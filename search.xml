<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>VMware虚拟化——使用 vCenter Server Appliance6.7（VCSA6.7）实现虚拟机不停机迁移</title>
    <url>/2019/06/07/%20VMware%E8%99%9A%E6%8B%9F%E5%8C%96%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%20vCenter%20Server%20Appliance6.7%EF%BC%88VCSA6.7%EF%BC%89%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8D%E5%81%9C%E6%9C%BA%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文给出基于最新版的VCSA6.7实现虚拟机业务从当前运行的ESXI主机不停机无缝迁移至另外一台ESXI主机的相关内容。该功能是VMware公司相当惊艳的一个技术，在某几种业务场景都非常需要此类需求。</p>
<p>（1）例如多台服务器ESXI主机分别位于局域网的不同物理位置，例如不同楼层，不同数据中心，那么按以往物理机业务迁移，显然需要把当前物理机所有业务代码和存储数据拷贝在另外一台物理服务器上，把新物理服务器跑起来，两者做lvs负载均衡后，把原物理机停掉，以此达到“业务不中断，且把服务器从一个物理位置换到另外一个物理位置”</p>
<p>（2）再或者采用简单粗暴的迁移方式，直接停掉当前物理机，当然业务会中断，把该物理机搬运至到其他物理位置，再跑起来</p>
<a id="more"></a>
<p>&#8195;&#8195;显然以上两种方式在当下IT技术看来，显得不够smart。接下来要介绍的是使用“不中断、不用搬运物理机”方式，将原业务服务“迁移”到新物理位置的服务器上（原业务非虚拟化环境，可先处理为虚拟化，解决方案自行处理，并不复杂。）。此类功能的配置，在很多文章也有提到，但个人看来还是比较浅薄，大多文章只给出配置成功的过程，却不分享配置过程出错的原因分析。</p>
<p>在VCSA管理中心（IP地址简单标示为：43），在其数据中心分组的一个主机集群，已经添加两台ESXI主机，一台为跑着业务的ESXI主机39（取IP地址最后两位作为标示），一台为接纳迁移虚拟机的ESXI主机40</p>
<p>（1）第一次迁移，对话框初步校验失败，提示目标主机40 Vmotion接口未配置，无法进行下一步操作</p>
<p><img src="https://img-blog.csdnimg.cn/20190616114139330.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这是因为ESXI39主机的VMkernel 虚拟网卡和 ESXI40主机的VMkernel虚拟网卡设置中，没有容许“V motion”报文通过，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616151332884.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p> 只有两台主机的VMkernel端口设置一致且打开Vmotion服务，可通过“迁移”的第一步的测试条件。这里不再贴图。</p>
<p>（2）解决第一个问题后，重写再操作迁移，走了几步后，发现又出现迁移条件测试不通过的提示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616153053311.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>“当前已连接的网络接口”“Network adapter 1”无法使用网络“VM Network”，因为 “在目标主机上为目标网络配置的卸载或安全策略不同于在源主机上为源网络配置的卸载或安全策略”</p>
<p>这个提示，是指目标ESXi主机40的“VM Network”模块配置的内容与ESXi主机39的不同，导致无法传输Vmotion数据包</p>
<p>这里的VM Network是指什么？对应下图内容：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616154049585.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>原理是ESXi主机内的网络模块：虚拟交换机下的某个端口组，这里有两个端口组，一个是管理network，一个是虚拟机的network。</p>
<p>查看40主机的VMnetwork属性（所有属性），如下图，在上面网络接口出错的关键字“安全策略”，就是对应VMnetwork的设置里的安全选项和策略选项，对比主机39，发现40的安全项目里：含杂模式设为拒绝，而39的设置为接受。所以两台主机的VM network 配置参数不一致，无法通过vmotion的校验。</p>
<p><img src="https://img-blog.csdnimg.cn/20190616162231176.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>综上，但这些问题或者设置都搞清楚后，在千兆局域网内，迁移一个几十G内存+1TB的虚拟机到另外一台ESXI主机上，仅需10分钟左右，全过程不停机，仅出现几次丢包。</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>迁移虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos7配置docker和docker-compose环境</title>
    <url>/2019/09/04/Centos7%E9%85%8D%E7%BD%AEdocker%E5%92%8Cdocker-compose%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>&#8195;&#8195;在测试机上搭建docker以及docker-compose环境目的还是为了快速构成开发环境，无需在裸机上为项目配置各种繁琐部署。</p>
<h4 id="1、安装docker"><a href="#1、安装docker" class="headerlink" title="1、安装docker"></a>1、安装docker</h4><p>查看centos版本以及内核版本，docker仅支持3.10以上的内核<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.6.1810 (Core) </span><br><span class="line">[root@dn2 ~]# uname -r</span><br><span class="line">3.10.0-957.27.2.el7.x86_64</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p><strong>更新yum包</strong></p>
<p><code>yum update</code></p>
<p><strong>安装必要的功能包</strong></p>
<p>yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动的依赖包</p>
<p><code>yum install -y yum-utils device-mapper-persistent-data lvm2</code></p>
<p><strong>新增阿里的docker镜像源</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在yum的repos.d目录下新增了一个docker-ce.repo</span></span><br><span class="line">[root@dn2 yum.repos.d]# ls</span><br><span class="line">CentOS-Base.repo      CentOS-Debuginfo.repo  CentOS-Sources.repo</span><br><span class="line">CentOS-Base.repo.bak  CentOS-fasttrack.repo  CentOS-Vault.repo</span><br><span class="line">CentOS-CR.repo        CentOS-Media.repo      docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看其镜像源地址，可以看到阿里镜像提供的stable版本</span></span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-stable-debuginfo]</span><br><span class="line">name=Docker CE Stable - Debuginfo $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/debug-$basearch/stable</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br></pre></td></tr></table></figure>
<p><strong>更新yum缓存</strong></p>
<p><code>yum makecache fast</code></p>
<p><strong>安装docker-ce 社区包</strong></p>
<p><code>yum -y install docker-ce</code></p>
<p><strong>启动docker以及开机自启</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line">systemctl enbale docker</span><br></pre></td></tr></table></figure>
<p><strong>查看docker版本</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# docker version         </span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           19.03.1</span><br><span class="line"> API version:       1.40</span><br><span class="line"> Go version:        go1.12.5</span><br><span class="line"> Git commit:        74b1e89</span><br><span class="line"> Built:             Thu Jul 25 21:21:07 2019</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Experimental:      false</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          19.03.1</span><br><span class="line">  API version:      1.40 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.12.5</span><br><span class="line">  Git commit:       74b1e89</span><br><span class="line">  Built:            Thu Jul 25 21:19:36 2019</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.2.6</span><br><span class="line">  GitCommit:        894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.0.0-rc8</span><br><span class="line">  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.18.0</span><br><span class="line">  GitCommit:        fec3683     </span><br></pre></td></tr></table></figure>
<p><strong>docker镜像加速</strong></p>
<p>这里的镜像是dockerhub的镜像，如果不设置为国内的docker镜像源，那么当使用docker pull 有些容量大镜像时，因走的官网下载链路，下载速度异常慢</p>
<p>通过新建/etc/docker/daemon.json文件设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;http:&#x2F;&#x2F;hub-mirror.c.163.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line"># 重启docker服务</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>
<h4 id="2、安装docker-compose"><a href="#2、安装docker-compose" class="headerlink" title="2、安装docker compose"></a>2、安装docker compose</h4><p>用于编排容器以及docker自动化部署，非常出色的容器编排工具</p>
<p>官网版本发布地址：<a href="https://github.com/docker/compose/releases">release</a></p>
<p>官网：<a href="https://docs.docker.com/compose/install/">install</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接安装二进制文件</span></span><br><span class="line">curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br><span class="line"><span class="meta">#</span><span class="bash"> 给docker-compose 加入可执行权限</span></span><br><span class="line">chmod +x /usr/local/bin/docker-compose </span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">[root@dn2 opt]# docker-compose --version</span><br><span class="line">docker-compose version 1.24.0, build 0aa59064</span><br></pre></td></tr></table></figure>
<p>以上完成docker和docker compose的环境部署，有关更多docker以及项目部署的文章会放在“docker”专栏里面。</p>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title>CopyOnWriteArrayList数据结构的源代码核心方法深入解析</title>
    <url>/2021/01/24/CopyOnWriteArrayList%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p>ArrayList并发读写不安全示例:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CopyOnWriteArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayListDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//        CopyOnWriteArrayList&lt;String&gt; list= new CopyOnWriteArrayList&lt;&gt;(Arrays.asList(&quot;foo&quot;,&quot;bar&quot;));</span></span><br><span class="line">        ArrayList&lt;String&gt; list= <span class="keyword">new</span> ArrayList&lt;&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>));</span><br><span class="line">        ArrayList&lt;Thread&gt; writeTreadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        ArrayList&lt;Thread&gt; readTreadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> threadNum=<span class="number">10</span>;</span><br><span class="line">        ExecutorService service= Executors.newFixedThreadPool(threadNum);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;threadNum;i++)&#123;</span><br><span class="line">            service.execute(<span class="keyword">new</span> Writer(list));</span><br><span class="line">            service.execute(<span class="keyword">new</span> Reader(list));</span><br><span class="line">        &#125;</span><br><span class="line">        service.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Writer</span> <span class="keyword">implements</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">    List&lt;String&gt; list;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Writer</span><span class="params">(List&lt;String&gt; list)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.list=list;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.list.add(<span class="string">&quot;writer&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reader</span> <span class="keyword">implements</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">    List&lt;String&gt; list;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Reader</span><span class="params">(List&lt;String&gt; list)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.list=list;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(Thread.currentThread().getName()+<span class="string">&quot;:&quot;</span>+list);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>提示ConcurrentModificationException：并发修改异常。这是因为有写线程和读线程同时操作list因为有写线程添加元素后，会改动modCount，导致某个读线程在读之前拿到的expectedModCount不等于正在读过程中modCount，故会在next()方法抛出异常</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pool-1-thread-4:[foo, bar, writer, writer]</span><br><span class="line">pool-1-thread-7:[foo, bar, writer, writer, writer]</span><br><span class="line">pool-1-thread-9:[foo, bar, writer, writer, writer, writer]</span><br><span class="line">pool-1-thread-11:[foo, bar, writer, writer, writer, writer, writer]</span><br><span class="line">pool-1-thread-11:[foo, bar, writer, writer, writer, writer, writer, writer]</span><br><span class="line">pool-1-thread-11:[foo, bar, writer, writer, writer, writer, writer, writer, writer]</span><br><span class="line">pool-1-thread-11:[foo, bar, writer, writer, writer, writer, writer, writer, writer, writer]</span><br><span class="line">pool-1-thread-11:[foo, bar, writer, writer, writer, writer, writer, writer, writer, writer, writer]</span><br><span class="line">pool-1-thread-11:[foo, bar, writer, writer, writer, writer, writer, writer, writer, writer, writer, writer]</span><br><span class="line">Exception in thread &quot;pool-1-thread-2&quot; java.util.ConcurrentModificationException</span><br><span class="line">	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)</span><br><span class="line">	at java.util.ArrayList$Itr.next(ArrayList.java:859)</span><br><span class="line">	at java.util.AbstractCollection.toString(AbstractCollection.java:461)</span><br><span class="line">	at java.lang.String.valueOf(String.java:2994)</span><br><span class="line">	at java.lang.StringBuilder.append(StringBuilder.java:131)</span><br><span class="line">	at concurrent.demo.Reader.run(ArrayListDemo.java:43)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>
<p>将<code>ArrayList</code>换成<code>CopyOnWriteArrayList</code>即可正常运行，因为<code>CopyOnWriteArrayList</code> 允许并发的读写操作，写操作不会影响到读操作，这就是所谓的读写分离设计。</p>
<a id="more"></a>
<h5 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h5><p>导读：CopyOnWriteArrayList是Java并发包中提供的一个并发容器，它是个线程安全且读操作无锁的ArrayList，写操作则通过创建底层数组的新副本来实现，是一种读写分离的并发策略，我们也可以称这种容器为”写时复制器”，Java并发包中类似的容器还有CopyOnWriteSet。本文会对CopyOnWriteArrayList的实现原理及源码进行分析</p>
<ul>
<li>CopyOnWriteArrayList在修改容器元素的时候并不是直接在原来的数组上进行修改，它是先拷贝一份，然后在拷贝的数组上进行修改，在修改完成之后将引用赋给原来的数组。</li>
</ul>
<h5 id="关键成员参数"><a href="#关键成员参数" class="headerlink" title="关键成员参数"></a>关键成员参数</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** The lock protecting all mutators */</span></span><br><span class="line"><span class="comment">// 全局锁：仅在clone方法和readObject方法上有效</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">transient</span> ReentrantLock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The array, accessed only via getArray/setArray. */</span></span><br><span class="line"><span class="comment">// 注意这个array是volatile类型，意味着array发生改变时，读线程对这种改变是可见的 </span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> Object[] array;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Gets the array.  Non-private so as to also be accessible</span></span><br><span class="line"><span class="comment"> * from CopyOnWriteArraySet class.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">final</span> Object[] getArray() &#123;</span><br><span class="line">    <span class="keyword">return</span> array;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sets the array.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">setArray</span><span class="params">(Object[] a)</span> </span>&#123;</span><br><span class="line">    array = a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 默认构造方法创建一个空数组</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CopyOnWriteArrayList</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        setArray(<span class="keyword">new</span> Object[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="相关读方法"><a href="#相关读方法" class="headerlink" title="相关读方法"></a>相关读方法</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">indexOf</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// 取出volatile的array，显然读操作不用加锁</span></span><br><span class="line">     Object[] elements = getArray(); </span><br><span class="line">     <span class="keyword">return</span> indexOf(o, elements, <span class="number">0</span>, elements.length);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IndexOutOfBoundsException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//取出volatile的array，因为volatile语义能保证array的修改能够对当前读线程可见</span></span><br><span class="line">    <span class="keyword">return</span> get(getArray(), index);</span><br></pre></td></tr></table></figure>
<p>这里只给出两个方法，可以看到读是不需要加锁的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEach</span><span class="params">(Consumer&lt;? <span class="keyword">super</span> E&gt; action)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (action == <span class="keyword">null</span>) <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="comment">// 取出volatile的array，其他线程对array的写操作对当前读线程可见</span></span><br><span class="line">    Object[] elements = getArray();</span><br><span class="line">    <span class="keyword">int</span> len = elements.length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; ++i) &#123;</span><br><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> E e = (E) elements[i];</span><br><span class="line">        action.accept(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="相关写方法"><a href="#相关写方法" class="headerlink" title="相关写方法"></a>相关写方法</h4><h5 id="add"><a href="#add" class="headerlink" title="add"></a>add</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//全局独占锁，写操作仅能由一个线程操作</span></span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 获取旧数组</span></span><br><span class="line">        Object[] elements = getArray();</span><br><span class="line">        <span class="keyword">int</span> len = elements.length;</span><br><span class="line">        <span class="comment">//创建新数组用于存放添加1个元素和旧数组元素，</span></span><br><span class="line">      	Object[] newElements = Arrays.copyOf(elements, len + <span class="number">1</span>); </span><br><span class="line">        newElements[len] = e;</span><br><span class="line">        <span class="comment">// 将volatile的array指向新数组，从这里也可以看出：只要写线程还未执行setArray(newElements)逻辑，那么同一时刻的读线程读取的数组必然是原数组，而非添加了元素的新数组。</span></span><br><span class="line">        setArray(newElements);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="remove"><a href="#remove" class="headerlink" title="remove"></a>remove</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//和add方法一样，</span></span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Object[] elements = getArray();</span><br><span class="line">        <span class="keyword">int</span> len = elements.length;</span><br><span class="line">        E oldValue = get(elements, index);</span><br><span class="line">        <span class="keyword">int</span> numMoved = len - index - <span class="number">1</span>; <span class="comment">// (len-1)-index</span></span><br><span class="line">        <span class="keyword">if</span> (numMoved == <span class="number">0</span>)</span><br><span class="line">            setArray(Arrays.copyOf(elements, len - <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">          	<span class="comment">//创建新数组用于存放删除1个节点的旧数组元素</span></span><br><span class="line">            Object[] newElements = <span class="keyword">new</span> Object[len - <span class="number">1</span>];</span><br><span class="line">            System.arraycopy(elements, <span class="number">0</span>, newElements, <span class="number">0</span>, index);</span><br><span class="line">            System.arraycopy(elements, index + <span class="number">1</span>, newElements, index,</span><br><span class="line">                             numMoved);</span><br><span class="line">          	<span class="comment">//将volatile的array指向新数组，从这里也可以看出：只要写线程还未执行setArray(newElements)逻辑，那么同一时刻的读线程读取的数组必然是包含删除元素的原数组。</span></span><br><span class="line">            setArray(newElements);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> oldValue;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里还有一个remove方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// 这里为何使用snapshot快照这样命名？ 因为这是在未加锁条件下读取的array数组，不保证读取的array是修改的新array，而是指在读取时刻的array，类似mysql的不可重复读机制.</span></span><br><span class="line">    Object[] snapshot = getArray();</span><br><span class="line">    <span class="keyword">int</span> index = indexOf(o, snapshot, <span class="number">0</span>, snapshot.length);</span><br><span class="line">    <span class="keyword">return</span> (index &lt; <span class="number">0</span>) ? <span class="keyword">false</span> : remove(o, snapshot, index);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o, Object[] snapshot, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Object[] current = getArray();</span><br><span class="line">        <span class="keyword">int</span> len = current.length;</span><br><span class="line">      	<span class="comment">// 前面说到snapshot是加锁之前的快照，因此在这里加锁之后，需要再次检查加锁前、后的过程中快照数组和当前array是否一致。</span></span><br><span class="line">        <span class="keyword">if</span> (snapshot != current) findIndex: &#123;</span><br><span class="line">          	<span class="comment">//snapshot时刻前，和snapshot时刻后两个时刻array元素个数不一样的话，取最小的</span></span><br><span class="line">            <span class="keyword">int</span> prefix = Math.min(index, len);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; prefix; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (current[i] != snapshot[i] &amp;&amp; eq(o, current[i])) &#123;</span><br><span class="line">                    index = i;</span><br><span class="line">                    <span class="keyword">break</span> findIndex;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (index &gt;= len)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">if</span> (current[index] == o)</span><br><span class="line">                <span class="keyword">break</span> findIndex;</span><br><span class="line">            index = indexOf(o, current, index, len);</span><br><span class="line">            <span class="keyword">if</span> (index &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Object[] newElements = <span class="keyword">new</span> Object[len - <span class="number">1</span>];</span><br><span class="line">        System.arraycopy(current, <span class="number">0</span>, newElements, <span class="number">0</span>, index);</span><br><span class="line">        System.arraycopy(current, index + <span class="number">1</span>,</span><br><span class="line">                         newElements, index,</span><br><span class="line">                         len - index - <span class="number">1</span>);</span><br><span class="line">        setArray(newElements);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="clear"><a href="#clear" class="headerlink" title="clear"></a>clear</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123; <span class="comment">// clear 过程中也加了独占锁，而且不需要遍历，clear性能很高</span></span><br><span class="line">        setArray(<span class="keyword">new</span> Object[<span class="number">0</span>]);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从以上相关的写操作可以得出一个结论：如果写操作很频繁，那么CopyOnWriteArrayList的写入性能很低，因为每次写操作都需要将原数组元素拷贝到一个新数组中，如果底层的array是大数组，那么将会占用两倍内存空间。</p>
<h5 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h5><p>sort也需要加独占锁，保证</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(Comparator&lt;? <span class="keyword">super</span> E&gt; c)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Object[] elements = getArray();</span><br><span class="line">        Object[] newElements = Arrays.copyOf(elements, elements.length);</span><br><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> E[] es = (E[])newElements;</span><br><span class="line">        Arrays.sort(es, c);</span><br><span class="line">        setArray(newElements);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="关于迭代器snapshot机制"><a href="#关于迭代器snapshot机制" class="headerlink" title="关于迭代器snapshot机制"></a>关于迭代器snapshot机制</h5><figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">import java.util.*;</span><br><span class="line">import java.util.concurrent.CopyOnWriteArrayList;</span><br><span class="line">import java.util.concurrent.ExecutorService;</span><br><span class="line">import java.util.concurrent.Executors;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">ArrayListDemo</span> &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(<span class="params">String[] args</span>) throws InterruptedException</span> &#123;</span><br><span class="line">        CopyOnWriteArrayList&lt;String&gt; list= <span class="keyword">new</span> CopyOnWriteArrayList&lt;&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>));</span><br><span class="line">        Thread t=<span class="keyword">new</span> Thread(<span class="keyword">new</span> Writer(list));</span><br><span class="line">        <span class="comment">// 在“写线程启动”之前，创建了iterator，该迭代器内部会在此刻创建一个snapshot快照数组，该snapshot引用指向的是未被修改的原数组：Arrays.asList(&quot;foo&quot;,&quot;bar&quot;)，</span></span><br><span class="line">        ListIterator&lt;String&gt; iter=list.listIterator();</span><br><span class="line">        t.start();</span><br><span class="line">        t.<span class="keyword">join</span>(); <span class="comment">// 写线程结束后，内部array引用指向的是新数组对象</span></span><br><span class="line">        <span class="comment">//iterator内部是用旧数组进行迭代：因此输出时foo、bar，而不是输出foo、bar、writer1、writer2</span></span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) System.<span class="keyword">out</span>.println(iter.next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title">Writer</span> <span class="title">implements</span> <span class="title">Runnable</span>&#123;</span><br><span class="line">    List&lt;String&gt; list;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Writer</span>(<span class="params">List&lt;String&gt; list</span>)</span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.list=list;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span>(<span class="params"></span>)</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.list.<span class="keyword">add</span>(<span class="string">&quot;writer1&quot;</span>);</span><br><span class="line">        <span class="keyword">this</span>.list.<span class="keyword">add</span>(<span class="string">&quot;writer2&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>理解以上demo输出，需要对其源代码掌握和理解：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> ListIterator&lt;E&gt; <span class="title">listIterator</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       Object[] elements = getArray();</span><br><span class="line">       <span class="keyword">int</span> len = elements.length;</span><br><span class="line">       <span class="keyword">if</span> (index &lt; <span class="number">0</span> || index &gt; len)</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(<span class="string">&quot;Index: &quot;</span>+index);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">new</span> COWIterator&lt;E&gt;(elements, index);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">COWIterator</span>&lt;<span class="title">E</span>&gt; <span class="keyword">implements</span> <span class="title">ListIterator</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">       <span class="comment">/** Snapshot of the array */</span></span><br><span class="line">   		<span class="comment">// 这里是关键，当执行Iterator&lt;String&gt; itr = list.iterator()时，迭代器内部其实先生成一份底层数组的snapshot快照，之后迭代器读取的数组都是这个快照数组（也即旧时刻下的数组），而不是读取最新的、正在修改的数组</span></span><br><span class="line">       <span class="keyword">private</span> <span class="keyword">final</span> Object[] snapshot; </span><br><span class="line">       <span class="comment">/** Index of element to be returned by subsequent call to next.  */</span></span><br><span class="line">       <span class="keyword">private</span> <span class="keyword">int</span> cursor;</span><br><span class="line"></span><br><span class="line">       <span class="function"><span class="keyword">private</span> <span class="title">COWIterator</span><span class="params">(Object[] elements, <span class="keyword">int</span> initialCursor)</span> </span>&#123;</span><br><span class="line">           cursor = initialCursor;</span><br><span class="line">           snapshot = elements;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">           <span class="keyword">return</span> cursor &lt; snapshot.length;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasPrevious</span><span class="params">()</span> </span>&#123;</span><br><span class="line">           <span class="keyword">return</span> cursor &gt; <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">           <span class="keyword">if</span> (! hasNext())</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">           <span class="keyword">return</span> (E) snapshot[cursor++];</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> E <span class="title">previous</span><span class="params">()</span> </span>&#123;</span><br><span class="line">           <span class="keyword">if</span> (! hasPrevious())</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">           <span class="keyword">return</span> (E) snapshot[--cursor];</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>
<h5 id="CopyOnWrite的缺点"><a href="#CopyOnWrite的缺点" class="headerlink" title="CopyOnWrite的缺点"></a><strong>CopyOnWrite的缺点</strong></h5><p>CopyOnWrite容器有很多优点，但是同时也存在两个问题，即内存占用问题和数据一致性问题。所以在开发的时候需要注意一下。</p>
<p><strong>「内存占用问题」</strong>。因为CopyOnWrite的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说200M左右，那么再写入100M数据进去，内存就会占用300M，那么这个时候很有可能造成频繁的Yong GC和Full GC。之前我们系统中使用了一个服务由于每晚使用CopyOnWrite机制更新大对象，造成了每晚15秒的Full GC，应用响应时间也随之变长。</p>
<p>ArrayList适合单线程情况下的数组场景，Vector适合多线程场景，但Vector每个方法前都有Synchronized修饰，因此写线程的操作会阻塞读线程的读操作，性能低。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>Java高级主题</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：jdk1.7的HashMap死循环问题深入分析</title>
    <url>/2020/10/23/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9Ajdk1.7%E7%9A%84HashMap%E6%AD%BB%E5%BE%AA%E7%8E%AF%E9%97%AE%E9%A2%98%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>首先介绍Java7版本的HashMap正常put和扩容过程对应的代码</p>
<h4 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h4><p>可以清晰看到1.7版本的HashMap内部结构是数组+冲突链，HashMap里面”桶”的字眼或者说法也是来源于Java7及之前版本的说法，在Java8它被称为bins</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/5d6e114e5204a1b71b060c92bbc7207d.png" alt="18-1"></p>
<a id="more"></a>
<h4 id="源码说明"><a href="#源码说明" class="headerlink" title="源码说明"></a>源码说明</h4><p>在源码查看方面，建议开发环境同时配置jdk7和jdk8，IDEA创建两个project，分别选择不同的jdk即可方便查阅和对比两个版本的源码。此外，如果已经深度理解了jdk8的HashMap设计原理，那么jdk7的HashMap的源码则更容易看懂。</p>
<h5 id="1-1-put方法"><a href="#1-1-put方法" class="headerlink" title="1.1 put方法"></a>1.1 put方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (table == EMPTY_TABLE) &#123;</span><br><span class="line">        inflateTable(threshold); <span class="comment">// 首次put，新建和初始化数组 -&gt; table = new Entry[capacity];</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> putForNullKey(value); <span class="comment">// key为null时，要么找到已存在key为null的节点并更新它；要么新增一个key为null的节点，因它hash为0，因此一定会被放在table[0]上。</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">int</span> hash = hash(key);</span><br><span class="line">    <span class="keyword">int</span> i = indexFor(hash, table.length); <span class="comment">// 和Java8设计一样：i=h &amp; (length-1);</span></span><br><span class="line"><span class="comment">/* 1、若e为null表示桶位上的table[i]还未放置任何节点</span></span><br><span class="line"><span class="comment">    * 2、若e.next为null表示table[i]是一个桶位节点</span></span><br><span class="line"><span class="comment">  	* 3、若 e.next不为null表名table[i]是一条冲突链(至少含有2个节点)</span></span><br><span class="line"><span class="comment">    * 这三种情况都可以放在一个for循环进行处理，显然比Java8的putVal简单</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">for</span> (Entry&lt;K,V&gt; e = table[i]; e != <span class="keyword">null</span>; e = e.next) &#123;</span><br><span class="line">        Object k;</span><br><span class="line">        <span class="comment">//找到一个节点与给定key相同，则更新value并返回旧值</span></span><br><span class="line">        <span class="keyword">if</span> (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123;</span><br><span class="line">            V oldValue = e.value;</span><br><span class="line">            e.value = value;</span><br><span class="line">            e.recordAccess(<span class="keyword">this</span>);</span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    modCount++; <span class="comment">// 这里可以看出java7先将modCount+1再添加节点，而java8先添加节点再将modCount+1</span></span><br><span class="line">    <span class="comment">// 若以上未找到key，说明需要新增一个node节点到HashMap</span></span><br><span class="line">    addEntry(hash, key, value, i); <span class="comment">// 用于添加节点</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-2-createEntry"><a href="#1-2-createEntry" class="headerlink" title="1.2 createEntry"></a>1.2 createEntry</h5><p>createEntry里面是一个很经典的情况：从这里可以看出java7的HashMap的冲突链每次都是链表头部插入新节点（头插法）这个设计在后面的resize也用上。而java8的是在链表使用尾插法：e.next=newNode </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addEntry</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">int</span> bucketIndex)</span> </span>&#123;        </span><br><span class="line">     <span class="keyword">if</span> ((size &gt;= threshold) &amp;&amp; (<span class="keyword">null</span> != table[bucketIndex])) &#123;</span><br><span class="line">         resize(<span class="number">2</span> * table.length); <span class="comment">// java7先resize再添加节点，而java8先添加节点再resize，</span></span><br><span class="line">         hash = (<span class="keyword">null</span> != key) ? hash(key) : <span class="number">0</span>;<span class="comment">//  java7与 java8的hash函数内部设计不一样</span></span><br><span class="line">         bucketIndex = indexFor(hash, table.length); <span class="comment">// 桶位索引计算，两者一样。</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     createEntry(hash, key, value, bucketIndex); <span class="comment">// 这里才是真正的添加节点</span></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">void</span> <span class="title">createEntry</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">int</span> bucketIndex)</span> </span>&#123;</span><br><span class="line">     Entry&lt;K,V&gt; e = table[bucketIndex];   <span class="comment">// 取出数组上桶节点</span></span><br><span class="line">     table[bucketIndex] = <span class="keyword">new</span> Entry&lt;&gt;(hash, key, value, e); <span class="comment">// 将新节点放在桶位上，然后新节点next再指向e（e代表是原节点或者原链表），因此每次</span></span><br><span class="line">     size++;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>为何设计为头插法？</p>
<p>个人的思考：</p>
<p>1、设计者考虑到新添加的节点有可能在未来时刻会被优先读，因此每次将新节点插在链表头部，可以快速被下次访问到，避免冗余遍历链表的操作，类似局部性原理或者类比FIFO思想的设计</p>
<p>2、头插法避免了对冲突链表遍历，在一些场合下能提升性能。如果采用尾插法，<code>while(e!=null)</code>遍历链表后，找到链表尾节点，再添加新节点到链表尾部，显然遍历消耗性能。</p>
<p>3、头插法设计也使得代码只需要2行，逻辑直观清晰，（体现设计者高度代码洁癖？），createEntry对比java8的putVal，putVal的添加节点可不止几行代码！</p>
<p>头插法createEntry示意图</p>
<p>假设数组容量16，那么对于3、35、19，他们的桶位索引<code>buctketIndex=key.hashCode &amp; (16-1)</code>，在这里假设hash计算方式就是直接使用hashCode进行计算，可以推出：数字作为key对应的hashCode就是数字本身。显然计算结果buctketIndex都是3，新增的节点51也将定位到3号位</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c6a3c211c03651cb6ecff2aa8e01837f.png" alt="19"></p>
<p>以上两个方法相对简单</p>
<p>1.3 resize</p>
<p>看看源码说明：Rehashes the contents of this map into a new array with a larger capacity. This method is called automatically when the number of keys in this map reaches its threshold.</p>
<p>这里提到一个Rehashes动词，也即重哈希，也即HashMap扩容</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">resize</span><span class="params">(<span class="keyword">int</span> newCapacity)</span> </span>&#123;</span><br><span class="line">    Entry[] oldTable = table;</span><br><span class="line">    <span class="keyword">int</span> oldCapacity = oldTable.length;</span><br><span class="line">    <span class="keyword">if</span> (oldCapacity == MAXIMUM_CAPACITY) &#123; <span class="comment">// 若旧表容量达到MAXIMUM_CAPACITY，则不再扩容</span></span><br><span class="line">        threshold = Integer.MAX_VALUE;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  	</span><br><span class="line">    Entry[] newTable = <span class="keyword">new</span> Entry[newCapacity]; <span class="comment">// 创建空的新数组实例</span></span><br><span class="line">    transfer(newTable, initHashSeedAsNeeded(newCapacity)); <span class="comment">// </span></span><br><span class="line">    table = newTable;</span><br><span class="line">    threshold = (<span class="keyword">int</span>)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重点在transfer方法实现上</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transfers all entries from current table to newTable.</span></span><br><span class="line"><span class="comment"> * 将旧数组（旧哈希表）的所有节点（entries）挪到新数组(新哈希表)上</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Entry[] newTable, <span class="keyword">boolean</span> rehash)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> newCapacity = newTable.length;</span><br><span class="line">    <span class="keyword">for</span> (Entry&lt;K,V&gt; e : table) &#123; <span class="comment">// 遍历旧表每一个捅位上，开始将节点根据一定规则挪到新数组，可以看到java7这个部分实现比java8简单。</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">null</span> != e) &#123;  </span><br><span class="line">            Entry&lt;K,V&gt; next = e.next; </span><br><span class="line">            <span class="keyword">if</span> (rehash) &#123;</span><br><span class="line">                e.hash = <span class="keyword">null</span> == e.key ? <span class="number">0</span> : hash(e.key);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">int</span> i = indexFor(e.hash, newCapacity);</span><br><span class="line">            e.next = newTable[i]; <span class="comment">// 当前节点e的next指针指向newTable[i]，newTable[i]代表null值或者一个节点或者一条冲突链表,若newTable[i]是节点或者冲突链，那么这一行代码使得e节点就作为冲突链的头节点</span></span><br><span class="line">            newTable[i] = e; <span class="comment">//将链表头节点e放到新表桶位上：table[i](e)-&gt;node-&gt;..-&gt;null，这就是Java7的扩容头插法。</span></span><br><span class="line">            e = next; <span class="comment">// 接力遍历链表下一个节点</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里也做了一个假设：桶位计算buctketIndex=key.hashCode &amp; (32-1)，注意这里32是新数组容量，这不影响分析resize过程，源码的hash算法只是让key更加分散分布在数组所有位置上。</p>
<p>ttransfer里面采用的头插法，根据前面createEntry方法的示意图可得到以下旧表扩容前后图：</p>
<p>根据buctketIndex=key.hashCode &amp; (32-1)可知，3和35这两个还是定位到3号桶位，由于头插法，因此35节点作为头结点，3作为后驱节点，而19被hash到新数组19号桶位。可以看出，Java 7没有红黑树，因此没有treerifyBin树化机制和树转链机制操作unTreeriFy等复杂的方法实现。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8cbbeeb337b4188240d9b923b6921f88.png" alt="20"></p>
<p>以上即是Java7的HashMap基本源码分析，下面看看在多线程情况下resize出现循环链表导致死循环的情况是怎么发生的。</p>
<h5 id="1-3-线程栈及其局部变量内存模型"><a href="#1-3-线程栈及其局部变量内存模型" class="headerlink" title="1.3 线程栈及其局部变量内存模型"></a>1.3 线程栈及其局部变量内存模型</h5><p>假设现在两个线程共享使用一个HashMap，在resize方法里面，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    Entry[] newTable = <span class="keyword">new</span> Entry[newCapacity];</span><br><span class="line">    transfer(newTable, initHashSeedAsNeeded(newCapacity));</span><br><span class="line">    table = newTable;</span><br><span class="line">...</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Entry[] newTable, <span class="keyword">boolean</span> rehash)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (Entry&lt;K,V&gt; e : table) &#123;</span><br><span class="line">                    Entry&lt;K,V&gt; next = e.next;</span><br><span class="line">           ...</span><br></pre></td></tr></table></figure>
<p>线程1和线程2分别在自己栈里面创建newTable、e、next局部变量，两个线程的newTable引用都指向共享堆（主存）里面的table对象，如下图所示</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/cfc120fc1068583f5bc2562429aeeca0.png" alt="21"></p>
<p>当然这里图并不是JVM内存全貌模型，图不是十分严谨，该图目的是为了下面的循环链表形成过程铺垫基本知识点。</p>
<h5 id="1-4-两个线程的执行流："><a href="#1-4-两个线程的执行流：" class="headerlink" title="1.4 两个线程的执行流："></a>1.4 两个线程的执行流：</h5><p>以下是线程1、线程2并发扩容执行流的说明，重点在线程1执行完<code>next=e.next</code>之后的扩容行为分析，由于代码逻辑是动态变化，因此这部分内容相对复杂，需要有耐心的跟着线程1和线程2的扩容步骤图所表达的思路，尤其线程1从中断位置继续运行后的逻辑，否则无法理清死循环（循环链表）的形成过程。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/dbab3de30eff5397d17d2d8a2aed11f6.png" alt="0"></p>
<h5 id="1-5-循环链表形成过程"><a href="#1-5-循环链表形成过程" class="headerlink" title="1.5 循环链表形成过程"></a>1.5 循环链表形成过程</h5><p>基本条件说明：</p>
<p>​        为了易于理解和便于作图，HashMap的数组初始容量为2，key为3和7，且在桶位1已经有冲突链<code>3-&gt;7-null</code>，hash计算以及桶位计算采用1.2提到的方法。table扩容后，容量为4，冲突链被rehash到桶位为3，由头插法可知，新数组桶位3的冲突链一定为：<code>7-&gt;3-&gt;null</code>，其他细节说明，参考图中文字</p>
<p>务必耐心过完每一张图和以下核心逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">next = e.next; </span><br><span class="line">e.next = newTable[i]; <span class="comment">// 当前节点e的next指针指向newTable[i]，newTable[i]代表null值或者一个节点或者一条冲突链表,若newTable[i]是节点或者冲突链，那么这一行代码使得e节点就作为当前冲突链的头节点</span></span><br><span class="line">newTable[i] = e; <span class="comment">//将链表头节点e放到新表桶位上：table[i](e)-&gt;node-&gt;..-&gt;null，这就是Java7的扩容头插法。</span></span><br><span class="line">e = next; <span class="comment">// 接力遍历链表下一个节点</span></span><br></pre></td></tr></table></figure>
<p>循环次数与步骤说明：</p>
<p>为了更加清晰理解以下循环图运行过程，在这里给出以下约定，循环次数用“循环n”表示，循环内部的代码用“步骤n”表示，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">transfer内部while(null !&#x3D; e)循环 &#x2F;&#x2F; 以下忽略i&#x3D;indexFor(e.hash,newCapacity)代码行</span><br><span class="line">第1次循环：</span><br><span class="line">循环1-步骤1 next &#x3D; e.next; </span><br><span class="line">循环1-步骤2 e.next &#x3D; newTable[i]</span><br><span class="line">循环1-步骤3 newTable[i] &#x3D; e </span><br><span class="line">循环1-步骤4 e &#x3D; next </span><br><span class="line"></span><br><span class="line">第2次循序：</span><br><span class="line">循环2-步骤1 next &#x3D; e.next; </span><br><span class="line">循环2-步骤2 e.next &#x3D; newTable[i]</span><br><span class="line">循环2-步骤3 newTable[i] &#x3D; e </span><br><span class="line">循环2-步骤4 e &#x3D; next</span><br><span class="line">......以此类推</span><br></pre></td></tr></table></figure>
<p>初始table结构</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6372f6672a6213c503196314af20c3f4.png" alt="1"></p>
<p>准备并发扩容</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/400ff663c5393341bd3af391fe841c29.png" alt="2"></p>
<p>在next=e.next执行完后线程1让出cpu时间片，操作系统调度线程2执行，且线程2完成了扩容</p>
<p>对于线程1的循环进度：循环1-步骤1 next=e.next</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/f96128757063cfb03f62eba25088b657.png" alt="3"></p>
<p>这里需要重点观察：线程1栈空间的 e1指向3节点、next1指向7节点的情况，对比线程2的newTable的7 节点和3节点</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/3822f48c15915aec2e6a496d29b3da67.png" alt="4"></p>
<p>线程2运行结束后，线程1得到cpu时间片接着从上次挂起的地方运行：</p>
<p>（利用线程控制块TCB恢复cpu现场，cpu程序计时器：存放cpu下一条运行的指令，在这里就是指：</p>
<p>i = indexFor(e.hash, newCapacity)。这里更想强调的是后面这行语句：<code>e.next = newTable[i]</code></p>
<p>此时线程1的循环进度：循环1-步骤2 <code>e.next = newTable[i]</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/863b1ca0488e5d5889e3294aba48de7c.png" alt="5"></p>
<p>此时线程1的循环进度：循环1-步骤3 <code>newTable[i]=e</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/a4d5bec409cb0b6fa1ab7fec8f6fe2ee.png" alt="6"></p>
<p>此时线程1的循环进度：循环1-步骤4 <code>e=next</code>，请注意这里next指针指向是table的7，而不是3在newTable指向的null，为何？就是因为循环1-步骤2 图出现的情况。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8209d703c780a7602e1d210fa6098699.png" alt="7"></p>
<p>此时线程1的循环进度：循环2-步骤1 <code>next=e.next</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c33790c1d027531efd4014d3d21f5b6d.png" alt="8"></p>
<p>此时线程1的循环进度：循环2-步骤2 <code>e.next=newTable[i]</code>，这里很特殊：因为上图已经是7指向newTable[i]，</p>
<p>在由于e当前就是7，因此执行<code>e.next=newTable[i]</code>后，还是7指向newTable[i]</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/fcb0e8356c9a80d113ca02e72397b366.png" alt="9"></p>
<p>此时线程1的循环进度：循环2-步骤3 <code>newTable[i]=e</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/d8be6aaa8098491801b082dbda72b447.png" alt="10"></p>
<p>此时线程1的循环进度：循环2-步骤4 <code>e=next</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/1c5d7a7ac7f2638f456acc6df6d9fa1f.png" alt="11"></p>
<p>此时线程1的循环进度：循环3-步骤1 <code>next=e.next</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/cfb58fd57b0accbf32d1bef861d94937.png" alt="12"></p>
<p>此时线程1的循环进度：循环3-步骤2 <code>e.next=newTable[i]</code>，有内味了！在这里开始出现节点间循环指向！</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8fdd674f820c59ab23918f3f1ab10928.png" alt="13"></p>
<p>此时线程1的循环进度：循环3-步骤3 <code>newTable[i]=e</code>，循环指向还在！</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/a90df12eaeda8a1d1dd5a35ced826617.png" alt="14"></p>
<p>此时线程1的循环进度：循环3-步骤4 <code>e=next</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/47918ba359182d0ab851741476774051.png" alt="15"></p>
<p>之后循环结束，形成环链总共用了3轮循环，线程1运行结束后，HashMap内部的table已经存在循环链表，此时还不会发生死循环。当下次使用<code>map.get(key)</code>，例如15这个key，该循环链表无法找到15这个key，由于链表是循环结构，恰好触发死循环遍历，cpu使用率飙高或者线程退出，查看堆栈可以看到死循环出现在调用方法：transfer</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/966b6284fe6b98c0e0cb3449e925ff9d.png" alt="16"></p>
<h5 id="1-6-诱发形成循环列表的根源点分析"><a href="#1-6-诱发形成循环列表的根源点分析" class="headerlink" title="1.6 诱发形成循环列表的根源点分析"></a>1.6 诱发形成循环列表的根源点分析</h5><p>从1.5的3次循环（共1+11次步骤）过程可以总结出导致循环链表的关键诱因：</p>
<p>1、头插法会使得扩容后链表顺序相反</p>
<p>2、在1的“助攻”下，线程1执行<code>next=e.next</code>并挂起后，当线程1再次恢复CPU线程执行时，e的指向和next的指向出现了“梦幻的倒换”：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/863b1ca0488e5d5889e3294aba48de7c.png" alt=""></p>
<p>这也反向证明Java8的尾插法链表扩容前后节点顺序保持一致则一定不会出现循环链表，因为对于Java8的HashMap尾插法逻辑：</p>
<p>线程1执行<code>next=e.next</code>并挂起后，当线程1再次恢复CPU线程执行时：如下图所示，可以清楚观察到</p>
<p>1）扩容前后链表的节点顺序保持不变</p>
<p>2)  线程1在挂起处继续执行时，e和next的指向保持不变</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ee707dbcb34467305d41023a137ae5e4.png" alt=""></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>​        从1.6小节的多张图可知，Java7的HashMap形成循环链表的过程需要3次循环共12个步骤，因此如果想直接在大脑中想象环链的构建过程，这当然是困难，其实可通过作图的方式，可以帮助大脑形成“记忆”快照，最终能将这个过程还原出来。(若还不能理解循环链表的形成过程，建议自行作图解析)。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce设计原理</title>
    <url>/2019/10/27/MapReduce%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h4 id="1、MR基本定义"><a href="#1、MR基本定义" class="headerlink" title="1、MR基本定义"></a>1、MR基本定义</h4><p>&#8195;&#8195;参考百度百科定义，简要概括如下：<br>MapReduce是分布式的计算框架或者解决方案，大致有基本内容：</p>
<ul>
<li>1）首先MapReduce重点是工作在集群的节点上，而非在单台服务器上做计算、做统计等</li>
<li>2）MapReduce把用户提交的任务以分布式放在多个节点上执行，自动划分计算数据和计算任务等，这些并行计算涉及到的系统底层的复杂细节交由MR框架负责处理。换句话：数据开发人员只需要定义“我要统计词频”的“任务”后，提交给MR框架即可，坐等计算结果，至于MR是如何从多台服务上找数据文件、计算统计、缓存中间计算结果、存储最终技术结果、CPU、内存、IO网络资源使用等，数据开发人员都无需关注，MR自己会处理。</li>
</ul>
<a id="more"></a>
<h4 id="2、HDFS和MR的关系"><a href="#2、HDFS和MR的关系" class="headerlink" title="2、HDFS和MR的关系"></a>2、HDFS和MR的关系</h4><p>&#8195;&#8195;HDFS和MR共同组成Hadoop分布式系统体系结构的核心。HDFS在集群上实现了分布式文件系统，MR在集群上实现了分布式计算和任务处理。HDFS在MR任务处理过程中提供了文件操作和存储等支持，MR在HDFS的基础上实现了任务的分发、计算、跟踪等工作，并收集结果，二者相互作用，完成分布式集群的主要任务。</p>
<h4 id="3、MR组件架构"><a href="#3、MR组件架构" class="headerlink" title="3、MR组件架构"></a>3、MR组件架构</h4><p><img src="https://img-blog.csdnimg.cn/20191016230840663.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;MapReduce框架设计为4部分：Client、JobTracker、TaskTracker以及Map Task &amp; Reduce Task。具体内容如下：</p>
<p>1）Client 客户端<br>Hadoop中，把用户提交的MapReduce程序成为“Job”（作业），每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 jar文件存储在 HDFS，也就是这个MR程序 jar包在集群中每个节点都存放在一份，而且是存放在hdfs文件系统上，并把jar包路径告诉 JobTracker ，由它创建每一个 Task（即 Map Task 和 Reduce Task） 将它们分发到各个 TaskTracker 服务中去执行。</p>
<p>2）JobTracker</p>
<p>JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，当然TaskTracker也会主动上报自己的情况，若JobTracker一旦发现某个TaskTracker失败，就将相应的任务转移到其他健康TaskTracker节点继续干活；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器Task Scheduler，而调度器会在TaskTracker所在节点资源出现空闲时，选择合适的任务工作交由这些空闲的TaskTracker来干活。这里说的调度器，就是我们后面要讨论的YARN，在hadoop 1，MR框架还需要运行资源调度和管理服务，在hadoop 2中，这个资源调度功能在MR框架中剥离出来，交由更专业的YARN框架统一处理。</p>
<p>3）TaskTracker</p>
<p>TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。</p>
<p>4）Map Task &amp; Reduce Task<br>Map Task、Reduce Task由Datanode节点上的TaskTracker 启动，HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce的处理单位是：split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。正因为屏蔽了物理文件层，在逻辑层，数据文件划分方法完全由数据开发者自行决定，这就是为何MR通过再次封装使用split来组织数据块的原因。这就像Linux LVM逻辑，多个物理卷PV组成一个大的卷组VG（或者物理卷池），逻辑卷LV在VG的上面，因为物理卷被逻辑化，因此可以由用户自行划分区或或者调整分区大小。<br>split 的多少决定了Map Task 的数目，每个split 只会交给一个Map Task 处理。<br>Split 和 Block的关系如下图所示：<br><img src="https://img-blog.csdnimg.cn/2019101721123387.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="4、MR运行流程"><a href="#4、MR运行流程" class="headerlink" title="4、MR运行流程"></a>4、MR运行流程</h4><p>4.1 分区原理</p>
<ul>
<li><p>1）、首先我们需要了解MR的分区partition原理，它是参考了一致性hash算法的概念：<br>Hash算法是为了保证数据均匀的分布，例如：有3个目录，编号分别为：0、1、2；现在有12个文件，如何把12个文件平均存放到3个目录下呢？按Hash算法的做法是，将12个文件从0开始编号，得到这样的一个数组：<br>[0，1，2，3，4，5，6，7，8，9，10，11]，每个文件所存放目录号=文件序号 mod 3，任何数字对3取模，最终得到的结果都是0，1，2。<br>最后将取模结果为0的文件放入0号目录下，结果为1的文件放入1号目录下，结果为2的文件放入2号目录，12个文件最终均匀的分布到3个目录下：<br>0、3、6、9、12号的文件存放在0号目录下<br>1、4、7、10号的文件存放在1号目录下<br>2、5、8、11号的文件存放在2号目录下<br>MR的分区partition就是这样的过程，partition_num=hash（key）mod N</p>
</li>
<li><p>2）MR为何做partition？<br>在进行MapReduce计算时，经常的需求是把最终的输出数据分到不同的文件中，例如：<br>按照年度（季度、月等）划分的话，需要把同一年份的数据放到一个文件中，因此不同年份对应不同的文件；<br>按性别业务划分，需要把同一性别的数据放到一个文件中，因此不同性别的数据放在不同的文件上。<br>从逆向来看：这些最终的输出是我们需要的数据，是由Reducer任务产生，而如果要得到最终输出的多个数据文件，意味着有同样数量的Reducer任务在跑；Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的Map阶段的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition，负责实现划分数据的类称作Partitioner。</p>
</li>
</ul>
<p>4.2 MapReduce统计词频的MapReduce流程<br><img src="https://img-blog.csdnimg.cn/20191027104923213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">该图统计词频流程主要分为以下几步。<br>第1步：假设一个文件有三行英文句子作为 MapReduce 的Input（输入），这里经过 Splitting 过程把文件分割为3块。分割后的3块数据就可以并行处理，每一块交给一个 map 线程处理，对应3个map task。</p>
<p>第2步：每个 map 线程中，以每个单词为key，以1作为词频数value，并建结果写到磁盘上，而非内存中（spark计算框架则会把此计算结果放在内存中，减少IO加速计算）。</p>
<p>第3步：每个 map 的输出要经过 shuffling（混洗），将相同的单词key放在同一分区，然后交给 reduce task处理。</p>
<p>第4步：reduce 接受到 shuffling 后的数据（reduce去读相应的map计算结果文件）， 会将相同的单词进行合并，得到每个单词的词频数，最后将统计好的每个单词的词频数作为输出结果。</p>
<p><strong>Map Reduce具体实现</strong><br>==Map阶段==<br>Map 阶段是由一定数量的 Map Task 组成。这些 Map Task 可以同时运行，每个Task所使用的计算资源由slot决定，每个 Map Task又是由以下三个部分组成。</p>
<ul>
<li><p>1）、对输入数据格式进行解析的一个组件：InputFormat。因为不同的数据可能存储的数据格式不一样，这就需要有一个 InputFormat 组件来解析这些数据的存放格式。默认情况下，它提供了一个 TextInputFormat 来读取输入数据格式（就像在Pandas中，使用read_csv()方法读取csv格式数据，用read_excel()读取Excel格式的数据）。TextInputFormat 会将文件的每一行解释成(key,value)，key代表每行偏移量，value代表每行数据内容。 通常不需要自定义 InputFormat，因为 MapReduce 提供了很多种InputFormat的实现，可直接引用。</p>
</li>
<li><p>2）、对输入数据进一步处理：Mapper——这个 Mapper 计算指具体业务逻辑，因为不同的业务对数据有不同的处理。</p>
</li>
<li><p>3)、数据分组：Partitioner。Mapper 数据处理之后输出导reduce之前，输出key会经过 Partitioner 分组选择不同的reduce。默认的情况下，Partitioner 会对 map 输出的key进行hash取模，比如有3个Reduce Task，它就是模（mod）3，如果key的hash值为0对应第0个 Reduce Task，hash值1对应第1个Reduce Task，hash值2对应第2个Reduce Task。如上图第一组map的bear和第二组map的bear hash值一样，因此被shuffling到同一组，然后交给第一个 reduce 来处理。</p>
</li>
</ul>
<p>==Reduce 阶段==<br>Reduce 阶段由一定数量的 Reduce Task 组成。这些 Reduce Task 可以同时运行，每个 Reduce Task又是由以下四个部分组成。</p>
<ul>
<li><p>1）、数据远程（http方式）或本地拷贝。Reduce Task 通过http拷贝每个 map 处理的结果，从每个 map 中读取一部分结果。每个 Reduce Task 拷贝哪些数据，是由上面 Partitioner 决定的(具体由reducer向ApplicationMaster请求拷贝数据的安排信息)。（这里要求服务器直接的网卡连接至少是Gb级别以上的光纤链路，提供拷贝数据的传输吞吐量）</p>
</li>
<li><p>2）、数据按照key排序。Reduce Task 读取完数据后，要按照key进行排序。按照key排序后，相同的key被分到一组，交给同一个 Reduce Task 处理。</p>
</li>
<li><p>3）、数据处理：Reducer。以WordCount为例，相同的单词key分到一组，交个同一个Reducer处理，这样就实现了对每个单词的词频统计。</p>
</li>
<li><p>4）、数据输出格式：OutputFormat。Reducer 统计的结果，将按照 OutputFormat 格式输出。默认情况下的输出格式为 TextOutputFormat，以WordCount为例，这里的key为单词，value为词频数。</p>
</li>
</ul>
<p>InputFormat、Mapper、Partitioner、Reducer和OutputFormat 都可以数据开发者自行实现，通常情况下，只需要实现 Mapper和Reducer。（数据开发者主要任务：集合数据业务需求，专注编写mapper和reducer二十年，该工作性质类似在关系型数据库中，数据分析岗根据市场运营提的数据报表需求写sql）</p>
<p>4.3  Shuffle（混洗）与reducer拉取数据细节<br>　　从4.2节内容可知，在map到reduce的过程，有一个环节为shuffling，那么shuffle是如何运作的呢？reducer又是如何从mapper节点去拷贝中间数据呢？按以下图示说明：<br>　<img src="https://img-blog.csdnimg.cn/20191017231015820.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>当map任务将数据output时，不仅仅是将结果输出到磁盘，它是将其写入内存缓冲区域，并进行一些预分类。</p>
<p>1）、Map阶段</p>
<p>首先map任务的output过程是一个环状的内存缓冲区，缓冲区的大小默认为100MB（可通过修改配置项mpareduce.task.io.sort.mb进行修改），当写入内存的大小到达一定比例，默认为80%（可通过mapreduce.map.sort.spill.percent配置项修改），便开始将这些缓存数据spill溢出写入磁盘。</p>
<p>在写入磁盘之前，线程将会指定数据写入与reduce相应的patitions中，最终传送给reduce。在每个partition中，后台线程将会在内存中进行Key的排序，（如果代码中有combiner方法，则会在output时就进行sort排序，这里，如果只有少于3个写入磁盘的文件，combiner将会在outputfile前启动，如果只有一个或两个，那么将不会调用）</p>
<p>这里将map输出的结果进行压缩会大大减少磁盘IO与网络传输的开销（配置参数mapreduce.map .output.compress 设置为true，如果使用第三方压缩jar，可通过mapreduce.map.output.compress.codec进行设置)</p>
<p>随后这些paritions输出文件将会通过HTTP发送至reducers，传送的最大启动线程通过mapreduce.shuffle.max.threads进行配置，如果mapper跟reducer同在一台服务器上，则reducer无需通过网络，直接读取本地文件，效率更高。</p>
<p>2）、The Reduce Side</p>
<p>首先上面每个节点的map都将结果写入了本地磁盘中，现在reduce需要将map的结果通过集群拉取过来，这里要注意的是，需要等到所有map任务结束后，reduce才会对map的结果进行拷贝，由于reduce函数有少数几个复制线程，以至于它可以同时拉取多个map的输出结果。默认的为5个线程（可通过修改配置mapreduce.reduce.shuffle.parallelcopies来修改其个数）</p>
<p>这里有个问题，那么reducers怎么知道从哪些机器拉取数据呢？ 当所有map的任务结束后，applicationMaster通过心跳机制（heartbeat mechanism)，由它负责mapping的输出结果与机器host的信息。所以reducer会定时的通过一个线程访问ApplicationMaster请求map的输出结果。</p>
<p>Map的结果将会被拷贝到reduce task的JVM的内存中（内存大小可在mapreduce.reduce.shuffle.input.buffer.percent中设置）如果不够用，则会写入磁盘。当内存缓冲区的大小到达一定比例时（可通过mapreduce.reduce.shuffle.merge.percent设置)或map的输出结果文件过多时（可通过配置mapreduce.reduce.merge.inmen.threshold)，将会触发合并(merged)随之写入磁盘。</p>
<p>这时要注意，所有的map结果这时都是被压缩过的，需要先在内存中进行解压缩，以便后续合并它们。（合并最终文件的数量可通过mapreduce.task.io.sort.factor进行配置） 最终reduce进行运算进行输出。<br>以上内容参考博文：<a href="https://blog.csdn.net/qq_36864672/article/details/78561375">shuffle和reducer拷贝mapper数据的细节</a></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>Python对象的属性增删改查的实质</title>
    <url>/2019/06/04/Python%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%B1%9E%E6%80%A7%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E7%9A%84%E5%AE%9E%E8%B4%A8/</url>
    <content><![CDATA[<p>&#8195;&#8195;在python对象实际使用过程中，会对其对象的属性进行增删改查，那么这背后是对哪种类型的”数据结构“进行增删改查呢？</p>
<a id="more"></a>
<p>下面以Blog类进行说明</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Python对象深度理解&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行实例化也即创建一个python对象</span></span><br><span class="line">myFirstBlog = Blog()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的增</span></span><br><span class="line">myFirstBlog.author=<span class="string">&#x27;Pysenen&#x27;</span></span><br><span class="line">print(myFirstBlog.author)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果：Pysenen</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的查</span></span><br><span class="line">print(myFirstBlog.title)</span><br><span class="line"><span class="comment"># 打印结果：Python对象深度理解</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的写（改）</span></span><br><span class="line">myFirstBlog.title=<span class="string">&#x27;新修改&#x27;</span></span><br><span class="line">print(myFirstBlog.title)</span><br><span class="line"><span class="comment"># 打印结果：新修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的删</span></span><br><span class="line"><span class="keyword">del</span> myFirstBlog.author</span><br><span class="line">print(myFirstBlog.author)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果：&#x27;Blog&#x27; object has no attribute &#x27;author&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;以上操作其实就是对 myFirstBlog这个对象里的<strong>dict</strong>数据结构进行操作</p>
<p>打印myFirstBlog.<strong>dict</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;author&#x27;</span>: <span class="string">&#x27;Pysenen&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;新修改&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195; 因此对对象的操作，其实就是对对象的<strong>dict</strong>属性操作</p>
<p>也即：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 增</span></span><br><span class="line">myFirstBlog.__dict__[<span class="string">&#x27;author&#x27;</span>]=<span class="string">&#x27;Python&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查</span></span><br><span class="line">myFirstBlog.__dict__[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改</span></span><br><span class="line">myFirstBlog.__dict__[<span class="string">&#x27;title&#x27;</span>]=<span class="string">&#x27;新的title&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删</span></span><br><span class="line">myFirstBlog.__dict__.pop(<span class="string">&#x27;author&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;因此容易扩展，对对象的操作，可以转为使用字典的方式进行灵活操作，可回到大家熟悉的python字典数据结构的概念</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_dict=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;26&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>:<span class="string">&#x27;test@gmail.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;company&#x27;</span>:<span class="string">&#x27;nonono&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">myFirstBlog.__dict__=batch_dict</span><br><span class="line"></span><br><span class="line">print(myFirstBlog.__dict__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> myFirstBlog.__dict__</span><br><span class="line">print(myFirstBlog.__dict__)</span><br><span class="line"><span class="comment"># 打印结果：&#123;&#125;</span></span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>————分割线————</p>
<p>&#8195;&#8195;由于这篇文章写得时间比较早，其实还有涉及更深的内容：类对象或者实例的所有属性都放在一个字典里，这里背后其实是由元类type掌控着。</p>
<p>&#8195;&#8195;所有的类都是type元类创建，它创建类的方式 normal_class=type(class_name,class_bases,class_dict)，这里的class_dict就是上面提到普通类的字典</p>
<p>&#8195;&#8195;例如上面定义的Blog类，用元类方式创建如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_dict=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;26&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>:<span class="string">&#x27;test@gmail.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;company&#x27;</span>:<span class="string">&#x27;nonono&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Blog=<span class="built_in">type</span>(<span class="string">&#x27;Blog&#x27;</span>,(<span class="built_in">object</span>,),batch_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">1</span>]: batch_dict=&#123;</span><br><span class="line">   ...:     <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">   ...:     <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;26&#x27;</span>,</span><br><span class="line">   ...:     <span class="string">&#x27;email&#x27;</span>:<span class="string">&#x27;test@gmail.com&#x27;</span>,</span><br><span class="line">   ...:     <span class="string">&#x27;company&#x27;</span>:<span class="string">&#x27;nonono&#x27;</span></span><br><span class="line">   ...: &#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: Blog=<span class="built_in">type</span>(<span class="string">&#x27;Blog&#x27;</span>,(<span class="built_in">object</span>,),batch_dict)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: Blog</span><br><span class="line">Out[<span class="number">3</span>]: __main__.Blog</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: b=Blog()</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: b.name</span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">&#x27;foo&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: b.age</span><br><span class="line">Out[<span class="number">6</span>]: <span class="string">&#x27;26&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: b.email</span><br><span class="line">Out[<span class="number">7</span>]: <span class="string">&#x27;test@gmail.com&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">type</span>(b)</span><br><span class="line">Out[<span class="number">8</span>]: __main__.Blog</span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;本文内容相对容易理解，如果要深入挖掘元类，可以查阅本博客文章<a href="https://blog.csdn.net/pysense/article/details/103516859">《 Python进阶——type与元类》</a></p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
  </entry>
  <entry>
    <title>Python开发常用的虚拟环境管理配置</title>
    <url>/2019/11/18/Python%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E7%9A%84%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>&#8195;&#8195;在某些python的开发项目中，或跑一些demo，例如tensorflow的demo，要求python3.5以上的版本，若原系统环境只有python2.7.5，显然无法满足测试环境。若为系统安装python3.5+，有些库又会造成版本冲突，因此需要使用python的虚拟环境工具来解决这些矛盾。当然也可采用python的docker镜像，使用一个镜像独立环境运行项目，但相比于python虚拟化工具来说，这种docker镜像显得有点重。</p>
<a id="more"></a>
<h4 id="1、python虚拟工具介绍"><a href="#1、python虚拟工具介绍" class="headerlink" title="1、python虚拟工具介绍"></a>1、python虚拟工具介绍</h4><p>目前有几种方式创建python的虚拟环境，在python3中有标准库venv，而第三方库例如virtualenv、virtualenvwrapper、pyenv，那么在项目或者说在实际开发里面，选哪种工具更为适合？</p>
<h5 id="1-1-virtualenv"><a href="#1-1-virtualenv" class="headerlink" title="1.1 virtualenv"></a>1.1 virtualenv</h5><p>virtualenv 是目前较为常用的 python 虚拟环境配置工具。它不仅同时支持 python2 和 python3，而且可以为每个虚拟环境指定 python 解释器（要求系统已经安装了不同版本的python），并选择不继承基础版本的site-packages。</p>
<h5 id="1-2-virtualenvwrapper"><a href="#1-2-virtualenvwrapper" class="headerlink" title="1.2 virtualenvwrapper"></a>1.2 virtualenvwrapper</h5><p>virtualenvwrapper是virtualenv的一个封装，目的是使后者更好用。virtualenv在使用中，每次得去虚拟环境所在目录下的 bin 目录下 source  activate，也即当有多个虚拟环境时，得每次都去找对应的目录，virtualenvwrapper将所有的虚拟环境目录全都集中起来统一管理，避免每次开启虚拟环境时候的source 项目目录操作。</p>
<h5 id="1-3-venv"><a href="#1-3-venv" class="headerlink" title="1.3 venv"></a>1.3 venv</h5><p>Python 从3.3 版本开始，自带了一个虚拟环境 <a href="https://docs.python.org/3/library/venv.html">venv</a>，在 <a href="http://legacy.python.org/dev/peps/pep-0405/">PEP-405</a> 中可以看到它的详细介绍。它的很多操作都和 virtualenv 类似。也支持linux和win。venv也有局限性，例如当前系统python版本为3.5，那么venv只能在当前安装的python3.5版本，不能创建其它Python 3.x的版本以及Python 2的环境。</p>
<h5 id="1-4-pyenv"><a href="#1-4-pyenv" class="headerlink" title="1.4 pyenv"></a>1.4 pyenv</h5><p><code>pyenv</code>主要用来安装、管理Python的版本及其虚拟环境，比如一个项目需要Python2.x，一个项目需要Python3.x。而virtualenv主要用来管理Python包的依赖。不同项目需要依赖的包版本不同，则需要使用虚拟环境。<code>pyenv</code>通过系统修改环境变量来实现Python不同版本的切换。前面的三个工具都是用于虚拟环境切换，pyenv是 Python 版本环境切换工具，将这两套工具结合使用，可以完美解决 python 多版本环境的问题。具体实例在第4节给出。</p>
<h4 id="2、使用virtualenv创建和管理虚拟环境"><a href="#2、使用virtualenv创建和管理虚拟环境" class="headerlink" title="2、使用virtualenv创建和管理虚拟环境"></a>2、使用virtualenv创建和管理虚拟环境</h4><h5 id="2-1-为多个python版本安装相应的pip"><a href="#2-1-为多个python版本安装相应的pip" class="headerlink" title="2.1 为多个python版本安装相应的pip"></a>2.1 为多个python版本安装相应的pip</h5><p>centos7.5默认没有pip包，因此需求手动安装，本文系统已经安装python2.7和python3.6。这里给出python2.7的pip安装和python3.6的pip3安装。<br>pip安装依赖setuptools，首先为python2和python3安装相应setuptools<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost local]# pwd</span><br><span class="line">/usr/local</span><br><span class="line">[root@localhost local]# wget https://files.pythonhosted.org/packages/ab/41/ab6ae1937191de0c9cbc115d0e91e335f268aa1cd85524c86e5970fdb68a/setuptools-42.0.0.zip</span><br><span class="line">[root@localhost local]# unzip setuptools-42.0.0.zip</span><br><span class="line">[root@localhost local] cd setuptools-42.0.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里的python命令是连接到python2.7，所以setuptools库只在python2.7环境生效</span></span><br><span class="line">[root@localhost setuptools-42.0.0]# python2.7 setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给python3.6安装setuptools</span></span><br><span class="line">[root@localhost setuptools-42.0.0]# python3.6 setup.py install</span><br></pre></td></tr></table></figure></p>
<p>这里为何使用python2.7或者python3.6，因为如果想要构建更多python版本，其shell执行命令例如python3.5，python3.7则会显得清晰而不混乱。</p>
<p>为python2和python3安装相应pip，跟setuptools安装流程一致。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@localhost local]# pwd</span><br><span class="line">/usr/local</span><br><span class="line">[root@localhost local] wget https://files.pythonhosted.org/packages/ce/ea/9b445176a65ae4ba22dce1d93e4b5fe182f953df71a145f557cffaffc1bf/pip-19.3.1.tar.gz</span><br><span class="line">[root@localhost local]# unzip pip-19.3.1.tar.gz</span><br><span class="line">[root@localhost local] cd pip-19.3.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 为python2.7 安装pip，最终命令执行路径在：/usr/<span class="built_in">local</span>/bin/pip</span></span><br><span class="line">[root@localhost pip-19.3.1] python2.7 setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 为pytho3.6 安装pip，最终命令执行路径：/usr/<span class="built_in">local</span>/bin/pip3</span></span><br><span class="line">[root@localhost pip-19.3.1] python3.6 setup.py install</span><br></pre></td></tr></table></figure>
<h5 id="2-2-安装virtualenv"><a href="#2-2-安装virtualenv" class="headerlink" title="2.2  安装virtualenv"></a>2.2  安装virtualenv</h5><p>上面已经配置了python2.7环境和python3.6环境，virtualenv库无需在两种环境安装，这里安装到python2.7库下即可。</p>
<p>这里要注意：如果系统已经安装python3版本，且shell已经设定python命令是软链接到python3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# ls -al /usr/bin/python</span><br><span class="line">**** /usr/bin/python -&gt; /usr/bin/python3</span><br></pre></td></tr></table></figure>
<p>那么需要使用pip3安装 virtualenv，这样virtualenv库才会安装到python3的site-packages目录下，</p>
<p>如果shell已经设定python命令是软链接到python2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# ls -al /usr/bin/python</span><br><span class="line">**** /usr/bin/python -&gt; /usr/bin/python2</span><br></pre></td></tr></table></figure>
<p>那么需要使用pip安装 virtualenv，这样virtualenv库才会安装到python2的site-packages目录下。</p>
<p>若不按照上述的环境情况，安装virtualenv，会出现以下情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# virtualenv -v</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/virtualenv&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    import virtualenv</span><br><span class="line">ModuleNotFoundError: No module named &#x27;virtualenv&#x27;</span><br></pre></td></tr></table></figure>
<p>当前python是软链到python3，而pip安装的virtualenv是在python2.7路径下，python3并没有virtualenv这个库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# python3</span><br><span class="line">Python 3.6.8 (default, Apr 25 2019, 21:02:35) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import virtualenv</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">ModuleNotFoundError: No module named &#x27;virtualenv</span><br></pre></td></tr></table></figure>
<p>virtualenv在python2.7路径下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# python</span><br><span class="line">Python 2.7.5 (default, Aug  7 2019, 00:51:29) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import virtualenv</span></span><br></pre></td></tr></table></figure>
<p>解决办法：将当前python命令软链到python2<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# ln -s /usr/bin/python2 /usr/bin/python</span><br></pre></td></tr></table></figure></p>
<h5 id="2-3、virtualenv创建虚拟环境"><a href="#2-3、virtualenv创建虚拟环境" class="headerlink" title="2.3、virtualenv创建虚拟环境"></a>2.3、virtualenv创建虚拟环境</h5><p>在/opt/pvenv_test目录下，创建一个使用python2.7解释器、且独立安装第三方库的运行环境、名字为crmapp的python2.7项目环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost pvenv_test]# virtualenv  --python=python2.7    --no-site-packages  crmapp</span><br><span class="line">Running virtualenv with interpreter /usr/bin/python2.7</span><br><span class="line">Already using interpreter /usr/bin/python2.7</span><br><span class="line">  No LICENSE.txt / LICENSE found in source</span><br><span class="line">New python executable in /opt/pvenv_test/crmapp/bin/python2.7</span><br><span class="line">Also creating executable in /opt/pvenv_test/crmapp/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...</span><br><span class="line"></span><br><span class="line">[root@localhost pvenv_test]# ls</span><br><span class="line">crmapp</span><br><span class="line">[root@localhost crmapp]# ls</span><br><span class="line">bin  include  lib  lib64</span><br><span class="line"><span class="meta">#</span><span class="bash"> 相关执行命令在bin目录下</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 激活当前环境</span></span><br><span class="line">[root@localhost crmapp]# source bin/activate</span><br><span class="line">(crmapp) [root@localhost crmapp]# </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前虚拟环境pip包没有引入系统的pip包，从而实现包不冲突</span></span><br><span class="line">(crmapp) [root@localhost crmapp]# pip list</span><br><span class="line">pip (9.0.1)</span><br><span class="line">setuptools (28.8.0)</span><br><span class="line">wheel (0.29.0)</span><br></pre></td></tr></table></figure>
<p>在/opt/pvenv_test目录下，创建一个使用python3.6解释器、且独立安装第三方库的运行环境、名字为djwebsocket的python3.6项目环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost pvenv_test]# virtualenv  --python=python3.6    --no-site-packages  djwebsocket</span><br><span class="line">Running virtualenv with interpreter /usr/bin/python3.6</span><br><span class="line">Already using interpreter /usr/bin/python3.6</span><br><span class="line">Using base prefix &#x27;/usr&#x27;</span><br><span class="line">  No LICENSE.txt / LICENSE found in source</span><br><span class="line">New python executable in /opt/pvenv_test/djwebsocket/bin/python3.6</span><br><span class="line">Also creating executable in /opt/pvenv_test/djwebsocket/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...</span><br><span class="line">done.</span><br><span class="line"></span><br><span class="line">[root@localhost pvenv_test]# source djwebsocket/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到当前pip包为干净的</span></span><br><span class="line">(djwebsocket) [root@localhost pvenv_test]# pip list</span><br><span class="line">Package    Version</span><br><span class="line">---------- -------</span><br><span class="line">pip        19.3.1 </span><br><span class="line">setuptools 42.0.0 </span><br><span class="line">wheel      0.33.6 </span><br></pre></td></tr></table></figure>
<p>这里的 —python=python3  其实就是 /usr/bin/python3对于的python3.6解释器</p>
<h4 id="3、pyenv终极python版本和虚拟化环境管理工具"><a href="#3、pyenv终极python版本和虚拟化环境管理工具" class="headerlink" title="3、pyenv终极python版本和虚拟化环境管理工具"></a>3、pyenv终极python版本和虚拟化环境管理工具</h4><p>有关virtualenvwrapper或者venv的用法，因内容较为简单，这里不再讨论，本文推荐使用pyenv管理任意基于python项目的虚拟环境。</p>
<h5 id="3-1-pyenv的设计原理"><a href="#3-1-pyenv的设计原理" class="headerlink" title="3.1 pyenv的设计原理"></a>3.1 pyenv的设计原理</h5><p><code>pyenv</code>主要用来管理Python的版本，比如一个项目需要Python2.x，一个项目需要Python3.x。而virtualenv主要用来管理Python包的依赖。不同项目需要依赖的包版本不同，则需要使用虚拟环境。</p>
<p><code>pyenv</code>通过系统修改环境变量来实现Python不同版本的切换。而vitualenv通过Python包安装到一个目录来作为Python虚拟包环境，通过切换目录来实现不同包环境间的切换。</p>
<p><code>pyenv</code>的设计巧妙的地方在于，在PATH 的最前面插入了一个垫片路径（shims）：~/.pyenv/shims:/usr/local/bin:/usr/bin:/bin。所有对 Python 可执行文件的查找都会首先被这个 shims 路径截获，从而使后方的系统路径失效。</p>
<p>对于系统环境变量 PATH ，里面包含了一串由冒号分隔的路径，例如 /usr/local/bin:/usr/bin:/bin。每当在系统中执行一个命令时，例如 python 或 pip，操作系统就会在 PATH 的所有路径中从左至右依次寻找对应的命令。因为是依次寻找，因此排在左边的路径具有更高的优先级。在PATH 最前面插入一个 <code>$(pyenv root)/shims</code>目录，<code>$(pyenv root)/shims</code>目录里包含名称为python以及pip等可执行脚本文件；当用户执行python或pip命令时，根据查找优先级，系统会优先执行shims目录中的同名脚本。pyenv 正是通过这些脚本，来灵活地切换至我们所需的Python版本。</p>
<p>需要手工去查找python版本的所在路径，如果有多个版本，这种手工管理显得有点繁琐。</p>
<h5 id="3-2-安装pyenv"><a href="#3-2-安装pyenv" class="headerlink" title="3.2 安装pyenv"></a>3.2 安装pyenv</h5><p>pyenv安装python需要依赖底层的系统库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost bin] yum install -y gcc make patch gdbm-devel openssl-devel sqlite-devel readline-devel zlib-devel bzip2-devel ncurses-devel libffi-devel</span><br></pre></td></tr></table></figure>
<p>若系统库不全，pyenv安装python后，会有如下提示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# pyenv install 3.7.5</span><br><span class="line">Installing Python-3.7.5...</span><br><span class="line">WARNING: The Python bz2 extension was not compiled. Missing the bzip2 lib?</span><br><span class="line">WARNING: The Python readline extension was not compiled. Missing the GNU readline lib?</span><br><span class="line">WARNING: The Python sqlite3 extension was not compiled. Missing the SQLite3 lib?</span><br><span class="line">Installed Python-3.7.5 to /root/.pyenv/versions/3.7.5</span><br></pre></td></tr></table></figure>
<p>有三个warning提示，系统环境缺少 bzip2、readline、SQLite3。</p>
<p>下载pyenv<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget git <span class="built_in">clone</span> https://github.com/pyenv/pyenv.git ~/.pyenv</span><br></pre></td></tr></table></figure></p>
<p>将<code>PYENV_ROOT</code>和<code>pyenv init</code>加入bash的<code>~/.bashrc</code>（或zsh的<code>~/.zshrc</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &#x27;export PATH=~/.pyenv/bin:$PATH&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &#x27;export PYENV_ROOT=~/.pyenv&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &#x27;eval &quot;$(pyenv init -)&quot;&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>激活<code>pyenv</code>（zsh为<code>~/.zshrc</code>）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>指定python版本在线安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# pyenv install 3.7.5</span><br></pre></td></tr></table></figure>
<p>常用命令<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyenv install --list # 列出可安装版本</span><br><span class="line">pyenv install &lt;version&gt; # 安装对应版本</span><br><span class="line">pyenv uninstall &lt;version&gt; # 卸载对应版本的python</span><br><span class="line">pyenv uninstall &lt;venv name&gt; # 删除指定虚拟环境</span><br><span class="line">pyenv install -v &lt;version&gt; # 安装对应版本，若发生错误，可以显示详细的错误信息</span><br><span class="line">pyenv versions # 显示当前使用的python版本</span><br><span class="line">pyenv which python # 显示当前python安装路径</span><br><span class="line">pyenv global &lt;version&gt; # 设置默认Python版本</span><br></pre></td></tr></table></figure></p>
<h5 id="3-3pyenv离线安装python各版本"><a href="#3-3pyenv离线安装python各版本" class="headerlink" title="3.3pyenv离线安装python各版本"></a>3.3pyenv离线安装python各版本</h5><p>pyenv自动去官网拉取python安装包，如果要为离线服务器安装，则只需在<code>.pyenv</code>创建cache目录，将指定版本的python安装包放在该目录。</p>
<p>python安装包下载地址：<a href="https://www.python.org/ftp/python/">https://www.python.org/ftp/python/</a></p>
<p>只需下载Python-<em>*</em>.tar.xz 即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost cache]# pwd</span><br><span class="line">/root/.pyenv/cache</span><br><span class="line">[root@localhost cache]# ls</span><br><span class="line">Python-3.7.5.tar.xz </span><br><span class="line"></span><br><span class="line">[root@localhost .pyenv]# pyenv install 3.7.5</span><br><span class="line">Installing Python-3.7.5...</span><br><span class="line">Installed Python-3.7.5 to /root/.pyenv/versions/3.7.5</span><br></pre></td></tr></table></figure>
<h5 id="3-4-使用多个python版本"><a href="#3-4-使用多个python版本" class="headerlink" title="3.4 使用多个python版本"></a>3.4 使用多个python版本</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pyenv设为系统原有python版本：</span><br><span class="line">[root@localhost opt]# pyenv global system </span><br><span class="line">[root@localhost opt]# python -V</span><br><span class="line">Python 2.7.5</span><br><span class="line"></span><br><span class="line">pyenv更换系统python3.7.5版本后：</span><br><span class="line">[root@localhost opt]# pyenv global 3.7.5</span><br><span class="line">[root@localhost opt]# python -V</span><br><span class="line">Python 3.7.5</span><br><span class="line">[root@localhost opt]# pyenv versions</span><br><span class="line">  system</span><br><span class="line">* 3.7.5 (set by /root/.pyenv/version)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python3.7.5所在目录，所有的安装python都会集中放置在.pyenv/versions目录下</span></span><br><span class="line">[root@localhost bin]# pwd</span><br><span class="line">/root/.pyenv/versions/3.7.5/bin</span><br><span class="line">[root@localhost bin]# ls</span><br><span class="line">2to3              idle     pip3    pydoc3.7   python3.7-config   python3-config</span><br><span class="line">2to3-3.7          idle3    pip3.7  python     python3.7-gdb.py   python-config</span><br><span class="line">easy_install      idle3.7  pydoc   python3    python3.7m         pyvenv</span><br><span class="line">easy_install-3.7  pip      pydoc3  python3.7  python3.7m-config  pyvenv-3.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>pyenv在python版本方面得心应手。</p>
<h5 id="3-5-pyenv结合-pyenv-virtualenv实现灵活的虚拟环境管理"><a href="#3-5-pyenv结合-pyenv-virtualenv实现灵活的虚拟环境管理" class="headerlink" title="3.5 pyenv结合/pyenv-virtualenv实现灵活的虚拟环境管理"></a>3.5 pyenv结合/pyenv-virtualenv实现灵活的虚拟环境管理</h5><p>在第3.2章节介绍了virtualenv用于管理pip包的虚拟环境，virtualenv有个不足地方是：系统需已安装多个python版本，一般会通过手工安装，也即编译时指定不同路径，避免冲突。pyenv作者同时也开发pyenv-virtualenv的工具，跟virtualenv功能一致，也是用于管理pip包以及虚拟环境，结合pyenv，可以实现任意python版本以及pip包环境的切换以及使用，高效提高个人开发效率。</p>
<p>pyenv-virtualenv放在.pyenv/plugins 这个插件目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost plugins]# pwd</span><br><span class="line">/root/.pyenv/plugins</span><br><span class="line">[root@localhost plugins]# git clone https://github.com/yyuu/pyenv-virtualenv.git</span><br><span class="line">[root@localhost plugins]# ls</span><br><span class="line">pyenv-virtualenv  python-build</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新系统环境变量</span></span><br><span class="line">[root@localhost plugins]# exec &quot;$SHELL&quot;</span><br></pre></td></tr></table></figure>
<p>创建python3.7.5版本、虚拟环境名为tf375</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv virtualenv 3.7.5 tf375</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有使用python3.7.5版本创建的虚拟环境都在此目录：/root/.pyenv/versions/3.7.5/envs</span></span><br><span class="line">[root@localhost envs]<span class="comment"># ls</span></span><br><span class="line">pyspark375  tf375</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf375虚拟环境的目录在这里：</span></span><br><span class="line">[root@localhost opt]<span class="comment"># ls ~/.pyenv/versions/3.7.5/envs/tf375/</span></span><br><span class="line"><span class="built_in">bin</span>/        include/    lib/        lib64/      pyvenv.cfg  </span><br><span class="line"></span><br><span class="line"><span class="comment">#pyenv uninstall tf375 # 删除虚拟环境,同时也会删除其目录</span></span><br></pre></td></tr></table></figure>
<p>激活tf375环境，pyenv支持对环境名的自动补全，非常方便，pip包默认不继承系统包。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv activate tf375 </span></span><br><span class="line"></span><br><span class="line">(tf375) [root@localhost envs]<span class="comment"># pip list</span></span><br><span class="line">Package    Version</span><br><span class="line">---------- -------</span><br><span class="line">pip        <span class="number">19.2</span><span class="number">.3</span> </span><br><span class="line">setuptools <span class="number">41.2</span><span class="number">.0</span> </span><br></pre></td></tr></table></figure>
<p>创建python3.6.5版本、虚拟环境名为spk的pyspark开发环境<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv virtualenv 3.6.5 pyspark_proj</span></span><br><span class="line">[root@localhost opt]<span class="comment"># pyenv activate pyspark_proj</span></span><br><span class="line">(pyspark_proj) [root@localhost opt]<span class="comment"># pip list</span></span><br><span class="line">pip (<span class="number">9.0</span><span class="number">.3</span>)</span><br><span class="line">setuptools (<span class="number">39.0</span><span class="number">.1</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>python虚拟环境</tag>
      </tags>
  </entry>
  <entry>
    <title>Python类私有变量覆盖特性</title>
    <url>/2019/06/03/Python%E7%B1%BB%E7%A7%81%E6%9C%89%E5%8F%98%E9%87%8F%E8%A6%86%E7%9B%96%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<p>&#8195;&#8195;此内容相对简单，仅在使用子类过程中，注意私有变量在子类继承过程的特性</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    __name=<span class="string">&#x27;Foo&#x27;</span> <span class="comment"># 命名空间变为：_Person__name</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__age=<span class="number">10</span>  <span class="comment"># 命名空间变为：_Person__age</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__height</span>(<span class="params">self</span>):</span> <span class="comment"># 命名空间变为：_Person__height</span></span><br><span class="line">        print(<span class="string">&#x27;form&#x27;</span>,self.__class__.__name__)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;180cm&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">outside_access</span>(<span class="params">self</span>):</span> <span class="comment"># 类内部访问类的私有方法或者私有数据属性</span></span><br><span class="line">        print(<span class="string">&#x27;form&#x27;</span>,self.__class__.__name__)</span><br><span class="line">        <span class="keyword">return</span> self.__height()</span><br><span class="line"></span><br><span class="line">print(Person.__dict__)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;_Person__name&#x27;: &#x27;Foo&#x27;, &#x27;__doc__&#x27;: None, &#x27;__module__&#x27;: &#x27;__main__&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;__dict__&#x27;: &lt;attribute &#x27;__dict__&#x27; of &#x27;Person&#x27; objects&gt;, &#x27;__weakref__&#x27;: &lt;attribute &#x27;__weakref__&#x27; of &#x27;Person&#x27; objects&gt;, </span></span><br><span class="line"><span class="string"> &#x27;__init__&#x27;: &lt;function Person.__init__ at 0x1143171e0&gt;, </span></span><br><span class="line"><span class="string"> &#x27;_Person__height&#x27;: &lt;function Person.__height at 0x114317268&gt;, </span></span><br><span class="line"><span class="string"> &#x27;outside_access&#x27;: &lt;function Person.outside_access at 0x1143172f0&gt;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">p=Person()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(p.__name) # Person&#x27; object has no attribute &#x27;__name&#x27;</span></span><br><span class="line"><span class="comment"># print(p.__height) # Person&#x27; object has no attribute &#x27;__height&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Python 为何要在类私有变量前加上类名作为变量名称</span></span><br><span class="line"><span class="string">目的：当子类覆盖父类时，保证了子类的同名私有变量_subclass__Foo不会覆盖同名的_parentclass__Foo</span></span><br><span class="line"><span class="string">或者说：父类不想让子类覆盖自己的方法，可通过将方法私有化达到目的,这个子类指：同一模块内的方法，或被引入到其他模块当中的子类</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SubPerson</span>(<span class="params">Person</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__height</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;170cm&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">outside_access</span>(<span class="params">self</span>):</span> <span class="comment"># 类内部访问类的私有方法或者私有数据属性</span></span><br><span class="line">        print(<span class="string">&#x27;form&#x27;</span>,self.__class__.__name__)</span><br><span class="line">        <span class="keyword">return</span> self.__height()</span><br><span class="line"></span><br><span class="line">s=SubPerson()</span><br><span class="line">print(s._Person__height())</span><br><span class="line"><span class="comment"># 打印结果：180cm</span></span><br><span class="line">print(s._SubPerson__height())</span><br><span class="line"><span class="comment"># 打印结果：170cm</span></span><br><span class="line"><span class="comment"># 说明父类__height方法与子类的_height完全独立</span></span><br><span class="line"></span><br><span class="line">print(s.outside_access())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">打印结果：</span></span><br><span class="line"><span class="string">form SubPerson</span></span><br><span class="line"><span class="string">170cm</span></span><br><span class="line"><span class="string">显然父类非私有方法已被子类覆盖</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
  </entry>
  <entry>
    <title>VMware虚拟化——基于VCSA6.7搭建生产可用小型服务器集群（非共享存储方式）</title>
    <url>/2019/06/10/VMware%E8%99%9A%E6%8B%9F%E5%8C%96%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8EVCSA6.7%E6%90%AD%E5%BB%BA%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8%E5%B0%8F%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%9B%86%E7%BE%A4%EF%BC%88%E9%9D%9E%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F%EF%BC%89/</url>
    <content><![CDATA[<h5 id="项目背景："><a href="#项目背景：" class="headerlink" title="项目背景："></a>项目背景：</h5><p>&#8195;&#8195;使用VMware做高可用集群的前提是服务器连接共享存储（当然VMware高level许可证以及网络都要保证），这种方式可以开启VM最惊艳的高可用功能：HA、DRS、FT，VM直接在PAAS层提供高可用，无需应用虚拟机配置。但也有一些特殊场景，不需要用HA、DRS、FT，服务器不连接共享存储，该业务仅是借助了VMware虚拟化技术，将多台独立带存储的物理机虚拟化后用VCSA6.7统一管理，这种业务中的虚拟机里跑的应用自带主备配置，分别在主ESXi开启主虚拟机，在备ESXi开启备虚拟机，在应用里面配置好master-slave，通过应用层实现的“应用层高可用”，物理层网络和物理服务器互相独立。</p>
<p>架构图如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20190711221651975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img">非共享存储服务器集群虚拟化后的大致架构图</p>
<a id="more"></a>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="1、服务器物理位置"><a href="#1、服务器物理位置" class="headerlink" title="1、服务器物理位置"></a>1、服务器物理位置</h5><p>&#8195;&#8195;机框1放置主服务器，机框2放置备服务器，两台构成主备虚拟化物理华三交换机可放置与机框1底下，或者放置到旁边位置（位置放太远不方便服务器网线连接交换机），参考第二点图：</p>
<h5 id="2、物理链路连接"><a href="#2、物理链路连接" class="headerlink" title="2、物理链路连接"></a>2、物理链路连接</h5><p><img src="https://img-blog.csdnimg.cn/20190711214645479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="拓扑图">拓扑图</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>（1）以机框1的物理服务器1为例，服务器一般配置4个物理网卡，eth0、eth1为业务网口，eth2作为管理ESXi网络网口，将eth0连至主交换机端口，将eth1连至备交换机端口，服务器1与上联接入层两台交换机构成链路冗余，也即两台交换机其中一台停机，或者服务器两个网口其中一个网口配置出错等，业务口都可冗余物理连接。</p>
<p>（2）eth2作为ESXi的管理口，连接至主接入层交互机，管理口冗余，避免过多占用网口和交互机端口（例如以华三某较为高端二层48端口，减去主备虚拟化IRF2个端口+上联连至核心交互机1个端口，还剩45个端口，若一台物理服务器使用两个管理口，再加两个业务口，可提供11台服务器连接（一个机框可放置服务器的数量也就是12台这样），若服务器仅用三个网口，可连15台服务器），按实际工作上配置的三个网口，两台48端口接入层交换机，可连接三个机框的服务器，数量大致为30台高度为2个U的服务器。</p>
<p>（3）接入层主交换机上联连至上联主核心交换机trunk端口，接入备交换机上联连至上联备核心交换机trunk端口，并配置两个trunk端口为聚合端口，实现接入层与核心层交换机构成冗余连接</p>
<p>（4）所有的物理连接都统一采用千兆电口，六类网线连接</p>
<h5 id="3、交换机IRF配置（又称堆叠，交换机虚拟化）"><a href="#3、交换机IRF配置（又称堆叠，交换机虚拟化）" class="headerlink" title="3、交换机IRF配置（又称堆叠，交换机虚拟化）"></a>3、交换机IRF配置（又称堆叠，交换机虚拟化）</h5><p>&#8195;&#8195;服务器上联交换机（业务接入层交换机）做了虚拟化后，相当于主备模式，当将主业务端口绑定到对应备交换机端口，那么交换机侧不仅实现主备模式，还实现负载均衡，流量叠加（跨交换机链路聚合），实现下联服务器集群在二层物理连接的高可用，生产已使用。此外，通过将业务接入层交换机接入到上联的核心交换机，实现连接更多的物理服务器</p>
<p>&#8195;&#8195;以两台华三某系列交换机为例，配置前提条件：支持irf，相同型号交换机，相同os版本</p>
<p>主备交换机物理链路连接。这里以23、24为千兆光口为连接说明，使用多模纤850交叉连接</p>
<p><img src="https://img-blog.csdnimg.cn/20190714112224579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>配置过程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 设置主交换机成员号1</span></span><br><span class="line">（1）irf member 1 renumber 1 </span><br><span class="line"></span><br><span class="line"> <span class="comment"># 设置本交换机优先级，值大优先级越高，越有可能成为master</span></span><br><span class="line"> (2) irf member 1 priority 20</span><br><span class="line"> (3) shutdown需要irf的物理端口23/24口</span><br><span class="line"></span><br><span class="line"> <span class="comment">#创建虚拟化逻辑端口 1/1表示 member=1/portID=1 并把23和24口加入（绑定）该逻辑端口</span></span><br><span class="line"> (4) irf-port 1/1</span><br><span class="line">    port group interface 23</span><br><span class="line">    port group interface 24</span><br><span class="line">“”“</span><br><span class="line">这里需要注意：主sw使用irf-port 1/1，那么在备sw必须使用irf-port 2/2，也即portID必须互为不     同，否则irf报错，提示portID冲突，无法建立irf环境，</span><br><span class="line">主sw的irf-port 1/1对应备sw的irf-port 2/2 </span><br><span class="line">或主sw的irf-port 1/2 对应备sw的irf-port 2/1,</span><br><span class="line">不能Irf-port 1/1 搭配Irf-port 2/1 或者Irf-port 1/2 搭配Irf-port 2/2</span><br><span class="line">”“”</span><br><span class="line"> (5)开启irf的23、24物理端口，并save保存配置</span><br><span class="line"></span><br><span class="line"> <span class="comment">#激活irf配置，并重启</span></span><br><span class="line"> (6) irf-port-configuration active </span><br><span class="line"></span><br><span class="line"><span class="comment"># 备sw的配置过程同上，不同的地方</span></span><br><span class="line"> (1) 成员号配置为2，备sw交换机的端口前缀从默认的1/0/~变为2/0/~（若有第三台，配置为3，端口前缀变为3/0/~，这个成员号配置是sw堆叠后，方便区别端口是属于哪台sw）</span><br><span class="line"> (2) sw的优先级配为10，比主sw的低</span><br><span class="line"> (2) 因主sw的逻辑端口配为irf-port 1/1，故备sw需配置为irf-port 2/2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后优化irf配置</span></span><br><span class="line"> undo irf mac-address persistent</span><br><span class="line"> irf auto-update <span class="built_in">enable</span></span><br><span class="line"> undo irf link-delay</span><br><span class="line"></span><br><span class="line">查看irf状态 dis irf</span><br><span class="line">MemberID    Role    Priority  CPU-Mac         Description</span><br><span class="line"> *+1        Master  20        00f0-a31p-6t02  ---</span><br><span class="line">   2        Standby 10        00f0-a31p-6t03  ---</span><br><span class="line"></span><br><span class="line">查看irf端口状态</span><br><span class="line">Member 1</span><br><span class="line"> IRF Port  Interface                             Status</span><br><span class="line"> 1         GigabitEthernet1/0/23                 UP    </span><br><span class="line">           GigabitEthernet1/0/24                 UP    </span><br><span class="line"> 2         <span class="built_in">disable</span>                               --    </span><br><span class="line">Member 2</span><br><span class="line"> IRF Port  Interface                             Status</span><br><span class="line"> 1         <span class="built_in">disable</span>                               --    </span><br><span class="line"> 2         GigabitEthernet2/0/23                 UP    </span><br><span class="line">           GigabitEthernet2/0/24                 UP  </span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="4、每台物理服务器存储配置"><a href="#4、每台物理服务器存储配置" class="headerlink" title="4、每台物理服务器存储配置"></a>4、每台物理服务器存储配置</h5><p>&#8195;&#8195;生产数据安全需优先，对每台物理服务器存储（SAS硬盘）配置RAID1，牺牲了一部分读写速度，这里RAID1的配置有个注意的地方，如果server提供的是非多媒体图片、视频等业务，读写都是“小块数据类”，那么RAID1 initial化stride设为最小256kb，若server提供的为图片、视频等业务，读写都是“大块数据类”，那么RAID1 initial化stride设为最大1024kb，以优化RAID的读写速度。</p>
<h5 id="5、业务网段和管理网段分开"><a href="#5、业务网段和管理网段分开" class="headerlink" title="5、业务网段和管理网段分开"></a>5、业务网段和管理网段分开</h5><p>&#8195;&#8195;在第二部分的物理链路拓扑图可以看到，数据链路和管理链路是分开的物理连接，实际使用场景中，一般会规划两个不同的网段使用，例如业务数据网段182.21.0.0/16，这个网段都是给vm虚拟机使用，网段大小可以自行根据vm数量规划，这里的掩码为16位，可以提供（<img src="https://private.codecogs.com/gif.latex?2%5E%7B16%7D-2" alt="2^{16}-2">）个VM使用，当然实际场景，很多企业不可能达到这么大的主机数，掩码可以设/24；另外一网段则用于ESXI主机管理口，例如182.22.10.0/24。一般企业来说，ESXI主机数量不会太多，可以设一个掩码8位的小网段，可以提供254个ESXI服务器使用。</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
  </entry>
  <entry>
    <title>VMware虚拟化——部署vCenter Server Appliance(VCSA6.7)完整全过程</title>
    <url>/2019/06/07/VMware%E8%99%9A%E6%8B%9F%E5%8C%96%E2%80%94%E2%80%94%E9%83%A8%E7%BD%B2vCenter%20Server%20Appliance(VCSA%206.7)%E5%AE%8C%E6%95%B4%E5%85%A8%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;采用vCenter+ESXi主机集群的部署方式可统一管理中小型数据中心，虚拟化使用vCenter最新的Linux版本，非网上大多数提及的Windows server版本，也即VCSA6.7，构建过程出现一些棘手问题，在全网都未找到相关问题的分析，在本文中得以解决，为此将解决方案以文章形式发布。</p>
<h4 id="1、VCSA6-7安装失败提示"><a href="#1、VCSA6-7安装失败提示" class="headerlink" title="1、VCSA6.7安装失败提示"></a>1、VCSA6.7安装失败提示</h4><p>VCSA6.7安装多次，卡在第二阶段52%或者60%后就无法安装，多次部署都出现该出错提示。</p>
<p><img src="https://img-blog.csdnimg.cn/20190616172734717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<a id="more"></a>
<p><img src="https://img-blog.csdnimg.cn/20190616172951334.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;在error的日志文件夹内，你会看到action.log、error.log、errors-ignored.log这三个文本（当然部署过程产生的还有其他多个文件）。其中在error.log文件里仅有一句：No file found matching /etc/vmware-vpx/vc-extn-cisreg.prop。很好，这些提示，都不可能给你明显的设置方向，你只能去检索+深度分析，在网上竟然检索不到该解决方案，以下为相关检索结果：</p>
<hr>
<p>这里有人提出同样的出错，但没有详细的解决方案：参考链接</p>
<p><code>https://bbs.51cto.com/thread-1569772-1-1.html</code></p>
<p>有人提出是VCSA版本低，说部署新的可成功安装，参考链接，这不是解决方案：</p>
<p><code>https://bbs.51cto.com/thread-1564358-1-1.html</code></p>
<p>这个链接是卡在安装过程，依然没人提过详细的解决方案：</p>
<p><code>https://bbs.csdn.net/topics/392349298</code></p>
<p>去<a href="http://www.baidu.com/link?url=vuGzAs8an5IJ6HdwZ94Sp3vWVs-SvvmFk-FiVW6dro4M2AUTwaASzEZ_q3VeC1n5"><em>Stack</em> <em>Overflow检索相关出错关键字，也无特别干货。</em></a></p>
<p><em>在VMware</em> troubleshoot 官方论坛，也没找到明显的提示，它有提示说到网络连接正常、DNS具备Vcenter域名的解析记录等，很含糊的提示。</p>
<p>而VCSA6.7部署成功的文章，却没进一步探讨，哪个环节是影响部署关键部分。</p>
<hr>
<h4 id="2、出现以上部署出错对应的配置过程（还原部署流程）："><a href="#2、出现以上部署出错对应的配置过程（还原部署流程）：" class="headerlink" title="2、出现以上部署出错对应的配置过程（还原部署流程）："></a>2、出现以上部署出错对应的配置过程（还原部署流程）：</h4><h5 id="2-1准备ESXI6-7环境"><a href="#2-1准备ESXI6-7环境" class="headerlink" title="2.1准备ESXI6.7环境"></a>2.1准备ESXI6.7环境</h5><p>&#8195;&#8195;首先需在一台物理机上安装好ESXI6.7环境，参考镜像：VMware-ESXi-6.7.0-8169922-LNV-20180404.iso ，参考安装方法：使用UltraISO将该镜像制作U盘启动器，注意，这里若用大白菜等工具，会出现无法在物理机安装ESXI环境</p>
<h5 id="2-2-准备VCSA6-7镜像"><a href="#2-2-准备VCSA6-7镜像" class="headerlink" title="2.2 准备VCSA6.7镜像"></a>2.2 准备VCSA6.7镜像</h5><p>&#8195;&#8195;首先使用虚拟光驱将VMware-VCSA-all-6.7.0-8217866.iso 镜像文件加载后在H:\vcsa-ui-installer\win32上，有exe界面安装器：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616174711487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;若不理解VCSA的概念，有人以为是这样的思路：该部署电脑是windows，那么在这里打开VCSA镜像的安装器，岂不是要安装在windows系统上？</p>
<p>&#8195;&#8195;实际情况：打开安装器后，会提示选择一台外部ESXI主机用来安装VSCA6.7（要求输入远程ESXI的IP账号密码），这个过程安装器会在选中的ESXI先创建一个linux环境（phonton os）的虚拟机，然后再把真正的VCSA6.7的OVA包注入到该虚拟机上进行安装。</p>
<h5 id="2-3-开始部署安装"><a href="#2-3-开始部署安装" class="headerlink" title="2.3 开始部署安装"></a>2.3 开始部署安装</h5><p>在配置网络这里，前几次部署失败都按这种网络配置</p>
<p><img src="https://img-blog.csdnimg.cn/20190616175934878.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>配置的大致逻辑：</p>
<p>FQDN：留空</p>
<p>IP地址：192.168.10.10</p>
<p>掩码：255.255.255.0</p>
<p>网关：192.168.10.1</p>
<p>DNS：20.101.191.1</p>
<p>ok，第一阶段都会正确安装，不会出现出错。</p>
<p>此时，登录部署vcenter的远程ESXi web 界面，发现该ESXi主机已经创建并运行一个venter的虚拟机，其shell提示可进入<br><code>https://photon-machine:5480</code> 进一步配置VCSA</p>
<p><img src="https://img-blog.csdnimg.cn/20190616214105649.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这里的域名值得质疑，</p>
<p><strong>“photon-machine”哪里设置的？</strong></p>
<p><code>https://photon-machine:5480</code> ，本地电脑DNS服务器肯定没有这条解析记录，部署电脑如何能打开该域名url？**</p>
<p><strong>如果必须让本地电脑可访问该域名url，那么本地电脑必须设置一个含有photon-machine&lt;&gt;192.168.10.10记录的DNS IP？</strong></p>
<p>如果<code>https://photon-machine:5480</code>可改为<code>https://192.168.10.10:5480</code> ，那么本地电脑无需设置DNS，即可访问，在哪里改？**</p>
<h5 id="2-4-带上以上疑问开始第二阶段安装"><a href="#2-4-带上以上疑问开始第二阶段安装" class="headerlink" title="2.4 带上以上疑问开始第二阶段安装"></a>2.4 带上以上疑问开始第二阶段安装</h5><p><img src="https://img-blog.csdnimg.cn/2019061621314249.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这里的single sign-on域名用了默认域名，也可以改成自己任意取定的域名（此设置不会引起第二阶段部署出错）</p>
<p><img src="https://img-blog.csdnimg.cn/20190616213544732.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;这里的IP地址DNS与2.3所填写的一致，到了这一步要注意了，这里的主机名称（指vcenter名称）：photo-machine，原来这是在第二阶段设完所有参数后，安装程序使用的默认的英文主机名称作为vcenter的主机名称，这样的配置设好后，开始第二阶段安装，不出意外，卡在60%后报错，安装失败。</p>
<p>&#8195;&#8195;如果你思维敏捷，你会去Vcenter shell看下什么情况，并输入<code>https://192.168.10.10</code>看看能有什么反馈？结果如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616220755366.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><img src="https://img-blog.csdnimg.cn/20190616220818565.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;用这两者里面的相关关键字去检索solution，然而似乎没什么收获。对于那些刚接触VCSA6.7且想使用其为搭建数据中心的工程师（这里指全栈工程师），2.1-2.4的配置过程几乎就是他们的配置过程，遇到这种bug应该很崩溃，因为所有的ESXi集群都搭建起来，就差Vcenter服务搭不起来，多台ESXI如何管理？如何构建高可用的数据中心？</p>
<h4 id="3、正确部署VCSA6-7的过程"><a href="#3、正确部署VCSA6-7的过程" class="headerlink" title="3、正确部署VCSA6.7的过程"></a>3、正确部署VCSA6.7的过程</h4><p>&#8195;&#8195;经过本人深度测试，目前两种部署方式可完整正确部署。</p>
<p>第一种：采用上面展示的UI安装方法，使用installer部署第一阶段&gt;&gt;进入VCenter shell设置&gt;&gt;登录Web界面进行第二阶段部署</p>
<p>第二种：采用将VCSA6.7的OVA文件导入到远程ESXi主机，使用ESXi导入ova创建&gt;&gt;进入VCenter shell设置&gt;&gt;登录Web界面进行第二阶段部署（理解了第一种方式后，第二种部署方式无需在此给出指引，可自行探索，ova文件一般位于iso文件夹里的vcsa目录下）</p>
<p>第一种方式</p>
<h5 id="3-1-第一阶段的installer，在此界面的设置参数中："><a href="#3-1-第一阶段的installer，在此界面的设置参数中：" class="headerlink" title="3.1 第一阶段的installer，在此界面的设置参数中："></a>3.1 第一阶段的installer，在此界面的设置参数中：</h5><p><img src="https://img-blog.csdnimg.cn/20190616223113370.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><strong>这里的FQDN可以留空，若要填入内容：建议务必填入V center的IP地址</strong></p>
<p><strong>这个案例中vcenter的IP地址：192.168.10.10，因为很多时候，内网并有没搭建专门的DNS服务器，直接填写为Vcenter的IP，客户机无需经过DNS解析即可访问</strong></p>
<p><strong>假设这里的FQDN填写的是域名：wow.mydatacenter，那么在第二阶段VCSA安装多个服务将采用域名去解析，若无法从所给的DNS解析出Vcenter的IP地址，导致相关服务继续安装出现错误。除非你设置的DNS服务器里，包含该域名和IP的映射记录。</strong></p>
<p><strong>所以：当FQDN填写vcenter为IP地址时，那么下方设置的DNS只需设为一个本网络段的可ping网关IP即可，因为vcenter第二阶段的服务安装直接通过IP查找，无需解析。</strong></p>
<p><strong>这里的DNS设为网关的IP：192.168.10.1</strong></p>
<h5 id="3-2-进入VCenter-shell设置"><a href="#3-2-进入VCenter-shell设置" class="headerlink" title="3.2 进入VCenter shell设置"></a>3.2 进入VCenter shell设置</h5><p>在3.1第一阶段完成后，退出installer（后面可通过web后台继续第二阶段部署），进入VCenter shell设置，在DNS里面，Hostname的值为photon-machine，这就是2.3节和2.4节内容提到<code>https://photon-machine:5480</code>等疑问的解决入口。</p>
<p><strong><img src="https://img-blog.csdnimg.cn/20190617230737363.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></strong></p>
<p>故将Hostname设为Vcenter的IP地址，shell restart 网络服务后，可看到其提示已经不再是域名的url，这意味着，</p>
<p><code>https://192.168.10.10:5480</code> 已经无需DNS解析即可访问，那么可以预测，在第二阶段部署中，某些服务的启动过程不再通过给定的DNS服务器去解析主机名称：photon-machine</p>
<p><img src="https://img-blog.csdnimg.cn/20190617230824719.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="3-3-登录web后台进行第二阶段部署：https-192-168-10-10-443"><a href="#3-3-登录web后台进行第二阶段部署：https-192-168-10-10-443" class="headerlink" title="3.3 登录web后台进行第二阶段部署：https://192.168.10.10:443"></a>3.3 登录web后台进行第二阶段部署：<code>https://192.168.10.10:443</code></h5><p>web的部署UI跟本地客户端UI一样，这里的网络配置，主机名称不再是photon-machine，而是vcenter的IP地址</p>
<p><img src="https://img-blog.csdnimg.cn/20190617231409508.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>ok，如果你的第二阶段安装最终配置如上，那么恭喜部署成功！</p>
<p><img src="https://img-blog.csdnimg.cn/20190617232019860.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="3-4-进入vCenter进程服务管理后台"><a href="#3-4-进入vCenter进程服务管理后台" class="headerlink" title="3.4  进入vCenter进程服务管理后台"></a>3.4  进入vCenter进程服务管理后台</h5><p><code>https://192.168.10.10:5480</code>又是啥管理后台？注意，该后台不是用于创建虚拟机和管理虚拟机的VCenter后台，是用于管理有关vCenter进程服务相关的后台。</p>
<p><img src="https://img-blog.csdnimg.cn/20190617232601668.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><img src="https://img-blog.csdnimg.cn/20190617232639689.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>基本是有关VCSA多个服务进程管理后台，以及一些基本配置，在这里我们再次可看到主机名为IP地址，在服务进程列表，可尝试停止VMware vCenter Server 进程，你会发现shell里面的Firstboot Error，访问<code>https://192.168.10.10:443</code>，出现熟悉的503</p>
<p><img src="https://img-blog.csdnimg.cn/20190616220818565.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>综上：</p>
<p>（1）如果你对DNS服务不知如何配置vCenter主机名正向和反向查询，请直接将vCenter hostname改为IP</p>
<p>（2）局域网DNS配置好相关域名记录后，也必须清楚hostname的默认值为photon-machine，需改为你自行设计的vCenter域名。</p>
<h4 id="4、VCSA6-7-web管理正确授权使用"><a href="#4、VCSA6-7-web管理正确授权使用" class="headerlink" title="4、VCSA6.7 web管理正确授权使用"></a>4、VCSA6.7 web管理正确授权使用</h4><p>&#8195;&#8195;如果两个产品的许可证不匹配，那么vcenter web面板会提示“ESXI主机的许可证与vcenter不匹配，将断开连接”，相当于无法添加ESXi主机</p>
<p>解决方案：</p>
<p>（1）Vcenter的许可证必须是：VMware vCenter Server 6 Standard</p>
<p>（2）ESXI6.7主机的许可证必须是：VMware vSphere 6 Enterprise Plus</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>vCenter - VCSA6.7</tag>
      </tags>
  </entry>
  <entry>
    <title>ZooKeeper特性适用的应用场景</title>
    <url>/2019/09/10/ZooKeeper%E7%89%B9%E6%80%A7%E9%80%82%E7%94%A8%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</url>
    <content><![CDATA[<p>&#8195;&#8195;随着对zk使用和了解更深入，不得不佩服Apache基金出品的技术，一直拥有着改变世界的能量！zookeeper结合大数据技术栈，实现无以伦比的高可用分布式大数据架构，单单这一点就非常让人兴奋，从zk的设计来看，传统的数据结构和算法以及底层网络知识和技术，仍然可以通过结合现代业务模型进行创造和创新，所有继续保持沉淀传统基础，以助力更高效吸收新技术！</p>
<p>&#8195;&#8195;以下引用了网上一些对zk的总结，内容比较简单，毕竟不是原理探讨，但对于zk的特性使用或者说发挥zk的特长，需要开发者深度理解数据模型或应用场景才能实现基于zk特性的相应逻辑。一些常用的zk特性和场合说明整理放在个人blog上，以便查阅</p>
<a id="more"></a>
<h3 id="数据发布与订阅（配置中心）"><a href="#数据发布与订阅（配置中心）" class="headerlink" title="数据发布与订阅（配置中心）"></a>数据发布与订阅（配置中心）</h3><p>&#8195;&#8195;数据发布与订阅，即所谓的配置中心，顾名思义就是发布者将数据发布到ZooKeeper节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和动态更新。<br>&#8195;&#8195;类似这样的需求：系统中需要使用一些通用的配置信息，例如机器列表信息、数据库配置信息等。这些全局配置信息通常具备以下3个特性。</p>
<ul>
<li>数据量通常比较小</li>
<li>数据内容在运行时动态变化。</li>
<li>集群中各机器共享，配置一致。</li>
</ul>
<p>对于这样的全局配置信息就可以发布到ZooKeeper上，让客户端（集群的机器）去订阅该消息。</p>
<p>发布/订阅系统一般有两种设计模式，分别是推（Push）和拉（Pull）模式。</p>
<ul>
<li>推：服务端主动将数据更新发送给所有订阅的客户端。</li>
<li>拉：客户端主动发起请求来获取最新数据，通常客户端都采用定时轮询拉取的方式。</li>
</ul>
<p>&#8195;&#8195;ZooKeeper采用的是推拉相结合的方式。如下：<br>客户端想服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据（推拉结合）。</p>
<h3 id="Naming-Service"><a href="#Naming-Service" class="headerlink" title="Naming Service"></a>Naming Service</h3><p>&#8195;&#8195;命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务，远程对象等等——都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表。通过在ZooKeepr里创建顺序节点，能够很容易创建一个全局唯一的路径，这个路径就可以作为一个名字。<br>==ZooKeeper的命名服务即生成全局唯一的ID==</p>
<h3 id="分布式协调-通知"><a href="#分布式协调-通知" class="headerlink" title="分布式协调/通知"></a>分布式协调/通知</h3><p>&#8195;&#8195;ZooKeeper中特有Watcher注册与异步通知机制，能够很好的实现分布式环境下不同机器，甚至不同系统之间的通知与协调，从而实现对数据变更的实时处理。使用方法通常是不同的客户端都对ZK上同一个ZNode进行注册，监听ZNode的变化（包括ZNode本身内容及子节点的），如果ZNode发生了变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并做出相应的处理。<br>==ZK的分布式协调/通知，是一种通用的分布式系统机器间的通信方式。==</p>
<h4 id="心跳检测"><a href="#心跳检测" class="headerlink" title="心跳检测"></a>心跳检测</h4><p>&#8195;&#8195;基于ZK的临时节点的特性，可以让不同的进程都在ZK的一个指定节点下创建临时子节点，不同的进程直接可以根据这个临时子节点来判断对应的进程是否存活。通过这种方式，检测和被检测系统直接并不需要直接相关联，而是通过ZK上的某个节点进行关联，大大减少了系统耦合。</p>
<h4 id="工作进度汇报"><a href="#工作进度汇报" class="headerlink" title="工作进度汇报"></a>工作进度汇报</h4><p>&#8195;&#8195;在一个常见的任务分发系统中，通常任务被分发到不同的机器上执行后，需要实时地将自己的任务执行进度汇报给分发系统。这个时候就可以通过ZK来实现。在ZK上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样便可以实现两个功能：</p>
<ul>
<li>通过判断临时节点是否存在来确定任务机器是否存活。</li>
<li>各个任务机器会实时地将自己的任务执行进度写到这个临时节点上去，以便中心系统能够实时地获取到任务的执行进度。</li>
</ul>
<h3 id="Master选举"><a href="#Master选举" class="headerlink" title="Master选举"></a>Master选举</h3><p>&#8195;&#8195;Master选举可以说是ZooKeeper最典型的应用场景了。比如HDFS中Active NameNode的选举、YARN中Active ResourceManager的选举和HBase中Active HMaster的选举等。</p>
<p>&#8195;&#8195;利用ZooKeepr的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即ZooKeeper将会保证客户端无法创建一个已经存在的ZNode。也就是说，如果同时有多个客户端请求创建同一个临时节点，那么最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很容易地在分布式环境中进行Master选举了。</p>
<p>&#8195;&#8195;成功创建该节点的客户端所在的机器就成为了Master。同时，其他没有成功创建该节点的客户端，都会在该节点上注册一个子节点变更的Watcher，用于监控当前Master机器是否存活，一旦发现当前的Master挂了，那么其他客户端将会重新进行Master选举。</p>
<h3 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h3><p>&#8195;&#8195;分布式锁是控制分布式系统之间同步访问共享资源的一种方式。分布式锁又分为排他锁和共享锁两种。</p>
<h4 id="排他锁"><a href="#排他锁" class="headerlink" title="排他锁"></a>排他锁</h4><p>排他锁（Exclusive Locks，简称X锁），又称为写锁或独占锁。</p>
<blockquote>
<p>如果事务T1对数据对象O1加上了排他锁，那么在整个加锁期间，只允许事务T1对O1进行读取和更新操作，其他任何事务都不能在对这个数据对象进行任何类型的操作（不能再对该对象加锁），直到T1释放了排他锁。</p>
</blockquote>
<p>可以看出，排他锁的核心是如何保证当前只有一个事务获得锁，并且锁被释放后，所有正在等待获取锁的事务都能够被通知到。</p>
<p>排他锁流程</p>
<h5 id="定义锁"><a href="#定义锁" class="headerlink" title="定义锁"></a>定义锁</h5><p>ZooKeeper上的一个znode可以表示一个锁。例如/exclusive_lock/lock节点就可以被定义为一个锁。</p>
<h5 id="获得锁"><a href="#获得锁" class="headerlink" title="获得锁"></a>获得锁</h5><p>&#8195;&#8195;如上所说，把ZooKeeper上的一个ZNode看作是一个锁，获得锁就通过创建znode的方式来实现。所有客户端都去/exclusive_lock节点下创建临时子节点/exclusive_lock/lock。ZooKeeper会保证在所有客户端中，最终只有一个客户端能够创建成功，那么就可以认为该客户端获得了锁。同时，所有没有获取到锁的客户端就需要到/exclusive_lock节点上注册一个子节点变更的Watcher监听，以便实时监听到lock节点的变更情况。</p>
<h4 id="释放锁"><a href="#释放锁" class="headerlink" title="释放锁"></a>释放锁</h4><p>&#8195;&#8195;因为/exclusive_lock/lock是一个临时节点，因此在以下两种情况下，都有可能释放锁。</p>
<ul>
<li>当前获得锁的客户端机器发生宕机或重启，那么该临时节点就会被删除，释放锁。</li>
<li>正常执行完业务逻辑后，客户端就会主动将自己创建的临时节点删除，释放锁。</li>
</ul>
<p>&#8195;&#8195;无论在什么情况下移除了lock节点，ZooKeeper都会通知所有在/exclusive_lock节点上注册了节点变更Watcher监听的客户端。这些客户端在接收到通知后，再次重新发起分布式锁获取，即重复『获取锁』过程。</p>
<h4 id="共享锁（也就是zk上实现的分布式锁）"><a href="#共享锁（也就是zk上实现的分布式锁）" class="headerlink" title="共享锁（也就是zk上实现的分布式锁）"></a>共享锁（也就是zk上实现的分布式锁）</h4><p>&#8195;&#8195;可以多个事务同时获得一个对象的共享锁（同时读），有共享锁就不能再加排他锁（因为排他锁是写锁），前面已经在文章给出基于分布式锁的非常详细的实现过程，这里不再重复。</p>
]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
  </entry>
  <entry>
    <title>centos存储扩容配置（基于LVM）</title>
    <url>/2019/06/30/centos%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9F%BA%E4%BA%8ELVM%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;在全栈开发项目中，当项目已经部署到服务器（虚拟机、物理机或者云主机）后，有些分区若空间分配不合理，数据库以及相关日志文件将很快占满磁盘空间，因此需在后期手动为项目所在服务器空间进行扩容。</p>
<a id="more"></a>
<h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li>查看</li>
<li>分区</li>
<li>格式化</li>
<li>挂载到相应目录上</li>
</ol>
<ul>
<li><h4 id="查看fdisk-l"><a href="#查看fdisk-l" class="headerlink" title="查看fdisk -l"></a>查看fdisk -l</h4><p>确认容量不足，可以使用dd命令创建一个1G文件看看</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# dd if=/dev/zero of=1.0G.img bs=1M count=1000</span><br><span class="line">dd: error writing ‘1.0G.img’: No space left on device</span><br><span class="line">134+0 records in</span><br><span class="line">133+0 records out</span><br><span class="line">139460608 bytes (139 MB) copied, 0.471449 s, 296 MB/s</span><br></pre></td></tr></table></figure>
<p><code>dd: error writing ‘1.0G.img’: No space left on device</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> [root@nn ~]# fdisk -l</span><br><span class="line">Disk /dev/sda: 4294 MB, 4294967296 bytes, 8388608 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line">Disk label type: dos</span><br><span class="line">Disk identifier: 0x000cef21</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sda1   *        2048     2099199     1048576   83  Linux</span><br><span class="line">/dev/sda2         2099200     8388607     3144704   8e  Linux LVM</span><br><span class="line"></span><br><span class="line">Disk /dev/sdb: 5368 MB, 5368709120 bytes, 10485760 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line"></span><br><span class="line">Disk /dev/mapper/centos-root: 2785 MB, 2785017856 bytes, 5439488 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line"></span><br><span class="line">Disk /dev/mapper/centos-swap: 432 MB, 432013312 bytes, 843776 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br></pre></td></tr></table></figure>
<p>以上指：<br>两个存储（硬盘）设备分别sda和sdb<br>Disk /dev/sda: 4294 MB，该硬盘已分区：sda1,sda2，原系统使用4G的存储空间<br>Disk /dev/sdb: 5368 MB，==该硬盘未分区，5G硬盘为新增==<br>==以下信息，扇区有分逻辑扇区和物理扇区，对应512字节和4096字节，为何如此规划？==<br>Disk /dev/sda: 4294 MB, 4294967296 bytes, 8388608 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 4096 bytes<br><a href="https://www.ibm.com/developerworks/cn/linux/l-4kb-sector-disks/">参考IBM官网开发者论坛的解释：</a><br>解释得挺通俗易懂</p>
</li>
</ul>
<blockquote>
<ul>
<li>大致意思：原计算机界存储底层设计一个物理扇区对应512字节，里面包括数据和纠错冗余码，但现在需要存储大量数据，需要 ==<strong>提高每个物理扇区的字节容量，才能从整体上扩大硬盘容量，也让每个扇区放入更强大的纠错算法</strong>==，从而达到增加磁盘容量+提高可靠性两方面提升</li>
<li>在以往计算机的软件产业链里面，都是基于512 字节扇区来设计和实现，例如在基本输入/输出系统（BIOS）、引导装载程序、操作系统内核、文件系统代码和磁盘工具等工具中，也就是“软件层”只认512标准，物理扇区却提供4096，如何处理这种矛盾？</li>
<li>采用“中间件（内核驱动？）”软件层， ==<strong>将底层物理扇区划分为8个512字节的扇区，对于“软件层”来说，它们还是基于在512字节的扇区去操作数据</strong>==，西部数据是第一家生产这种磁盘的制造商，它用“ Advanced Format ”来代表带 4096 字节物理扇区且向 512 字节逻辑扇区转换的磁盘<br>既然涉及到数据写入4096字节物理扇区的的磁盘，中间需要分8个逻辑扇区去写入，那么这个过程是性能损耗的，具体说明如下：<br>最新的文件系统使用 4096 字节或更大尺寸的数据结构，因此，大部分磁盘 I/O 操作占用4k倍数的大小扇区，假设现在Linux系统要将一个文件写入到磁盘，<br>（1）当文件系统数据结构正好与底层物理分区4k大小一致，对 4k字节数据结构的读，就是直接对单一扇区的读写，因此硬盘的固件不需要做其他操作，不影响性能；<br>（2） 但当文件系统数据结构与底层物理扇区4k不完全一致时，例如文件为7k大小，读写操作必须使用两个物理扇区，对于读操作，因为读写头总是以大概率扫过连续两个扇区，不会消耗而外时间，==<strong>而对于写操作，磁盘的固件首先读取两个物理扇区，修改两个扇区的分区（分成2*8=16个逻辑扇区），然后入两个物理扇区。该操作所需时间比 4k占用一个扇区时所需时间多。因此，性能下降。</strong>==<br>（3）如何判断文件数据结构是否得到合理对齐（放置到扇区上）？ 大多数文件系统将其数据结构与包含其本身的分区开头对齐。因此，如果一个分区起始于以一个 4096 字节（或8 个扇区的分界）边界，例如下表所示，则表示它得到合理对齐</li>
</ul>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>分区</th>
<th>start</th>
<th>end</th>
</tr>
</thead>
<tbody>
<tr>
<td>sda1</td>
<td>2048</td>
<td>4096</td>
</tr>
<tr>
<td>sda2</td>
<td>4096</td>
<td>8192</td>
</tr>
<tr>
<td>sda3</td>
<td>8192</td>
<td>(8192+4096*10000)</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4>在使用fdisk /dev/sdb 进行分区时，由于centos已经默认第一个分区的边界时从2048开始，而是不是从0，128等小值开始，以下是以4096作为分区的起始值，为何要以一个大值作为第一个扇区边界值：==<strong>为 MBR 与第一个分区之间的未分配空间中的装载引导程序代码留出空间</strong>==<br>Partition number (1-4, default 1):<br>First sector  ==(2048-10485759, default 2048)== 4096<br>Last sector, +sectors or +size{K,M,G} (4096-10485759, default 10485759):<br>Using default value 10485759<br>Partition 1 of type Linux and of size 5 GiB is set<br>==要设为LVM分区，t命令更改==<br>Command (m for help): t<br>Selected partition 1<br>Hex code (type L to list all codes): 8e<br>Changed type of partition ‘Linux’ to ‘Linux LVM’</li>
</ul>
<p>Command (m for help): w<br>The partition table has been altered!<br>查看最新分区<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sdb1            2048    10485759     5241856   8e  Linux LVM</span><br></pre></td></tr></table></figure><br>5G硬盘已经被分区sdb1，LVM类型</p>
<ul>
<li><h3 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h3>格式化为一定格式的文件系统，与目标挂载目录的文件系统一致<br>通过命令查看<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs       2G  1.9G  1  80% /</span><br><span class="line">devtmpfs                devtmpfs  482M     0  482M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     494M  7.0M  487M   2% /run</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  162M  853M  16% /boot</span><br><span class="line">tmpfs                   tmpfs      99M     0   99M   0% /run/user/0  </span><br><span class="line">....</span><br></pre></td></tr></table></figure>
以上可以看到根目录/ 文件系统格式为 xfs<br>故将sdb1格式为xfs<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可以看到mkfs有常见的文件系统格式</span></span><br><span class="line">[root@nn ~]# mkfs</span><br><span class="line">mkfs         mkfs.btrfs   mkfs.cramfs  mkfs.ext2    mkfs.ext3    mkfs.ext4    mkfs.minix   mkfs.xfs </span><br><span class="line"><span class="meta">#</span><span class="bash"> 将新分区格式为xfs</span></span><br><span class="line">[root@nn ~]# mkfs.xfs /dev/sdb1</span><br></pre></td></tr></table></figure></li>
<li>调整目标LVM大小<br>（1）查看现有Volume Group名称</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               centos</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  .....</span><br></pre></td></tr></table></figure>
<p>这里vg名称为默认值：centos<br>（2）对新分区sdb1创建对应的物理卷<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pvcreate /dev/sdb1</span><br></pre></td></tr></table></figure><br> (3) 用以上物理卷扩展VG：centos<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vgextend centos /dev/sdb1</span><br><span class="line"><span class="meta">#</span><span class="bash"> No physical volume label <span class="built_in">read</span> from /dev/sdb1 Writing physical volume</span> </span><br><span class="line"><span class="meta">#</span><span class="bash">data to disk <span class="string">&quot;/dev/sdb1&quot;</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash">Physical volume <span class="string">&quot;/dev/sdb1&quot;</span> successfully created</span> </span><br><span class="line"><span class="meta">#</span><span class="bash">Volume group <span class="string">&quot;centos&quot;</span> successfully extended</span></span><br></pre></td></tr></table></figure><br> (4) 扩展 LVM 的逻辑卷 centos-root<br> 查看有哪些LVM逻辑卷<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# lvdisplay </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/centos/swap</span><br><span class="line">  LV Name                swap</span><br><span class="line">  VG Name                centos</span><br><span class="line">  LV Size                412.00 MiB</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/centos/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                centos</span><br><span class="line">  LV Size                2.59 GiB</span><br></pre></td></tr></table></figure><br> 截取关键行，以上显示有两个逻辑卷，swap和root，都归属于VG：centos<br> 现在要用sdb1增加的容量来扩展的root 逻辑卷，对于的LV path：/dev/centos/root<br> ==扩展==<br> ==<code>lvextend  /dev/centos/root    /dev/sdb1</code>==<br> ==调整大小==<br>==<code>xfs_growfs  /dev/centos/root</code>==</p>
<p>(5) 查看扩展效果<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# lvscan </span><br><span class="line">  ACTIVE            &#x27;/dev/centos/swap&#x27; [412.00 MiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/root&#x27; [&lt;7.59 GiB] inherit</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs       7.6G  2.9G  4.8G  38% /</span><br><span class="line">devtmpfs                devtmpfs  482M     0  482M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     494M  7.0M  487M   2% /run</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  162M  853M  16% /boot</span><br><span class="line">tmpfs                   tmpfs      99M     0   99M   0% /run/user/0</span><br></pre></td></tr></table></figure>
<p>以上都说明逻辑卷root对于根目录，容量从2.6G，扩展到7.6G</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>LVM扩容</tag>
      </tags>
  </entry>
  <entry>
    <title>flume集群高可用连接kafka集群</title>
    <url>/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面blog文章中：<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>和<a href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;flume NG集群前向的source是各类实时的log数据，通过flume sink将这些日志实时sink到后向kafka集群，所有flume sink其实是本架构里kafka的producer角色，kafka集群后向连接spark streaming，用于消费kafka的实时消息（log日志数据）流。</p>
<a id="more"></a>
<p>组件版本：<br>flume-1.9.0、kafka-2.12 </p>
<h4 id="1-在kafka集群上创建相应的topic"><a href="#1-在kafka集群上创建相应的topic" class="headerlink" title="1.在kafka集群上创建相应的topic"></a>1.在kafka集群上创建相应的topic</h4><p>&#8195;&#8195;在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是直接sink到hdfs上。<br>创建topic需要做一定规划，考虑到目前有三个broker节点，分别为nn、dn1以及dn2节点，所以创建了3个分区，每个分区有三个replica，topic名为：sparkapp<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181&#x2F;kafka-zk --replication-factor 3 --partitions 3 --topic sparkapp </span><br><span class="line">Created topic sparkapp.</span><br><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181&#x2F;kafka-zk --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>-zookeeper nn:2181/kafka-zk：因为kafka在zk的所有znode都统一放置在/kafka-zk路径下，所以启动时需要注意加上该路径。</p>
<h4 id="2-单节点配置flume的agent-sink"><a href="#2-单节点配置flume的agent-sink" class="headerlink" title="2.单节点配置flume的agent sink"></a>2.单节点配置flume的agent sink</h4><h5 id="2-1-配置flume-文件"><a href="#2-1-配置flume-文件" class="headerlink" title="2.1 配置flume 文件"></a>2.1 配置flume 文件</h5><p>&#8195;&#8195;这里首先给出单节点的flume是如何连接到kafka集群，在nn节点上启动flume进程。在第3章节，将给出flume集群连接kafka集群，实现两组件之间的高可用实时数据流。<br>flume source的数据源为<code>/opt/flume_log/web_log/access.log</code><br>这里拷贝一份新的配置文件，配置过程简单，相关组件的参数说明可以参考flume官网最新文档：<br><a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-sink">kafka-sink章节</a><br><code>[root@nn conf]# cp flume-conf.properties flume-kafka.properties</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">vi flume-kafka.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">设置<span class="built_in">source</span>组件<span class="built_in">exec</span>模式用 tail -F实时读取文本新的数据行</span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel组件，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定sinktype 为kafka sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的broker列表，用逗号（英文）隔开</span></span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> 前面创建的topic名称</span></span><br><span class="line">a1.sinks.k1.kafka.topic = sparkapp</span><br><span class="line"><span class="meta">#</span><span class="bash"> How many messages to process <span class="keyword">in</span> one batch. flume一次写入kafka的消息数</span></span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line"><span class="meta">#</span><span class="bash"> ack=1 说明只要求producer写入leader主分区即完成（<span class="built_in">wait</span> <span class="keyword">for</span> leader only)）</span></span><br><span class="line"><span class="meta">#</span><span class="bash">How many replicas must acknowledge a message before its considered successfully written.</span> </span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以设置分区ID，这里使用默认，也即kafka自己分区器</span></span><br><span class="line"><span class="meta">#</span><span class="bash">a1.sinks.k1.kafka.defaultPartitionId</span></span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 5</span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息的压缩类型</span></span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>组件和sinks组件绑定channel组件</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里sink的channel是单数</span></span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<p>这里的linger.ms=5主要是处理下情况：<br>producer是按照batch进行发送，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。<br>这里设置producer请求延时5ms才会被发送。</p>
<h5 id="2-2-测试数据消费情况"><a href="#2-2-测试数据消费情况" class="headerlink" title="2.2 测试数据消费情况"></a>2.2 测试数据消费情况</h5><p>在dn1节点上启动kafka consumer进程，等待flume sink，因为kafka已经集群，所以—bootstrap-server 选任意一个节点都可以，前提所选节点需在线<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  saprkapp</span><br></pre></td></tr></table></figure></p>
<p>在nn节点上启动flume agent，这里不是后台启动，目的是为了实时观测flume agent 实时数据处理情况。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-kafka.properties --name a1</span><br></pre></td></tr></table></figure><br>手动在source文件最加新的文本行<br>[root@nn web_log]# echo “testing flume to kafka” &gt;&gt;access.log<br>在dn1 上可以看到实时输出nn节点上flume sink过来的消息<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  sparkapp</span><br><span class="line">testing flume to kafka</span><br></pre></td></tr></table></figure><br>以上完成单节点flume实时数据到kafka的配置和测试，下面将使用flume集群模式sink到kafka集群</p>
<h4 id="3-flume-NG集群连接kafka集群"><a href="#3-flume-NG集群连接kafka集群" class="headerlink" title="3.flume NG集群连接kafka集群"></a>3.flume NG集群连接kafka集群</h4><p><img src="https://img-blog.csdnimg.cn/20191201225119507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">图3 为flume NG集群连接kafka集群的示意图<br>配置也相对简单，只需要把<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>第4.2章节的collector配置的sinks部分改为kafkasink，agent1、agent2和agent3用第4.1章节所提的配置文件内容即可，本文不再给出。<br>给个flume节点角色分布表<br>| 节点 |  flume 角色|kafka角色|<br>|—|—|—|<br>| nn | agent1，collector 1|kafka broker<br>| dn1 | agen2 |kafka broker<br>| dn2 | agent3，collector2 |kafka broker</p>
<h5 id="3-1-配置collector"><a href="#3-1-配置collector" class="headerlink" title="3.1 配置collector"></a>3.1 配置collector</h5><p>因为测试环境计算资源有限，每个flume进程和kafka进程都在同一服务器上运行，实际生产环境flume和kafka分别在不同服务器上。<br>配置collector：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume-1.9.0&#x2F;conf</span><br><span class="line">[root@nn conf]# vi avro-collector.properties</span><br><span class="line"># 定义三个组件</span><br><span class="line">collector1.sources &#x3D; r1</span><br><span class="line">collector1.sinks &#x3D; k1</span><br><span class="line">collector1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 定义source：这里的source配成avro，连接flume agent端sink avro</span><br><span class="line">collector1.sources.r1.type &#x3D; avro</span><br><span class="line"># bind的属性：dn2节点对应改为dn2</span><br><span class="line">collector1.sources.r1.bind &#x3D; nn</span><br><span class="line">collector1.sources.r1.port &#x3D; 52020</span><br><span class="line"></span><br><span class="line">#定义channel</span><br><span class="line">collector1.channels.c1.type &#x3D; memory</span><br><span class="line">collector1.channels.c1.capacity &#x3D; 500</span><br><span class="line">collector1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># 指定sinktype 为kafka sink，从而使得flume collector成为kafka的producer</span><br><span class="line">collector1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">collector1.sinks.k1.kafka.bootstrap.servers &#x3D; nn:9092,dn1:9092,dn2:9092</span><br><span class="line">collector1.sinks.k1.kafka.topic &#x3D; sparkapp</span><br><span class="line">collector1.sinks.k1.kafka.flumeBatchSize &#x3D; 20</span><br><span class="line">collector1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line">collector1.sinks.k1.kafka.producer.linger.ms &#x3D; 5</span><br><span class="line">collector1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"></span><br><span class="line"># source组件和sinks组件绑定channel组件</span><br><span class="line">collector1.sources.r1.channels &#x3D; c1</span><br><span class="line">collector1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure></p>
<h5 id="3-2-启动flume-ng集群服务"><a href="#3-2-启动flume-ng集群服务" class="headerlink" title="3.2 启动flume-ng集群服务"></a>3.2 启动flume-ng集群服务</h5><p>在nn和dn2节点上使用nohup &amp; 后台启动flume collector进程<br>nn节点上collector进程，jps可以看到Application进程<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[1] 26895</span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">27168 Jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">23589 Kafka</span><br><span class="line">26895 Application</span><br></pre></td></tr></table></figure><br> dn2同样操作，这里不再给出。</p>
<p>分别在nn、dn1和dn2节点上使用nohup &amp; 后台启动启动flume agent进程<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-agent.properties --name  agent1 -Dflume.root.logger=INFO,console &amp; </span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">324 Jps</span><br><span class="line">23589 Kafka</span><br><span class="line">30837 Application</span><br><span class="line">32462 Application</span><br></pre></td></tr></table></figure><br>可以在nn节点看到两个 Application进程，一个是flume collector进程，另外一个是flume agent进程。<br>dn1和dn2取同样的后台启动方式，这里不再给出。</p>
<h5 id="3-3-测试flume与kafka高可用"><a href="#3-3-测试flume与kafka高可用" class="headerlink" title="3.3 测试flume与kafka高可用"></a>3.3 测试flume与kafka高可用</h5><p>任找一个节点，启动kafka consumer 进程，这里在dn1节点上启动<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# .&#x2F;bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092,dn1:9092,dn2:9092 --topic sparkapp</span><br></pre></td></tr></table></figure><br>因为nn、dn1、dn2三个节点上都有flume agent进程，分别在每个节点下的access.log追加新数据行，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># nn节点追加新数据行</span><br><span class="line">[root@nn web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@nn web_log]# echo &quot;flume&amp;kafka HA--msg from nn&quot; &gt;&gt;access.log</span><br><span class="line"># dn1节点追加新数据行</span><br><span class="line">[root@dn1 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn1 web_log]#echo &quot;flume&amp;kafka HA--msg from dn1&quot; &gt;&gt;access.log</span><br><span class="line"># dn2节点追加新数据行</span><br><span class="line">[root@dn2 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn2 web_log]# echo &quot;flume&amp;kafka HA--msg from dn2&quot; &gt;&gt;access.log</span><br></pre></td></tr></table></figure><br>在kafka consumer shell可以看到实时接收三条记录：<br><img src="https://img-blog.csdnimg.cn/20191203235843219.png" alt="在这里插入图片描述"><br>能否关闭其中collector所在服务器中的一台用于同时测试flume和kafka的高可用，例如关闭nn服务器？<br>不能，因为这里只有三个节点，zookeeper集群必须要三个及以上才能保证高可用，因此这里只需要kill nn节点上的collector，此时flume集群只有dn2的collector在工作，按上面的步骤，在三个节点上都给access.log新增数据行，同样可以正常观测到dn1的kafka consumer拿到3条message</p>
<h4 id="4-小结"><a href="#4-小结" class="headerlink" title="4.小结"></a>4.小结</h4><p>本文给出了flume与kafka连接的高可用部署过程，设置相对简单，考虑到测试环境资源限制，这里把flume集群和kafka集群放在同一服务器，生产环境中，flume集群有独立的服务器提供，kafka集群也由独立的服务器提供。后面几篇文章将给出有关spark的深度理解和kafka与spark streaming整合内容，目的为用于实现实时处理批数据的环节。</p>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume连接kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop集群平台网络配置bond模式实现高可用</title>
    <url>/2019/11/19/hadoop%E9%9B%86%E7%BE%A4%E5%B9%B3%E5%8F%B0%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AEbond%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面文章关于hadoop大数据项目开发中，每台服务器之间其实存在大量的IO，例如NameNode服务器和DataNode服务器同步fimag文件件，DataNode之间的数据IO，为压榨服务器对于超大数据的吞吐量，在服务器的网络层使用bond模式配置Linux网卡，可以将该服务器上多个物理网卡虚拟成一张网卡，由该虚拟网卡对外提供统一网络服务，实现多个网卡自适应负载均衡以及提高数据的吞吐量，同时也实现链路双备份功能，保证服务器底层网络高可用性。bond虚拟网卡可以绑定多个网络，例如绑定2两个网卡，一般物理服务器有4个千兆网卡，所以也可以绑定4个网卡，绑定的网卡类型需要一致，否则无法进行bond模式配置。<br>&#8195;&#8195;以centos7.5 的两个网卡配成bond模式作为示例，其中在之前的hadoop的相关文章里，nn节点与dn2节点构成HA，dn节点作为DataNode，当这三个节点的底层网络都配成bond模式，将进一步提高本blog之前搭建大数据开发平台的高可用性。作为全栈开发者，这些涉及简单网络的配置，应该需要掌握。</p>
<a id="more"></a>
<h4 id="1、为测试服务器添加多个网卡"><a href="#1、为测试服务器添加多个网卡" class="headerlink" title="1、为测试服务器添加多个网卡"></a>1、为测试服务器添加多个网卡</h4><p>在VMware workstations中，给相应服务器新增一个网卡，选择NAT模式，这样这是基于虚拟机操作，实际生成环境中，我的开发项目会部署在真实服务器上，真实服务器不需要NAT模式。</p>
<h4 id="2、查看测试服务器的网卡信息"><a href="#2、查看测试服务器的网卡信息" class="headerlink" title="2、查看测试服务器的网卡信息"></a>2、查看测试服务器的网卡信息</h4><p>原网卡ens33是配有IP的，新增的网卡ens37无IP绑定。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ip a</span><br><span class="line"></span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.188.0.5/24 brd 192.188.0.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: ens37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 ***/64 scope link noprefixroute </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>可以看到两个网卡ens33和ens37，都是千兆网速，都为up状态</p>
<h4 id="3、配置两个网卡"><a href="#3、配置两个网卡" class="headerlink" title="3、配置两个网卡"></a>3、配置两个网卡</h4><h5 id="3-1-这里需要先把原网卡配置拷贝一份作为备份。"><a href="#3-1-这里需要先把原网卡配置拷贝一份作为备份。" class="headerlink" title="3.1 这里需要先把原网卡配置拷贝一份作为备份。"></a>3.1 这里需要先把原网卡配置拷贝一份作为备份。</h5><p>备份网卡ens33的配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# pwd</span><br><span class="line">/etc/sysconfig/network-scripts</span><br><span class="line">[root@nn network-scripts]# cp ifcfg-ens33 ifcfg-ens33.bak</span><br></pre></td></tr></table></figure>
<p>新增网卡ens37是没有对应配置文件，只需要从ens33拷贝一份即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# cp ifcfg-ens33 ifcfg-ens37</span><br></pre></td></tr></table></figure>
<h5 id="3-2-将ens33和ens37配成slave模式"><a href="#3-2-将ens33和ens37配成slave模式" class="headerlink" title="3.2 将ens33和ens37配成slave模式"></a>3.2 将ens33和ens37配成slave模式</h5><p>这里不再需要配置ip和dns，mac地址也不需要配置，linux自动识别。</p>
<p>配置ens33，</p>
<p>注意该网卡已配置有IP，在改完该网卡配置后，请勿直接重启网卡，否则ssh无法远程连接，因为slave模式下，该网卡配置文件里面是无IP地址的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=ens33</span><br><span class="line">NAME=ens33</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line"><span class="meta">#</span><span class="bash"> bond网卡的名字</span></span><br><span class="line">MASTER=bond0</span><br><span class="line"><span class="meta">#</span><span class="bash"> ens33网卡启用slave模式</span></span><br><span class="line">SLAVE=yes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>配置ens37 ，同上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=ens37</span><br><span class="line">NAME=ens37</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line"><span class="meta">#</span><span class="bash"> bond网卡的名字</span></span><br><span class="line">MASTER=bond0</span><br><span class="line"><span class="meta">#</span><span class="bash"> ens33网卡启用slave模式</span></span><br><span class="line">SLAVE=yes</span><br></pre></td></tr></table></figure>
<p>若有更多的网卡需要绑定为bond0模式，则按以上配置改即可。</p>
<h4 id="4、配置bond0虚拟网卡"><a href="#4、配置bond0虚拟网卡" class="headerlink" title="4、配置bond0虚拟网卡"></a>4、配置bond0虚拟网卡</h4><p>在 <code>/etc/sysconfig/network-scripts</code>下，需要创建一个<code>ifcfg-bond0</code>文件，用于配置名字为bond0的虚拟网卡，注意这里bond0只是一个名字，可以按需要命名，例如mybond。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# vi ifcfg-bond0</span><br></pre></td></tr></table></figure>
<p>配置文件如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=bond0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">NM_CONTROLLER=no</span><br><span class="line">BONDING_OPTS=&quot;miimon=100 mode=6&quot;</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.188.0.5</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.188.0.1</span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line">DNS2=114.114.114.115</span><br></pre></td></tr></table></figure>
<p>若不想在bond0配置文件写入<code>BONDING_OPTS=&quot;miimon=100 mode=6&quot;</code></p>
<p>也可以在/etc/modprobe.d/dist.conf</p>
<p>新增如下两行：</p>
<p><code>alias bond0 bonding
options bond0 miimon=100 mode=6</code></p>
<p>但个人建议直接在bond0网卡配置mod模式，方便后期查看和更改。</p>
<h4 id="5、bond0的配置说明和工作原理"><a href="#5、bond0的配置说明和工作原理" class="headerlink" title="5、bond0的配置说明和工作原理"></a>5、bond0的配置说明和工作原理</h4><p>重点配置项为：BONDING_OPTS=”miimon=100 mode=6”</p>
<p>该配置文件:miimon=100，意思是linux每100ms监测一次本服务器网络链路连接状态（这里说的网络连接，是指该服务器网卡与接入层交换机端口连接的状态），如果有其中网卡例如ens33中断，那么bond0会将网络连接切到可用ens37；</p>
<p>mode的值表示工作模式，共有0，1，2，3，4，5，6六种模式，常用为0，6，1三种</p>
<ul>
<li>mode=0，表示load balancing (round-robin)为负载均衡方式，两块网卡都工作，但是与网卡相连的交换必须做特殊配置（ 这两个端口需要配置成聚合方式），因为做bonding的这两块网卡是使用同一个MAC地址</li>
<li><p>mode=1，表示fault-tolerance (active-backup)提供冗余功能，工作方式是主备的工作方式，也就是说默认情况下只有一块网卡工作，另一块做备份 。</p>
</li>
<li><p>mode=6，表示load balancing (round-robin)为负载均衡方式，两块网卡都工作，该模式下无需配置交换机，因为做bonding的这两块网卡是使用不同的MAC地址。</p>
<p>mode6因为可以做负载均衡，因此实际场景使用效果好，mode6的工作原理：bond0虚拟网卡的mac地址为两张网卡mac地址其中的一个，bond0通过更改自身与某个网卡一样的Mac地址，以达到随时切换转发数据包到相应正常工作的网卡中。从外部交换机来看，交换机只看到bond0这个网卡，bond0就像Nginx，都是代理角色，代理后面有多个实体。</p>
<p>mode6负载均衡的是这么实现的：</p>
<p>假设测试服务器的网卡ens33连接交换机33号端口，ens37连接交换机37端口。</p>
<p>当bond0检测到ens33流量超过阈值时，则bond0会将自己Mac地址切换到en37的mac地址，所以交换机通过发arp广播包，找到37端口就是连接ens37网卡，所以网络流量就从交换机37端口转发到ens37网卡。</p>
</li>
</ul>
<h4 id="6、-加载内核bond模块-modprobe-bonding"><a href="#6、-加载内核bond模块-modprobe-bonding" class="headerlink" title="6、 加载内核bond模块 modprobe bonding"></a>6、 加载内核bond模块 modprobe bonding</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# modprobe bonding</span><br><span class="line">[root@nn network-scripts]# lsmod |grep bond </span><br><span class="line">bonding               152656  0 </span><br></pre></td></tr></table></figure>
<p>重启网络</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# service network restart</span><br><span class="line">Restarting network (via systemctl):                        [  OK  ]</span><br></pre></td></tr></table></figure>
<h4 id="7、查看bond0虚拟网卡状态并测试主备网卡切换"><a href="#7、查看bond0虚拟网卡状态并测试主备网卡切换" class="headerlink" title="7、查看bond0虚拟网卡状态并测试主备网卡切换"></a>7、查看bond0虚拟网卡状态并测试主备网卡切换</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# cat /proc/net/bonding/bond0 </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 负载均衡模式</span></span><br><span class="line">Bonding Mode: adaptive load balancing</span><br><span class="line">Primary Slave: None</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前对外连接网络额度是网卡ens33</span></span><br><span class="line">Currently Active Slave: ens33</span><br><span class="line"><span class="meta">#</span><span class="bash"> 状态up</span></span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens33</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f1</span><br><span class="line">Slave queue ID: 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f2</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>
<p>查看bond0的mac地址，跟ens33的mac地址一样：12:1d:21:fg:43:f1 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ip a</span><br><span class="line"></span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: ens37: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line"></span><br><span class="line">191: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.142.4/24 brd 192.168.142.255 scope global noprefixroute bond0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>关闭ens33网卡，看看bond0是否会切换到ens37</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# ifdown ens33</span><br><span class="line">Device &#x27;ens33&#x27; successfully disconnected.</span><br><span class="line"></span><br><span class="line">[root@localhost network-scripts]# cat /proc/net/bonding/bond0 </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line">Bonding Mode: adaptive load balancing</span><br><span class="line">Primary Slave: None</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到当ens33不工作后，网络连接已经切换到ens37</span></span><br><span class="line">Currently Active Slave: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f2</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop集群网络配置</tag>
      </tags>
  </entry>
  <entry>
    <title>supervisor管理web服务进程</title>
    <url>/2019/11/06/supervisor%E7%AE%A1%E7%90%86web%E6%9C%8D%E5%8A%A1%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;部分较小的项目例如flask，对内部使用，可无需使用web server，直接用flask自带服务即可完成需求，但考虑到直接使用<code>python app.py</code>启动flask后，运行过程进程可能会退出，因此有必要对其进行监控并自动重启，supervisor可满足需求，加上其用python写成，可以独自进行二次开发（supervisor的代码值得学习）。本文是对以往部分项目进行一个整理，作为参考资料归档。</p>
<a id="more"></a>
<h4 id="1、离线安装supervisor"><a href="#1、离线安装supervisor" class="headerlink" title="1、离线安装supervisor"></a>1、离线安装supervisor</h4><p>supervisor下载地址：</p>
<p><code>https://files.pythonhosted.org/packages/ca/1f/07713b0e1e34c312450878801d496bce8b9eff5ea9e70d41ff4e299b2df5/supervisor-4.1.0-py2.py3-none-any.whl</code></p>
<p>setuptools的下载地址:</p>
<p><code>https://files.pythonhosted.org/packages/d9/de/554b6310ac87c5b921bc45634b07b11394fe63bc4cb5176f5240addf18ab/setuptools-41.6.0-py2.py3-none-any.whl</code></p>
<p>先安装setuptools<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip  setuptools-41.6.0.zip &amp;&amp; cd  setuptools-41.6.0</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p>
<p>再安装supervisor</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip  supervisor-4.1.0.tar.gz &amp;&amp; cd supervisor-4.1.0</span><br><span class="line">python setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试是否安装成功</span></span><br><span class="line"><span class="meta">[root@~d]#</span><span class="bash"> python</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import supervisor</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;</span> </span><br></pre></td></tr></table></figure>
<h4 id="2、配置supervisor环境变量"><a href="#2、配置supervisor环境变量" class="headerlink" title="2、配置supervisor环境变量"></a>2、配置supervisor环境变量</h4><p>很多网上教程在安装完supervisor后，都会给出如下提示：</p>
<p>在/etc/supervisor目录下生成配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">echo_supervisord_conf&gt;</span><span class="bash">/etc/supervisord.conf</span></span><br></pre></td></tr></table></figure>
<p>但如果echo_supervisord_conf或者supervisord 二进制程序所在目录没有配到PATH环境变量的话，那么提示未找到对应的命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# echo_supervisord_conf&gt;/etc/supervisord.conf</span><br><span class="line">-bash: echo_supervisord_conf: command not found</span><br></pre></td></tr></table></figure>
<p>查找echo_supervisord_conf  在哪个目录下</p>
<h5 id="2-1-PATH的问题"><a href="#2-1-PATH的问题" class="headerlink" title="2.1 PATH的问题"></a>2.1 PATH的问题</h5><p>一般会在python的安装目录下，例如/usr/local/python27/bin 或者在编译安装python所设定的路径</p>
<p><code>[root@wap~]# which python3.5
/usr/local/bin/python3.5</code></p>
<p>但会有例外：</p>
<p>很多人安装python后，并没有把python的bin路径设到PATH里，而是直接把新安装的python 路径通过软链接的方式覆盖旧python，例如下面所示：</p>
<p><code>ln -s /usr/bin/python3.5 /usr/bin/python</code></p>
<p>虽然这种方式在可以启动python3.5，但一旦安装新的库后，这些库所在路径不是位于python3.5目录下，而是位于系统默认的bin目录下，如下面所示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# ls /usr/bin/</span><br><span class="line">flask      python3.5      pyvenv-3.5   pip3    supervisorctl  supervisord  echo_supervisord_conf  </span><br></pre></td></tr></table></figure>
<p>这里flask库和supervisor库都在<code>/usrl/bin/</code>目录下<br>而此时查看系统系统的path变量，centos最初始的path配置如下<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .bash_profile</span></span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash"> User specific environment and startup programs</span></span><br><span class="line">PATH=$PATH:$HOME/bin</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure><br>显然没有加入python的环境变量，修改后如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Get the aliases and <span class="built_in">functions</span></span></span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line">PATH=$PATH:$HOME/bin:/usr/bin/</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure>
<p>到此，supervisor的相关命令可在shell直接使用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# echo_supervisord_conf </span><br><span class="line">; Sample supervisor config file.</span><br><span class="line">;</span><br><span class="line">; For more information on the config file, please see:</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<h4 id="3、-将supervisor配置成service，并加入开机自启"><a href="#3、-将supervisor配置成service，并加入开机自启" class="headerlink" title="3、 将supervisor配置成service，并加入开机自启"></a>3、 将supervisor配置成service，并加入开机自启</h4><h5 id="3-1-加入service"><a href="#3-1-加入service" class="headerlink" title="3.1 加入service"></a>3.1 加入service</h5><p>离线安装supervisor后，还需配置开机启动service，这里要注意：</p>
<p>需要将 <code>daemon &quot;supervisord -c /etc/supervisord.conf &quot;</code></p>
<p>改为<code>daemon &quot;/usr/bin/supervisord -c /etc/supervisord.conf &quot;</code></p>
<p>完整的service配置如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# vi /etc/rc.d/init.d/supervisord</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> <span class="keyword">function</span> library</span></span><br><span class="line">. /etc/rc.d/init.d/functions</span><br><span class="line">RETVAL=0</span><br><span class="line">start() &#123;</span><br><span class="line">  echo -n $&quot;Starting supervisord: &quot;</span><br><span class="line">  daemon &quot;/usr/bin/supervisord -c /etc/supervisord.conf &quot;</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  echo</span><br><span class="line">  [ $RETVAL -eq 0 ] &amp;&amp; touch /var/lock/subsys/supervisord</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop() &#123;</span><br><span class="line">  echo -n $&quot;Stopping supervisord: &quot;</span><br><span class="line">  killproc supervisord</span><br><span class="line">  echo</span><br><span class="line">  [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/supervisord</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">restart() &#123;</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line"> start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line"> stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line"> restart|force-reload|reload)</span><br><span class="line">  restart</span><br><span class="line">  ;;</span><br><span class="line"> condrestart)</span><br><span class="line">  [ -f /var/lock/subsys/supervisord ] &amp;&amp; restart</span><br><span class="line">  ;;</span><br><span class="line"> status)</span><br><span class="line">  status supervisord</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  ;;</span><br><span class="line"> *)</span><br><span class="line">  echo $&quot;Usage: $0 &#123;start|stop|status|restart|reload|force-reload|condrestart&#125;&quot;</span><br><span class="line">  exit 1</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">exit $RETVAL</span><br></pre></td></tr></table></figure>
<p>修改文件权限为755，并设置开机启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod 755 /etc/rc.d/init.d/supervisord</span><br><span class="line">chkconfig supervisor on</span><br></pre></td></tr></table></figure>
<h5 id="3-2-修改默认的-etc-supervisord-conf"><a href="#3-2-修改默认的-etc-supervisord-conf" class="headerlink" title="3.2 修改默认的/etc/supervisord.conf"></a>3.2 修改默认的/etc/supervisord.conf</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[unix_http_server]</span><br><span class="line">file=/var/run/supervisor.sock  </span><br><span class="line"></span><br><span class="line">[supervisord]</span><br><span class="line">logfile=/var/log/supervisor/supervisord.log ; 修改为 /var/log 目录，避免被系统删除</span><br><span class="line">pidfile=/var/run/supervisord.pid </span><br><span class="line">...</span><br><span class="line">[supervisorctl]</span><br><span class="line">; 和&#x27;unix_http_server&#x27;里面的设定匹配</span><br><span class="line">serverurl=unix:///var/run/supervisor.sock </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：<code>/var/log/supervisor/supervisord.log</code>需要手动创建</p>
<h5 id="3-4、配置supervisord的web界面"><a href="#3-4、配置supervisord的web界面" class="headerlink" title="3.4、配置supervisord的web界面"></a>3.4、配置supervisord的web界面</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[inet_http_server]      </span><br><span class="line">port=*:36700      </span><br><span class="line">username=***           </span><br><span class="line">password=***         </span><br></pre></td></tr></table></figure>
<h5 id="3-5、配置管理进程"><a href="#3-5、配置管理进程" class="headerlink" title="3.5、配置管理进程"></a>3.5、配置管理进程</h5><p>配置管理进程有两种方式，一种是直接在supervisord.conf文件写入相关进程配置，另外一种是在某个目录下，为每个进程单独创建一份配置文件，然后把这些配置文件include到supervisord.conf，已防止supervisord.conf的全局配置被错误修改。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> /etc/supervisor/config，用于存放全部进程管理的配置文件</span></span><br><span class="line">[root@wap ~]# mkdir -p /etc/supervisor/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在/etc/supervisord.conf全局配置文件中的include参数，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将/etc/supervisor/config目录添加到include中</span></span><br><span class="line"></span><br><span class="line">[include]</span><br><span class="line">files =/etc/supervisor/config/*.ini</span><br></pre></td></tr></table></figure>
<p>这里以配置flask两个小项目作为示例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]#  vi /etc/supervisor/config/flask_app_1.init</span><br><span class="line">[program:flask_app_1]</span><br><span class="line">command=python /opt/flask_app_1/app.py </span><br><span class="line"><span class="meta">#</span><span class="bash"> 日志目录需要手动创建，文件名由supervisor创建</span></span><br><span class="line">stdout_logfile=/etc/supervisor/logs/flask_app_1.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> stdout日志文件大小，默认50MB，这里设为10M</span></span><br><span class="line">stdout_logfile_maxbytes=10MB  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 限制保存最近10个日志</span></span><br><span class="line">stdout_logfile_backups=10   </span><br><span class="line"><span class="meta">#</span><span class="bash"> 在 supervisord 启动的时候也自动启动</span></span><br><span class="line">autostart=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 把stderr重定向到stdout，默认<span class="literal">false</span></span></span><br><span class="line">redirect_stderr = true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 程序异常退出后自动重启</span></span><br><span class="line">autorestart=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动5秒后若没有异常退出，则认为该服务已经正常启动</span></span><br><span class="line">startsecs=5</span><br><span class="line">user=root ;默认使用root用户</span><br><span class="line">priority=10  ;该进程的优先级</span><br><span class="line">stopasgroup=true   ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup=true   ;默认为false，向进程组发送kill信号，包括子进程</span><br></pre></td></tr></table></figure>
<p>flask_app_2同上配置，注意实际使用中，应与相关需求或功能命名。</p>
<h4 id="4、启动supervisor并测试"><a href="#4、启动supervisor并测试" class="headerlink" title="4、启动supervisor并测试"></a>4、启动supervisor并测试</h4><p>这里因为已经将supervisor配置成service，故不需要再使用</p>
<p><code>supervisord -c /etc/supervisord.conf</code> 这样的命令启动supervisor</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# service supervisor start</span><br><span class="line">Starting supervisord:                                      [  OK  ]</span><br><span class="line"></span><br><span class="line">[root@wap ~]# service supervisor status</span><br><span class="line">supervisord (pid  18921) is running...</span><br><span class="line"></span><br><span class="line">[root@wap ~]# service supervisor stop</span><br><span class="line">Stopping supervisord:                                      [  OK  ]</span><br><span class="line">[root@wap ~]# service supervisor restart</span><br><span class="line">Stopping supervisord:                                      [  OK  ]</span><br><span class="line">Starting supervisord:                                      [  OK  ]</span><br></pre></td></tr></table></figure>
<p>以上说明supervisor的service配置是成功的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看已经被管理进程，这里有两种方式，一种在web 页面查看与操作，另外一种则用cli方式查看</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cli查看相关被管理进程服务</span></span><br><span class="line">[root@portal py_apps]# supervisorctl </span><br><span class="line">flask_app_1                     RUNNING   pid 191047, uptime 0:11:04</span><br><span class="line">flask_app_2                     RUNNING   pid 191047, uptime 0:11:04</span><br><span class="line"><span class="meta">supervisor&gt;</span><span class="bash"> </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启几次flask，查看其日志</span></span><br><span class="line">[root@wap ~]# cat  /etc/supervisor/logs/flask_app_1.log   </span><br><span class="line"> * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line"> * Environment: production</span><br><span class="line">   WARNING: Do not use the development server in a production environment.</span><br><span class="line">   Use a production WSGI server instead.</span><br><span class="line"> * Debug mode: off</span><br><span class="line"> * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line"> * Environment: production</span><br><span class="line">   WARNING: Do not use the development server in a production environment.</span><br><span class="line">   Use a production WSGI server instead.</span><br><span class="line"> * Debug mode: off</span><br></pre></td></tr></table></figure>
<p>通过web查看和管理进程<br><img src="https://img-blog.csdnimg.cn/20191106145102844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="5、supervisor分布式管理服务"><a href="#5、supervisor分布式管理服务" class="headerlink" title="5、supervisor分布式管理服务"></a>5、supervisor分布式管理服务</h4><p>&#8195;&#8195;例如在serverA放了app1，serverB放了app2，serverC放了app3，而且每个server还有自行开发的其他采集模块等，因supervisor只能管理单机上的进程，对于不同服务器上的进程管理，则需要借助其他工具：例如cesi，地址：<code>https://github.com/Gamegos/cesi</code>。它可以实现跨服务器管理进程。当然我们完全可以基于supervisor进行开发，利用它提供的rpc接口，也可以开发出集群管理工具。但不建议这么做，考虑到还需要主力攻项目，这些通用工具能用第三方可优先使用。本文不再对cesi进行测试。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title>使用kazoo连接zookeeper并监听节点数量以及值变化</title>
    <url>/2019/09/10/%E4%BD%BF%E7%94%A8kazoo%E8%BF%9E%E6%8E%A5zookeeper%E5%B9%B6%E7%9B%91%E5%90%AC%E8%8A%82%E7%82%B9%E6%95%B0%E9%87%8F%E4%BB%A5%E5%8F%8A%E5%80%BC%E5%8F%98%E5%8C%96/</url>
    <content><![CDATA[<p>&#8195;&#8195;目前kazoo是连接zk的最新第三方库，最新更新时间为2019年1月，其他第三方连接zk的库都长时间未更新，所以推荐使用kazoo。前面有几篇文章都已经详细给出了zk的部署，接下来是zk最核心的地方，将zk的数据结构特性跟业务场景相结合，实现复杂需求，本文给出基本demo用法介绍。</p>
<a id="more"></a>
<h5 id="1、监控节点数量的变化"><a href="#1、监控节点数量的变化" class="headerlink" title="1、监控节点数量的变化"></a>1、监控节点数量的变化</h5><p>&#8195;&#8195;基本操作，创建、更新、删除，kazoo接口已经足够简单，入参类型如果不懂，可以直接看源码，同时也有助于深入了解别人是如何构思python“中间件”</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo <span class="keyword">import</span> exceptions</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_test</span>(<span class="params">zk_path,host,port,node_list</span>):</span></span><br><span class="line">    zk=KazooClient(hosts=host+<span class="string">&#x27;:&#x27;</span>+port,timeout=<span class="number">5</span>)</span><br><span class="line">    zk.start(timeout=<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> zk.exists(zk_path):</span><br><span class="line">        print(<span class="string">&quot;node:&#123;&#125; does&#x27;t exists&quot;</span>.<span class="built_in">format</span>(zk_path))</span><br><span class="line">        <span class="comment"># 创建当前节点,持久性节点,值需要设为byte类型</span></span><br><span class="line">        zk.create(path=zk_path,value=<span class="string">b&#x27;bar&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里是获取当前节点的子节点列表，可以设定watch以及是否返回节点数据</span></span><br><span class="line">    child_node_list=zk.get_children(zk_path,watch=<span class="literal">None</span>,include_data=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 创建多个子节点，值可以设为一样，因为这里关注子节点是否存在，不关心其值</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> child_node_list:</span><br><span class="line">        <span class="keyword">for</span> sub_node <span class="keyword">in</span> node_list:</span><br><span class="line">            zk.create(zk_path + <span class="string">&#x27;/&#x27;</span> + sub_node,<span class="string">b&#x27;1&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;subnode list:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(child_node_list))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取当前节点的znode对象：含data和ZnodeStat对象</span></span><br><span class="line">    data,stat=zk.get(zk_path)</span><br><span class="line">    print(<span class="string">&#x27;current node data:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data))</span><br><span class="line">    print(<span class="string">&#x27;data version:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(stat.version))</span><br><span class="line">    print(<span class="string">&#x27;data length:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(stat.data_length))</span><br><span class="line">    print(<span class="string">&#x27;children node numbers:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(stat.numChildren))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新节点数据,可以指定值和版本，成功更新则ZnodeStat 对象</span></span><br><span class="line">    stat_new=zk.<span class="built_in">set</span>(zk_path,value=<span class="string">b&#x27;foo&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;node &#123;0&#125; is updated:&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(zk_path,stat_new))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除当前节点，若当前节点有子节点，则提示无法删除，需要使用递归删除</span></span><br><span class="line">    zk.delete(zk_path,recursive=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        last=zk.get_children(zk_path)</span><br><span class="line">        print(<span class="string">&#x27;children nodes :&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(last))</span><br><span class="line">    <span class="keyword">except</span> exceptions.NoNodeError:</span><br><span class="line">        print(<span class="string">&#x27;no children nodes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    zk.stop()</span><br><span class="line">    zk.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    normal_test(zk_path=<span class="string">&#x27;/app_conf&#x27;</span>,host=<span class="string">&#x27;192.168.100.5&#x27;</span>,port=<span class="string">&#x27;2181&#x27;</span>,node_list=[<span class="string">&#x27;foo1&#x27;</span>,<span class="string">&#x27;foo2&#x27;</span>,<span class="string">&#x27;foo3&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;==监控节点数量的变化，可以应用到相关的场景：<br>1）把节点名称设为服务器IP，可以实现服务器集群管理，服务（服务接口）上、下线通知等，又称服务发现、服务监控等<br>2）主备切换，把最小临时节点设为master角色，其他临时节点为salve角色<br>3）独占锁，若只监听一个固定临时节点，当该临时节点创建，则获得锁，否则释放锁<br>4）分布式锁，不同客户端创建不同临时顺序节点，链式监听节点是否删除事件==</p>
<h5 id="2、简单的wacher"><a href="#2、简单的wacher" class="headerlink" title="2、简单的wacher"></a>2、简单的wacher</h5><p>&#8195;&#8195;kazoo支持使用装饰器实现一个简单的wacher，kazoo有两种wacher，一个是监听子节点变化，另外一个是监听节点值的变化。监听子节点变化示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_child_node</span>(<span class="params">zk_path</span>):</span></span><br><span class="line">    zkc=KazooClient(hosts=<span class="string">&#x27;192.168.100.5:2181&#x27;</span>,timeout=<span class="number">5</span>)</span><br><span class="line">    zkc.start(timeout=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 直接用装饰器完成监听</span></span><br><span class="line"><span class="meta">    @ChildrenWatch(<span class="params">client=zkc,path=zk_path,send_event=<span class="literal">True</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_changes_with_event</span>(<span class="params">children,event</span>):</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;Children nodes are %s&quot;</span> % children)</span><br><span class="line">         <span class="keyword">if</span> event:</span><br><span class="line">            print(<span class="string">&quot;catched nodes a children nodes event &quot;</span>,event)</span><br><span class="line">            </span><br><span class="line"><span class="meta">    @ChildrenWatch(<span class="params">client=zkc,path=zk_path</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_changes_without_event</span>(<span class="params">children</span>):</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;Children are %s&quot;</span> % children)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    	time.sleep(<span class="number">5</span>)</span><br><span class="line">    	print(<span class="string">&#x27;watching children node changes.....&#x27;</span>)</span><br><span class="line">    	</span><br><span class="line">watch_child_node(/app_conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在zk上连续创建几个子节点，可以看到监听到变化</span></span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">2</span>] create /app_conf/foo1 <span class="number">1</span></span><br><span class="line">Created /app_conf/foo1</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">3</span>] create /app_conf/foo2 <span class="number">1</span></span><br><span class="line">Created /app_conf/foo2</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">4</span>] create /app_conf/foo3 <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出，捕捉到节点变化的事件，但zk不会给出这个事件的具体发生情况：子节点被删除的事件、子节点新增的事件</span></span><br><span class="line">需要客户端根据事件的发生写一段逻辑去获取zk的节点到底是增加了还是减少了。</span><br><span class="line">Children nodes are []</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>, <span class="string">&#x27;foo2&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>, <span class="string">&#x27;foo2&#x27;</span>, <span class="string">&#x27;foo3&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>, <span class="string">&#x27;foo2&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：在注册监听环节，可以监听当前节点本身是否删除的事件，以及子节点的增、删事件，若需要zk返回event，那么需要将send_event设为True,才可以在watch函数传入event位置参数,这个逻辑可以在kazoo的源码看到<br>if self._send_event<br>    result = self._func(children, event)<br>else:<br>    result = self._func(children)<br>装饰器有两种写法，一种是从引用kazoo import的ChildrenWatch，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ChildrenWatch(<span class="params">client=zkc,path=zk_path,send_event=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_changes_with_event</span>(<span class="params">children,event</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>另外一种是从已创建的zk实例中调用ChildrenWatch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@zkc.ChildrenWatch(<span class="params">path=zk_path,send_event=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_changes_with_event</span>(<span class="params">children,event</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h6 id="2-1-监听节点自身的数据变化"><a href="#2-1-监听节点自身的数据变化" class="headerlink" title="2.1 监听节点自身的数据变化"></a>2.1 监听节点自身的数据变化</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_data</span>(<span class="params">zk_path</span>):</span></span><br><span class="line">    zkc = KazooClient(hosts=<span class="string">&#x27;192.168.100.5:2181&#x27;</span>, timeout=<span class="number">5</span>)</span><br><span class="line">    zkc.start(timeout=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接用装饰器完成监听,节点值的监听还可以拿到zk的事件</span></span><br><span class="line">    <span class="comment">#使用@DataWatch(client=zkc,path=zk_path)或者两种写法都可以</span></span><br><span class="line"><span class="meta">    @zkc.DataWatch(<span class="params">path=zk_path</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_watch</span>(<span class="params">data, stat,event</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">&quot;Data is &#123;0&#125; and data type is &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(data, <span class="built_in">type</span>(data)))</span><br><span class="line">        print(<span class="string">&quot;Version is %s&quot;</span> % stat.version)</span><br><span class="line">        <span class="keyword">if</span> event:</span><br><span class="line">            print(<span class="string">&quot;catching a data event &quot;</span>,event)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        print(<span class="string">&#x27;watching current node data changes.....&#x27;</span>)</span><br><span class="line"></span><br><span class="line">watch_data(<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line"><span class="comment"># 在zk 将/app_conf set不同的值，可以看到监听到数据变化</span></span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">20</span>] <span class="built_in">set</span> /app_conf foo</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">19</span>] <span class="built_in">set</span> /app_conf <span class="number">2</span></span><br><span class="line"><span class="comment"># 输出，注意kazoo返回的是bytes类型的数据,data变化的事件已经捕捉到</span></span><br><span class="line">Data <span class="keyword">is</span> <span class="string">b&#x27;foo&#x27;</span> <span class="keyword">and</span> data <span class="built_in">type</span> <span class="keyword">is</span> &lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">bytes</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"><span class="title">Version</span> <span class="title">is</span> 22</span></span><br><span class="line"><span class="class"><span class="title">watching</span> <span class="title">current</span> <span class="title">node</span> <span class="title">data</span> <span class="title">changes</span>.....</span></span><br><span class="line"><span class="class"><span class="title">Data</span> <span class="title">is</span> <span class="title">b</span>&#x27;<span class="title">bar</span>&#x27; <span class="title">and</span> <span class="title">data</span> <span class="title">type</span> <span class="title">is</span> &lt;<span class="title">class</span> &#x27;<span class="title">bytes</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"><span class="title">Version</span> <span class="title">is</span> 23</span></span><br><span class="line"><span class="class"><span class="title">catching</span> <span class="title">a</span> <span class="title">data</span> <span class="title">event</span>  <span class="title">WatchedEvent</span>(<span class="params"><span class="built_in">type</span>=<span class="string">&#x27;CHANGED&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;节点数据的变化，很容易联想相关应用场景：<br>1）集中配置管理，各个客户端监听放置配置文件内容的节点，若配置有变化，则可以各个客户端拉取配置更新<br>2）消息队列：<br>&#8195;&#8195;在特定节点下创建持久顺序节点，创建成功时Watcher通知等待的队列，队列删除序列号最小的节点用以消费。此场景下znode存储的数据就是消息队列中的消息内容，持久顺序节点就是消息的编号，按排序后取出最小编号节点（先get后delete）。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题</p>
<h6 id="2-2-封装一个同时监听子节点变化和当前节点数据变化的watcher类"><a href="#2-2-封装一个同时监听子节点变化和当前节点数据变化的watcher类" class="headerlink" title="2.2 封装一个同时监听子节点变化和当前节点数据变化的watcher类"></a>2.2 封装一个同时监听子节点变化和当前节点数据变化的watcher类</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZKWatcher</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host,port,timeout=<span class="number">5</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self._old_node_list=[]</span><br><span class="line">        self._node_name=<span class="string">&#x27;&#x27;</span></span><br><span class="line">        self._host=host</span><br><span class="line">        self._port=port</span><br><span class="line">        self._time_out=timeout</span><br><span class="line">        self._ip_port=self._host+<span class="string">&#x27;:&#x27;</span>+self._port</span><br><span class="line">        self._zkc=KazooClient(hosts=self._ip_port,timeout=self._time_out)</span><br><span class="line">        self._zkc.start(self._time_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">watcher</span>(<span class="params">self,zk_path</span>):</span></span><br><span class="line">        <span class="comment"># 获取原子节点列表</span></span><br><span class="line">        self._old_node_list=self._zkc.get_children(zk_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 为所要监听的节点开启一个子节点监听器</span></span><br><span class="line">            ChildrenWatch(client=self._zkc,path=zk_path,func=self._node_change,send_event=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为所要监听的节点开启一个该节点值变化的监听器</span></span><br><span class="line">            DataWatch(client=self._zkc,path=zk_path,func=self._data_change)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_node_change</span>(<span class="params">self,new_node_list,event</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里的new_node_list是指当前最新的子节点列表</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> event:</span><br><span class="line">            print(<span class="string">&#x27;未有事件发生&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">		<span class="comment"># 当前节点列表与上次拿到的节点列表相等，注意不是长度相等，是列表值和长度都要相等</span></span><br><span class="line">        <span class="keyword">if</span> new_node_list == self._old_node_list:</span><br><span class="line">            print(<span class="string">&#x27;子节点列表未发生变化&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(new_node_list)&gt;<span class="built_in">len</span>(self._old_node_list):</span><br><span class="line">            <span class="keyword">for</span> new_node <span class="keyword">in</span> new_node_list:</span><br><span class="line">                <span class="keyword">if</span> new_node <span class="keyword">not</span> <span class="keyword">in</span> self._old_node_list:</span><br><span class="line">                    print(<span class="string">&#x27;监听到一个新的节点：%s&#x27;</span>%<span class="built_in">str</span>(new_node))</span><br><span class="line">                    self._old_node_list=new_node_list</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> old_node <span class="keyword">in</span> self._old_node_list:</span><br><span class="line">                <span class="keyword">if</span> old_node <span class="keyword">not</span> <span class="keyword">in</span> new_node_list:</span><br><span class="line">                    print(<span class="string">&#x27;监听到一个删除的节点：%s&#x27;</span>%<span class="built_in">str</span>(old_node))</span><br><span class="line">                    self._old_node_list=new_node_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data_change</span>(<span class="params">self,data,stat,event</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">            print(<span class="string">&#x27;节点已删除，无法获取数据&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> event:</span><br><span class="line">            print(<span class="string">&#x27;未有事件发生&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;监听到数据变化&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;数据为&#x27;</span>,data)</span><br><span class="line">        print(<span class="string">&#x27;数据长度&#x27;</span>,stat.dataLength)</span><br><span class="line">        print(<span class="string">&#x27;数据版本号：&#x27;</span>,stat.version)</span><br><span class="line">        print(<span class="string">&#x27;子节点数据版本号&#x27;</span>,stat.cversion)</span><br><span class="line">        print(<span class="string">&#x27;子节点数量&#x27;</span>,stat.numChildren)</span><br><span class="line">        print(<span class="string">&#x27;事件&#x27;</span>,event)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        zk = ZKWatcher(host=<span class="string">&#x27;192.168.100.5&#x27;</span>,port=<span class="string">&#x27;2181&#x27;</span>)</span><br><span class="line">        zk.watcher(<span class="string">&#x27;/app_locker&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            time.sleep(<span class="number">5</span>)</span><br><span class="line">            print(<span class="string">&#x27;watching......&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>kazoo</tag>
      </tags>
  </entry>
  <entry>
    <title>使用连接池方式和多线程方式连接mysql的测试说明</title>
    <url>/2019/08/29/%E4%BD%BF%E7%94%A8%E8%BF%9E%E6%8E%A5%E6%B1%A0%E6%96%B9%E5%BC%8F%E5%92%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5mysql%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面文章讨论了mysql做高可用的配置，参考<a href="https://blog.csdn.net/pysense/article/details/99892680">文章链接</a>，而本文则是开发项目过程需要用的部分，从配置数据库到实用数据库，以及再用SQL做BI分析再到SQL优化，这些都是全栈工程师的基本功。</p>
<a id="more"></a>
<h4 id="1、连接池测试mysql默认连接配置"><a href="#1、连接池测试mysql默认连接配置" class="headerlink" title="1、连接池测试mysql默认连接配置"></a>1、连接池测试mysql默认连接配置</h4><p>&#8195;&#8195;先出简单的测试连接池或多线程并发的脚本，这里先借用DBUtils创建连接池，文章后面会给出无须借用第三方库也可以实现实用的连接池。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> DBUtils.PooledDB <span class="keyword">import</span> PooledDB</span><br><span class="line"></span><br><span class="line">db_info=&#123;</span><br><span class="line">            <span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;port&#x27;</span>:<span class="number">34312</span>, <span class="comment"># 改掉默认端口号，安全考虑</span></span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;***&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;***&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;db&#x27;</span>:<span class="string">&#x27;erp_app&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;charset&#x27;</span>:<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">db_pool = PooledDB(</span><br><span class="line">    creator=pymysql,</span><br><span class="line">    maxconnections=<span class="number">3000</span>,  <span class="comment"># 连接池允许的最大连接数，0和None表示不限制连接数</span></span><br><span class="line">    mincached=<span class="number">0</span>,  <span class="comment"># 初始化时，连接池中至少创建的空闲的链接，0表示不创建</span></span><br><span class="line">    maxcached=<span class="number">0</span>,  <span class="comment"># 连接池中最多闲置的连接，0和None不限制</span></span><br><span class="line">    maxshared=<span class="number">0</span>, <span class="comment"># 连接池中最多共享的连接数量，0和None表示全部共享。</span></span><br><span class="line">    blocking=<span class="literal">True</span>,  <span class="comment"># 连接池中如果没有可用连接后，是否阻塞等待。True</span></span><br><span class="line">    maxusage=<span class="literal">None</span>,  <span class="comment"># 一个连接最多被重复使用的次数，None表示无限制</span></span><br><span class="line">    **db_info <span class="comment"># 数据库账户等信息</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试插入数据</span></span><br><span class="line">insert_sql = (<span class="string">&quot;insert into apps_name &quot;</span></span><br><span class="line">              <span class="string">&quot;(id,app_log_name,log_path,log_date) &quot;</span></span><br><span class="line">              <span class="string">&quot;values(null,%(app_log_name)s,%(log_path)s,null)&quot;</span>)</span><br><span class="line">insert_data=&#123;</span><br><span class="line">    <span class="string">&#x27;app_log_name&#x27;</span>:<span class="string">&#x27;BI-Access-Log&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;log_path&#x27;</span>:<span class="string">&#x27;/opt/data/apps_log/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">mode,inst_sql,inst_data</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    m:多线程模式，c:连接池模式</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;m&#x27;</span>:</span><br><span class="line">        conn=pymysql.connect(**db_info)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        conn = db_pool.connection()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> conn.cursor() <span class="keyword">as</span> cur:</span><br><span class="line">            resl=cur.execute(inst_sql,inst_data)</span><br><span class="line">            conn.commit()</span><br><span class="line">    <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> err:</span><br><span class="line">        conn.rollback()</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="comment"># PooledDB连接池关闭方法其实不是真的把该连接关闭，而是将该连接由放入池的队列里，在后文会看到该逻辑的实现</span></span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_insert</span>(<span class="params">mode,nums=<span class="number">151</span></span>):</span></span><br><span class="line">    treads=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nums):</span><br><span class="line">        t=threading.Thread(target=save_data,args=(mode,insert_sql,insert_data))</span><br><span class="line">        treads.append(t)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> treads:</span><br><span class="line">        t.start()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> treads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">mode,request_nums</span>):</span></span><br><span class="line">    start=time.time()</span><br><span class="line">    multi_insert(mode,request_nums)</span><br><span class="line">    end=time.time()</span><br><span class="line">    cost=end-start</span><br><span class="line">    print(<span class="string">&#x27;cost:&#123;0:.3&#125; s&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run(<span class="string">&#x27;c&#x27;</span>,<span class="number">100000</span>)</span><br></pre></td></tr></table></figure></p>
<p>查看mariadb默认设置的最大连接数为151</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; show variables like &#x27;%max_con%&#x27;;</span><br><span class="line">+---------------------------------------+-------+</span><br><span class="line">| Variable_name                         | Value |</span><br><span class="line">+---------------------------------------+-------+</span><br><span class="line">| extra_max_connections                 | 1     |</span><br><span class="line">| max_connect_errors                    | 100   |</span><br><span class="line">| max_connections                       | 151   |</span><br><span class="line">| performance_schema_max_cond_classes   | 80    |</span><br><span class="line">| performance_schema_max_cond_instances | -1    |</span><br><span class="line">+---------------------------------------+-------+</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;运行测试脚本，注意这里是连接池10万个连接的并发，对于server端的mysql来说，也就是同时有200个并发connections，就已经出错了，这就是模拟了客户端多线程大量并发消耗完mysql 最大连接资源引起error<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#x27;t connect to MySQL server on &#x27;192.168.100.5&#x27; ([Errno 24] Too many open files)&quot;)</span><br></pre></td></tr></table></figure></p>
<h4 id="2、高并发连接数据库出错分析与测试"><a href="#2、高并发连接数据库出错分析与测试" class="headerlink" title="2、高并发连接数据库出错分析与测试"></a>2、高并发连接数据库出错分析与测试</h4><p>&#8195;&#8195;出现以上情况，可通过设置mysql max_connections最大值，来保证并发量,测试mysql在有限物理资源条件下可达到的最大连接数，随便设一个大值例如10000000，最后可mysql单机可设定的最大连接数为10万<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; set GLOBAL max_connections=10000000;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [erp_app]&gt; show variables like &#x27;%max_con%&#x27;;</span><br><span class="line">+---------------------------------------+--------+</span><br><span class="line">| Variable_name                         | Value  |</span><br><span class="line">+---------------------------------------+--------+</span><br><span class="line">| extra_max_connections                 | 1      |</span><br><span class="line">| max_connect_errors                    | 100    |</span><br><span class="line">| max_connections                       | 100000 |</span><br><span class="line">| performance_schema_max_cond_classes   | 80     |</span><br><span class="line">| performance_schema_max_cond_instances | -1     |</span><br><span class="line">+---------------------------------------+--------+</span><br><span class="line">5 rows in set (0.001 sec)</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;在这里，==使用多线程方式==，测试脚本发起10万个线程并发请求，运行后程序很快出现socket请求打满客户端系统缓冲区，导致系统级别出错，如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#39;t connect to MySQL server on &#39;192.168.</span><br><span class="line">100.5&#39; ([WinError 10055] 由于系统缓冲区空间不足或队列已满，不能执行套接字上的操</span><br><span class="line">作。)&quot;)</span><br></pre></td></tr></table></figure><br>再查看服务器响应的最大连接数，成功连接仅有745个</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; show global status like &#x27;Max_used_connections&#x27;;</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Variable_name        | Value |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Max_used_connections | 745   |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">1 row in set (0.001 sec)</span><br></pre></td></tr></table></figure>
<h4 id="3、为何选用连接池优化连接？"><a href="#3、为何选用连接池优化连接？" class="headerlink" title="3、为何选用连接池优化连接？"></a>3、为何选用连接池优化连接？</h4><p>&#8195;&#8195;从第二部分的测试可知，因为每个线程创建单独的连接，当并发量大时，会造成client和msyql server之间的频繁“线程创建tcp连接-登录-线程退出关闭tcp连接”，<br>若采用连接池方式连接mysql，则是重用数据库服务端的tcp通道，以达到client和MySQL之间只需维持较少的连接，单个客户端可以提高其并发量，且消耗较低的物理资源。</p>
<p>打个不一定恰当的通俗比喻：</p>
<p>有10000辆车同时要从A点到达B点，出发前，A、B之间没有路，需要先搭建</p>
<p>1）多线程方式：需要1000个路面施工队同时搭建完1000条“高速路”后，才能同时出发，可见需要消耗非常多资源（10000个施工队以及10000条高速路资源），等10000辆车到达B点后，10000个施工队又得去拆除10000条高速路，非常耗资源</p>
<p>2）连接池方式（假设连接池大小为1000）：需要1000个路面施工队同时搭建完1000条“高速路”后，前面1000辆车到达B点，后面9000辆车出发时，施工队不需要再新建高速路，继续重用前面搭建的1000条“高速路”，极大降低的物理资源浪费。</p>
<p>连接池重要两个逻辑：</p>
<ul>
<li><p>在程序创建连接的时候，可以从连接池队列中取出一个空闲的连接，不需要重新初始化连接，提升获取连接的速度</p>
</li>
<li><p>关闭连接的时候，把连接放回存放连接池的队列中，而不是真正的关闭，所以可以减少频繁地打开和关闭tcp通道</p>
</li>
</ul>
<h4 id="4、继续测试两种连接效果"><a href="#4、继续测试两种连接效果" class="headerlink" title="4、继续测试两种连接效果"></a>4、继续测试两种连接效果</h4><p>&#8195;&#8195;mysql的默认最大连接数已设置为1万个<br>1）开启2000个线程，重复1次，使用多线程并发，不出意外，mysql接收到已用连接数为1022个，之后的请求连接全部error 中断</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@localhost opt]# python insert_test.py</span><br><span class="line">cost:1.99 s</span><br><span class="line"></span><br><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#x27;t connect to MySQL server on &#x27;192.168.100.4&#x27; ([Errno 24] Too many open files)&quot;)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show global status like &#x27;Max_used_connections&#x27;;</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Variable_name        | Value |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Max_used_connections | 1022  |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">1 row in set (0.000 sec)</span><br></pre></td></tr></table></figure>
<p>2) 开启多线程6000个并发，运行出错，6000个线程直接把客户端的系统缓冲区打满，无法继续运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#39;t connect to MySQL server on &#39;192.168.100.4&#39; ([WinError 10055] 由于系统缓冲区空间不足或队列已满，不能执行套接字上的操作。)&quot;)</span><br></pre></td></tr></table></figure>
<p>3）连接池开启6000个并发，连接池最大连接限制3000个，运行没有问题，cost:24.3 s</p>
<p>在mysql也可以看到最大已用连接数为3000个</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show global status like &#x27;Max_used_connections&#x27;;</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Variable_name        | Value |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Max_used_connections | 3000  |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">1 row in set (0.001 sec)</span><br></pre></td></tr></table></figure>
<h4 id="5-、自行实现简易使用的mysql连接池"><a href="#5-、自行实现简易使用的mysql连接池" class="headerlink" title="5 、自行实现简易使用的mysql连接池"></a>5 、自行实现简易使用的mysql连接池</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> pymysql.cursors <span class="keyword">import</span> DictCursor</span><br><span class="line"></span><br><span class="line">db_info=&#123;</span><br><span class="line">            <span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.4&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;port&#x27;</span>:<span class="number">3306</span>,</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;***&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;****&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;db&#x27;</span>:<span class="string">&#x27;erp_app&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;charset&#x27;</span>:<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConnPoolException</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;连接池出错 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MariaDBPool</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    _inst_lock=threading.RLock()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, connections, **db_conf</span>):</span></span><br><span class="line">        self.__connections = connections</span><br><span class="line">        self.__pool = Queue(connections)</span><br><span class="line">        <span class="comment"># 在init阶段，就已经创建好指定的连接，全部put到共享队列</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.__connections):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                conn = pymysql.connect(**db_conf)</span><br><span class="line">                self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">except</span> ConnPoolException <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">raise</span> IOError</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 单例模式创建连接池，个人喜欢用__new__方法创建，简洁，且使用了递归锁，保证在多线程方式创建单例模式的对象都是同一对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">with</span> cls._inst_lock:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(cls,<span class="string">&#x27;_inst&#x27;</span>):</span><br><span class="line">                cls._inst=<span class="built_in">object</span>.__new__(cls)</span><br><span class="line">        <span class="keyword">return</span> cls._inst</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute_insert</span>(<span class="params">self,sql,data_dict=<span class="literal">None</span></span>):</span></span><br><span class="line">        conn = self.__pool.get()</span><br><span class="line">        cursor = conn.cursor(DictCursor)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result=cursor.execute(sql,data_dict) <span class="keyword">if</span> data_dict <span class="keyword">else</span> cursor.execute(sql)</span><br><span class="line">            conn.commit()</span><br><span class="line">        <span class="keyword">except</span> ConnPoolException <span class="keyword">as</span> e:</span><br><span class="line">           <span class="comment"># 这里就是重点，只是关闭了游标，连接对像又返回池里   </span></span><br><span class="line">            conn.rollback()</span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 这里就是重点，只是关闭了游标，连接对像又返回池里</span></span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">executemany_insert</span>(<span class="params">self,sql,data_dict_list=<span class="literal">None</span></span>):</span></span><br><span class="line">        conn = self.__pool.get()</span><br><span class="line">        cursor = conn.cursor(DictCursor)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result=cursor.execute(sql,data_dict) <span class="keyword">if</span> data_dict_list <span class="keyword">else</span> cursor.executemany(sql)</span><br><span class="line">        <span class="keyword">except</span> ConnPoolException <span class="keyword">as</span> e:</span><br><span class="line">            conn.rollback()</span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 这里才是真正的关闭所有连接池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.__connections):</span><br><span class="line">            self.__pool.get().close()</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;就本文测试的数据库以及简单表结构而言，使用该连接池模块，注意测试之前，需重启mysql，保证数据库最大已连接数为1，也即清空历史残留连接，以便做新测试对比。12000个并发，mysql设定最大可用连接数：3000，单例模式，cost:22.2 s，程序不会出现任何异常，当然因表结构和服务器性能情况而不同。</p>
<p>&#8195;&#8195;这里顺便提下oracle线程池，部分项目使用oracle数据库，cx_Oracle也支持使用连接池方式连接，大致流程如下，也可根据使用习惯封装更适合自身业务需要的模块。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cx_Oracle</span><br><span class="line">db_user=<span class="string">&#x27;foo&#x27;</span></span><br><span class="line">db_pwd=<span class="string">&#x27;barbar&#x27;</span></span><br><span class="line">db_host=<span class="string">&#x27;192.168.100.7&#x27;</span></span><br><span class="line">db_name=<span class="string">&#x27;erp_app&#x27;</span></span><br><span class="line">dsn = cx_Oracle.makedsn(db_host, <span class="string">&quot;1521&quot;</span>, db_name)</span><br><span class="line">db_config = &#123;</span><br><span class="line">    <span class="string">&#x27;user&#x27;</span>: db_user,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>: db_pwd,</span><br><span class="line">    <span class="string">&#x27;dsn&#x27;</span>: dsn,</span><br><span class="line">    <span class="string">&#x27;min&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;max&#x27;</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">&#x27;increment&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;threaded&#x27;</span>: <span class="literal">True</span></span><br><span class="line">&#125;</span><br><span class="line">orc_pool = cx_Oracle.SessionPool(**dbConfig)</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;若使用多线程方式，尤其数量大的情况下（500以上），很容易把底层bug爆出来，cx_Oracle会引发python解释器崩溃。其实建议，只要是连接数据库，所引入的第三方库支持线程池方法的话，都建议用线程池，哪怕你的插入数据不是太频繁或并发量不大，减少程序自身出错，也降低数据库连接压力。</p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql连接池</tag>
      </tags>
  </entry>
  <entry>
    <title>在HadoopHA节点上部署Kafka集群组件</title>
    <url>/2019/11/28/%E5%9C%A8HadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2kafka%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章中<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件 》</a>已经介绍了flume实时收集acces.log，同时给出flume是如何实现数据流向的高可用环境测试。在后面的文章中会给出实时大数据项目的开发，实时数据源由flume sink到kafka的topic里，而不是前面提到的hdfs，目的是利用kafka强大的分布式消息组件用于分发来自flume的实时数据流。<br>kafka集群在Hadoop实时大数据项目的位置，如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191124162957995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p>
<a id="more"></a>
<h4 id="1、Kafka的基本介绍"><a href="#1、Kafka的基本介绍" class="headerlink" title="1、Kafka的基本介绍"></a>1、Kafka的基本介绍</h4><h5 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h5><p>Kafka 是一种分布式的，基于发布/订阅的消息系统（redis也可以实现该功能），主要设计目标如下：<br>以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。<br>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。<br>支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。<br>同时支持离线数据处理和实时数据处理。<br>Scale out：支持在线水平扩展。</p>
<h5 id="1-2-kafka-应用场景"><a href="#1-2-kafka-应用场景" class="headerlink" title="1.2 kafka 应用场景"></a>1.2 kafka 应用场景</h5><ul>
<li>日志收集：可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer。不过在本文中，flume用于收集数据日志，kafka组件用于接受来自flume的event</li>
<li>流式处理：spark streaming，在上面的架构图也可以清楚看到kafka组件的下游为spark streaming，它消费来自kafka topic的实时数据消息。</li>
<li>消息系统：解耦生产者和消费者、缓存消息等。</li>
<li>用户活动跟踪：kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后消费者通过订阅这些topic来做实时的监控分析，亦可保存到hbase、mangodb等数据库。</li>
<li>运营指标：kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，</li>
<li>生产各种操作的集中反馈，比如报警和报告。<br>可以看出kafka在大数据实时处理以及互联网产品方面应用最为突出。</li>
</ul>
<h5 id="1-3-kafka相关术语"><a href="#1-3-kafka相关术语" class="headerlink" title="1.3 kafka相关术语"></a>1.3 kafka相关术语</h5><ul>
<li>producer : 生产者，生产message发送到topic，例如flume sink就是生产者</li>
<li>consumer : 消费者，订阅topic并消费message, consumer作为一个线程来消费，例如实时处理的spark streaming。</li>
<li><p>Broker：Kafka节点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群，在大数据项目中，直接利用已有的hadoop节点服务器配置成kafka集群。整个 Kafka 集群架构会有一个 zookeeper集群，通过 zookeeper 管理集群配置，选举 kafka Leader，以及在 Consumer Group 发生变化时进行 Rebalance。</p>
</li>
<li><p>topic：一类消息，消息存放的目录即主题，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发</p>
</li>
<li>massage： Kafka中最基本的传递对象。</li>
<li>partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的segment以及index：partition在物理上已一个文件夹的形式存在，由多个segment文件组成和多个index文件，它们是很对出现，每个Segment存着message信息，每个index存放着message的offset</li>
<li>replica：partition 的副本，保障 partition 的高可用。个人建议写成replica partition—副本分区</li>
<li>leader：这里的leader要理解为某个partition 作为主分区，也即称为leader partition，要注意该partition所在的服务器不能称为leader，否认会被误认为是kafka集群的master服务器（Kafka把master服务器称为controller）。 producer 和 consumer 只跟 leader petition交互。</li>
<li>replicas：leader 角色的partition加上replica角色的partition，一起成为replicas，也就是该topic总共有多少个副本数，副本数包含一个主分区副本和其余的副本分区。</li>
<li>controller：为了避免更leader这个词混淆，开发者将kafka 集群中的其中一台服务器称为controller，用于对每个topic的partition leader选举以及实现对partition的failover。</li>
<li>consumer Group：消费者组，一个Consumer Group包含多个consumer</li>
<li>offset：偏移量，理解为消息partition中的索引</li>
</ul>
<h4 id="2、kafka-单点部署与测试"><a href="#2、kafka-单点部署与测试" class="headerlink" title="2、kafka 单点部署与测试"></a>2、kafka 单点部署与测试</h4><h5 id="2-1-配置文件"><a href="#2-1-配置文件" class="headerlink" title="2.1 配置文件"></a>2.1 配置文件</h5><p>目前官方kafka最新稳定版本为2.3.1<br>按官方建议以下建议，项目用到scala2.1.3，kafka用了官方的建议版本2.1.2<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">We build for multiple versions of Scala. This only matters if you are using Scala and you want a version built for the same Scala version you use. Otherwise any version should work (2.12 is recommended). </span><br></pre></td></tr></table></figure><br>kafka组件同样被放置在/opt目录下，该目录放置所有Hadoop及其组件，便于统一管理<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hbase-2.1.7   kafka-2.12      scala-2.13.1             </span><br><span class="line">flume_log     hive-3.1.2    mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    xcall.sh          </span><br></pre></td></tr></table></figure></p>
<p>配置server.properties。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn config]# vi server.properties </span><br><span class="line">[root@nn config]# pwd</span><br><span class="line">/opt/kafka-2.12/config</span><br><span class="line"><span class="meta">#</span><span class="bash"> The id of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果是kafka集群，需配置全局id</span></span><br><span class="line">broker.id=10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Socket Server Settings #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以不设置，kafka自动获取hostname</span></span><br><span class="line">listeners=PLAINTEXT://nn:9092</span><br><span class="line">advertised.listeners=PLAINTEXT://nn:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Basics #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终存放消息的路径,建议放在kafka组件目录下，方便管理</span></span><br><span class="line">log.dirs=/opt/kafka-2.12/kafka-logs</span><br><span class="line">num.partitions=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Zookeeper #############################</span></span></span><br><span class="line">zookeeper.connect=nn:2181,dn1:2181,dn2:2181/kafka-zk</span><br><span class="line"><span class="meta">#</span><span class="bash"> Timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br></pre></td></tr></table></figure><br>已更新配置：<del>zookeeper.connect=nn:2181,dn1:2181,dn2:2181</del><br>考虑到后面项目中，对kafka在zk上方便更为管理，用了新的配置：zookeeper.connect=nn:2181,dn1:2181,dn2:2181/kafka-zk<br>因为kafka默认在zk的根路径下创建多个节点路径，当需要去zk查看kafka相关的元数据时显得有点混乱，因此这里要求kafka将它要创建的所有znode都统一放在/kafka-zk这个路径下，方便集中查看和管理kafka的元数据，如下所示：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0]  ls /kafka-zk</span><br><span class="line">[cluster, controller_epoch, controller, brokers, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config]</span><br></pre></td></tr></table></figure><br>本文后面内容所有kafka命令中，若有<code>--zookeeper nn:2181</code> 这样启动参数，都需要改为<code>--zookeeper nn:2181/kafka-zk</code></p>
<h5 id="2-2-启动kafka进程"><a href="#2-2-启动kafka进程" class="headerlink" title="2.2 启动kafka进程"></a>2.2 启动kafka进程</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh config/server.properties </span><br></pre></td></tr></table></figure>
<p>启动后提示内存不足<br>“There is insufficient memory ”<br>因为kafka的启动脚本为最大堆申请1G内存，由于使用虚拟机跑项目，资源有限，将 kafka-server-start.sh的export KAFKA_HEAP_OPTS=”-Xmx1G -Xms1G”修改为export KAFKA_HEAP_OPTS=”-Xmx256M -Xms128M”，最大堆空间为256M，初始堆空间为128M。<br>使用后台进程方式启动kafka服务<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过jps也可以看到kafka进程</span></span><br><span class="line">[root@nn kafka-2.12]# jps</span><br><span class="line">4609 QuorumPeerMain</span><br><span class="line">14436 JournalNode</span><br><span class="line">2454 HMaster</span><br><span class="line">2552 Jps</span><br><span class="line">14185 DataNode</span><br><span class="line">15017 NodeManager</span><br><span class="line">2365 Kafka</span><br><span class="line">13983 NameNode</span><br><span class="line">14879 ResourceManager</span><br></pre></td></tr></table></figure></p>
<h5 id="2-3-测试topic"><a href="#2-3-测试topic" class="headerlink" title="2.3 测试topic"></a>2.3 测试topic</h5><p>创建无备份的topic,名称为hadoop，分区数1<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]#bin/kafka-topics.sh --create --zookeeper nn:2181 --replication-factor 1  --partitions 1 --topic hadoop</span><br><span class="line">Created topic hadoop.</span><br></pre></td></tr></table></figure><br>查看新建的topic<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn bin]# kafka-topics.sh --list --zookeeper nn:2181</span><br><span class="line">hadoop</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看kafka在zookeeper上创建的topic znode上可以看到 hadoop这个topic</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls /brokers</span><br><span class="line">[ids, topics, seqid]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /brokers/topics</span><br><span class="line">[hadoop]</span><br></pre></td></tr></table></figure><br>启动producer进程，这是一个console，可以命令式发送message<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list nn:9092 --topic hadoop</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">spark</span></span><br></pre></td></tr></table></figure></p>
<p>新打开一个shell用于启动consumer进程，订阅hadoop这个topic，该进程会持续监听9092端口，一旦上面producer的console写入信息，这边consumer就会立刻打印同样信息。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic hadoop</span><br><span class="line">hello kafka</span><br><span class="line">spark</span><br></pre></td></tr></table></figure></p>
<p>查看hadoop这个topic的所在的物理文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里的hadoop-0就是hadoop topic的parition</span></span><br><span class="line">[root@nn hadoop-0]# pwd</span><br><span class="line">/opt/kafka-2.12/kafka-logs/hadoop-0</span><br><span class="line"></span><br><span class="line">[root@nn hadoop-0]# ls</span><br><span class="line">00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  leader-epoch-checkpoint</span><br></pre></td></tr></table></figure><br>有index、log文件，新版本的kafka还多了timeindex时间索引。至此完成kafka单节点的配置和测试。</p>
<h4 id="3、kafka集群部署与测试"><a href="#3、kafka集群部署与测试" class="headerlink" title="3、kafka集群部署与测试"></a>3、kafka集群部署与测试</h4><h5 id="3-1-配置server-properties"><a href="#3-1-配置server-properties" class="headerlink" title="3.1 配置server.properties"></a>3.1 配置server.properties</h5><p>kafka集群部署要求所在节点上已经运行zookeeper集群。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn config]# vi server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个节点id需唯一nn设10，dn1设11，dn2设12</span></span><br><span class="line">broker.id=10</span><br><span class="line">ip和端口这里可以不配置，kafka自动读取，也方便把整个kafka目录分发到其他节点上</span><br><span class="line"><span class="meta">#</span><span class="bash">listeners=PLAINTEXT://:9092</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放的日志，kafka自动创建</span></span><br><span class="line">log.dirs=/opt/kafka-2.12/kafka-logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置zk集群</span></span><br><span class="line">zookeeper.connect=nn:2181,dn1:2181,dn2:2181</span><br></pre></td></tr></table></figure><br>其他属性项基本是调优项目，这里不再一一给出，后面用单独一篇文章给出讨论。<br>将kafka-2.12目录拷贝到dn1和dn2节点上，修改对应的broker.id即可</p>
<h5 id="3-2-集群测试"><a href="#3-2-集群测试" class="headerlink" title="3.2 集群测试"></a>3.2 集群测试</h5><p>分布在三个节点上启动kafka服务<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@dn1 kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@dn2 kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line"><span class="meta">#</span><span class="bash"> jps可以看到每个节点上都已经有kafka进程</span></span><br><span class="line">[root@nn opt]# sh xcall.sh jps |grep ka</span><br><span class="line">10569 Kafka</span><br><span class="line">12836 Kafka</span><br><span class="line">28243 Kafka</span><br></pre></td></tr></table></figure></p>
<p>创建一个新的topic：sparkapp，3份拷贝，3分区<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-topics.sh --create --zookeeper nn:2181,dn1:2181,dn2:2181 --replication-factor 3 --partitions 3 --topic sparkapp</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看sparkapp主分区及其副本分区的情况</span></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-topics.sh --describe --zookeeper nn:2181 --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br></pre></td></tr></table></figure><br>该命令其实就是读取/brokers/topics/sparkapp/partitions/**/state 所有分区的state节点值<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] get  &#x2F;brokers&#x2F;topics&#x2F;sparkapp&#x2F;partitions&#x2F;0&#x2F;state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:22,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[10,11,12]&#125;</span><br></pre></td></tr></table></figure></p>
<p>在nn节点启动producer进程，连接broker分别为nn自己、dn1节点和dn2节点，都能正常连接，同理，dn1、dn2的producer进程使用dn1、dn2、nn节点都能正常连接<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list nn:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br><span class="line"></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list dn1:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br><span class="line"></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list dn2:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br></pre></td></tr></table></figure></p>
<p>在nn节点启动producer进程，然后在dn1节点、dn2节点以及nn新shell分别启动consumer，看看一个producer生产msg，其他三个节点能否同时收到<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br><span class="line">[root@dn2 kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br></pre></td></tr></table></figure></p>
<p>查看kafka-cluster这个topic的partition在物理文件上的分布<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-logs]# ls sparkapp-</span><br><span class="line">sparkapp-0/ sparkapp-1/ sparkapp-2/ </span><br><span class="line">[root@nn kafka-logs]# ls sparkapp-0/</span><br><span class="line">00000000000000000000.index  00000000000000000000.timeindex</span><br><span class="line">00000000000000000000.log    leader-epoch-checkpoint</span><br></pre></td></tr></table></figure><br>可以看到三个分区对于三个文件目录，每个目录有索引文件和数据文件</p>
<h5 id="3-3-在zk上查看集群情况"><a href="#3-3-在zk上查看集群情况" class="headerlink" title="3.3 在zk上查看集群情况"></a>3.3 在zk上查看集群情况</h5><p>kafka在zk上的数据存储结构：<br>brokers列表：ls /brokers/ids<br>某个broker信息：get /brokers/ids/10<br>topic信息：get /brokers/topics/sparkapp<br>partition信息：get /brokers/topics/sparkapp/partitions/0/state<br>controller中心节点变更次数：get /controller_epoch<br>conrtoller信息：get /controller<br>[zk: localhost:2181(CONNECTED) 2] get /controller<br>{“version”:1,”brokerid”:10,”timestamp”:”<em>*</em>“}，可以看到当前kafka集群的controller节点为nn服务器brokerid为10.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 集群的brokers信息在/brokers持久节点下，ids节点用于存放上线的brokers id号，topics：集群上所有的topces都放在在节点下</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 20] ls /brokers</span><br><span class="line">[ids, topics, seqid]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> brokers在持久ids节点下注册临时节点，节点名称就是broker自己的id号，这里说明为何在server.properties里面的broker.id要设为唯一，因为利用zookeeper的临时节点以及保证节点命名唯一。</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 19] ls /brokers/ids</span><br><span class="line">[10,11,12]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取其中一个broker id节点的信息，例如dn2这个broker</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 24] get /brokers/ids/12</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://dn2:9092&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;dn2&quot;,&quot;timestamp&quot;:&quot;*****&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看sparkapp这个topics的分区数量</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /brokers/topics/sparkapp/partitions</span><br><span class="line">[0, 1, 2]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前kafka集群的leader状态，通过在topic的分区的state节点可以看到当前leader是节点dn1，对应的broker id为1</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 36] get /brokers/topics/sparkapp/partitions/1/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:6,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:2,&quot;isr&quot;:[11,12,10]&#125;</span><br></pre></td></tr></table></figure><br>至此，完成Kafka的集群配置和测试</p>
<h4 id="4、-小结"><a href="#4、-小结" class="headerlink" title="4、 小结"></a>4、 小结</h4><p>为hadoop环境配置kafka组件的过程相对简单，鉴于Kafka这个中间件具有非常不错应用价值，本blog继续用1到2篇文章深入探讨有关Kafka核心内容。此外还用另外一篇文章用于给出flume和kafka两者的整合——<a href="https://blog.csdn.net/pysense/article/details/103335495">《flume集群高可用连接kafka集群》</a>。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka集群</tag>
        <tag>hadoop HA</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Keepalived实现双机主备</title>
    <url>/2019/06/30/%E5%9F%BA%E4%BA%8EKeepalived%E5%AE%9E%E7%8E%B0%E5%8F%8C%E6%9C%BA%E4%B8%BB%E5%A4%87/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文使用keepalived快速配置实现双机主备模式，该模式为keepalived入门使用，生产使用需要谨慎，当然可用于帮助理解keepalived</p>
<p>步骤：</p>
<p>1）主备server安装keepalived</p>
<p>2）主备server配置keepalived.conf</p>
<p>3）主备server安装httpd web服务（用于测试）</p>
<p>4）主备启动keepalived，并测试master、backup各自中断服务后，访问情况</p>
<a id="more"></a>
<h5 id="1、yum-y-install-keepalived-httpd"><a href="#1、yum-y-install-keepalived-httpd" class="headerlink" title="1、yum -y install keepalived httpd"></a>1、yum -y install keepalived httpd</h5><h5 id="2、主server的配置文件："><a href="#2、主server的配置文件：" class="headerlink" title="2、主server的配置文件："></a>2、主server的配置文件：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id s0  ！ 集群作用域的全局路由id，每台serverID要求唯一</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER ! 主server为master，备server为BACKUP</span><br><span class="line">    interface eth0 ！该server的网卡</span><br><span class="line">    virtual_router_id 51  ！集群作用域所有server 相同id</span><br><span class="line">    priority 100 ！优先级，同一个vrrp_instance里，主server必须要高于备server</span><br><span class="line">    advert_int 1 </span><br><span class="line">    authentication &#123;  ！主与备之间的认证机制</span><br><span class="line">        auth_type PASS  </span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123; ！ 虚拟IP</span><br><span class="line">        192.168.1.10</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>备机的配置同上，只需将state和priority改下 。从该配置文件可知，kl提供的功能极为简单，因此可实现一主多备的模式，而且非常容易配置，例如通过ansible 批量配置备机。</p>
<h5 id="3、安装apache-httpd-web-server"><a href="#3、安装apache-httpd-web-server" class="headerlink" title="3、安装apache httpd web server"></a>3、安装apache httpd web server</h5><p>修改web主页内容，以便显示访问的是来自哪台server：</p>
<p>cd /var/www/html</p>
<p>vi index.html</p>
<p>内容：from keepalived master server</p>
<p>同理备机：from keepalived slave server</p>
<p>（聪明的你，可用docker去启动一个web服务，甚至keepalived也被docker化）</p>
<h5 id="4、主备分别启动keepalived"><a href="#4、主备分别启动keepalived" class="headerlink" title="4、主备分别启动keepalived"></a>4、主备分别启动keepalived</h5><p>server keepalived start</p>
<p>在主server的linux日志可看到相关运行info</p>
<p>vi /var/log/messages</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn systemd: Started LVS <span class="keyword">and</span> VRRP High Availability Monitor.</span><br><span class="line"><span class="comment"># 启动LVS和VRRP HA监测服务</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_healthcheckers[<span class="number">11285</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line"><span class="comment"># 读取ka配置文件</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Registering Kernel netlink reflector</span><br><span class="line"><span class="comment"># ka使用Linux内核netlink创建与ka进程的通信</span></span><br><span class="line"><span class="comment"># netlink是用户进程与内核的IP网络配置之间的通信接口，同时它也可以作为内核内部与多个用户空间进程之</span></span><br><span class="line"><span class="comment"># 间的消息传输系统.基于netlink，用户进程和内核之间的通信支持全双工、组播、异步，像一台纯软件实现的“虚拟机三层交换机”，从这一点可感受到Linux内核有多强大！</span></span><br><span class="line"></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Registering Kernel netlink command channel</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Registering gratuitous ARP shared channel</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) removing protocol VIPs.</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Using LinkWatch kernel netlink reflector...</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP sockpool: [ifindex(<span class="number">2</span>), proto(<span class="number">112</span>), unicast(<span class="number">0</span>), fd(<span class="number">10</span>,<span class="number">11</span>)]</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">11</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) Transition to MASTER STATE</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) Entering MASTER STATE</span><br><span class="line"><span class="comment"># 通过VRRP协议和ka的配置文件，将这台server选为master角色</span></span><br><span class="line"></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) setting protocol VIPs.</span><br><span class="line"><span class="comment"># eth0网口上设置VIP</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line"><span class="comment"># 在eth0发送ARP广播，对外问下192.168.1.10有没有机器在使用</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) Sending/queueing gratuitous ARPs on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.88</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>备机的message：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 systemd: Starting LVS <span class="keyword">and</span> VRRP High Availability Monitor...</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11248</span>]: Starting Keepalived v1<span class="number">.3</span><span class="number">.5</span> (03/<span class="number">19</span>,<span class="number">2017</span>), git commit v1<span class="number">.3</span><span class="number">.5</span>-<span class="number">6</span>-g6fa32f2</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11248</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun 27 03:25:23 dn2 systemd: PID file /var/run/keepalived.pid not readable (yet?) after start.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11249</span>]: Starting Healthcheck child process, pid=<span class="number">11250</span></span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11249</span>]: Starting VRRP child process, pid=<span class="number">11251</span></span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 systemd: Started LVS <span class="keyword">and</span> VRRP High Availability Monitor.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_healthcheckers[<span class="number">11250</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Registering Kernel netlink reflector</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Registering Kernel netlink command channel</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Registering gratuitous ARP shared channel</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: VRRP_Instance(VI_1) removing protocol VIPs.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Using LinkWatch kernel netlink reflector...</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: VRRP_Instance(VI_1) Entering BACKUP STATE</span><br><span class="line"><span class="comment"># 备机进入BACKUP状态</span></span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: VRRP sockpool: [ifindex(<span class="number">2</span>), proto(<span class="number">112</span>), unicast(<span class="number">0</span>), fd(<span class="number">10</span>,<span class="number">11</span>)]</span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>对比主备ka的运行日志发现：</p>
<p>主server比被server多了在eth0网口设置VIP：192.168.1.10的过程，使用ip a命令查看网口配置可看到：</p>
<p>主server网口多了VIP，而备机网口还是单IP，这里涉及到keepalived的设计原理，以下为原理解析部分</p>
<h5 id="5、keepalived工作原理"><a href="#5、keepalived工作原理" class="headerlink" title="5、keepalived工作原理"></a>5、keepalived工作原理</h5><h6 id="5-1-模块设计"><a href="#5-1-模块设计" class="headerlink" title="5.1 模块设计"></a>5.1 模块设计</h6><p>keepalived采用是模块化设计，不同模块实现不同的功能，keepalived主要有三个模块，分别是core、check和vrrp。</p>
<p>core：是keepalived的核心，负责主进程的启动和维护，全局配置文件的加载解析等</p>
<p>check： 负责healthchecker(健康检查)，包括了各种健康检查方式，以及对应的配置的解析包括LVS的配置解析；</p>
<p>可基于脚本检查对IPVS后端服务器健康状况进行检查。</p>
<p>vrrp：VRRPD子进程，VRRPD子进程就是来实现VRRP协议，虚拟冗余路由协议。</p>
<p>下面是其他库： libipfwc：iptables/ipchains库，配置LVS会用到 libipvs*：配置LVS会用到，注意，keepalived和LVS完全不同的技术方向，各司其职相互配合</p>
<p>架构图：</p>
<p><img src="https://img-blog.csdnimg.cn/20190630203253526.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;keepalived启动后会有三个进程:</p>
<p>父进程：内存管理，IO调度、子进程管理等等</p>
<p>子进程：vrrpd子进程</p>
<p>子进程：healthchecker子进程</p>
<p>&#8195;&#8195;由上图可知，两个子进程都被系统WatchDog看管，两个子进程各自实现自己的事，healthchecker子进程实现检查各自服务器的健康程度，例如HTTP，LVS等等，如果healthchecker子进程检查到MASTER上服务不可用，就会通知本机上的兄弟VRRP子进程，让他删除通告，并且去掉虚拟IP，转换为BACKUP状态</p>
<h6 id="5-2-工作原理"><a href="#5-2-工作原理" class="headerlink" title="5.2 工作原理"></a>5.2 工作原理</h6><p>&#8195;&#8195;kpa是一个类似于layer3，4 &amp; 5交换机制的软件，类似第3层、第4层和第5层物理交换机，分别工作在IP/TCP协议栈的IP层、TCP层、应用层。原理分别如下：</p>
<p>&#8195;&#8195;在layer3，kpa使用layer3的方式工作式时，kpa会定期向服务器群中的服务器发送一个ICMP的数据包（既Ping），如果发现某台服务的IP地址没有激活，kpa便报告这台服务器失效，并将它从服务器群中剔除(这种情况的典型例子是某台服务器被非法关机)。Layer3方式是以服务器的IP地址是否有效作为服务器工作正常与否的标准。</p>
<p>&#8195;&#8195;在layer4: layer4主要以TCP端口的状态来决定服务器工作正常与否。如web server的服务端口一般是80，8080或者443，如果kpa检测到80端口没有启动，则kpa将把这台服务器从服务器群中剔除。</p>
<p>&#8195;&#8195;Layer5： Layer5就是工作在具体的应用层了，比Layer3,Layer4要复杂，在网络上占用的带宽也要大一些。kpa将根据用户的设定规则检查服务器相应服务是否运行正常，如果没有正常运行，则Keepalived将把服务器从服务器群中剔除。</p>
<h6 id="5-3-VRRP虚拟冗余路由协议"><a href="#5-3-VRRP虚拟冗余路由协议" class="headerlink" title="5.3 VRRP虚拟冗余路由协议"></a>5.3 VRRP虚拟冗余路由协议</h6><p>&#8195;&#8195;在现实的网络环境中，两台需要通信的主机大多数情况下并没有直接的物理连接。对于这样的情况，它们之间路由怎样选择？主机如何选定到达目的主机的下一跳路由，这个问题通常的解决方法有二种：在主机上使用动态路由协议(RIP、OSPF等)或者在主机上配置静态路由。显然，在主机上配置动态路由是不切实际的，因为管理、维护成本以及是否支持等诸多问题。配置静态路由则变得十分流行，但路由器(或者说默认网关default gateway)却经常成为单点。</p>
<p>VRRP的目的就是为了解决静态路由单点故障问题。VRRP通过一竞选(election)协议来动态的将路由任务交给LAN中虚拟路由器中的某台VRRP路由器。</p>
<p>​      VRRP工作机制</p>
<p>​      在一个VRRP虚拟路由器中，有多台物理的VRRP路由器，但是这多台的物理的机器并不能同时工作，而是由一台称为MASTER的负责路由工作，其它的都是BACKUP，MASTER并非一成不变，VRRP让每个VRRP路由器参与竞选，最终获胜的就是MASTER。MASTER拥有一些特权，比如 拥有虚拟路由器的IP地址，我们的主机就是用这个IP地址作为静态路由的。拥有特权的MASTER要负责转发发送给网关地址的包和响应ARP请求。VRRP通过竞选协议来实现虚拟路由器的功能，所有的协议报文都是通过IP多播(multicast)包形式发送的。虚拟路由器由VRID(范围0-255)和一组IP地址组成，对外表现为一个周知的MAC地址。所以，在一个虚拟路由 器中，不管谁是MASTER，对外都是相同的MAC和IP(称之为VIP)。客户端主机并不需要因为MASTER的改变而修改自己的路由配置，对他们来说，这种主从的切换是透明的。</p>
<p> &#8195;&#8195;在一个虚拟路由器中，只有作为MASTER的VRRP路由器会一直发送VRRP广告包(VRRPAdvertisement message)，BACKUP不会抢占MASTER，除非它的优先级(priority)更高。当MASTER不可用时(BACKUP收不到广告包)， 多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(&lt;1s)，以保证服务的连续性</p>
<h5 id="6、入门级kpa优缺点"><a href="#6、入门级kpa优缺点" class="headerlink" title="6、入门级kpa优缺点"></a>6、入门级kpa优缺点</h5><p>&#8195;&#8195;优点当然是配置简单，缺点：浪费服务器资源，因为配置模式为主备，正常对外服务时，仅有一台主机对外提供服务，而多台备机只能处于未使用状态。在之后的几篇文章中，将开始使用keepalived+nginx，keepalived+lvs实现负载均衡高可用的主从、双主模式。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Zookeeper的临时顺序节点实现分布式锁</title>
    <url>/2019/09/11/%E5%9F%BA%E4%BA%8EZookeeper%E7%9A%84%E4%B8%B4%E6%97%B6%E9%A1%BA%E5%BA%8F%E8%8A%82%E7%82%B9%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章中，已经给出基于kazoo操作zk的逻辑，接下来利用zk的临时序列节点机制实现分布式锁，分布式锁场景使用非常广，在读写方面都非常适合，个人认为比基于redis实现的分布式锁更具可靠性（但性能方面，redis应该更强？）。</p>
<a id="more"></a>
<h5 id="1、zk的临时顺序节点"><a href="#1、zk的临时顺序节点" class="headerlink" title="1、zk的临时顺序节点"></a>1、zk的临时顺序节点</h5><p>&#8195;&#8195;临时顺序节点其实是结合临时节点和顺序节点的特点：在某个固定的持久节点(例如/locker)下创建子节点时，zk通过将10位的序列号附加到原始名称来设置znode的路径。例如，使用路径 /locker/foo创建为临时顺序节点，zk会将路径设为为 /locker/foo0000000001 ，并将下一个临时迅速节点设为/locker/foo0000000002，以此类推。如果两个顺序节点是同时创建的，那么zk不会对每个znode使用相同的数字。当创建节点的客户端与zk断开连接时（socket断开，更深一层应该是收到客户端发来的TCP挥手FIN 报文），服务端zk底层收到客户端的FIN报文后将由该客户端创建的临时节点删除掉。<br>&#8195;&#8195;临时顺序节点结构大致如下，/locker节点为持久节点，该节点下有多个子节点，这些子节点由不同客户端创建。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">locker&#x2F;</span><br><span class="line">├── foo0000000001</span><br><span class="line">├── foo0000000002</span><br><span class="line">├── foo0000000003</span><br><span class="line">├── foo0000000004</span><br><span class="line">└── foo0000000005</span><br></pre></td></tr></table></figure></p>
<h5 id="2、分布式锁的实现流程"><a href="#2、分布式锁的实现流程" class="headerlink" title="2、分布式锁的实现流程"></a>2、分布式锁的实现流程</h5><p>&#8195;&#8195;注意：本文提供的是基于zk的共享锁，而非排他锁（独占锁），看完本文后，实现独占锁会简单很多<br>&#8195;&#8195;==共享锁定义：又称读锁。如果事务T1对数据对象O1加上了共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放。zk实现的“共享锁”就是有多个序号的临时节点。==<br>&#8195;&#8195;共享锁与排他锁的区别在于，加了排他锁之后，数据对象只对当前事务可见，而加了共享锁之后，数据对象对所有事务都可见。</p>
<h5 id="分布式锁流程："><a href="#分布式锁流程：" class="headerlink" title="分布式锁流程："></a>分布式锁流程：</h5><pre><code>(1) 客户端发起请求，在zk指定持久节点/locker下（若不存在该locker节点则创建），创建临时顺序节点/locker/foo0000000003
(2) 获取/locker下所有子节点，例如有三个不同客户端各自创建的节点`all_nodes=[&#39;/locker/foo0000000001&#39;,&#39;/locker/foo0000000002&#39;,&#39;/locker/foo0000000003&#39;]`
(3) 对子节点按节点自增序号从小到大排序
(4) 判断本节点/locker/foo0000000003是否为节点列表中最小的子节点，若是，则获取锁，处理业务后，删除本节点/locker/foo0000000003；若不是，则监听排在该节点前面的那个节点/locker/foo0000000002“是否存在”事件
注意：这里产生这样的节点监听链，有两个监听链：
`/locker/foo0000000002监听/locker/foo0000000001是否存在的事件`
`/locker/foo0000000003监听/locker/foo0000000002是否存在的事件`
(5) 若被监听的节点路径“是否存在”的事件触发，处理业务，删除本节点；否则客户端阻塞自己，继续等待监听事件触发。
</code></pre><h5 id="图示说明"><a href="#图示说明" class="headerlink" title="图示说明"></a>图示说明</h5><p>流程用OmniGraffle（Mac）画成，比Visio好用<br><img src="https://img-blog.csdnimg.cn/2019091000072398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="3、show-me-the-code"><a href="#3、show-me-the-code" class="headerlink" title="3、show me the code"></a>3、show me the code</h5><p>&#8195;&#8195;用python实现的zk临时顺序节点分布式锁的文章，在csdn等貌似没看到过，很多文章都是使用别人已经封装好的zklock或者直接使用kazoo提供zklock来做例子说明。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用上下文管理协议，对于使用者友好以及简单易用</span></span><br><span class="line"><span class="keyword">with</span> ZkDistributedLock(**conf):</span><br><span class="line">     <span class="comment"># 调用者的业务逻辑代码</span></span><br><span class="line">     doing_jobs(*args,**kwargs)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZkDistributedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于zk的临时顺序节点实现分布式锁</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hosts, locker_path, sub_node_name, timeout, default_value=<span class="string">b&#x27;1&#x27;</span></span>):</span></span><br><span class="line">        self.hosts = hosts</span><br><span class="line">        <span class="comment"># 持久节点路径</span></span><br><span class="line">        self.locker_path = locker_path</span><br><span class="line">        self.timeout = timeout</span><br><span class="line">        <span class="comment"># 子节点路径</span></span><br><span class="line">        self.sub_node_path = os.path.join(self.locker_path, sub_node_name)</span><br><span class="line">        <span class="comment"># 创建子节点为临时顺序节点的默认值（只需要有值就行）</span></span><br><span class="line">        self.default_value = default_value</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用于客户端自己首次发起请求为获得锁后，用线程的事件阻塞自己不退出，继续等待zk的删除事件通知</span></span><br><span class="line">        <span class="comment"># 这比使用while True+time.sleep()方式更优雅</span></span><br><span class="line">        self.thread_event = threading.Event()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建zk连接，若未创建成功，直接raise Kazoo定义的连接错误，这里无需再给出try except的错误。</span></span><br><span class="line">        self.zkc = KazooClient(hosts=self.hosts, timeout=self.timeout)</span><br><span class="line">        self.zkc.start(self.timeout)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.zkc.exists(self.locker_path):</span><br><span class="line">            self.zkc.create(self.locker_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lock</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 这里是直接返回临时顺序节点的完整路径，例如返回：&#x27;/locker/foo0000000002&#x27;</span></span><br><span class="line">        self.current_node_path = self.zkc.create(path=self.sub_node_path, value=self.default_value, ephemeral=<span class="literal">True</span>,</span><br><span class="line">                                                 sequence=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取固定节点下的所有临时顺序节点列表</span></span><br><span class="line">        all_nodes = self.zkc.get_children(self.locker_path)</span><br><span class="line">        <span class="comment"># 对临时顺序节点列表进行排序，小到大，kazoo返回是节点名称，不是路径：[&#x27;foo0000000001&#x27;,&#x27;foo0000000002&#x27;,&#x27;foo0000000003&#x27;....]</span></span><br><span class="line">        all_nodes = <span class="built_in">sorted</span>(all_nodes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(all_nodes) == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果仅有zk的/locker路径下仅有一个临时顺序节点，说明没有其他客户端争抢，本客户端直接获得锁</span></span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;current node &#123;0&#125; got the locker at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            <span class="comment"># 线程阻塞事件被set为True，通知客户端无需再阻塞自己，已经获得锁。</span></span><br><span class="line">            self.thread_event.<span class="built_in">set</span>()</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取最小节点名例如&#x27;foo0000000001&#x27;</span></span><br><span class="line">        min_node = all_nodes[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 拼接最小节点路径:&#x27;/locker/foo0000000001&#x27;</span></span><br><span class="line">        min_node_path = os.path.join(self.locker_path, min_node)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果自身节点为最小节点，说明获得锁，进行操作后可以是释放锁</span></span><br><span class="line">        <span class="keyword">if</span> self.current_node_path == min_node_path:</span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;current node &#123;0&#125; got the locker at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            self.thread_event.<span class="built_in">set</span>()</span><br><span class="line">            <span class="comment"># 线程阻塞事件被set为True，通知客户端无需再阻塞自己，已经获得锁。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 当前节点不是最小节点，获取当前节点的前面的节点，并对该节点进行路径存在监听（注意这里不是对最小节点监听,避免羊群效应）</span></span><br><span class="line">            current_node = os.path.split(self.current_node_path)[<span class="number">1</span>]</span><br><span class="line">            pre_node_index = all_nodes.index(current_node) - <span class="number">1</span></span><br><span class="line">            pre_node = all_nodes[pre_node_index]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获得在当前节点前面的那个节点路径</span></span><br><span class="line">            self.pre_node_path = os.path.join(self.locker_path, pre_node)</span><br><span class="line">            print(<span class="string">&#x27;current node：&#123;0&#125; is watching the pre node：&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, self.pre_node_path))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对当前节点前面的那个节点增加&quot;exists事件&quot;监听</span></span><br><span class="line">            self.zkc.exists(path=self.pre_node_path, watch=self.watch_node_is_exist)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">watch_node_is_exist</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;当前节点前面的那个节点被删除，触发删除事件，该函数被回调，获得锁</span></span><br><span class="line"><span class="string">        若</span></span><br><span class="line"><span class="string">        :param event:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> event:</span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;current node &#123;0&#125; got the locker at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            self.thread_event.<span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">release</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 释放锁，通过删除当前子节点路径实现</span></span><br><span class="line">        <span class="keyword">if</span> self.zkc.exists(self.current_node_path):</span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;deleted node &#123;0&#125; at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            self.zkc.delete(self.current_node_path)</span><br><span class="line">            self.zkc.stop()</span><br><span class="line">            self.zkc.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 客户端首次发起请求锁，线程事件为False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.thread_event.is_set():</span><br><span class="line">            <span class="comment"># 去zk获取锁</span></span><br><span class="line">            self.get_lock()</span><br><span class="line">            <span class="comment"># 如果本客户端首次请求锁却未能获得，那么客户端可以阻塞自己不退出，这里没限制重新获取锁的次数</span></span><br><span class="line">            <span class="comment"># （也可以设计为retry次数到达前，阻塞自己，超过retry次数后，客户端退出并提示获取锁失败）</span></span><br><span class="line">            self.thread_event.wait()</span><br><span class="line">            <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        self.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">a, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    模拟业务处理逻辑</span></span><br><span class="line"><span class="string">    :param a:</span></span><br><span class="line"><span class="string">    :param b:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c = a + b</span><br><span class="line">    print(<span class="string">&#x27;doing jobs&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    print(<span class="string">&#x27;jobs is done!&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    conf = &#123;</span><br><span class="line">        <span class="string">&#x27;hosts&#x27;</span>: <span class="string">&#x27;192.168.100.5:2181&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;locker_path&#x27;</span>: <span class="string">&#x27;/locker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sub_node_name&#x27;</span>: <span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;timeout&#x27;</span>: <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ZkDistributedLock(**conf):</span><br><span class="line">        doing_jobs(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h5 id="4、测试分布式锁运行效果"><a href="#4、测试分布式锁运行效果" class="headerlink" title="4、测试分布式锁运行效果"></a>4、测试分布式锁运行效果</h5><p>1）单个客户端请求锁：<br>单个客户端请求模拟情况较为简单：</p>
<p>2）模拟多个客户端并发请求锁：<br>启动多个客户端程序前，先手动在zk服务器上创建一个临时顺序节点并保持shell不退出，如下所示，当前已有一个最小节点，以后创建的下一个节点要监听该节点：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">5</span>] create -e -s /locker/foo <span class="number">1</span>  </span><br><span class="line">Created /locker/foo0000000605</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">6</span>] ls /locker</span><br><span class="line">[foo0000000602]</span><br></pre></td></tr></table></figure></p>
<p>接着运行多个以上程序，这里已三个为例：<br>==以上foo0000000602未删除前==<br>第一个客户端程序，可以看到第一个客户端创建603临时顺序节点，并监听着602节点并保持运行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node：&#x2F;locker&#x2F;foo0000000603 is watching the pre node：&#x2F;locker&#x2F;foo0000000602</span><br></pre></td></tr></table></figure></p>
<p>第二个客户端程序，603节点因为602节点还未删除所以还存在，因此第二个客户端创建的604临时顺序节点要监听它前面的603节点并保持运行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node：&#x2F;locker&#x2F;foo0000000604 is watching the pre node：&#x2F;locker&#x2F;foo0000000603</span><br></pre></td></tr></table></figure></p>
<p>==foo0000000602删除后（在zk服务器上手动删除602节点后，第一个客户端获得锁马上打印相关操作）==<br>第一个客户端打印，10:35:36获得锁，业务逻辑耗时5秒，并在10:35:41释放锁：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node &#x2F;locker&#x2F;foo0000000603 got the locker at *** 10:35:36</span><br><span class="line">doing jobs</span><br><span class="line">jobs is done!</span><br><span class="line">deleted node &#x2F;locker&#x2F;foo0000000603 at *** 10:35:41</span><br></pre></td></tr></table></figure></p>
<p>第二个客户端打印，在第一个客户端释放锁的时刻10:35:41，第二个客户端同时获得锁，业务逻辑耗时5秒，并在10:35:46释放锁：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node &#x2F;locker&#x2F;foo0000000603 got the locker at *** 10:35:41</span><br><span class="line">doing jobs</span><br><span class="line">jobs is done!</span><br><span class="line">deleted node &#x2F;locker&#x2F;foo0000000603 at *** 10:35:46</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title>基于redis实现分布式锁（单实例）</title>
    <url>/2019/09/19/%E5%9F%BA%E4%BA%8Eredis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%88%E5%8D%95%E5%AE%9E%E4%BE%8B%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;zookeeper的分布式方案当然最优雅也最可靠，如果redis集群服务已经搭起或者哨兵模式已经部署的条件下，那么基于多个redis实例实现的分布式锁同样高可用，而且redis性能凸显，本文给出的是在单个redis服务上使用setnx+expire实现可用的分布式锁，也可使用redis的事务MULTI+WATCH机制实现分布式锁，只不过这种方式相对简单，本文不再赘述。</p>
<a id="more"></a>
<h4 id="1、基于redis单实例实现的分布式锁"><a href="#1、基于redis单实例实现的分布式锁" class="headerlink" title="1、基于redis单实例实现的分布式锁"></a>1、基于redis单实例实现的分布式锁</h4><h5 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h5><p>加锁实际上就是在redis中，给Key键设置一个全局唯一值，为避免死锁（客户端加锁后，一直没有释放锁），并该key设一个过期时间，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET locker uuid NX PX 20000</span><br><span class="line"></span><br><span class="line"># locker 所有客户端都统一在redis设置的key，名称可以根据业务逻辑，例如app_write_locker</span><br><span class="line"># uuid是客户端自己生成的全局唯一的字符串标识，在python中可以通过UUID库生成唯一标识。</span><br><span class="line"># NX 代表只在键不存在时，才能成功设置key也即成功加锁，否则给客户端返回false</span><br><span class="line"># PX 设置键的过期时间为2000毫秒。</span><br><span class="line"># EX 若要设置秒数，则用EX </span><br><span class="line"># 如果上面的命令执行成功，则证明客户端获取到了锁。</span><br></pre></td></tr></table></figure>
<h5 id="延期锁的过期时间"><a href="#延期锁的过期时间" class="headerlink" title="延期锁的过期时间"></a>延期锁的过期时间</h5><p>假设有这样的场景：A客户端从加锁-业务执行-释放锁，这一过程需要5秒，锁的过期时间仅为2秒，显然2秒后，A的锁已经失效，B客户端加锁成功，但A还未处理完业务，也即出现两个客户端都加锁成功的情况。当然你可以将锁的失效时间设为更大值，这取决你对业务逻辑执行时长的熟悉度。</p>
<p>事实上，无需熟悉业务执行时长的情况下，也可以让客户端加锁后，再开启一个子线程不断对该客户端创建的锁延期，以保证足够的时间让业务逻辑执行完。</p>
<p>这个“延期锁的过期时间”在分布式锁当中，不是强制要实现的，正如前面所说，你非常清楚业务执行流程耗时基本不超过1秒，那么设置锁过期5秒，也完全OK。</p>
<h5 id="解锁"><a href="#解锁" class="headerlink" title="解锁"></a>解锁</h5><p>解锁就是客户端将自己设置的Key删除，而且只能限制客户端A的删除自己设置的key，而且不能删除其他客户端设置的key，通过比加锁时设置的<code>uuid</code>，以及释放锁拿到的uuid是否一致作为实现。为了保证删除key操作的原子性，这里借用redis官方建议LUA脚本key的删除操作。</p>
<p>为何客户端只能删除自己设定key？</p>
<p>例如客户端A加锁locker成功，业务执行需要5秒，而key过期时间为2秒；2秒后导致客户端B加锁成功，业务执行需要5秒，当时间线来到第5秒时，A把B的锁删除了（客户端C此时加锁成功），而此时B还在执行业务，显然不合理。</p>
<h4 id="2、python代码实现"><a href="#2、python代码实现" class="headerlink" title="2、python代码实现"></a>2、python代码实现</h4><p>基于redis单实例的分布式锁流程图<br><img src="https://img-blog.csdnimg.cn/20190915103408140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这里用with协议封装，使得使用者简单调用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> _thread <span class="keyword">import</span> start_new_thread</span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host, port, redis_timeout, retries, retry_itv, locker_key, expire, watch,extend, extend_interval</span>):</span></span><br><span class="line">        self.host = host</span><br><span class="line">        self.port = port</span><br><span class="line">        <span class="comment"># 客户端加锁的重试次数</span></span><br><span class="line">        self.retries = retries</span><br><span class="line">        <span class="comment"># 客户端加锁请求间隔时长</span></span><br><span class="line">        self.retry_itv = retry_itv</span><br><span class="line">        <span class="comment"># 客户端连接redis服务超时时长</span></span><br><span class="line">        self.r_timeout = redis_timeout</span><br><span class="line">        <span class="comment"># 客户端加锁的key</span></span><br><span class="line">        self.locker_key = locker_key</span><br><span class="line">        <span class="comment"># 锁的过期时长</span></span><br><span class="line">        self.expire = expire</span><br><span class="line">        <span class="comment"># 每次在锁原有过期时长的基础上再延长多长时间，秒或者毫秒</span></span><br><span class="line">        self.extend = extend</span><br><span class="line">        <span class="comment"># 每次延长锁的过期时间的wait间隔时长</span></span><br><span class="line">        self.extend_interval = extend_interval</span><br><span class="line">        <span class="comment"># 是否开启延长客户端锁的子线程,默认开启</span></span><br><span class="line">        self.watch_dog_thread = watch</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 客户端全局唯一ID标识</span></span><br><span class="line">        self.app_id=<span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 是否获得锁</span></span><br><span class="line">        self.is_get_lock=<span class="literal">False</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 使用连接池</span></span><br><span class="line">        self.conn_pool = redis.ConnectionPool(host=self.host, port=self.port, socket_connect_timeout=self.r_timeout)</span><br><span class="line">        self.r = redis.Redis(connection_pool=self.conn_pool)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">watch_dog</span>(<span class="params">self</span>):</span></span><br><span class="line">       <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">       为客户端的锁延长过期时间</span></span><br><span class="line"><span class="string">       &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            new_app_id = self.r.get(self.locker_key)</span><br><span class="line">            <span class="comment"># 客户端还未设置key时watch_dog线程抢先get key导致查询到为空</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> new_app_id:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 只能延长由自己id加锁的失效时间</span></span><br><span class="line">            <span class="keyword">if</span> self.app_id == new_app_id.decode(<span class="string">&#x27;utf-8&#x27;</span>):</span><br><span class="line">                <span class="comment"># 对原有的过期时间延长</span></span><br><span class="line">                self.expire=self.expire+self.extend</span><br><span class="line">                self.r.<span class="built_in">set</span>(self.locker_key, self.app_id, ex=self.expire)</span><br><span class="line">                ttl=self.r.pttl(self.locker_key)</span><br><span class="line">                print(<span class="string">&#x27;watch_dog已延长该锁的过期时间至：&#123;&#125;s&#x27;</span>.<span class="built_in">format</span>(ttl/<span class="number">1000</span>))</span><br><span class="line">                time.sleep(self.extend_interval)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">               <span class="comment"># 说明主线程以及完成业务逻辑且成功释放说，该watch_dog子线程退出</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire</span>(<span class="params">self,retries</span>):</span></span><br><span class="line"></span><br><span class="line">        self.app_id = <span class="built_in">str</span>(uuid.uuid1())</span><br><span class="line">        <span class="comment"># 尝试加锁</span></span><br><span class="line">        is_set = self.r.<span class="built_in">set</span>(self.locker_key, self.app_id, ex=self.expire, nx=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> is_set:</span><br><span class="line">            <span class="comment"># 加锁成功</span></span><br><span class="line">            print(<span class="string">&#x27;已获得锁&#123;&#125;开始watch dog 线程&#x27;</span>.<span class="built_in">format</span>(self.app_id))</span><br><span class="line">            <span class="comment"># 开启延长锁的过期时间的子线程</span></span><br><span class="line">            <span class="keyword">if</span> self.watch_dog_thread:</span><br><span class="line">                self.start_new_thread(self.watch_dog, ())</span><br><span class="line">            self.is_get_lock=<span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> self.is_get_lock</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 重试加锁，超过尝试次数则加锁失败</span></span><br><span class="line">            retries=retries-<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> retries == <span class="number">0</span>:</span><br><span class="line">                self.is_get_lock=<span class="literal">False</span></span><br><span class="line">                <span class="keyword">return</span> self.is_get_lock</span><br><span class="line">            print(<span class="string">&#x27;未获得锁，重试获锁剩余次数：&#123;0&#125;,每次获取锁的间隔时长：&#123;1&#125;s&#x27;</span>.<span class="built_in">format</span>(retries,self.retry_itv))</span><br><span class="line">            time.sleep(self.retry_itv)</span><br><span class="line">            self.acquire(retries)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">atomic_delete</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在redis服务端执行原生lua脚本，保证原子删除key，也即保证一定能解锁</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># lua的用法可以redis官网查阅到，KEYS：存放键的列表，ARGV：存放值得列表</span></span><br><span class="line">        <span class="comment"># KEYS[1]取第一个键，ARGV[1]取第一个值</span></span><br><span class="line">        lua_del_script = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then</span></span><br><span class="line"><span class="string">            return redis.call(&quot;del&quot;,KEYS[1])</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            return 0</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        lua_del_func = self.r.register_script(lua_del_script)</span><br><span class="line">        result= lua_del_func(keys=[self.locker_key], args=[self.app_id])</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">release</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 解锁前，先获取该锁的值</span></span><br><span class="line">        now_app_id = self.r.get(self.locker_key)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> now_app_id:</span><br><span class="line">            <span class="comment"># 客户端已完成业务逻辑，但锁已失效，此时不影响其他客户端请求锁，直接return即可</span></span><br><span class="line">            print(<span class="string">&#x27;客户端未完成任务，但锁提前过期了，无法完成更新数据&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只能释放自己申请的锁，若别人申请的锁，自己不能删除，直接返回</span></span><br><span class="line">        <span class="keyword">if</span> self.app_id == now_app_id.decode(<span class="string">&#x27;utf-8&#x27;</span>):</span><br><span class="line">            self.atomic_delete()</span><br><span class="line">            print(<span class="string">&#x27;执行原子删除，锁释放：&#x27;</span>, self.app_id)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 加锁入口</span></span><br><span class="line">        self.acquire(self.retries)</span><br><span class="line">        <span class="keyword">return</span> self.is_get_lock</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        <span class="comment"># 解锁出口</span></span><br><span class="line">        self.release()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">n</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;doing jobs at:&#x27;</span>,datetime.datetime.now())</span><br><span class="line">    time.sleep(n)</span><br><span class="line">    print(<span class="string">&#x27;finish jobs at&#x27;</span>,datetime.datetime.now())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    db_conf = &#123;</span><br><span class="line">        <span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;port&#x27;</span>: <span class="number">6379</span>,</span><br><span class="line">        <span class="string">&#x27;retries&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;retry_itv&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;redis_timeout&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&#x27;locker_key&#x27;</span>: <span class="string">&#x27;locker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;expire&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend_interval&#x27;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> RedLock(**db_conf) <span class="keyword">as</span> lock:</span><br><span class="line">        <span class="keyword">if</span> lock:</span><br><span class="line">            doing_jobs(<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&#x27;获取锁超时&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：在释放锁的逻辑中，引用了lua脚本执行redis删除键，这是非常微妙且核心的细节，如果使用非原生删除，有可能会出现以下极端情况：<br>流程顺序：(1)\==&gt;(2)\==&gt;(3)==&gt;(4)<br><img src="https://img-blog.csdnimg.cn/20190916215733472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">（补充极限情况（2）的另外一个突发场景：key过期时间设置为毫秒单位，<br>redis设置为AOF持久化数据时，AOF同步到磁盘的方式默认每秒1次，如果在这1秒内断电，会导致内存数据丢失，立即重启服务器后，A设置的key已不在，故B加锁成功，A此时开始执行删除key的操作，导致互斥性失效）<br>==redis作者给出的key的有效期可使用毫秒精度的UNIX 时间戳，显然有较高的精度，因此场景（2）是可能发生的，因此直接用redis.del()命令删除key，不保证原子性<br>至于redis官网给出的原子删除lua脚本声称是保证原子操作，那么可认为以下两个操作：判断uuid是否为加锁者的uuid操作和删除该key的操作，它们一起消耗的时刻将足够微小，认为是原子的。（暂且相信官网，就像你相信mysql的事务原子操作）==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if redis.call(&quot;get&quot;,KEYS[1]) &#x3D;&#x3D; ARGV[1] then</span><br><span class="line">      return redis.call(&quot;del&quot;,KEYS[1])</span><br></pre></td></tr></table></figure></p>
<p>单线程运行结果：</p>
<p>任务运行需要5秒，锁过期时长为1秒，那么watch_dog每隔1秒延长锁的过期时间1秒，那么在任务运行5秒这个过程中，总共将锁过期时长延长至6秒，足以保证任务完整运行且锁不失效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">已获得锁0381e3c6-d6fa-11e9-bfb9-a45e60c5a11d开始watch dog 线程</span><br><span class="line">doing jobs at: ** 10:14:50.119315</span><br><span class="line">watch_dog已延长该锁的过期时间至：1.999s</span><br><span class="line">watch_dog已延长该锁的过期时间至：2.999s</span><br><span class="line">watch_dog已延长该锁的过期时间至：4.0s</span><br><span class="line">watch_dog已延长该锁的过期时间至：5.0s</span><br><span class="line">watch_dog已延长该锁的过期时间至：6.0s</span><br><span class="line">finish jobs at ** 10:14:55.121694</span><br><span class="line">执行原子删除，锁释放： 0381e3c6-d6fa-11e9-bfb9-a45e60c5a11d</span><br></pre></td></tr></table></figure>
<p>不开启watch_dog，模拟任务运行时间过长，锁先失效的情况，程序运行1秒后，锁已经失效，该客户端任务结束后，发现自己加的锁没有了，为了数据安全，客户端只能放弃本次更新记录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">已获得锁e04f94b0-d6fa-11e9-b8cf-a45e60c5a11d开始watch dog 线程</span><br><span class="line">doing jobs at: ** 10:21:00.531881</span><br><span class="line">finish jobs at ** 10:21:05.532265</span><br><span class="line">客户端未完成任务，但锁提前过期了，无法完成更新数据</span><br></pre></td></tr></table></figure>
<h4 id="3、多线程模拟多个客户端并发争取分布式锁"><a href="#3、多线程模拟多个客户端并发争取分布式锁" class="headerlink" title="3、多线程模拟多个客户端并发争取分布式锁"></a>3、多线程模拟多个客户端并发争取分布式锁</h4><p>这里改下一部分代码：也即，每个线程共享redis连接池对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,r,redis_timeout, retries, retry_itv, locker_key, expire, extend, extend_interval</span>):</span></span><br><span class="line">        ......</span><br><span class="line">        self.r = r</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r</span>):</span></span><br><span class="line">    thread_name = threading.currentThread().name</span><br><span class="line">    bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">    total = r.get(bonus)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">        print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">redis_conn</span>):</span></span><br><span class="line">    info = &#123;</span><br><span class="line">        <span class="string">&#x27;r&#x27;</span>:redis_conn,</span><br><span class="line">        <span class="string">&#x27;retries&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;retry_itv&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;redis_timeout&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;locker_key&#x27;</span>: <span class="string">&#x27;locker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;expire&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend_interval&#x27;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">with</span> RedLock(**info) <span class="keyword">as</span> lock:</span><br><span class="line">        <span class="keyword">if</span> lock:</span><br><span class="line">            doing_jobs(redis_conn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&#x27;获取锁超时&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    pool_obj = redis.ConnectionPool(host=<span class="string">&#x27;192.168.100.5&#x27;</span>, port=<span class="number">6379</span>, socket_connect_timeout=<span class="number">5</span>)</span><br><span class="line">    redis_conn_obj = redis.Redis(connection_pool=pool_obj)</span><br><span class="line">    threads = []</span><br><span class="line">    <span class="comment"># 开启100个线程模拟客户端争取redis锁</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        t = threading.Thread(target=run, args=(redis_conn_obj,))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 在redis服务端设置值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set money 10</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
<p>测试结果，资源为10份，故对应有10个客户端可获得资源，且是有序的扣减，其他客户端要么未抢到锁，要么抢到锁后发现没资源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">客户端:Thread-14抢到奖金，还剩9</span><br><span class="line">客户端:Thread-3抢到奖金，还剩8</span><br><span class="line">客户端:Thread-78抢到奖金，还剩7</span><br><span class="line">客户端:Thread-16抢到奖金，还剩6</span><br><span class="line">客户端:Thread-11抢到奖金，还剩5</span><br><span class="line">客户端:Thread-17抢到奖金，还剩4</span><br><span class="line">客户端:Thread-5抢到奖金，还剩3</span><br><span class="line">客户端:Thread-66抢到奖金，还剩2</span><br><span class="line">客户端:Thread-38抢到奖金，还剩1</span><br><span class="line">客户端:Thread-9抢到奖金，还剩0</span><br><span class="line">获取锁超时</span><br><span class="line">获取锁超时</span><br><span class="line">奖金已被抢完</span><br><span class="line">奖金已被抢完</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>以上的实现都是基于redis单服务，若redis服务不可用或宕机之类的，所有客户端都无法加锁，显然单点故障不可靠，因此要实现高可用的redis分布式锁，还需设计如何在多个redis服务上实现，这就需要参考RedLock算法，后面的文章将进一步讨论。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title>堡垒机系统自定义Chrome代填程序开发</title>
    <url>/2019/06/19/%E5%A0%A1%E5%9E%92%E6%9C%BA%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%AE%9A%E4%B9%89Chrome%E4%BB%A3%E5%A1%AB%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>&#8195;&#8195;企业内部有多个业务系统、管理系统，而这些系统相互独立，也无相关审计工具对操作人员进行审计，使用人员或者管理员在对账户等也存在一定疏忽，为加强网络安全以及保证内部信息安全，一般都要求这些管理系统接入堡垒机，堡垒机的审计原理：使用人员登录堡垒机系统，选中要登录的管理系统，堡垒机自动打开远程桌面，并代自动填入需要访问系统的账户和密码，这一过程对使用者透明，但堡垒机可通过录制使用者操作视频以及记录相关登入登出操作，达到对多个管理系统统一管理。</p>
<a id="more"></a>
<p>&#8195;&#8195;关键功能配置在此：</p>
<p>&#8195;&#8195;需在堡垒机系统上配置纳入审计的URL、账号和密码，其自带功能仅支持ie浏览器登录，而内部使用多个系统已不支持ie或者IE体验极差（包括旧管理系统和新管理系统），在应用类型选项仅有“IE代填以及自定义代填”两项选择，却没有类似“IE代填、Chrome代填、FireFox代填这些选项”， 而实际使用场景为使用Chrome浏览器代填，堡垒机厂家提供的自定义代填功能竟然无相关程序。</p>
<p><img src="https://img-blog.csdnimg.cn/20190731130451317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;如上图：厂家提供的默认选项IE代填，无需给出账户以及密码输入框的Xpath，即可实现通过IE打开相应web管理页面，但仅限IE打开！</p>
<p><img src="https://img-blog.csdnimg.cn/2019073113053794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;选择自定义代填，从界面选项来看，它仅提供目标地址url、登录账户、登录密码，如果要用chrome打开该url，首先要解决的是如何定位账户输入框位置和密码输入框位置，以便使用selenium打开webdriver定位到相应元素并自动填入信息。显然厂家无法提供该脚本，需自行开发！故需构造类似这样的脚本应用路径：</p>
<p><code>python autoFillingIn.py  -url ** -user ** -psw ** -wait ** -hup ** -hpp ** -lp**  -al **</code></p>
<p>其中-hup –hpp这两个入参为用户名输入框的xpath和密码输入框的xpath</p>
<p>堡垒机通过打开远程桌面后并运行该autoFillingIn.py，该程序通过入参调用chrome driver.exe从而打开chrome浏览器</p>
<p>以下为开发过程：</p>
<h4 id="（1）环境准备："><a href="#（1）环境准备：" class="headerlink" title="（1）环境准备："></a>（1）环境准备：</h4><p>由堡垒机打开的远程桌面所在服务器需安装python3.5环境、selenium库，以及chrome driver.exe</p>
<p>chromedriver下载</p>
<p>在阿里的镜像源：<code>http://npm.taobao.org/mirrors/chromedriver/2.8/</code></p>
<p>chromedriver_win32.zip （可适用32位和64位windows）</p>
<p>selenium离线安装包</p>
<p><code>git clone https://github.com/SeleniumHQ/selenium.git</code></p>
<p>里面已有setup.py，将该离线包上传到远程桌面所在服务器</p>
<p>python setup.py install</p>
<p>注：如果使用其他浏览器例如Firefox、Edge 、Safari，可在以下官网下载相应driver</p>
<p><code>https://pypi.org/project/selenium/</code></p>
<h4 id="（2）建议将chromedriver-exe和autoFillingIn-py放在同一目录下"><a href="#（2）建议将chromedriver-exe和autoFillingIn-py放在同一目录下" class="headerlink" title="（2）建议将chromedriver.exe和autoFillingIn.py放在同一目录下"></a>（2）建议将chromedriver.exe和autoFillingIn.py放在同一目录下</h4><p>这里放在c:\ chromeTools目录下</p>
<p>程序逻辑如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keep_double_quotes</span>(<span class="params">xpath</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过Chrome右键Copy Xpath获取目标url账户、密码、登录按钮输入框的Xpath有两种通用形式：</span></span><br><span class="line"><span class="string">    第一种在其html元素显示给出id值的：//*[@id=&quot;usernameInput&quot;] 以及 //*[@id=&quot;loginForm&quot;]/fieldset/div[1]/input</span></span><br><span class="line"><span class="string">    第二种在其html元素没有id索引的：/html/body/form/table[1]/tbody/tr[5]/td[3]/input 或 /html/body/form/table[1]/tbody/tr[5]/td[3]/img</span></span><br><span class="line"><span class="string">    对于第一种Xpath，当使用入参 -up //*[@id=&quot;login_input&quot;]时，会被argparse解析为 //*[@id=login_input],双引号没了！因此需要还原输入参数的格式，</span></span><br><span class="line"><span class="string">    否则find_element_by_xpath将无法定位，无法实现自动代填</span></span><br><span class="line"><span class="string">    该bug相对隐藏！</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m=re.search(<span class="string">r&#x27;=(.*?)\]&#x27;</span>,xpath)</span><br><span class="line">    <span class="keyword">if</span> m:</span><br><span class="line">        id_name=m.group(<span class="number">1</span>)</span><br><span class="line">        quotes_id_name=<span class="string">&#x27;&quot;&#x27;</span>+id_name+<span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">        double_quotes_path=re.sub(<span class="string">r&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(id_name),quotes_id_name,xpath)</span><br><span class="line">        <span class="keyword">return</span> double_quotes_path</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> xpath</span><br><span class="line"></span><br><span class="line">parser=argparse.ArgumentParser(description=<span class="string">&#x27;自动代填&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-url&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;网站url,支持http和https&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-user&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;用户名&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-psw&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;密码&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-wait&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,default=<span class="number">5</span>,<span class="built_in">help</span>=<span class="string">&#x27;网页加载等待秒数，保证元素加载出来后才能定位成功&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-up&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,<span class="built_in">help</span>=<span class="string">&#x27;用户input提示元素的Xpath,取第一个input元素的id，style为&quot;display: block;&quot;，如没有该id，则可忽略该-up选项&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-hup&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;用户input存放用户名元素的Xpath,style为&quot;display: none;&quot;，表示隐藏用户名输入&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-pp&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,<span class="built_in">help</span>=<span class="string">&#x27;密码input提示元素的Xpath,取第一个input元素的id，style为&quot;display: block;&quot;，如没有该id，则可忽略该-pp选项&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-hpp&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;密码input真实密码填入元素的Xpath,style为&quot;display: none;&quot;，表示隐藏密码输入&#x27;</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-lp&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,<span class="built_in">help</span>=<span class="string">&#x27;登录按钮的Xpath，使用Chrome的Copy Xpath定位元素路径&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-al&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;指定是否自动登录,如指定自动，填入Y或y，如需要填入验证码、短信等二次认证的输入框，请填入N或n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输入参数</span></span><br><span class="line">inputs=parser.parse_args()</span><br><span class="line">target_url=inputs.url</span><br><span class="line">username=inputs.user</span><br><span class="line">password=inputs.psw</span><br><span class="line">sleep_time=inputs.wait</span><br><span class="line">u_inp=inputs.up</span><br><span class="line">hidden_u_inp=inputs.hup</span><br><span class="line">psw_inp=inputs.pp</span><br><span class="line">hidden_psw_inp=inputs.hpp</span><br><span class="line">login_input=inputs.lp</span><br><span class="line">auto_login=inputs.al</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取chrome_driver所在路径</span></span><br><span class="line">chrome_name = <span class="string">&#x27;chromedriver.exe&#x27;</span></span><br><span class="line">dir_name = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">chrome_driver_path = os.path.join(dir_name, chrome_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加chrome的一些优化设置例如不弹出无意义的提示栏、容许不安全访问、最大窗口访问、禁止gpu加速，</span></span><br><span class="line"><span class="comment"># 如不设置，Chrome会弹出一些警告栏甚至提示安全禁止打开url，将无法自动代填或自动代填后自动登入系统进入首页</span></span><br><span class="line">options=webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">&#x27;--disable-popup-blocking&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--disable-infobars&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--allow-running-insecure-content&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--start-maximized&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--disable-gpu&#x27;</span>)</span><br><span class="line">driver=webdriver.Chrome(chrome_driver_path,chrome_options=options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Chrome浏览器打开一个窗口并打开该url</span></span><br><span class="line">driver.get(target_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待</span></span><br><span class="line">time.sleep(sleep_time)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定位相关输入框或按钮位置并填入值</span></span><br><span class="line">selector=driver.find_element_by_xpath</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户名含有隐藏输入时，有两个input id，需要对第一个input进行focus</span></span><br><span class="line"><span class="keyword">if</span> u_inp:</span><br><span class="line">    u_inp_selector=selector(u_inp)</span><br><span class="line">    driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>,u_inp_selector)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户名输入框第二个input id，隐藏输入用户名的id，也要对其进行focus</span></span><br><span class="line">hidden_u_inp_selector=selector(hidden_u_inp)</span><br><span class="line">driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>, hidden_u_inp_selector)</span><br><span class="line">hidden_u_inp_selector.send_keys(username)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 密码含有隐藏输入时，有两个input id，需要对第一个input进行focus</span></span><br><span class="line"><span class="keyword">if</span> psw_inp:</span><br><span class="line">    psw_inp_selector=selector(psw_inp)</span><br><span class="line">    driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>,psw_inp_selector)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 密码输入框第二个input id，隐藏输入用户名的id，也要对其进行focus</span></span><br><span class="line">hidden_psw_inp_selector=selector(hidden_psw_inp)</span><br><span class="line">driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>, hidden_psw_inp_selector)</span><br><span class="line">hidden_psw_inp_selector.send_keys(username)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">login=selector(login_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否自动登录</span></span><br><span class="line"><span class="keyword">if</span> auto_login <span class="keyword">in</span> (<span class="string">&#x27;Y&#x27;</span>,<span class="string">&#x27;y&#x27;</span>):</span><br><span class="line">    login.click()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这里为何不设计为以下代码结构：</p>
<p>def auto_filling_in(**kargs):</p>
<p>​     pass</p>
<p>if <strong>name</strong>==”<strong>main</strong>”:</p>
<p>​     获取入参</p>
<p>​     调用auto_filling_in(**kargs)</p>
<p>因为使用该形式运行时，selenium会以子进程运行chromedriver.exe</p>
<p>if <strong>name</strong>==”<strong>main</strong>”:以下的代码作为父进程</p>
<p>而父进程在获取入参和调用auto_filling_in()后会结束，导致子进程运行的chrome也退出</p>
<h4 id="（3）程序使用说明"><a href="#（3）程序使用说明" class="headerlink" title="（3）程序使用说明"></a>（3）程序使用说明</h4><p>最后在堡垒机上的配置该脚本：</p>
<p>应用路径:c:\ chromeTools\ autoFillingIn.py</p>
<p>运行参数：-url %Target -user %Username -psw %Password -wait 5 -fup /html/body/form/table[1]/tbody/tr[5]/td[1]/input  -fpp /html/body/form/table[1]/tbody/tr[5]/td[4]/input -lp /html/body/form/table[1]/tbody/tr[5]/td[5]/img  -al Y</p>
<p>登录地址：需要审计的管理系统url</p>
<p>登录账户：账户名</p>
<p>登录密码：密码</p>
<p>以上部分参数说明：</p>
<p>-fup账户输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[1]/input，若为隐藏元素，则需要加入up选项</p>
<p>-fpp密码输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[4]/input，若为隐藏元素，则需要加入pp选项</p>
<p>-lp登录输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[5]/img</p>
<p>==重点内容==：</p>
<p>以上都是基于用户输入框和密码输入框都为输入字符可见的条件下，可以代成功，但是对于用户名、密码输入框设为输入字符隐藏时，需要加入以下两个选项才能定位到元素并正确代填，需要将鼠标先focus到用户名输入框或者密码输入框。</p>
<p>-up账户输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[2]/input</p>
<p>-pp密码输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[3]/input</p>
<h4 id="4-运行效果："><a href="#4-运行效果：" class="headerlink" title="(4) 运行效果："></a>(4) 运行效果：</h4><p><img src="https://img-blog.csdnimg.cn/20190805092838974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><img src="https://img-blog.csdnimg.cn/20190805092853755.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>​    任务栏开启了两个远程桌面窗口，一个是autoFillingIn.py运行窗口，另一个为chrome打开的被审计web系统</p>
<p>​    综上，最关键的设计：入参双引号处理，以及wait等待页面加载完毕的时间，因为某些旧管理系统服务器性能较低，打开页面慢，容易导致selenium无法定位元素位置。另外对于有验证码、短信等二次认证的web管理页面，自动登录选项 -al 要设为N或n，即不自动登录到首页，由使用人员自行填入验证码（账户和密码已经后台自动填到输入框内，无需再填入，除非使用人刷新了以打开的登录窗口，那么账户和密码输入框会被清空）。</p>
<p>​    由于解决了argparse对入参Xpath字符串路径问题，本文的解决方案适用所有管理系统需要通过Chrome自动代填达到审计目的场景，之前在全网以及检索国外论坛，都未找到合适参考的方案，用了几天时间自行研究到部署可用，恰好满足实际场景（否则堡垒机系统使用受限，而且厂家无法提供该技术支持！）。在这里，也要介绍国内也有非常出色的开源堡垒机系统例如：Jumpserver，使用python+django开发（个人擅长的技术栈），而且该开源堡垒机系统还符合4A（认证Authentication、账号Account、授权Authorization、审计Audit）的专业运维审计系统，可docker化部署！</p>
<p>Jumpserver的官网：<code>http://www.jumpserver.org/</code></p>
<p>github：<code>https://github.com/jumpserver/jumpserver</code></p>
<p>相关开源堡垒机系统的对比参考文章：</p>
<p><code>https://blog.csdn.net/enweitech/article/details/88840456</code></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>深入理解RDD弹性分布式数据集</title>
    <url>/2019/12/26/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3RDD%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的博客<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》 </a>深入探讨了Spark架构原理内容，该文提到Stage的划分，为什么要做Stage划分？是为了得到更小的Task计算单元，分发给Executor的线程运行，将规模庞大、多流程的计算任务划分为有序的小颗粒的计算单元，实现更高效的计算。那么Stage划分怎么实现？需依赖RDD(Resilient Distributed Datasets，弹性分布式数据集)，可以说，RDD是Spark最为核心的概念。本文内容部分参考了<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_RDD.md">《弹性式数据集RDDs》</a>以及<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">《Spark官方文档》</a></p>
<a id="more"></a>
<h4 id="1、RDD简介"><a href="#1、RDD简介" class="headerlink" title="1、RDD简介"></a>1、RDD简介</h4><p>&#8195;&#8195;RDD 是分布式的、可容错的、只读的、分区记录的弹性数据集合（在开发角度来看，它是一种数据结构，并是绑定了多个方法和属性的一种object），支持并行操作，可以由外部数据集或其他 RDD 转换而来，细读Resilient Distributed  Dataset这三个单词，更深层的含义如下：</p>
<ul>
<li>Resilient: 弹性的，RDD如何体现出弹性呢？通过使用RDD血缘关系图——DAG，在丢失节点上重新计算上，弹性容错。</li>
<li>Distributed:分布式的，RDD的数据集驻留在多个节点的内存中或者磁盘上（一分多）。</li>
<li>Dataset: 在物理文件存储的数据</li>
</ul>
<p>更具体的说明：</p>
<ul>
<li>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数。<br>（这一特点体现出RDD的分布式，RDD的分区是在多个节点上指定的，注意不是指把master分区拷贝到其他节点上，spark强调的是”移动数据不如移动计算“，避免跨节点拷贝分区数据。做这样假设：RDD如果不设计为多个分区，那么一个RDD就是代表一个超大数据集，而且只能在单机行运行，这跟Pandas的DataFrame区别就不大了。）</li>
<li>Spark一个计算程序，往往会产生多个RDD，这些RDD会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。<br>（这个特点就体现了RDD可恢复性，）</li>
<li><p>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)。<br>（其实很多中间件或者组件只要涉及到Partition，必然少不了使用Partitioner(分区器)，例如kafka的partition，Producer可通过对消息key取余将消息写入到不同副本分区上，例如redis的key在slot上分配，也是通过对key取余。）</p>
</li>
<li><p>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</p>
</li>
</ul>
<h4 id="2、创建RDD"><a href="#2、创建RDD" class="headerlink" title="2、创建RDD"></a>2、创建RDD</h4><ul>
<li><p>由一个已经存在的Scala 数组创建或者Python列表创建。<br>val rdd0 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) # Scala<br>val rdd0 = sc.parallelize([1,2,3,4,5,6,7,8]) # Python</p>
</li>
<li><p>由外部存储系统的文件创建。包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、HBase等，其他流式数据：Socket流等。<br>val rdd2 = sc.textFile(“hdfs://nn:9000/data_files”)<br>sc是spark-shell内置创建的sparkcontext</p>
</li>
<li><p>已有的RDD经过算子转换生成新的RDD<br><code>val rdd1=rdd0.flatMap(_.split(&quot; &quot;))</code><br><code>val rdd2=rdd1.map((_, 1))</code></p>
</li>
</ul>
<h4 id="3、宽依赖和窄依赖"><a href="#3、宽依赖和窄依赖" class="headerlink" title="3、宽依赖和窄依赖"></a>3、宽依赖和窄依赖</h4><p>&#8195;&#8195;在<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》 </a>的第6章节”理解Spark Stage的划分“内容，正是RDD 与它的父 RDD(s) 之间的依赖关系，才有Stage划分的基础，主要为以下两种不同的依赖关系：</p>
<ul>
<li>窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖，一对一关系。例如Map、Filter、Union等算子。（你可以这样形象理解Spark为什么要用narrow这个词：从下图中例如Map算子，一个父rdd到一个子rdd的依赖关系，一条线直连关系，用窄形容恰当）</li>
<li>宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDD 的多个分区所依赖，一对多关系。例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffling。（你可以这样形象理解Spark为什么要用wide这个词：从下图中例如groupByKey算子，一个父RDD有多条线连接到不同子RDD，用宽形容恰当）</li>
<li></li>
<li>Lineage（血统）<br>例如RDD0转换为下一个RDD1，RDD1转换为下一个RDD2，RDD2转换为下一个RDD3，…，这一系列的转换过程叫做血统，可以构成DAG图（执行计划）<br>有何作用？当RDD3丢失，只需要使用Lineage关系图和重新计算RDD2，即可恢复出RDD3数据集。而无需从头RDD0重新计算，省时省力。<br>对于下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：<br><img src="https://img-blog.csdnimg.cn/20191222202637659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">对于map、filter、union这些算子可以很直观看到它们所计算的rdd依赖关系是窄依赖<br>注意：对于join算子，如果是协同划分的话，两个父RDD之间， 父RDD与子RDD之间能形成一致的分区安排，即同一个Key保证被映射到同一个分区，这种join是窄依赖。（协同划分就是指定分区器以产生前后一致的分区安排）<br>如果不是协同划分，就会形成宽依赖。</li>
</ul>
<p>这两种依赖关系除了可以把一个Job划分多个Stage，还有以下最为重要的两点作用：<br>&#8195;&#8195;第一：窄依赖可实现在当前节点上（数据无需夸节点传输）以流水线的方式（pipeline管道方式）对父分区数据进行流水线计算，例如先执行 Map算子，接着执行Filter算子，这两个操作一气呵成。而宽依赖则需要先计算好所有父分区的数据，接着将数据通过跨节点传递后执行shuffling（不跨节点，又怎么能把不同节点但key相同的项归并到同一分区上呢？所以宽依赖必然要进行磁盘IO和Socket跨节点传数据），这一过程与 MapReduce 类似。<br>&#8195;&#8195;第二：窄依赖能够更有效地进行数据恢复，根据上面”Lineage（血统）“所提的逻辑，只需重新对丢失子rdd的父rdd进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据重新计算并再次 shuffling，效率低而且耗时。</p>
<h4 id="4、通过RDD的依赖关系构建DAG计算图"><a href="#4、通过RDD的依赖关系构建DAG计算图" class="headerlink" title="4、通过RDD的依赖关系构建DAG计算图"></a>4、通过RDD的依赖关系构建DAG计算图</h4><p>&#8195;&#8195;上面提到的多个RDD之间的两种依赖关系组成了 DAG，DAG 定义了这些 RDD之间的 Lineage链，通过血统关系（就像你手上拿了一张关系图谱），如果一个 RDD 的部分或者全部计算结果丢失了，也可以根据”这张关系图谱“重新进行计算出结果。Spark根据RDD依赖关系的不同将 DAG 划分为不同的执行阶段 (Stage)：</p>
<ul>
<li>对于窄依赖，由于分区的依赖关系是确定的，分区在当前节点上（数据无需夸节点传输）进行转换操作，也就是说可在同一个线程执行当前阶段计算任务，而且多个分区可以直接并行运行（因此分区数就决定并发计算的粒度，可用于Spark计算性能调优）。因此窄依赖的RDD可以划分到同一个执行阶段Stage；</li>
<li>对于宽依赖，由于 Shuffle 的存在，只能等多个父 RDD被 Shuffle 处理完成后（不同父分区的Shuffle导致数据需夸节点传输），才能开始对子RDD计算，因此遇到宽依赖就需要重新划分阶段。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191224193522623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>正是有了以上对Stage的划分设计，Spark在执行作业时， 生成一个完整的、最优的执行计划，从而比MapReduce”更加聪明地利用资源地“完成计算作业。</p>
<h4 id="5、RDD-持久化"><a href="#5、RDD-持久化" class="headerlink" title="5、RDD 持久化"></a>5、RDD 持久化</h4><p>&#8195;&#8195;这一章节内容直接翻译spark：<br>&#8195;&#8195;RDD最重要的特点是可将数据集缓存在内存（或者磁盘上），这也是Spark之所以快的原因。使用RDD persist时，每个节点都会存储当前RDD计算的dataset，以便被其他RDD直接在内存加载使用，这种效果将使得之后的action算子至少提高10倍上计算速度。<br>&#8195;&#8195;通过调用persist()或者cache()方法即可触发RDD persist，而且持久化另外一个作用：Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.（可根据已有的算子重新计算丢失的RDD）<br>&#8195;&#8195;Spark 在持久化 RDDs 的时候提供了 3 种storage level：存在内存中的非序列化的 java 对象、存在内存中的序列化的数据以及存储在磁盘中。第一种选择的性能是最好的，因为 JVM 可以很快的访问 RDD 的每一个元素。第二种选择是在内存有限的情况下，使的用户可以以很低的性能代价而选择的比 java 对象图更加高效的内存存储的方式。如果内存完全不够存储的下很大的 RDDs，而且计算这个 RDD 又很费时的，那么选择第三种方式。</p>
<p>The full set of storage levels is:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Storage Level</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>MEMORY_ONLY</td>
<td>Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will    not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the    partitions that don’t fit on disk, and read them from there when they’re needed.</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER   (Java and Scala)</td>
<td>Store RDD as <em>serialized</em> Java objects (one byte array per partition).    This is generally more space-efficient than deserialized objects, especially when using a    <a href="http://spark.apache.org/docs/latest/tuning.html">fast serializer</a>, but more CPU-intensive to read.</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER   (Java and Scala)</td>
<td>Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of    recomputing them on the fly each time they’re needed.</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>Store the RDD partitions only on disk.</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
<td>Same as the levels above, but replicate each partition on two cluster nodes.</td>
</tr>
<tr>
<td>OFF_HEAP (experimental)</td>
<td>Similar to MEMORY_ONLY_SER, but store the data in    <a href="http://spark.apache.org/docs/latest/configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to be enabled.</td>
</tr>
</tbody>
</table>
</div>
<p>Note: In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2.<br>注意对使用Python来开发spark的话，它存储的对象都是由Pickle库实现序列化，因此对于python，除了OFF_HEAP这个level，其他level都适用。<br>Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it.<br>此外Spark在shuffle操作例如reduceByKey这类操作时会自动对RDD做持久化，以防止某些节点再做shuffle过程中挂了导致需要重新计算父RDD。Spark建议使用者在RDD需要重新使用的场景下可先持久化RDD。</p>
<h4 id="6、RDD的checkpointing机制"><a href="#6、RDD的checkpointing机制" class="headerlink" title="6、RDD的checkpointing机制"></a>6、RDD的checkpointing机制</h4><p>&#8195;&#8195;前面提到对于要恢复某些丢失的RDD，可根据父 RDDs 的血缘关系recomputed，但是如果这个血缘关系链很长的话（例如业务逻辑里面有10来个transmissions以及有多个actions），则recomputed需要耗费很长时间，因此在这种场景下，将一些 RDDs 的数据持久化到稳定存储系统中是有必要的。<br>&#8195;&#8195;checkpointing 对具有很长的血缘关系链且包含了宽依赖的 RDDs 是非常有用的，比如spark给出的PageRank 例子，在这些场景下，集群中的某个节点的失败会导致每一个父亲 RDD 的一些数据的丢失，最惨的是这些父RDD都是宽依赖以及很长的窄依赖链关系，显然需要重新所有的计算。<br>&#8195;&#8195;对于普通的窄依赖的 RDDs（spark给出例子中线性回归例子中的 points 和 PageRank 中的 link 列表数据），checkpointing 可能一点用都没有。如果一个节点失败了，spark照应可以很快在其他的节点中并行的重新计算出丢失了数据的分区，这个成本只是备份整个 RDD 的成本的一点点而已。</p>
<p>&#8195;&#8195;Spark 目前提供了一个 checkpointing 的 api（persist 中的标识为 REPLICATE，还有 checkpoint()），用户自行选择在一些最佳的 RDDs 来进行 checkpointing，以达到最小化恢复时间。（这就像你玩过关类游戏，总共20关，这个游戏本身没有自动保存检查点，必须用户自己触发，在第19关是最难的关卡耗费一周时间拿下，而你确忘记存档，能不崩溃吗）</p>
<h4 id="7、Spark分区与RDD分区（待更新）"><a href="#7、Spark分区与RDD分区（待更新）" class="headerlink" title="7、Spark分区与RDD分区（待更新）"></a>7、Spark分区与RDD分区（待更新）</h4><p>这部分内容难度较大，但确实能真正理解Spark和RDD并行计算的底层逻辑，相关知识点需回顾hadoop文件分区。这部分内容待更新</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：AQS核心源代码实现之共享模式的深入解析</title>
    <url>/2021/05/16/AQS%E6%A0%B8%E5%BF%83%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%85%B1%E4%BA%AB%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>信号量是典型的共享模式：</p>
<p>何为共享模式：同一时间可以有多个线程同时持有锁资源，强调的是线程并行概念。</p>
<p>独占模式：同一时间只能有一个线程持有锁资源</p>
<h4 id="1、Semaphore信号量说明"><a href="#1、Semaphore信号量说明" class="headerlink" title="1、Semaphore信号量说明"></a>1、Semaphore信号量说明</h4><p>底层实现原理：使用AQS同步状态的state来保存信号量的当前计数。<code>release(1)</code>表示线程释放了1个锁资源，对应的state值加1，<code>acquire(1)</code>表示线程消耗1个数量的锁资源，对应的state值减1。一般用于限制线程并发工作数量。</p>
<h4 id="2、Semaphore使用acquire-的小demo"><a href="#2、Semaphore使用acquire-的小demo" class="headerlink" title="2、Semaphore使用acquire()的小demo"></a>2、Semaphore使用acquire()的小demo</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SemaphoreDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Semaphore semaphore = <span class="keyword">new</span> Semaphore(<span class="number">5</span>);</span><br><span class="line">        List&lt;Thread&gt;  threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            Thread t= <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    semaphore.acquire();</span><br><span class="line">                    System.out.println(<span class="string">&quot;线程&quot;</span> + Thread.currentThread().getId() + <span class="string">&quot;已拿到锁资源&quot;</span>);</span><br><span class="line">                     Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">                    semaphore.release();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread t : threadList) &#123;</span><br><span class="line">            t.start();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread t : threadList) &#123;</span><br><span class="line">            t.join();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以观察到结果：5个线程能够同时成功抢到锁资源并执行了5秒，后面5个线程因为已经没有可用资源，只能等待前面5个线程运行结束后才能继续。</p>
<p>注意Semaphore在初始化的构造器也是可以指定公平模式和非公平模式，以下将以默认的公平模式去讨论AQS是如何支撑Semaphore的工作过程</p>
<h4 id="3、Semaphore获取锁资源acquire的工作流程"><a href="#3、Semaphore获取锁资源acquire的工作流程" class="headerlink" title="3、Semaphore获取锁资源acquire的工作流程"></a>3、Semaphore获取锁资源acquire的工作流程</h4><a id="more"></a>
<p>构造器默认使用非公平模式，指定最大可用资源数（也可以称为可用信号量、许可证permits、最大可用锁资源等）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Semaphore</span><span class="params">(<span class="keyword">int</span> permits)</span> </span>&#123;</span><br><span class="line">    sync = <span class="keyword">new</span> NonfairSync(permits);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实Semaphore类也是跟ReentrantLock设计如出一辙，内部定义了AQS的子类Sync类，并分别定义非公平模式实现类<code>NonfairSync</code>和公平模式的实现类<code>FairSync</code>。</p>
<p>流程如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">		<span class="comment">// ① 用户代码：</span></span><br><span class="line">        Semaphore semaphore =<span class="keyword">new</span> Semaphore(<span class="number">10</span>); <span class="comment">// 默认非公平模式</span></span><br><span class="line">        semaphore.acquire();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		<span class="comment">// ② Semaphore 内部的acquire方法其实调用的是可立即响应中断的获取锁资源方法：acquireSharedInterruptibly</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        sync.acquireSharedInterruptibly(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> 		<span class="comment">// ③ AQS定义的acquireSharedInterruptibly方法：在共享模式下获取锁资源，如果当前正在抢锁资源的线程被外界中断，则该线程会马上抛出中断异常。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquireSharedInterruptibly</span><span class="params">(<span class="keyword">int</span> arg)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">				<span class="comment">// 这里通过读取线程自己的中断标记来判断释放需要响应外界中断</span></span><br><span class="line">        <span class="keyword">if</span> (Thread.interrupted())</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">      	<span class="comment">// 如果没有外界中断当前线程，且当前线程使用子类的tryAcquireShared具体逻辑去尝试获取锁资源时，已无剩余量，则进入排队逻辑</span></span><br><span class="line">        <span class="keyword">if</span> (tryAcquireShared(arg) &lt; <span class="number">0</span>)</span><br><span class="line">            doAcquireSharedInterruptibly(arg);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		<span class="comment">//④ 上面的tryAcquireShared(arg)显然就是Semaphore子类Sync要实现的模板方法，也即NonfairSync的tryAcquireShared方法</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * NonFair version</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NonfairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">2694183684443567898L</span>;</span><br><span class="line"></span><br><span class="line">        NonfairSync(<span class="keyword">int</span> permits) &#123;</span><br><span class="line">            <span class="keyword">super</span>(permits);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">int</span> <span class="title">tryAcquireShared</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> nonfairTryAcquireShared(acquires);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">//⑤ 来自Semaphore子类Sync的非公平方式抢占共享锁资源</span></span><br><span class="line">        <span class="function"><span class="keyword">final</span> <span class="keyword">int</span> <span class="title">nonfairTryAcquireShared</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">              	<span class="comment">// 非公平性是指：所有新来的线程都通过自旋去竞争锁资源</span></span><br><span class="line">                <span class="keyword">int</span> available = getState(); <span class="comment">// 可以看到，state的值表示当前可用资源数</span></span><br><span class="line">                <span class="keyword">int</span> remaining = available - acquires; <span class="comment">// 剩余量=当前可用数量-请求量</span></span><br><span class="line">              	<span class="comment">// 使用CAS更新扣减之后的剩余量，将剩余量返回给外面调用方，例如`tryAcquireShared`方法</span></span><br><span class="line">                <span class="keyword">if</span> (remaining &lt; <span class="number">0</span> || </span><br><span class="line">                    compareAndSetState(available, remaining))</span><br><span class="line">                    <span class="keyword">return</span> remaining;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* ⑥</span></span><br><span class="line"><span class="comment">如果tryAcquireShared没有可用资源量，则线程可能会放入阻塞队列里面，具体由AQS里面的doAcquireSharedInterruptibly方法实现</span></span><br><span class="line"><span class="comment">    if (tryAcquireShared(arg) &lt; 0)</span></span><br><span class="line"><span class="comment">        doAcquireSharedInterruptibly(arg);        </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">		<span class="comment">// AQS的可响应中断的共享模式获取锁资源方法，注意区别于doAcquireShared</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Acquires in shared interruptible mode.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arg the acquire argument</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doAcquireSharedInterruptibly</span><span class="params">(<span class="keyword">int</span> arg)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Node node = addWaiter(Node.SHARED); <span class="comment">// new一个有共享标记的节点，将其添加到阻塞队列的队尾</span></span><br><span class="line">        <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">       <span class="comment">// 以下的设计流程其实跟独占模式的acquireQueued类似：当前线程节点node进入队尾后，立即检查它的前驱节点p，若p是head节点，说明node是阻塞队列的第一个线程节点，则可以再利用tryAcquireShared去尝试请求可用资源。这种设计就是所谓的“fast try,failed acquire then do enq”</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">                <span class="comment">// ① 前驱节点是头节点，则可以进入尝试获取资源逻辑</span></span><br><span class="line">                <span class="keyword">if</span> (p == head) &#123;</span><br><span class="line">                  	<span class="comment">// 这里还是使用Semaphore内部的NonfairSync的tryAcquireShared方法实现</span></span><br><span class="line">                    <span class="keyword">int</span> r = tryAcquireShared(arg);</span><br><span class="line">                     <span class="comment">/* ② 如果剩余的可用资源&gt;=0，那么将node设为head，并且将唤醒动作传播到下一个线程节点上。这里要重点解释为何共享模式下不能用setHead，而是需要使用setHeadAndPropagate(node, r)：</span></span><br><span class="line"><span class="comment">                     例如有10个线程，5个可用资源，假设前面5个线程抢用完资源后，第6到10个线程将进入阻塞队列并在等待中，在某一刻，前面5个线程同时释放资源，那么第6个线程唤醒后使用r = tryAcquireShared(arg)就会剩下4个可用资源，意味着第7、8、9、10这四个在阻塞队列的线程根本无需再等待，外面已经有4个资源够他们抢用了(除非此时外界又来了新线程直接跟7到10号线程竞争资源)，因此需要在第6个线程唤醒获得资源后，马上将唤醒动作传播给第7个线程（第7个线程传播给第8个线程，类似多米诺骨牌效益），这就是共享模式为何采用Propagating wake up——将唤醒动作传播持续下去的设计思路</span></span><br><span class="line"><span class="comment">                        */</span></span><br><span class="line">                    <span class="keyword">if</span> (r &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                        setHeadAndPropagate(node, r); <span class="comment">// 获取到锁资源的第6号线程可以出队，同时将第7号线程唤醒，完成“唤醒传播”操作</span></span><br><span class="line">                        p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">                        failed = <span class="keyword">false</span>;</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//③ 说明线程节点不是第一个线程节点，阻塞自己，避免浪费cpu时间片，这一点设计跟acquireQueued是一致的。</span></span><br><span class="line">                <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">                    parkAndCheckInterrupt())</span><br><span class="line">                  	<span class="comment">// ④在进入对尾阻塞自己后，parkAndCheckInterrupt()唤醒后，若发现自己阻塞期间被外界中断过，那么就通过抛出中断异常来响应“外界的中断请求”，try捕获到此异常后进入finally的“取消排队处理”</span></span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          	<span class="comment">//⑤ 既然当前线程唤醒后，发现自己阻塞期间被外界中断过，那么线程取消排队，不再和阻塞队列里面的线程竞争共享锁资源</span></span><br><span class="line">            <span class="keyword">if</span> (failed)</span><br><span class="line">                cancelAcquire(node);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="4、doAcquireShared"><a href="#4、doAcquireShared" class="headerlink" title="4、doAcquireShared"></a>4、doAcquireShared</h4><p>注意其源码解释：Acquires in shared uninterruptible mode，强调了该方法是不响应中断的，而上面的<code>doAcquireSharedInterruptibly</code> 能响应外界设置中断标志且会抛出中断异常。</p>
<p><code>doAcquireShared</code>在什么情况下会被调用呢？</p>
<p>用户层代码使用<code>semaphore.acquireUninterruptibly()</code>请求锁资源</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// Semaphore</span></span><br><span class="line">  	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">acquireUninterruptibly</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       sync.acquireShared(<span class="number">1</span>);</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">// AQS</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquireShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (tryAcquireShared(arg) &lt; <span class="number">0</span>)</span><br><span class="line">           doAcquireShared(arg); <span class="comment">// 此处可以看到semaphore.acquireUninterruptibly()底层调用的是不响应中断的doAcquireShared</span></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>以下是doAcquireShared的设计解析：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Acquires in shared uninterruptible mode.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arg the acquire argument</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doAcquireShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node node = addWaiter(Node.SHARED); </span><br><span class="line">    <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">            <span class="comment">// ① 假设node的前驱节点还不是head节点，则进入④流程，等到该node唤醒后重新来到①且此时p==head</span></span><br><span class="line">            <span class="keyword">if</span> (p == head) &#123;</span><br><span class="line">                <span class="keyword">int</span> r = tryAcquireShared(arg);</span><br><span class="line">                <span class="keyword">if</span> (r &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    setHeadAndPropagate(node, r); </span><br><span class="line">                    p.next = <span class="keyword">null</span>; <span class="comment">// help GC </span></span><br><span class="line">                    <span class="comment">// ③ 接④的interrupted = true的标志后，唤醒的线程将外界中断信号在这里补充了一个自我中断，注意：别错误认为此时线程就是抛出异常，doAcquireShared是被设计为“获取锁资源的过程中都不会响应中断处理”</span></span><br><span class="line">                  	<span class="comment">// 如果线程是被unpark正常唤醒正常唤醒，那么就会跳过以下if的selfInterrupt()逻辑。</span></span><br><span class="line">                    <span class="keyword">if</span> (interrupted)</span><br><span class="line">                        selfInterrupt();</span><br><span class="line">                  	<span class="comment">// 虽然补充了自我中断标志，但是线程已经成功获得锁资源，因此不是“failed”，可直接返回。</span></span><br><span class="line">                    failed = <span class="keyword">false</span>;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// ④</span></span><br><span class="line">            <span class="comment">// 说明线程节点不是第一个线程节点，那么入队后，阻塞自己，避免浪费cpu时间片。</span></span><br><span class="line">          	<span class="comment">// shouldParkAfterFailedAcquire(p, node) 主要负责将node的前驱节点waitStatus改为SIGNAL，parkAndCheckInterrupt()主要负责阻塞线程并且在线程唤醒后，返回该线程有被中断的标志</span></span><br><span class="line">            <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">                parkAndCheckInterrupt())</span><br><span class="line">                interrupted = <span class="keyword">true</span>;  <span class="comment">// 这里指明了线程节点使用doAcquireShared成功获取锁资源前，是不会抛出中断异常的。当线程节点被唤醒时，对于被唤醒的情况有两种：一种是使用unpark正常唤醒，一种是被外界线程使用interrupt()中断，对于后面这种中断情况，当前唤醒线程并不会马上响应它（并将interrupted标志设为ture），而是继续后面的流程：在下一轮自旋将回到①处，如果p==head成立，则尝试获取锁资源，如果剩余的可用资源&gt;=0,那么就会进入②位置，然后在③位置去处理“外界的中断信号”</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment">// 如果线程节点node请求可用资源出现其他异常（显然不会是中断异常），则取消该线程节点排队，这一点设计跟acquireQueued是一致的。</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (failed)</span><br><span class="line">            cancelAcquire(node);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5、setHeadAndPropagate"><a href="#5、setHeadAndPropagate" class="headerlink" title="5、setHeadAndPropagate"></a>5、setHeadAndPropagate</h4><p><code>setHeadAndPropagate</code>方法是共享模式获取锁资源的的一个核心设计，需要独立再给出详细分析</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">	    <span class="comment">/**</span></span><br><span class="line"><span class="comment">			因为阻塞队列第一个线程节点使用 tryAcquireShared获取可用资源后，剩余量r&gt;=0,因此它可以出队，并且检查后驱节点是否以共享模式等待，如果是那么只要满足剩余量大于0或者后驱节点的PROPAGATE已经设置，那么就要讲唤醒动作传播下去，也即调用doReleaseShared来实现唤醒动作接力。注意到Doug lea有提到该方法会引起不必要的唤醒动作。                 </span></span><br><span class="line"><span class="comment">     * Sets head of queue, and checks if successor may be waiting</span></span><br><span class="line"><span class="comment">     * in shared mode, if so propagating if either propagate &gt; 0 or</span></span><br><span class="line"><span class="comment">     * PROPAGATE status was set.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> propagate the return value from a tryAcquireShared</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setHeadAndPropagate</span><span class="params">(Node node, <span class="keyword">int</span> propagate)</span> </span>&#123;</span><br><span class="line">        Node h = head; <span class="comment">// Record old head for check below 保留旧头节点，用于之后检查</span></span><br><span class="line">        setHead(node);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Try to signal next queued node if:</span></span><br><span class="line"><span class="comment">         *   Propagation was indicated by caller,</span></span><br><span class="line"><span class="comment">         *     or was recorded (as h.waitStatus either before</span></span><br><span class="line"><span class="comment">         *     or after setHead) by a previous operation</span></span><br><span class="line"><span class="comment">         *     (note: this uses sign-check of waitStatus because</span></span><br><span class="line"><span class="comment">         *      PROPAGATE status may transition to SIGNAL.)</span></span><br><span class="line"><span class="comment">         * and</span></span><br><span class="line"><span class="comment">         *   The next node is waiting in shared mode,</span></span><br><span class="line"><span class="comment">         *     or we don&#x27;t know, because it appears null</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * The conservatism in both of these checks may cause</span></span><br><span class="line"><span class="comment">         * unnecessary wake-ups, but only when there are multiple</span></span><br><span class="line"><span class="comment">         * racing acquires/releases, so most need signals now or soon</span></span><br><span class="line"><span class="comment">         * anyway.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">      </span><br><span class="line">      	<span class="comment">/*</span></span><br><span class="line"><span class="comment">      	这里设计很机智：</span></span><br><span class="line"><span class="comment">      	条件① 如果可用资源量propagate&gt;0,当然可以和后驱节点（如果存在）说：你们可以醒来啦，state这边还有资源可以抢占</span></span><br><span class="line"><span class="comment">      	条件②条件③是联合设计：如果旧头节点不为null那么就可以判断h.waitStatus头结点同步状态值，如果&lt;0,说明waitStatus可能是SIGNAL或者PROPAGATE，故考虑唤醒传播逻辑</span></span><br><span class="line"><span class="comment">      	条件④：其实条件④就是Doug Lea经常用的一个trick写法，对于高并发场景下，可以在判断条件里面进行双重检查，也即前一刻读取一次，在后一刻马上读取一次</span></span><br><span class="line"><span class="comment">        在这里，直接在条件里面重新读取新的阻塞队列头节点，如果为不空，就会跟前一刻的条件②条件③联合起来的逻辑一样。</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        <span class="keyword">if</span> (propagate &gt; <span class="number">0</span> || h == <span class="keyword">null</span> || h.waitStatus &lt; <span class="number">0</span> ||</span><br><span class="line">            (h = head) == <span class="keyword">null</span> || h.waitStatus &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            Node s = node.next; <span class="comment">// 取出第一个线程节点的后驱节点，它可能null也可能是一个正在等待的线程节点</span></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">            唤醒后继共享节点</span></span><br><span class="line"><span class="comment">            ① s后驱节点为null，为何还要将唤醒动作接力下去？ 这是因为虽然上一课刻读取node.next为null，但是在下一刻如果恰好有新的线程节点入队，那么此时doReleaseShared就可以派上用场，但如果下一刻还是没有新线程入队，那么doReleaseShared也会被调用，这就是注释提到的“may cause unnecessary wake-ups”，也即引起不必要的唤醒调用。</span></span><br><span class="line"><span class="comment">            ② 若s不为null，恰好保证第二个条件s.isShared()不会发生空指针异常且可以只要s.isShared()为true，说明后驱节点是共享模式，需要被唤醒去抢锁资源的。</span></span><br><span class="line"><span class="comment">            这里之所以加入判断后驱节点s是共享模式节点的前提下才能执行唤醒操作，是因为在ReentrantReadWriteLock的读写锁同步器设计内部的阻塞队列中可能同时存在读线程节点（共享模式，需要继续被唤醒）和写线程节点（独占模式），而对于Semaphore同步器，只有s.isShared()为true才会将唤醒动作接力下去，否则说明此s节点是独占模式的节点（或者说此时用户使用的是ReentrantReadWriteLock同步器场景），不需要执行唤醒传播逻辑。</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            <span class="keyword">if</span> (s == <span class="keyword">null</span> || s.isShared())  <span class="comment">// 如果s不是null且s.isShared()不是true,说明当前使用setHeadAndPropagate是ReentrantReadWriteLock使用场景，那么此时该节点是一个写节点，不需要传播唤醒，因为这个写线程会等之前那个获得独占锁的写线程去唤醒。</span></span><br><span class="line"><span class="comment">// 当然这部分内容最好有了ReentrantReadWriteLock设计原理的基础后再来理解则更容易明白。              </span></span><br><span class="line">                doReleaseShared();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="6、doReleaseShared"><a href="#6、doReleaseShared" class="headerlink" title="6、doReleaseShared()"></a>6、doReleaseShared()</h4><p>正如注释里面的说明：signals successor and ensures propagation，通知后驱节点以及保证唤醒传播（接力）下去</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Release action for shared mode -- signals successor and ensures</span></span><br><span class="line"><span class="comment"> * propagation. (Note: For exclusive mode, release just amounts</span></span><br><span class="line"><span class="comment"> * to calling unparkSuccessor of head if it needs signal.)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doReleaseShared</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">/* 由于在ReentrantReadWriteLock的阻塞队列里面可能同时存在独占模式的线程节点和共享模式的线程节点，因此该唤醒方法分为两种情况：如果遇到阻塞队列第一个线程节点是需要唤醒的</span></span><br><span class="line"><span class="comment">     * Ensure that a release propagates, even if there are other</span></span><br><span class="line"><span class="comment">     * in-progress acquires/releases.  This proceeds in the usual</span></span><br><span class="line"><span class="comment">     * way of trying to unparkSuccessor of head if it needs</span></span><br><span class="line"><span class="comment">     * signal. But if it does not, status is set to PROPAGATE to</span></span><br><span class="line"><span class="comment">     * ensure that upon release, propagation continues.</span></span><br><span class="line"><span class="comment">     * Additionally, we must loop in case a new node is added</span></span><br><span class="line"><span class="comment">     * while we are doing this. Also, unlike other uses of</span></span><br><span class="line"><span class="comment">     * unparkSuccessor, we need to know if CAS to reset status</span></span><br><span class="line"><span class="comment">     * fails, if so rechecking.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">for</span> (;;) &#123; <span class="comment">// 注意到这里用到自旋，确保唤醒动作能够传播到下一个节点</span></span><br><span class="line">        Node h = head;</span><br><span class="line">      	<span class="comment">// 只要阻塞队列不是空链表（至少存在一个线程节点）</span></span><br><span class="line">        <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h != tail) &#123;</span><br><span class="line">            <span class="keyword">int</span> ws = h.waitStatus;</span><br><span class="line">            <span class="comment">// ① 共享模式下，阻塞队列里面的线程节点的前驱节点waitStatus都会被设置SIGNAL，因此可以到这里如果直到head的ws是SIGNAL，就可以进行唤醒操作</span></span><br><span class="line">            <span class="keyword">if</span> (ws == Node.SIGNAL) &#123;</span><br><span class="line">              	<span class="comment">// 确保能将头节点从SIGNAL改为0才能进行唤醒后驱节点的操作</span></span><br><span class="line">                <span class="keyword">if</span> (!compareAndSetWaitStatus(h, Node.SIGNAL, <span class="number">0</span>))</span><br><span class="line">                    <span class="keyword">continue</span>;            <span class="comment">// loop to recheck cases</span></span><br><span class="line">                unparkSuccessor(h);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// ② 如果此时头节点ws是0，就将其设为PROPAGATE，以保证唤醒操作能够传播到下一个节点</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (ws == <span class="number">0</span> &amp;&amp;</span><br><span class="line">                     !compareAndSetWaitStatus(h, <span class="number">0</span>, Node.PROPAGATE))</span><br><span class="line">                <span class="keyword">continue</span>;                <span class="comment">// loop on failed CAS</span></span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">// 如果头节点已经被更新（也即inconsistent read），只能回到for循环继续重试</span></span><br><span class="line">        <span class="keyword">if</span> (h == head)                   <span class="comment">// loop if head changed</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="7、Semaphore的释放资源release"><a href="#7、Semaphore的释放资源release" class="headerlink" title="7、Semaphore的释放资源release()"></a>7、Semaphore的释放资源release()</h4><p>release的逻辑相对简单，这里不再过多说明。对比<code>doAcquireShared</code>方法，原来共享模式的同步器，不仅在释放资源时要求去唤醒阻塞队列的节点，被唤醒的线程节点在请求资源后，也要将唤醒操作传播下去。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// 用户代码</span></span><br><span class="line">        semaphore.release();</span><br><span class="line">        </span><br><span class="line"> <span class="comment">// 在成功是否锁资源后，底层是调用AQS的doReleaseShared唤醒阻塞队列的首个等待的线程节点，tryReleaseShared(arg)是模板方法     </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">releaseShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tryReleaseShared(arg)) &#123;</span><br><span class="line">            doReleaseShared();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Semaphore内部tryReleaseShared实现逻辑，对state进行CAS累加</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryReleaseShared</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                <span class="keyword">int</span> current=getState();</span><br><span class="line">                <span class="keyword">int</span> next=current+releases;</span><br><span class="line">                <span class="keyword">if</span> (next&lt;current)</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Permit count underflow&quot;</span>);</span><br><span class="line">                <span class="keyword">if</span> (compareAndSetState(current,next))</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果释放资源成功，也即state值加1（假设每次请求1个资源），那么进入AQS的共享模式下的唤醒操作：</span></span><br><span class="line">        <span class="keyword">if</span> (tryReleaseShared(arg)) &#123;</span><br><span class="line">            doReleaseShared();</span><br></pre></td></tr></table></figure>
<p>其实释放资源的<code>release()</code>方法内部的<code>doReleaseShared</code>的唤醒设计在CountDownLatch的countDown里面也是同样的设计。</p>
<h4 id="8、共享模式下的公平模式"><a href="#8、共享模式下的公平模式" class="headerlink" title="8、共享模式下的公平模式"></a>8、共享模式下的公平模式</h4><p>此设计相对简单，和ReentrantLock的公平模式如出一辙，通过<code>hasQueuedPredecessors()</code>判断阻塞队列是否有正在排队的线程节点，如果有，那么只能乖乖的入队：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">FairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">2014338818796000944L</span>;</span><br><span class="line"></span><br><span class="line">    FairSync(<span class="keyword">int</span> permits) &#123;</span><br><span class="line">        <span class="keyword">super</span>(permits);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">int</span> <span class="title">tryAcquireShared</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="keyword">if</span> (hasQueuedPredecessors())</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> available = getState();</span><br><span class="line">            <span class="keyword">int</span> remaining = available - acquires;</span><br><span class="line">            <span class="keyword">if</span> (remaining &lt; <span class="number">0</span> ||</span><br><span class="line">                compareAndSetState(available, remaining))</span><br><span class="line">                <span class="keyword">return</span> remaining;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>ArrayList数据结构设计原理的简要深入分析.md</title>
    <url>/2021/01/16/ArrayList%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E7%9A%84%E7%AE%80%E8%A6%81%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h5 id="成员变量"><a href="#成员变量" class="headerlink" title="成员变量"></a>成员变量</h5><p>空参构造非常简单，它会为我们创建一个空的集合。elementData成员变量是用来存放数据的对象,是一个Object[]，DEFAULTCAPACITY_EMPTY_ELEMENTDATA则是一个空的数组。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">　* Constructs an empty list with an initial capacity of ten.</span></span><br><span class="line"><span class="comment">　*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayList</span><span class="params">()</span> </span>&#123;</span><br><span class="line">　　<span class="keyword">this</span>.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>空参构造非常简单，它会为我们创建一个空的集合。elementData成员变量是用来存放数据的对象,是一个Object[]，DEFAULTCAPACITY_EMPTY_ELEMENTDATA则是一个空的数组。注意DEFAULTCAPACITY_EMPTY_ELEMENTDATA类型为static final，表明其在内存中只有一份且禁止修改。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">　* Shared empty array instance used <span class="keyword">for</span> <span class="keyword">default</span> sized empty instances. We</span><br><span class="line">　* distinguish <span class="keyword">this</span> from EMPTY_ELEMENTDATA to know how much to inflate when</span><br><span class="line">　* first element is added.</span><br><span class="line">　*/</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;</span><br></pre></td></tr></table></figure>
<p>注意elementData使用transient修饰。表明在采用Java默认的序列化机制的时候，被该关键字修饰的属性不会被序列化。而ArrayList类实现了java.io.Serializable接口，即采用了Java默认的序列化机制。但是elementData在网络传输的时候不序列化肯定是不行的，翻看源码会发现ArrayList自己实现了序列化和反序列化的方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> Object[] elementData; <span class="comment">// non-private to simplify nested class access</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h5 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayList</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (initialCapacity &gt; <span class="number">0</span>) &#123; <span class="comment">//如果指定了初始容量，则用该容量去new一个10的null数组</span></span><br><span class="line">        <span class="keyword">this</span>.elementData = <span class="keyword">new</span> Object[initialCapacity];</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (initialCapacity == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.elementData = EMPTY_ELEMENTDATA;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Illegal Capacity: &quot;</span>+</span><br><span class="line">                                           initialCapacity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Constructs an empty list with an initial capacity of ten.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayList</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// 10个容量的空数组,注意this.elementData一定是指向数组本身</span></span><br><span class="line">    <span class="keyword">this</span>.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayList</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">    elementData = c.toArray(); <span class="comment">//取出集合的数组元素，放到Object[]数组中</span></span><br><span class="line">    <span class="keyword">if</span> ((size = elementData.length) != <span class="number">0</span>) &#123; </span><br><span class="line">        <span class="comment">// c.toArray might (incorrectly) not return Object[] (see 6260652)</span></span><br><span class="line">        <span class="keyword">if</span> (elementData.getClass() != Object[].class)</span><br><span class="line">          	<span class="comment">// 将c.toArray()拷贝到一个新Object数组上，并用elementData指向它</span></span><br><span class="line">            elementData = Arrays.copyOf(elementData, size, Object[].class);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// replace with empty array.</span></span><br><span class="line">        <span class="keyword">this</span>.elementData = EMPTY_ELEMENTDATA; <span class="comment">//DEFAULTCAPACITY_EMPTY_ELEMENTDATA用在空参数的构造方法中，而EMPTY_ELEMENTDATA用在入参为集合的构造方法中。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="add方法"><a href="#add方法" class="headerlink" title="add方法"></a>add方法</h5><p>首次add一个元素时：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//首次添加时，size=0，将size+1看看是否有超过默认容量10，注意minCapacity=size + 1),size+1表示elementData所需要的最小长度。这里的size变量，是用来记录ArrayList包含元素的多少的，初始值为0</span></span><br><span class="line">    ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// Increments modCount!!</span></span><br><span class="line">    elementData[size++] = e; <span class="comment">//注意这里是指elementData[size] = e 然后再将size自增1</span></span><br><span class="line">  	<span class="comment">//切勿以为index=size+1然后elementData[index]=e，否则就会出现首个元素对应elementData[1]这样的误解</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureCapacityInternal</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>calculateCapacity</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">calculateCapacity</span><span class="params">(Object[] elementData, <span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">//首次add一个元素时,elementData就是指向DEFAULTCAPACITY_EMPTY_ELEMENTDATA，而minCapacity=size+1=0+1，因此calculateCapacity返回10给到ensureExplicitCapacity</span></span><br><span class="line">    <span class="keyword">if</span> (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;</span><br><span class="line">        <span class="keyword">return</span> Math.max(DEFAULT_CAPACITY, minCapacity);</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">// 每次添加一个节点后，minCapacity=size+1</span></span><br><span class="line">    <span class="keyword">return</span> minCapacity; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首次add一个元素时：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureExplicitCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    modCount++; <span class="comment">//首次add一个元素，modCount当然为1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// overflow-conscious code</span></span><br><span class="line">   	<span class="comment">// 首次add一个元素，前面计算出minCapacity=10，而elementData.length=0，因此需要grow增加数组容量（因为空构造方法创建的是空数组，而添加一个元素前，就需要将数组容量扩容到10）</span></span><br><span class="line">    <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)</span><br><span class="line">        grow(minCapacity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>grow</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// overflow-conscious code</span></span><br><span class="line">   <span class="comment">// 首次add一个元素时，elementData还是空数组</span></span><br><span class="line">    <span class="keyword">int</span> oldCapacity = elementData.length;<span class="comment">// 0</span></span><br><span class="line">  	<span class="comment">// 扩容值：new=old+0.5old=1.5old</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>); <span class="comment">//0</span></span><br><span class="line">    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>) <span class="comment">//前面算出minCapacity=10</span></span><br><span class="line">        newCapacity = minCapacity; <span class="comment">// 因此使用10作为elementData首次add的数组容量</span></span><br><span class="line">    <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>) <span class="comment">// MAX_ARRAY_SIZE=Integer.MAX_VALUE - 8</span></span><br><span class="line">        newCapacity = hugeCapacity(minCapacity);</span><br><span class="line">    <span class="comment">// minCapacity is usually close to size, so this is a win:</span></span><br><span class="line">  	<span class="comment">// 首先elementData还是空数组，经过以下数组扩容后（本质是利用拷贝到新数组实现），elementData指向了一个容量10的null数组</span></span><br><span class="line">    elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">  	<span class="comment">//也即以上执行完后，elementData.length=10,但此时size还是=1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>hugeCapacity</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hugeCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (minCapacity &lt; <span class="number">0</span>) <span class="comment">// overflow </span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> OutOfMemoryError();</span><br><span class="line">   	<span class="comment">// 看看是否需要取到最大值：2^31-1</span></span><br><span class="line">    <span class="keyword">return</span> (minCapacity &gt; MAX_ARRAY_SIZE) ?</span><br><span class="line">        Integer.MAX_VALUE :</span><br><span class="line">        MAX_ARRAY_SIZE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ensureCapacity</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Increases the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance, if</span></span><br><span class="line"><span class="comment"> * necessary, to ensure that it can hold at least the number of elements</span></span><br><span class="line"><span class="comment"> * specified by the minimum capacity argument.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>   minCapacity   the desired minimum capacity</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)</span><br><span class="line">        <span class="comment">// any size if not default element table</span></span><br><span class="line">        ? <span class="number">0</span></span><br><span class="line">        <span class="comment">// larger than default for default empty table. It&#x27;s already</span></span><br><span class="line">        <span class="comment">// supposed to be at default size.</span></span><br><span class="line">        : DEFAULT_CAPACITY;</span><br><span class="line">		<span class="comment">//上面看的是是否达到10的默认容量，如果计算出的最小需求容量大于最小扩容容量，那么就需要进行扩容了</span></span><br><span class="line">  	<span class="comment">// 例如minCapacity=11，minExpand要么是0，要么就是10，显然是需要扩容的</span></span><br><span class="line">    <span class="keyword">if</span> (minCapacity &gt; minExpand) &#123;</span><br><span class="line">        ensureExplicitCapacity(minCapacity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="trimToSize"><a href="#trimToSize" class="headerlink" title="trimToSize"></a>trimToSize</h5><p>例如elementData是一个容量为10的数组，而当前elementData仅拥有一个元素，那么使用trimToSize后，</p>
<p>elementData就是一个容量为1的数组，“感觉像被压实了”。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Trims the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance to be the</span></span><br><span class="line"><span class="comment"> * list&#x27;s current size.  An application can use this operation to minimize</span></span><br><span class="line"><span class="comment"> * the storage of an &lt;tt&gt;ArrayList&lt;/tt&gt; instance.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 官方都说这样裁剪可以使用最小空间来存储一个ArrayList</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">trimToSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    modCount++;</span><br><span class="line">    <span class="keyword">if</span> (size &lt; elementData.length) &#123;</span><br><span class="line">        elementData = (size == <span class="number">0</span>)</span><br><span class="line">          ? EMPTY_ELEMENTDATA</span><br><span class="line">          : Arrays.copyOf(elementData, size);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面也可以看出，只要实际元素个数少于数组容量，那么就可以进行“数组裁剪”，如果如果size为0，那么</p>
<p>就让elementData指向EMPTY_ELEMENTDATA这个空数组，（在这里再次可以感受到与DEFAULTCAPACITY_EMPTY_ELEMENTDATA的区别），否则就会使用Arrays.copyOf方法，创建一个只有size容量的数组，然后elementData再指向它。</p>
<h5 id="indexOf"><a href="#indexOf" class="headerlink" title="indexOf"></a>indexOf</h5><p>返回给定元素所在的下标，可以看到：如果存在，则返回值就是0到size-1方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">indexOf</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">            <span class="keyword">if</span> (elementData[i]==<span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">            <span class="keyword">if</span> (o.equals(elementData[i]))</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>contains复用了indexOf；显然如果检索不到，返回-1，那么下面就会返回false</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> indexOf(o) &gt;= <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="尾部遍历lastIndexOf"><a href="#尾部遍历lastIndexOf" class="headerlink" title="尾部遍历lastIndexOf"></a>尾部遍历lastIndexOf</h5><p>显然最后一个元素对应的下标是size-1</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lastIndexOf</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = size-<span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">            <span class="keyword">if</span> (elementData[i]==<span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = size-<span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">            <span class="keyword">if</span> (o.equals(elementData[i]))</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="注意toArray的官方解释"><a href="#注意toArray的官方解释" class="headerlink" title="注意toArray的官方解释"></a>注意toArray的官方解释</h5><p>返回的array是不存在有任何引用指向它，因为它是全新分配的，因此caller可以自由修改toArray() 返回的数组。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns an array containing all of the elements in this list</span></span><br><span class="line"><span class="comment"> * in proper sequence (from first to last element).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The returned array will be &quot;safe&quot; in that no references to it are</span></span><br><span class="line"><span class="comment"> * maintained by this list.  (In other words, this method must allocate</span></span><br><span class="line"><span class="comment"> * a new array).  The caller is thus free to modify the returned array.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;This method acts as bridge between array-based and collection-based</span></span><br><span class="line"><span class="comment"> * APIs.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> an array containing all of the elements in this list in</span></span><br><span class="line"><span class="comment"> *         proper sequence</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Object[] toArray() &#123;</span><br><span class="line">    <span class="keyword">return</span> Arrays.copyOf(elementData, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Positional-Access-Operations"><a href="#Positional-Access-Operations" class="headerlink" title="Positional Access Operations"></a>Positional Access Operations</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Positional Access Operations</span></span><br><span class="line"></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  <span class="function">E <span class="title">elementData</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> (E) elementData[index];</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">	 <span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        rangeCheck(index);</span><br><span class="line">        <span class="keyword">return</span> elementData(index);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">        rangeCheck(index);</span><br><span class="line">		</span><br><span class="line">        E oldValue = elementData(index);	<span class="comment">//取出旧值</span></span><br><span class="line">        elementData[index] = element; <span class="comment">// 设置新值</span></span><br><span class="line">        <span class="keyword">return</span> oldValue;	<span class="comment">//返回旧值</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>rangeCheck</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheck</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">//只要给定的下标index大于等于size，即可抛出异常</span></span><br><span class="line">    <span class="keyword">if</span> (index &gt;= size)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="add"><a href="#add" class="headerlink" title="add"></a>add</h5><p>Shifts the element currently at that position (if any) and any subsequent elements to the right (adds one to their indices).</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">    rangeCheckForAdd(index);</span><br><span class="line">    ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// Increments modCount!!</span></span><br><span class="line">  	<span class="comment">/* 从源数组的index到index+(length-1)个元素要拷贝，因为length=size - index，因此对于src来说，拷贝的元素范围为[index,index+(size - index -1)]，也即src拷贝元素的范围是[index,size-1]</span></span><br><span class="line"><span class="comment">  	放在目标数组：index+1位置到index+1+(length-1)</span></span><br><span class="line"><span class="comment">  	而length=size - index个，这里是size是src的实际元素个数，故index+1+(size - index-1)=size</span></span><br><span class="line"><span class="comment">  	对于dest来说，就是讲src的元素放置在[index+1,size]位置上，由于dest就是源数组，因此需要确保新的源数组容量至少比之前大1个空位。</span></span><br><span class="line"><span class="comment">  	将[index,size-1]的元素整体往右挪动1格，变成[index+1,size]，这就是官方解释的</span></span><br><span class="line"><span class="comment">  	 Shifts the element currently at that position (if any) and any subsequent elements to the right，然后adds one to their indices</span></span><br><span class="line"><span class="comment">   	*/</span> </span><br><span class="line">    System.arraycopy(elementData, index, elementData, index + <span class="number">1</span>,</span><br><span class="line">                     size - index);</span><br><span class="line">    elementData[index] = element; <span class="comment">//再将原index位置元素覆盖即可</span></span><br><span class="line">    size++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>检查所插入的位置index是否合法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A version of rangeCheck used by add and addAll.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheckForAdd</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (index &gt; size || index &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="size和index的区别"><a href="#size和index的区别" class="headerlink" title="size和index的区别"></a>size和index的区别</h5><p>例如：如果一个数组a的元素个数有3个，那么size就等于3，也即size是从1开始计算</p>
<p>那么索引呢？ index最大值为2，因为索引是从0开始计算，因此为了简化思维：</p>
<p>如果要计算元素个数差值：需要将index和size对齐，再来做加减</p>
<p>index+1即可对齐size的元素计数</p>
<p>因此在下面的remove方法即可用到这种计算策略！！</p>
<h5 id="remove"><a href="#remove" class="headerlink" title="remove"></a>remove</h5><p>跟add相反：把index+1~size的元素向左边整体挪动一格，挪到index~size-1</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Removes the element at the specified position in this list.</span></span><br><span class="line"><span class="comment">     * Shifts any subsequent elements to the left (subtracts one from their</span></span><br><span class="line"><span class="comment">     * indices).</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> index the index of the element to be removed</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the element that was removed from the list</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IndexOutOfBoundsException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    rangeCheck(index); <span class="comment">// 确保给定下标不越界</span></span><br><span class="line"></span><br><span class="line">    modCount++; </span><br><span class="line">    E oldValue = elementData(index);  <span class="comment">// eturn (E) elementData[index]</span></span><br><span class="line">		<span class="comment">/*(1) 索引index对齐到size</span></span><br><span class="line"><span class="comment">		size元素个数的计算视角是从1开始，index索引计算视角是从0开始，先对齐index+1，那么要迁移的元素个数就是</span></span><br><span class="line"><span class="comment">  	 numMoved=size-(index-1)</span></span><br><span class="line"><span class="comment">  	（2）size转为索引视角</span></span><br><span class="line"><span class="comment">  	size-1就是变成从0开始计数，也即跟索引index对齐</span></span><br><span class="line"><span class="comment">  	因此计算元素个数：索引a-索引b就是元素差的个数，也即numMoved=size-1-index</span></span><br><span class="line"><span class="comment">  	*/</span></span><br><span class="line">    <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>; ;</span><br><span class="line">    <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">      	<span class="comment">//从[index+1,size-1]的元素整体向左迁移一格变成[index,size-2]</span></span><br><span class="line">        System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,</span><br><span class="line">                         numMoved);</span><br><span class="line">    elementData[--size] = <span class="keyword">null</span>; <span class="comment">// clear to let GC do its work 将elementData数组的第size-1个桶位置空用于GC</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> oldValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下案例模拟remove操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String[] src=&#123;<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>&#125;;</span><br><span class="line"><span class="comment">// 将[1,2]的元素迁移到[0,1]中，迁移的元素个数2个 </span></span><br><span class="line">System.arraycopy(src,<span class="number">1</span>,src,<span class="number">0</span>,<span class="number">2</span>);</span><br><span class="line">System.out.println(Arrays.toString(src));  <span class="comment">// [bar, foobar, foobar]</span></span><br><span class="line">src[<span class="number">2</span>]=<span class="keyword">null</span>; <span class="comment">// elementData[--size] = null</span></span><br><span class="line">System.out.println(Arrays.toString(src)); <span class="comment">// [bar, foobar, null]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>删除一个对象</p>
<p>先使用遍历桶位的方式去处理，如果找到，则调用fastRemove删除</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; size; index++)</span><br><span class="line">            <span class="keyword">if</span> (elementData[index] == <span class="keyword">null</span>) &#123;</span><br><span class="line">                fastRemove(index);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; size; index++)</span><br><span class="line">            <span class="keyword">if</span> (o.equals(elementData[index])) &#123;</span><br><span class="line">                fastRemove(index);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>fastRemove：其实就是remove方法里面跳过index检查以及目标删除值返回</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">fastRemove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    modCount++;</span><br><span class="line">    <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">        System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,</span><br><span class="line">                         numMoved);</span><br><span class="line">    elementData[--size] = <span class="keyword">null</span>; <span class="comment">// clear to let GC do its work</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="clear"><a href="#clear" class="headerlink" title="clear"></a>clear</h5><p>注意：虽然clear会把所有桶位置null，但是只能算一次结构修改！！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    modCount++;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// clear to let GC do its work</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">        elementData[i] = <span class="keyword">null</span>;</span><br><span class="line">    size = <span class="number">0</span>; <span class="comment">// 这里可以再次看到size是实际元素个数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="System-arraycopy"><a href="#System-arraycopy" class="headerlink" title="System.arraycopy"></a>System.arraycopy</h5><p>src: the source array 源数组</p>
<p>srcPoc: 从该位置开始拷贝拷贝元素</p>
<p>dest: 目标数组</p>
<p>destPos：从该位置开始放置拷贝过来元素</p>
<p>length：要将源数组拷贝多少个元素到目标数组中</p>
<pre><code> * @param      src      . //
 * @param      srcPos   starting position in the source array.
 * @param      dest     the destination array.
 * @param      destPos  starting position in the destination data.
 * @param      length   the number of array elements to be copied.
 System.arraycopy(elementData, index, elementData, index + 1,
                 size - index);
</code></pre><p>官方注释：</p>
<pre><code>/**
 * Copies an array from the specified source array, beginning at the
 * specified position, to the specified position of the destination array.
 * A subsequence of array components are copied from the source
 * array referenced by &lt;code&gt;src&lt;/code&gt; to the destination array
 * referenced by &lt;code&gt;dest&lt;/code&gt;. The number of components copied is
 * equal to the &lt;code&gt;length&lt;/code&gt; argument. The components at
 * positions &lt;code&gt;srcPos&lt;/code&gt; through
 * &lt;code&gt;srcPos+length-1&lt;/code&gt; in the source array are copied into
 * positions &lt;code&gt;destPos&lt;/code&gt; through
 * &lt;code&gt;destPos+length-1&lt;/code&gt;, respectively, of the destination
 * array.
 * &lt;p&gt;
</code></pre><p>也即将src中范围为<code>srcPoc~srcPoc+length-1</code> 的节点，拷贝到<code>destPos~destPos+length-1</code>，拷贝的节数数量有length个</p>
<p>实例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String[] src=&#123;<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>&#125;;</span><br><span class="line">String[] dest=&#123;<span class="string">&quot;public&quot;</span>,<span class="string">&quot;private&quot;</span>,<span class="string">&quot;protected&quot;</span>,<span class="string">&quot;this&quot;</span>&#125;;</span><br><span class="line">System.arraycopy(src,<span class="number">1</span>,dest,<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line">System.out.println(Arrays.toString(dest)); <span class="comment">//输出：[public, bar, foobar, this]</span></span><br></pre></td></tr></table></figure>
<p>将src数组中的“bar”、“foobar”这两个元素拷贝到dest的index=1中，覆盖两个元素，显然”private”,”protected”会被覆盖</p>
<h5 id="异常情况"><a href="#异常情况" class="headerlink" title="异常情况"></a>异常情况</h5><ul>
<li>ArrayIndexOutOfBoundsException</li>
</ul>
<p>那么如果拷贝的元素数量length大于dest数组长度，那么显然会抛出异常</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String[] src=&#123;<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>&#125;;</span><br><span class="line">String[] dest=&#123;<span class="string">&quot;public&quot;</span>,<span class="string">&quot;private&quot;</span>,<span class="string">&quot;protected&quot;</span>,<span class="string">&quot;this&quot;</span>&#125;;</span><br><span class="line">System.arraycopy(src,<span class="number">1</span>,dest,<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">System.out.println(Arrays.toString(dest)); <span class="comment">//ArrayIndexOutOfBoundsException</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>因为destPos+length大于dest.length，也即destPos+length=2+3=5，大于dest.length=4</p>
<p>或者srcPos+length大于src.length，表示指定要拷贝的元素数量超过实际拥有的元素</p>
<p>srcPos+length=1+3=4,大于src.length=3</p>
<ul>
<li>NullPointerException</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">System.arraycopy(null,1,dest,1+1,src.length-1);</span><br><span class="line">System.arraycopy(src,1,null,1+1,src.length-1);</span><br></pre></td></tr></table></figure>
<p>源数组为空，没元素可供拷贝，目标数组为空，拷贝的元素没地方放置</p>
<ul>
<li>ArrayStoreException</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.arraycopy(src,<span class="number">1</span>,<span class="string">&quot;test&quot;</span>,<span class="number">1</span>+<span class="number">1</span>,src.length-<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>有其中一个不是数组类型的</p>
<p>或者src和</p>
<h5 id="addAll"><a href="#addAll" class="headerlink" title="addAll"></a>addAll</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">    Object[] a = c.toArray();</span><br><span class="line">    <span class="keyword">int</span> numNew = a.length; <span class="comment">// 这里的length就是c集合实际元素的个数，不是c集合底层数组的容量长度</span></span><br><span class="line">    ensureCapacityInternal(size + numNew);  <span class="comment">// Increments modCount</span></span><br><span class="line">  	<span class="comment">// 将c集合的0~length-1范围内的元素拷贝到，elementData的[size,length-1]位置上</span></span><br><span class="line">    System.arraycopy(a, <span class="number">0</span>, elementData, size, numNew);</span><br><span class="line">    size += numNew;</span><br><span class="line">    <span class="keyword">return</span> numNew != <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="removeRange"><a href="#removeRange" class="headerlink" title="removeRange"></a>removeRange</h5><p>Removes from this list all of the elements whose index is between fromIndex, inclusive, and toIndex, exclusive。Shifts any succeeding elements to the left (reduces their index). This call shortens the list by (toIndex - fromIndex) elements</p>
<p>例如，[0,3]，size=4，现在要删除fromIndex=1，toIndex=3，其中不包括toIndex=3指向的元素</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ArrayList&lt;String&gt; src= <span class="keyword">new</span> ArrayList&lt;String&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>,<span class="string">&quot;test&quot;</span>));</span><br><span class="line">System.out.println(src.subList(<span class="number">1</span>,<span class="number">3</span>)); <span class="comment">// [bar, foobar]</span></span><br></pre></td></tr></table></figure>
<p>截取子链的数量：toIndex-fromIndex</p>
<p>底层调用：</p>
<p>[0,….fromIndex,….,toIndex,…..size-1]</p>
<p>实际要拿走的元素访问是[formIndex,toIndex-1]</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">removeRange</span><span class="params">(<span class="keyword">int</span> fromIndex, <span class="keyword">int</span> toIndex)</span> </span>&#123;</span><br><span class="line">    modCount++;  <span class="comment">//删除再多也是被计为改动一次</span></span><br><span class="line">  	<span class="comment">/*（1）</span></span><br><span class="line"><span class="comment">  	使用size对齐到索引index,因为removeRange不包括toIndex指向的节点，因此实际只删除到toIndex-1</span></span><br><span class="line"><span class="comment">  	(size-1)-(toIndex-1)=size-toIndex</span></span><br><span class="line"><span class="comment">  	（2）或者说size本身元素个数视角，toIndex相当于元素个数视角，因此要挪动元素个数就是size-toIndex个</span></span><br><span class="line"><span class="comment">  	*/</span></span><br><span class="line">    <span class="keyword">int</span> numMoved = size - toIndex; <span class="comment">//也可通过索引角度来计算</span></span><br><span class="line">  	<span class="comment">// 将toIndex到toIndex+length-1向左迁移到fromIndex~fromIndex+length-1</span></span><br><span class="line">    System.arraycopy(elementData, toIndex, elementData, fromIndex,</span><br><span class="line">                     numMoved);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// clear to let GC do its work</span></span><br><span class="line">  	<span class="comment">// 将size转为索引视角：size-1，将toIndex转为索引视角:toIndex-1，fromIndex本身就是索引视角</span></span><br><span class="line">  	<span class="comment">// 0-&gt;fromIndex-&gt;toIndex-1-&gt;size-1</span></span><br><span class="line">  	<span class="comment">//剩余的元素个数为：(size-1)-(toIndex-1-fromIndex)=size-(toIndex-fromIndex)</span></span><br><span class="line">    <span class="keyword">int</span> newSize = size - (toIndex-fromIndex);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = newSize; i &lt; size; i++) &#123;</span><br><span class="line">        elementData[i] = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    size = newSize;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="removeAll"><a href="#removeAll" class="headerlink" title="removeAll"></a>removeAll</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> ArrayList&lt;String&gt; src= <span class="keyword">new</span> ArrayList&lt;String&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>,<span class="string">&quot;test&quot;</span>));</span><br><span class="line"> ArrayList&lt;String&gt; dest= <span class="keyword">new</span> ArrayList&lt;String&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>));</span><br><span class="line">	System.out.println(src.removeAll(dest)); <span class="comment">//true</span></span><br><span class="line">System.out.println(src); <span class="comment">// [test]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">removeAll</span><span class="params">(Collection&lt;?&gt; c)</span> </span>&#123;</span><br><span class="line">    Objects.requireNonNull(c);</span><br><span class="line">    <span class="keyword">return</span> batchRemove(c, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="batchRemove"><a href="#batchRemove" class="headerlink" title="batchRemove"></a>batchRemove</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">batchRemove</span><span class="params">(Collection&lt;?&gt; c, <span class="keyword">boolean</span> complement)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Object[] elementData = <span class="keyword">this</span>.elementData; <span class="comment">// 原列表</span></span><br><span class="line">    <span class="keyword">int</span> r = <span class="number">0</span>, w = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">boolean</span> modified = <span class="keyword">false</span>;</span><br><span class="line">  	<span class="comment">// r是读索引指针，w是写索引指针</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (; r &lt; size; r++) <span class="comment">//size是原列表元素个数</span></span><br><span class="line">          	<span class="comment">// 首先原列表要删除c这个子集，那么就说要保留只在list且不在c中元素即可</span></span><br><span class="line">          <span class="comment">// c.contains(elementData[r])==fasle,说明不在c里面，就将写入到elementData的w位置</span></span><br><span class="line">          <span class="comment">// 因此最终就是0到w个保留了，而w+1到size-1被删除</span></span><br><span class="line">            <span class="keyword">if</span> (c.contains(elementData[r]) == complement)</span><br><span class="line">                elementData[w++] = elementData[r];</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// Preserve behavioral compatibility with AbstractCollection,</span></span><br><span class="line">        <span class="comment">// even if c.contains() throws.</span></span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">      比如写到第r个元素发生了异常，那么由于elementData保留到[0,w]个元素，那么直接再把r+1到size-1这一段节点迁移到从w开始处</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">if</span> (r != size) &#123;</span><br><span class="line">            System.arraycopy(elementData, r,</span><br><span class="line">                             elementData, w,</span><br><span class="line">                             size - r);</span><br><span class="line">            w += size - r; <span class="comment">// w是个数，加上拷贝过来的(length= size - r)个就是</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (w != size) &#123;</span><br><span class="line">            <span class="comment">// clear to let GC do its work</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = w; i &lt; size; i++)</span><br><span class="line">                elementData[i] = <span class="keyword">null</span>;</span><br><span class="line">            modCount += size - w;</span><br><span class="line">            size = w;</span><br><span class="line">            modified = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> modified;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>retainAll是removeAll的相反版本：</p>
<p>将list保留和c一样的元素，也即只保留c和list的交集</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public boolean retainAll(Collection&lt;?&gt; c) &#123;</span><br><span class="line">    Objects.requireNonNull(c);</span><br><span class="line">    return batchRemove(c, true);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  ArrayList&lt;String&gt; src= <span class="keyword">new</span> ArrayList&lt;String&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>,<span class="string">&quot;test&quot;</span>));</span><br><span class="line">  ArrayList&lt;String&gt; dest= <span class="keyword">new</span> ArrayList&lt;String&gt;(Arrays.asList(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foobar&quot;</span>));</span><br><span class="line">System.out.println(src.retainAll(dest));</span><br><span class="line">System.out.println(src); <span class="comment">//[foo, bar, foobar]</span></span><br></pre></td></tr></table></figure>
<h5 id="transient-在ArrayList的作用！"><a href="#transient-在ArrayList的作用！" class="headerlink" title="transient 在ArrayList的作用！"></a>transient 在ArrayList的作用！</h5><p>然后我们看一下ArrayList的源码中是实现了java.io.Serializable序列化了的，也就是transient Object[] elementData; 这行代码的意思是不希望elementData被序列化，那这时候我们就有一个疑问了，为什么elementData不进行序列化？这时候我去网上找了一下答案，觉得这个解释是最合理且易懂的“</p>
<blockquote>
<p>在ArrayList中的elementData这个数组的长度是变长的,java在扩容的时候,有一个扩容因子,也就是说这个数组的长度是大于等于ArrayList的长度的,我们不希望在序列化的时候将其中的空元素也序列化到磁盘中去，只需要那些桶位不为空的节点，所以需要手动的序列化数组对象,所以使用了transient来禁止自动序列化这个数组</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// Write out all elements in the proper order.</span></span><br><span class="line"><span class="comment">//从遍历的细节可以看出：size是实际的元素个数，因此以下是把实际元素写入流中，所以这里不是用 elementData.length作为遍历极值，因为elementData.length是数组长度容量</span></span><br><span class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;size; i++) &#123;</span><br><span class="line">     s.writeObject(elementData[i]);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>以上需要改为下面的逻辑吗：</p>
<p>注意：不能这么写，因为ArrayList的元素可以为null，如果按下面这么写，就把需要序列化的元素给排除了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;size; i++) &#123;</span><br><span class="line">  	<span class="keyword">if</span>(elementData[i] != <span class="keyword">null</span>)</span><br><span class="line">    		s.writeObject(elementData[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Iterator"><a href="#Iterator" class="headerlink" title="Iterator"></a>Iterator</h5><p>Returns an iterator over the elements in this list in proper sequence.</p>
<p>该迭代器是从列表的左到右方向迭代的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Iterator&lt;E&gt; <span class="title">iterator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Itr();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An optimized version of AbstractList.Itr</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">Itr</span> <span class="keyword">implements</span> <span class="title">Iterator</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">  	<span class="comment">// 注意以下两个指针都是索引视角</span></span><br><span class="line">    <span class="keyword">int</span> cursor;       <span class="comment">// index of next element to return</span></span><br><span class="line">    <span class="keyword">int</span> lastRet = -<span class="number">1</span>; <span class="comment">// index of last element returned; -1 if no such</span></span><br><span class="line">    <span class="keyword">int</span> expectedModCount = modCount;</span><br><span class="line"></span><br><span class="line">    Itr() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cursor != size; <span class="comment">// cursor有效指向范围跟索引一样：o到size-1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  </span><br><span class="line">  	<span class="comment">/*</span></span><br><span class="line"><span class="comment">  	观测cursor指向</span></span><br><span class="line"><span class="comment">        Iterator&lt;String&gt; iterator= src.iterator();</span></span><br><span class="line"><span class="comment">        iterator.next(); //cursor=1 ,返回是的lastRet=cursor-1值，cursor指向的值</span></span><br><span class="line"><span class="comment">        iterator.next();//cursor=2</span></span><br><span class="line"><span class="comment">        while (iterator.next()!=null)&#123;</span></span><br><span class="line"><span class="comment">            iterator.remove();</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 每次执行next()过程都会检查fail-fast</span></span><br><span class="line">        checkForComodification();</span><br><span class="line">        <span class="keyword">int</span> i = cursor; </span><br><span class="line">        <span class="keyword">if</span> (i &gt;= size)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">        Object[] elementData = ArrayList.<span class="keyword">this</span>.elementData;</span><br><span class="line">        <span class="keyword">if</span> (i &gt;= elementData.length)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">      	<span class="comment">//首次调用next()方法时：放置值是lastRet = i指向的元素，也即0指向的元素，此时cursor指针会指向下一个元素</span></span><br><span class="line">        cursor = i + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> (E) elementData[lastRet = i];</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">      	每次remove方法前必须调用next()方法，以下是错误示范</span></span><br><span class="line"><span class="comment">      	（1）</span></span><br><span class="line"><span class="comment">        while (iterator.hasNext())&#123;</span></span><br><span class="line"><span class="comment">            iterator.remove();</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        （2）仅有一次next()调用，但在第一次remove()执行完后，lastRet、cursor都会置为初始值，因此第二次remove()必然IllegalStateException</span></span><br><span class="line"><span class="comment">        iterator.next();</span></span><br><span class="line"><span class="comment">        while (iterator.hasNext())&#123;</span></span><br><span class="line"><span class="comment">            iterator.remove();</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">       </span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">        <span class="keyword">if</span> (lastRet &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">        checkForComodification();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ArrayList.<span class="keyword">this</span>.remove(lastRet);</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        ArrayList&lt;String&gt; src= new ArrayList&lt;String&gt;(Arrays.asList(&quot;foo&quot;,&quot;bar&quot;,&quot;foobar&quot;));</span></span><br><span class="line"><span class="comment">        Iterator&lt;String&gt; iterator= src.iterator();</span></span><br><span class="line"><span class="comment">        while (iterator.next()!=null)&#123;  </span></span><br><span class="line"><span class="comment">            iterator.remove();</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        可以看到，如果是上面这样的while循环，那么lastRet和cursor就会在-1,0到0，1来回变化</span></span><br><span class="line"><span class="comment">  */</span>          </span><br><span class="line">            cursor = lastRet;</span><br><span class="line">            lastRet = -<span class="number">1</span>;</span><br><span class="line">            expectedModCount = modCount;  <span class="comment">//这就是为何在迭代器中使用remove方法不会造成fail-fast原因</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IndexOutOfBoundsException ex) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  	<span class="comment">//该方法也是用于处理“剩余需要遍历的元素”</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEachRemaining</span><span class="params">(Consumer&lt;? <span class="keyword">super</span> E&gt; consumer)</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">        Objects.requireNonNull(consumer);</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> size = ArrayList.<span class="keyword">this</span>.size; <span class="comment">//元素个数</span></span><br><span class="line">        <span class="keyword">int</span> i = cursor; <span class="comment">//如果当前游标指针指向size，说明无元素处理了</span></span><br><span class="line">        <span class="keyword">if</span> (i &gt;= size) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">final</span> Object[] elementData = ArrayList.<span class="keyword">this</span>.elementData;</span><br><span class="line">        <span class="keyword">if</span> (i &gt;= elementData.length) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">// 只要i未越界且迭代过程没有其他结构性修改，</span></span><br><span class="line">        <span class="keyword">while</span> (i != size &amp;&amp; modCount == expectedModCount) &#123;</span><br><span class="line">          	<span class="comment">//也即consumer.accept((E) elementData[i])，然后在i++</span></span><br><span class="line">            consumer.accept((E) elementData[i++]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// update once at end of iteration to reduce heap write traffic</span></span><br><span class="line">        cursor = i; <span class="comment">//循环内部已经更新i指针了，其实此时i=size,cursor=size，lastRet=cursor-1</span></span><br><span class="line">        lastRet = i - <span class="number">1</span>;</span><br><span class="line">        checkForComodification();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (modCount != expectedModCount)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>forEachRemaining</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.out.println(<span class="string">&quot;first:&quot;</span>+iterator.next());</span><br><span class="line">System.out.println(<span class="string">&quot;remain:&quot;</span>);</span><br><span class="line">iterator.forEachRemaining(p-&gt;System.out.println(p)); <span class="comment">//最终cursor会指向size，lastRet指向size-1</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>first:foo<br>remain:<br>bar<br>foobar</p>
<h5 id="ListIterator具有增强功能"><a href="#ListIterator具有增强功能" class="headerlink" title="ListIterator具有增强功能"></a>ListIterator具有增强功能</h5><p>对比iterator：可以反向迭代、可以指定任意索引位置开始迭代</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An optimized version of AbstractList.ListItr</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ListItr</span> <span class="keyword">extends</span> <span class="title">Itr</span> <span class="keyword">implements</span> <span class="title">ListIterator</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">    ListItr(<span class="keyword">int</span> index) &#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        cursor = index; <span class="comment">// 可以指定从哪个索引位置开始启用迭代器</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasPrevious</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cursor != <span class="number">0</span>; <span class="comment">// 当cursor指向0时，显然迭代器当然没有previous节点了，再比如cursor=1时，迭代器还有一个previous节点即elementData[0]</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">nextIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cursor; <span class="comment">//因为cursor本身就是指向下一个索引位置,当index=0，那么cursor=0，nextIndex()也是指向0</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">previousIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> cursor - <span class="number">1</span>;  <span class="comment">// 因为cursor本身就是指向下一个索引位置,那么 cursor - 1就是前一个索引位置</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">previous</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        checkForComodification();</span><br><span class="line">        <span class="keyword">int</span> i = cursor - <span class="number">1</span>;  <span class="comment">// 例如cursor=size，由于previous是不断向左边迭代，第一次遍历时，尾部节点显然是size-1也即cursor-1指向的节点</span></span><br><span class="line">        <span class="keyword">if</span> (i &lt; <span class="number">0</span>) <span class="comment">//i的有效取值范围0到cursor-1</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">        Object[] elementData = ArrayList.<span class="keyword">this</span>.elementData;</span><br><span class="line">        <span class="keyword">if</span> (i &gt;= elementData.length)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">        cursor = i; <span class="comment">// 更新游标，将cursor指向当前处理的节点索引i</span></span><br><span class="line">        <span class="keyword">return</span> (E) elementData[lastRet = i];<span class="comment">//</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 要启动next()方法才能调用set</span></span><br><span class="line">        <span class="keyword">if</span> (lastRet &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">        checkForComodification();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ArrayList.<span class="keyword">this</span>.set(lastRet, e); <span class="comment">// 注意this.set方法里面有rangeCheck(index)，其中index&gt;=size就会抛出异常</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IndexOutOfBoundsException ex) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">      </span><br><span class="line">        checkForComodification();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> i = cursor; </span><br><span class="line">            ArrayList.<span class="keyword">this</span>.add(i, e);</span><br><span class="line">            cursor = i + <span class="number">1</span>; <span class="comment">// 增加1个元素后，cursor也要指向下一个位置</span></span><br><span class="line">            lastRet = -<span class="number">1</span>; <span class="comment">// 因为add无需返回值，因此可需要设为初始值-1</span></span><br><span class="line">            expectedModCount = modCount;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IndexOutOfBoundsException ex) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>Java高级主题</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase架构分析</title>
    <url>/2019/11/02/HBase%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的<a href="https://blog.csdn.net/pysense/article/details/102635656">文章</a>中，详细给出了HBase HA高可用部署以及测试的内容，本篇文章将对HBase架构进行分析。</p>
<h3 id="1、有关HBase基本介绍"><a href="#1、有关HBase基本介绍" class="headerlink" title="1、有关HBase基本介绍"></a>1、有关HBase基本介绍</h3><h4 id="1-1-HBase解决的痛点："><a href="#1-1-HBase解决的痛点：" class="headerlink" title="1.1 HBase解决的痛点："></a>1.1 HBase解决的痛点：</h4><ul>
<li>解决随机近实时的高效的读写</li>
<li>解决非结构化的数据存储</li>
</ul>
<h4 id="1-2-HBase应用："><a href="#1-2-HBase应用：" class="headerlink" title="1.2 HBase应用："></a>1.2 HBase应用：</h4><ul>
<li>可以存储非结构化的数据</li>
<li>被用来做实时数据分析(整合flume、storm、streaming等)<blockquote>
<p>引用HBase作为业务存储，则需要注意的点（按官方指引的翻译）：<br>首先确保使用场景有足够多的数据，上亿或者十几亿行的数据，HBase将非常适合<br>第二，确保业务需求中不需要用到关系型数据库那种严格的索引、事务、高级查询等<br>第三点，确保足够多的硬件服务器，至少5台个HDFS节点以上，以便发挥HDFS性能。</p>
</blockquote>
</li>
</ul>
<h4 id="1-3-Hbase特性："><a href="#1-3-Hbase特性：" class="headerlink" title="1.3 Hbase特性："></a>1.3 Hbase特性：</h4><ul>
<li>Hadoop的分布式、开源的、多版本的非关系型数据库</li>
<li><p>Hbase存储Key-Value格式，面向列存储，Hbase底层为byte[]比特数组数据，不存在数据类型一说。</p>
</li>
<li><p>严格一致的读写</p>
</li>
<li><p>表的自动和可配置分片</p>
</li>
<li><p>RegionServer之间的自动故障转移支持</p>
</li>
<li><p>方便的基类，用于通过Apache HBase表备份Hadoop MapReduce作业</p>
</li>
<li><p>块缓存和布隆过滤器用于实时查询</p>
</li>
</ul>
<h3 id="2、HBase架构"><a href="#2、HBase架构" class="headerlink" title="2、HBase架构"></a>2、HBase架构</h3><p><img src="https://img-blog.csdnimg.cn/20191027170545189.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">如上图所示，架构主要有以下4个部分</p>
<a id="more"></a>
<h4 id="2-1-HMaster"><a href="#2-1-HMaster" class="headerlink" title="2.1 HMaster"></a>2.1 HMaster</h4><ul>
<li>负责管理HBase的元数据，表结构，表的Region信息</li>
<li>负责表的创建，删除和修改</li>
<li>负责为HRegionServer分配Region，分配后将元数据写入相应位置</li>
</ul>
<h4 id="2-1-HRegionServer"><a href="#2-1-HRegionServer" class="headerlink" title="2.1 HRegionServer"></a>2.1 HRegionServer</h4><ul>
<li>存储多个HRegion</li>
<li>处理Client端的读写请求（根据从HMaster返回的元数据找到对应的HRegionServer）</li>
<li>管理Region的Split分裂、StoreFile的Compaction合并。</li>
<li>一个RegionServer管理着多个Region，在HBase运行期间，可以动态添加、删除HRegionServer。</li>
</ul>
<h4 id="2-3-HRegion"><a href="#2-3-HRegion" class="headerlink" title="2.3 HRegion"></a>2.3 HRegion</h4><ul>
<li>一个HRegion里可能有1个或多个Store（参考上图）。</li>
<li>HRegionServer维护一个HLog。</li>
<li>HRegion是HBase分布式存储和负载的最小单元，但不是HBase数据存储到文件的最小单元。</li>
<li>表通常被保存在多个HRegionServer的多个Region中。</li>
</ul>
<h4 id="2-4-Store"><a href="#2-4-Store" class="headerlink" title="2.4 Store"></a>2.4 Store</h4><ul>
<li>Store由内存中的MemStore和磁盘中的若干StoreFile组成（参考上图）</li>
<li>一个Store里有1个或多个StoreFile和一个memStore。</li>
<li><p>每个Store存储一个列族。</p>
<h4 id="2-5-MemStore、StoreFile、HFile"><a href="#2-5-MemStore、StoreFile、HFile" class="headerlink" title="2.5 MemStore、StoreFile、HFile"></a>2.5 MemStore、StoreFile、HFile</h4></li>
<li>MemStore：<br>首先，memStore 是在内存中存在，保存修改key-value数据；当memStore的大小达到一个阀值（默认64MB）时，memStore会被flush到文件，也就是存在磁盘上。</li>
<li>StoreFile：<br>接上面内容，memStore的每次flush操作都会生成一个新的StoreFile，StoreFile底层是以HFile的格式保存，当有多个StoreFile后，将会触发为合并为一个大的StoreFile。</li>
<li>HFile：<br>HFile是HBase中KeyValue数据的存储格式，在hdfs上是二进制格式文件，一个StoreFile对应着一个HFile。HFile有自己的数据结构。<br>HFile写入的时候，分一个块一个块的写入，每个Block块64KB，这样有利于数据的随机访问，不利于连续访问，若连续访问需求大，可将Block块设为较大值。<br>HFile物理文件形式参考下面4.3内容</li>
</ul>
<h4 id="2-6-WALs——Write-Ahead-Log预写日志（HLog）"><a href="#2-6-WALs——Write-Ahead-Log预写日志（HLog）" class="headerlink" title="2.6 WALs——Write-Ahead-Log预写日志（HLog）"></a>2.6 WALs——Write-Ahead-Log预写日志（HLog）</h4><p>&#8195;&#8195;数据库在事务机制中常见的一致性的实现方式就是通过记录日志，通过日志文件的方式实现写入一致性、数据恢复或者数据备份，那么对于HBases也有同样的逻辑，因为大型分布式系统中硬件故障很常见，如果MemStore还没有及时flush到HFile，服务器宕机或者断电，那么MemStore部分的数据肯会丢失。<br>&#8195;&#8195;HBase给出的解决方案：先写入MemStore中，然后更新hlog中，只有成功更新到hlog之后，写操作才能被认为是成功完成。如果在MemStore没有写到hlog之前宕机，HBase重启后可以从hlog恢复。Hbase集群中每台服务器维护一个hlog文件。</p>
<p>==<strong>hlog过期</strong>==<br>&#8195;&#8195;当数据从memstore写入到磁盘中，Hlog就已经没有用了，会把/hbase/WALs目录下的数据移动到/hbase/oldWALs 目录下，oldWALs目录下的数据会根据 hbase.master.cleaner.interval (默认1分钟)配置的时间定期去检查，如发现有数据会清除，清除前还会检验一个参数 hbase.master.logcleaner.ttl ，也就是说数据保存1分钟以上才会删除，如果一分钟内数据直接从memstore写入到了磁盘，oldWALs下的数据也不会被删除</p>
<h4 id="2-7-zookeeper"><a href="#2-7-zookeeper" class="headerlink" title="2.7 zookeeper"></a>2.7 zookeeper</h4><ul>
<li>通过选举（抢占zk临时节点），保证任何时候，集群中只有一个master，Master与RegionServers 启动时会向ZooKeeper注册</li>
<li>RegionServer 向zookeeper的临时znode注册，提供RegionsSrver状态信息（是否在线）。</li>
<li>存放所有Region的寻址入口</li>
<li>存储HBase的schema和table元数据</li>
<li>HMaster启动时候会将hbase系统表-ROOT- 加载到 zookeeper cluster，通过zookeeper cluster可以获取当前系统表元信息的存储所对应的RegionServer信息。</li>
</ul>
<h3 id="3、数据模型"><a href="#3、数据模型" class="headerlink" title="3、数据模型"></a>3、数据模型</h3><p>&#8195;&#8195;这里以Amandeep Khurana的<a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf">《Introduction to HBase Schema Design》</a>作为参考<br>&#8195;&#8195;HBase的表结构如下图3所示：<br><img src="https://img-blog.csdnimg.cn/20191027173035586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;该表有两个列簇： Personal and Office，每个列簇有两个列，例如Personal：姓名和家庭电话，每个小格子cells用来存储真正数据，例如John及其电话，但Row Key 不属于cell。</p>
<p>&#8195;&#8195;HBase行记录的数据结构，其实就是一个嵌套map（嵌套字典），HBase用这种方式组织了数据，以上图的第一行John为例，该0001行字典如下：<br><img src="https://img-blog.csdnimg.cn/20191027173150411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;因为大家更熟悉hash map或字典结构，从上图可以看到，HBase的数据单元cell，通过把timestamp作为最里层字典的key，从而数据多版本的控制。看到该map结构，大家可以联想到mangodb，这伙计大有赶超hadoop之势，同样具备分布式大数据的存储、查询能力，它也用这种结构来存储数据，只不过mangodb用的粒度更小是json结构。</p>
<h3 id="4、表结构组成（对应图3）"><a href="#4、表结构组成（对应图3）" class="headerlink" title="4、表结构组成（对应图3）"></a>4、表结构组成（对应图3）</h3><p>Table（表格）<br>一个HBase表格由多行组成。</p>
<p>Row（行）<br>&#8195;&#8195;RowKey没有特定的数据类型，都是字节数组类型，任何字符串都可以作为行键表的中行数据按照Rowkey的字节（byte order) 排序存储HBase中的行里面包含一个key和一个或者多个包含值的列。行按照行的key字母顺序存储在表格中。因为这个原因，行的key的设计就显得非常重要。数据的存储设计目的是让数据类型相近的数据存储到一起。</p>
<p>Column Family（列族）<br>&#8195;&#8195;因为性能的原因，列族物理上包含一组列和它们的值。每一个列族拥有一系列的存储属性，例如值是否缓存在内存中，数据是否要压缩或者他的行key是否要加密等等。表格中的每一行拥有相同的列族，尽管一个给定的行可能没有存储任何数据在一个给定的列族中。</p>
<p>Column Qualifier（列名）<br>&#8195;&#8195;列族必须定义表时给出，每个列族可以有一个或多个列成员（Column Qualigier)，列成员不需要在定义表时给出，新的列族成员（name、phone）可以随后按需动态加入。列族Personal在创建表格时已被确定，但是列名则可以以后业务需要时动态追加，例如给Personal增加一个列名:company字段，用于存储个人的公司信息</p>
<p>Cell（单元）</p>
<p>&#8195;&#8195;单元是由行、列族、列限定符、值和代表值版本的时间戳组成的。Cell由RowKey，列族：列名（Column Qualifier），时间戳唯一决定。Cell中的数据是没有类型的，全部以字节码形式存储。每个单元格保存着同一份数据的多个版本<br>，不同时间版本的数据按照时间顺序倒序排序。</p>
<p>Timestamp（时间戳）<br> &#8195;&#8195;每个Cell可能有多个版本，它们之间用时间戳区分。</p>
<h3 id="5、HBase物理文件存储过程"><a href="#5、HBase物理文件存储过程" class="headerlink" title="5、HBase物理文件存储过程"></a>5、HBase物理文件存储过程</h3><h4 id="5-1-HRegion在表的上位置"><a href="#5-1-HRegion在表的上位置" class="headerlink" title="5.1 HRegion在表的上位置"></a>5.1 HRegion在表的上位置</h4><p>&#8195;&#8195;HBase的一个典型Big Table 在行的方向上水平分割为多个HRegion，所有行都按照row key的字典序排列。（注意如果Table表刚建初期，数据量不多，此时Table仅有一个HRegion）<br><img src="https://img-blog.csdnimg.cn/20191028225523874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="4-2-HRegion自动拆分"><a href="#4-2-HRegion自动拆分" class="headerlink" title="4.2 HRegion自动拆分"></a>4.2 HRegion自动拆分</h4><p>&#8195;&#8195;HRegion按照大小拆分，每个Table初始只有一个HRegion，随着数据不断插入表，Rowkey越来越多，HRegion不断增大，当增大到一个阀值（默认256M）的时候，HRegion会被拆分为两个新的HRegion，如下图所示，所以当Table中的行不断增多，就会有越有更多HRegion，这些HRegion分布式存在多台HRegion Server上，只要有足够的节点服务器，那么HBase就可以继续被扩容，存下百亿行都OK。在传统关系型数据库中，这种体量的行数，很难想象！<br><img src="https://img-blog.csdnimg.cn/20191028232225435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="4-3-HRegion-分布式存储"><a href="#4-3-HRegion-分布式存储" class="headerlink" title="4.3 HRegion 分布式存储"></a>4.3 HRegion 分布式存储</h4><p>&#8195;&#8195;HRegion是Hbase中分布式存储和负载均衡的最小单元，同一个table的不同HRegion可以分布式存储在不同的HRegion Server上，这里说的“负载均衡”是指：例如下图，如果table的所有HRegion都扔到一台服务器上存储，容易出现不平衡分布（数据倾斜），因此需要考虑所有的Region平衡分布到每个节点上，如下图所示，但一个HRegion是不会拆分到多个节点上，因为HRegion是HMaster分布式存储可以管理的最小单元。（机灵的同学此时会想到：同一个HRegion需要拷贝三份或多份再存到不同节点上吗？注意：这里完全不需要，因为HRegion最终存放是在HDFS上的一个二进制文件block，hdfs本身就会将这个block自动拷贝多份，再存到其他节点。）<br><img src="https://img-blog.csdnimg.cn/2019102900132276.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="5-2-Hbase数据在HDFS的存储"><a href="#5-2-Hbase数据在HDFS的存储" class="headerlink" title="5.2 Hbase数据在HDFS的存储"></a>5.2 Hbase数据在HDFS的存储</h4><p>&#8195;&#8195;HRegion虽然是分布式存储的最小单元，但并不是存储的最小单元。HRegion由一个或者多个Store组成，每个Store保存一个Columns Family。每个Store又由一个memStore和0至多个StoreFile组成。StoreFile最终以HFile格式保存在HDFS上<br><img src="https://img-blog.csdnimg.cn/20191029213447409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;对于company表，上面提到每个Store保存一个Columns Family，那么在HDFS上是怎样的文件呢？通过查看HBase在hdfs文件系统创建的data目录如下所示：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# hdfs dfs -ls /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 root supergroup         42 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/.regioninfo</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/recovered.edits</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/staff_info</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;Hbase的数据在HDFS中的路径结构如下：<br>==hdfs://dn2:9000/hbase/data/{名字空间}/{表名}/{区域名称}/{列族名称}/{文件名}==<br>具体说明：<br>hdfs://dn2:9000：hdfs分布式节点dn2为该HBase提供底层文件服务</p>
<ul>
<li>{名字空间}：在本文的示例中，因为在hbase shell建表做测试时，没有创建新的名字空间（相当于关系型数据库的database），所以hbase为company table提供默认default空间</li>
<li>{表名}:具体的HBase table名称，如本例的company table</li>
<li>{区域名称}：指HRegion的字符串名称：38445bbc84c64c82b8273edafbd19b07  由每张表切割形成，table创建开始阶段，仅有一个HRegion，当table越来越大，路径<code>hdfs://dn2:9000/hbase/data/&#123;名字空间&#125;/&#123;表名&#125;/</code>下将会有多个“区域名称”（也即分割为多个HRegion），这就是HRegion在hdfs的存在形式。</li>
<li>{列族名称}：例如本例创建的depart_info和staff_info 这两个列簇</li>
<li>{文件名}：这个文件就是Hfile，每个列簇下有自己存放key-value数据的最终物理文件，这里就是HBase存放数据的最小物理文件。</li>
</ul>
<p>以depart_info列簇查看它包含的文件有：bdccf10bf1a344baa68c4404b385da7b和dddb8e7e43134c16823db24416<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@nn ~]# hdfs dfs -ls /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup       5082 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info/bdccf10bf1a344baa68c4404b385da7b</span><br><span class="line">-rw-r--r--   3 root supergroup       4859 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info/dddb8e7e43134c16823db24416</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;继续查看depart_info路径下bdccf10bf1a344baa68c4404b385da7b文件内容，该文件为二进制文件，从内容可以大致看到该文件已经存储了R1和R2两行数据内容以及一些元信息等<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DATABLK*ˇˇˇˇˇˇˇˇ@&quot;&quot;R1depart_infoinner_telmÌµçS108R1depart_infolevelmÌµq19</span><br><span class="line">R1depart_infonamemÌµWOHR</span><br><span class="line">&quot;R2depart_infoinner_telmÌ∑£106R2depart_infolevelmÌ∑äF9R2depart_infonamemÌ∑n6Finance$©…]BLMFBLK2ˇˇˇˇˇˇˇˇ@%ÈíT@3+D4IDXROOT23&#x2F;ˇˇˇˇˇˇˇˇ@P&amp;&quot;R1depart_infoinner_telmÌµçS…“?IDXROOT2O@!ÁµãÏFILEINF2îêˇˇˇˇˇˇˇˇ@±PBUFä</span><br><span class="line"></span><br><span class="line">BLOOM_FILTER_TYPEROW</span><br><span class="line"></span><br><span class="line">DELETE_FAMILY_COUNT</span><br><span class="line"></span><br><span class="line">EARLIEST_PUT_TSmÌµWO</span><br><span class="line"></span><br><span class="line">KEY_VALUE_VERSION</span><br><span class="line"></span><br><span class="line">LAST_BLOOM_KEYR2</span><br><span class="line"></span><br><span class="line">MAJOR_COMPACTION_KEY</span><br><span class="line"></span><br><span class="line">MAX_MEMSTORE_TS_KEY</span><br><span class="line"></span><br><span class="line">MAX_SEQ_ID_KEY</span><br><span class="line"></span><br><span class="line">	TIMERANGEmÌµWOmÌ∑£</span><br><span class="line"></span><br><span class="line">hfile.AVG_KEY_LEN</span><br><span class="line"></span><br><span class="line">hfile.AVG_VALUE_LEN</span><br><span class="line"> </span><br><span class="line">hfile.CREATE_TIME_TSmÌÔ∂</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">hfile.LASTKEYR2depart_infonamemÌ∑n6˙x%ªBLMFMET2&lt;8ˇˇˇˇˇˇˇˇ@Y&amp;)R1Z8˚TRABLK&quot;$H»œ&#x2F; Ú&amp;(08@HPZ-org.apache.hadoop.hbase.KeyValue$KVComparator&#96;</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;在namenode web页面：<a href="http://dn2:50070/">http://dn2:50070/</a> 可以查看HFile相关信息，如下图所示，该HFile在HDFS集群上是3份拷贝的分布式存储。<br><img src="https://img-blog.csdnimg.cn/20191029225007131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="6、HBase目录结构"><a href="#6、HBase目录结构" class="headerlink" title="6、HBase目录结构"></a>6、HBase目录结构</h3><p>&#8195;&#8195;HMaser在hdfs文件系统上创建了HBase目录，该目录用于存放所有关于HBase文件、数据、元信息等内容。</p>
<ul>
<li>/hbase/.tmp: 临时目录，当对表做创建和删除操作时，会将表move到该目录下，然后进行操作。</li>
<li>/hbase/WALs:在2.6章节已经提到，该目录为保存操作日志hlog，如下所示：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]#  hdfs dfs -ls &#x2F;hbase&#x2F;WALs&#x2F;</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   * &#x2F;hbase&#x2F;WALs&#x2F;dn1,16020,*</span><br><span class="line">drwxr-xr-x   * &#x2F;hbase&#x2F;WALs&#x2F;dn2,16020,*</span><br><span class="line">drwxr-xr-x   * &#x2F;hbase&#x2F;WALs&#x2F;nn,16020,*</span><br></pre></td></tr></table></figure>
从WALs目录下可以看到每个HRegionServer维护自己一个hlog</li>
<li>/hbase/data:核心目录，存储Hbase表的数据默认情况下该目录下有两个目录</li>
<li>/hbase/data/default:当在用户创建表的时候，没有指定namespace时，表就创建在此目录下<br>– /hbase/data/hbase：系统内部创建的表，hbase:meta，namespace</li>
<li>/hbase/hbase.id：存储集群唯一cluster id(UUID)</li>
<li>/hbase/hbase.version：集群版本号</li>
<li>/hbase/oldWALs：参考2.6章节内容——hlog过期</li>
</ul>
<h3 id="7、HBase的读写流程"><a href="#7、HBase的读写流程" class="headerlink" title="7、HBase的读写流程"></a>7、HBase的读写流程</h3><p>&#8195;&#8195;之所以把该章节安排在文章最后，是因为基于前面讨论HBase内部结构已经有一定了解后，再分析其读写流程则显得更容易理解。</p>
<h4 id="7-1-写流程"><a href="#7-1-写流程" class="headerlink" title="7.1 写流程"></a>7.1 写流程</h4><p>1）Client从Zookeeper中获取表region相关信息，根据要插入的rowkey，获取指定的Regionserver信息。<br>2）数据被写入Region的MemStore（操作也同时写入到Hlog中），若持续写入数据量超过MemStore达到预设阈值，MemStore会flush到StoreFile，当数据都同时写入到MemStore和Hlog后，那么对于client来说，本次写入即完成。<br>3）随着StoreFile文件的不断增多，当其数量增长到一定阈值后，触发Compact合并操作，将多个StoreFile合并成一个StoreFile。StoreFiles通过不断的Compact合并操作，逐步形成越来越大的StoreFile。<br>4）单个StoreFile大小超过一定阈值后，触发Split操作，把当前Region Split成2个新的Region。旧Region会下线，新Split出的2个Region会被HMaster分配到相应的RegionServer上，使得原先1个Region的压力得以分流到2个Region上。</p>
<h4 id="7-2-读流程"><a href="#7-2-读流程" class="headerlink" title="7.2 读流程"></a>7.2 读流程</h4><p>1）跟写流程一样，client先连接ZooKeeper，访问Meta Regionserver节点信息，把HBase的meta表缓存到client端。<br>2）根据rowkey找到Region，再去相应Regionserver 发起读请求。<br>3）RegionServer接收该读请求之后，既然是来询问key值是否存在，那么HBase针对检索，有自己一套方法：先查询MemStore，如果未找到，再去查询BlockCache加速读内容的缓冲区，如果还没有找到，就会到StoreFile（HFile）中查找这条数据，然后将这条数据返回给client。（注意：这里忽略了一个有关key高效检索是否存在与于HFIle的知识点：bloom-filter）<br>（从client在HBase读写请求过程可知，client其实无需与HMaster通信，只需要知道ZooKeeper地址即可）</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本文已经完成对HBase架构及其数据模型较为全面的讨论，接下来在另外一篇文章开启对Hive的部署和架构分析，Hive跟HBase、Hadoop结合使用后，形成一套可用的数据仓库。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：LongAdder高并发计数性能分析</title>
    <url>/2021/10/22/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9ALongAdder%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AE%A1%E6%95%B0%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>本文基于本博客已发布的CHM文章中分析fullAddCount方法基础上，介绍LongAdder这个高并发计数性能，并通过与原子计数器AtomicLong进行比较，最后给出Striped64类相关分析（因其内部真正实现了高并发计数逻辑）。</p>
<p>为何需要介绍下LongAdder类？<br>因为在fullAddCount方法的定义上，因为在CHM源代码上，Doug Lea有提到：fullAddCount的设计原理可通过LongAdder这个类得到相关解释：：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// See LongAdder version for explanation</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">fullAddCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">boolean</span> wasUncontended)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line"><span class="comment">// 省略部分  ......</span></span><br></pre></td></tr></table></figure>
<h4 id="1、两种并发计数器的性能比较"><a href="#1、两种并发计数器的性能比较" class="headerlink" title="1、两种并发计数器的性能比较"></a>1、两种并发计数器的性能比较</h4><p>longAdderCost和atomicLongCost的目的：指定一定数量的线程并发执行该逻辑：每个线程计数从0增加到1000*1000</p>
<a id="more"></a>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> juc.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.LongAdder;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicLong;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongAdderDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span>[] scales=&#123;<span class="number">1</span>,<span class="number">4</span>,<span class="number">16</span>,<span class="number">128</span>,<span class="number">512</span>,<span class="number">1024</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> loops=<span class="number">1000</span>*<span class="number">1000</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> scale : scales) &#123;</span><br><span class="line">            longAdderCost(scale,loops);</span><br><span class="line">            atomicLongCost(scale,loops);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">longAdderCost</span><span class="params">(<span class="keyword">int</span> threadNums,<span class="keyword">int</span> loops)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        LongAdder longAdder=<span class="keyword">new</span> LongAdder();</span><br><span class="line">        List&lt;Thread&gt; threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> start= System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadNums; i++) &#123;</span><br><span class="line">            Thread t= <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> loop = <span class="number">0</span>; loop &lt; loops; loop++) &#123;</span><br><span class="line">                    longAdder.increment();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.join();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> duration=System.currentTimeMillis()-start;</span><br><span class="line">        String s= String.format(<span class="string">&quot;LongAdder：线程数为%s,计算%s次,总用时%sms&quot;</span>,threadNums,loops,duration);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">atomicLongCost</span><span class="params">(<span class="keyword">int</span> threadNums,<span class="keyword">int</span> loops)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        AtomicLong atomicLong=<span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">        List&lt;Thread&gt; threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> start= System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadNums; i++) &#123;</span><br><span class="line">           Thread t= <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">               <span class="keyword">for</span> (<span class="keyword">int</span> loop = <span class="number">0</span>; loop &lt; loops; loop++) &#123;</span><br><span class="line">                   atomicLong.incrementAndGet();</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;);</span><br><span class="line">           threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">           thread.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">           thread.join();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> duration=System.currentTimeMillis()-start;</span><br><span class="line">        String s= String.format(<span class="string">&quot;AtomicLong：线程数为%s,计算%s次,总用时%sms&quot;</span>,threadNums,loops,duration);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">LongAdder：线程数为<span class="number">1</span>,计算<span class="number">1000000</span>次,总用时205ms</span><br><span class="line">AtomicLong：线程数为<span class="number">1</span>,计算<span class="number">1000000</span>次,总用时17ms</span><br><span class="line">LongAdder：线程数为<span class="number">4</span>,计算<span class="number">1000000</span>次,总用时38ms</span><br><span class="line">AtomicLong：线程数为<span class="number">4</span>,计算<span class="number">1000000</span>次,总用时102ms</span><br><span class="line">LongAdder：线程数为<span class="number">16</span>,计算<span class="number">1000000</span>次,总用时64ms</span><br><span class="line">AtomicLong：线程数为<span class="number">16</span>,计算<span class="number">1000000</span>次,总用时351ms</span><br><span class="line">LongAdder：线程数为<span class="number">128</span>,计算<span class="number">1000000</span>次,总用时503ms</span><br><span class="line">AtomicLong：线程数为<span class="number">128</span>,计算<span class="number">1000000</span>次,总用时2833ms</span><br><span class="line">LongAdder：线程数为<span class="number">512</span>,计算<span class="number">1000000</span>次,总用时2092ms</span><br><span class="line">AtomicLong：线程数为<span class="number">512</span>,计算<span class="number">1000000</span>次,总用时12953ms</span><br><span class="line">LongAdder：线程数为<span class="number">1024</span>,计算<span class="number">1000000</span>次,总用时4007ms</span><br><span class="line">AtomicLong：线程数为<span class="number">1024</span>,计算<span class="number">1000000</span>次,总用时26639ms</span><br></pre></td></tr></table></figure>
<p>对以上数据进行作图如下：</p>
<p><img src="https://img-blog.csdnimg.cn/6c72a0eb2a7940a6a934d5cfe2633257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>可以非常清楚看到两者存在性能差异：</p>
<p>（1）剔除单线程数据，LongAdder并发计数耗时最少，并且随着线程数量增加，并发计数耗时趋势相对稳定</p>
<p>（2）剔除单线程数据，AtomicLong并发计数耗时长，并且随着线程数量增加，并发计数性耗时基本呈指数上升趋势</p>
<h4 id="2、两者性能差异的原因分析"><a href="#2、两者性能差异的原因分析" class="headerlink" title="2、两者性能差异的原因分析"></a>2、两者性能差异的原因分析</h4><h5 id="2-1-单线程情况"><a href="#2-1-单线程情况" class="headerlink" title="2.1 单线程情况"></a>2.1 单线程情况</h5><p>首先对于单线程情况下，也即线程数量为1，因为LongAdder的increment方法里面调用add方法且在add方法内部需要做一些基本判断，这些代码的执行需要消耗一定时间：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Adds the given value.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> x the value to add</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 相当于ConcurrentHashMap的AddCount方法的分支1逻辑</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">long</span> x)</span> </span>&#123;</span><br><span class="line">      Cell[] as; <span class="keyword">long</span> b, v; <span class="keyword">int</span> m; Cell a;</span><br><span class="line">      <span class="keyword">if</span> ((as = cells) != <span class="keyword">null</span> || !casBase(b = base, b + x)) &#123;</span><br><span class="line">          <span class="keyword">boolean</span> uncontended = <span class="keyword">true</span>;</span><br><span class="line">          <span class="keyword">if</span> (as == <span class="keyword">null</span> || (m = as.length - <span class="number">1</span>) &lt; <span class="number">0</span> ||</span><br><span class="line">              (a = as[getProbe() &amp; m]) == <span class="keyword">null</span> ||</span><br><span class="line">              !(uncontended = a.cas(v = a.value, v + x)))</span><br><span class="line">            	<span class="comment">// 相当于ConcurrentHashMap的fullAddCount的逻辑</span></span><br><span class="line">              longAccumulate(x, <span class="keyword">null</span>, uncontended);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Equivalent to &#123;<span class="doctag">@code</span> add(1)&#125;.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">increment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      add(<span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对比atomicLong.incrementAndGet()方法：该方法直接进行计数，代码执行流程显然比上面要快</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">incrementAndGet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> unsafe.getAndAddLong(<span class="keyword">this</span>, valueOffset, <span class="number">1L</span>) + <span class="number">1L</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此单线程情况下，从实际测试结果来看，AtomicLong肯定会比LongAdder快</p>
<h5 id="2-2-多线程并发计数情况"><a href="#2-2-多线程并发计数情况" class="headerlink" title="2.2 多线程并发计数情况"></a>2.2 多线程并发计数情况</h5><p>当线程数量开始增加时，LongAdder计数比AtomicLong快，要知道这两个都是使用lockness机制也即Unsafe的CAS去实现的，为何两者性能差距这么明显？</p>
<p>主要还是两者的设计思路导致的</p>
<p>（1）对于AtomicLong来说：incrementAndGet方法如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">incrementAndGet</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> unsafe.getAndAddLong(<span class="keyword">this</span>, valueOffset, <span class="number">1L</span>) + <span class="number">1L</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>内部调用的是getAndAddLong方法，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">getAndAddLong</span><span class="params">(Object o, <span class="keyword">long</span> offset, <span class="keyword">long</span> delta)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> v;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        v = getLongVolatile(o, offset);</span><br><span class="line">    &#125; <span class="keyword">while</span> (!compareAndSwapLong(o, offset, v, v + delta));</span><br><span class="line">    <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到其设计相对简陋，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/8addaedf6ca74783b40c1e63e963c062.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>while自旋+对offset进行CAS加delta值计数，也即当有1000个线程并发去incrementAndGet时，由于线程竞争激烈，导致每轮自旋只能有1个线程成功拿到本轮的CAS，那么最后效果：越多线程参与计数效率就越慢，因为竞争激烈时，可能有一部分线程一直竞争CAS都失败，它们占用cpu时间片不说，还未做“加1计数”的贡献。</p>
<p>打个通俗比喻：</p>
<p>有一个窗口放着一个小黑版，上面写着一个count，小黑板每次只让一个人对其加1计数，如果采用AtomicLong的“while自旋+CAS”设计，当有1000个人同时想在小黑板做加1计数时，每轮只能有一个人走到窗口前在小黑板加1，只要这个人还没写完，那么其他999个人就得一起不断来回走到窗口前看看能否抢到“小黑班写的权利”，显然这种“一起来回走动”浪费资源，效率也低。</p>
<p>（2）对于LongAdder来说：increment内部其实使用是相对高效的、能降低线程竞争的设计：longAccumulate，其实也是ConcurrentHashMap里面的fullAddCount方法，原理图在CHM的源代码已经给出，如下图所示<br><img src="https://img-blog.csdnimg.cn/5c320a9b9aa543d2aa19a0f41840f54c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>设计思想：</p>
<p>（1）当线程竞争不激烈情况下，通过自旋+cas对baseCount进行加1计数，这一阶段类似AtomicLong的计数逻辑</p>
<p>（2）当线程竞争十分激烈的情况下，有一部分线程很幸运能够抢到cas权力成功对baseCount加1，而剩下对baseCount加1cas失败的线程，它们就会创建一个CounterCells计数的数组，然后线程给对应自己的桶位Cell对象进行cas加1操作，这样一来就实现了“线程分流”，减少竞争。</p>
<p>亮点设计在于这个“CounterCells计数的数组”</p>
<p>打个通俗比喻（同上）：</p>
<p>当人数量少时，开1个窗口给他们在小黑板写</p>
<p>当人数量多，多开几个窗口，例如开8个窗口，每个窗口都放置一块小黑板，那么</p>
<p>每个窗口平均下来也就125个人在竞争，相比于之前1000个人激烈竞争1个窗口，</p>
<p>这个窗口最大数量跟cpu数量一致，能充分利用每个cpu core，显然性能就上来了。</p>
<p>其实两者的性能说明在LongAdder的源代码注释已给出：</p>
<blockquote>
<p>This class is usually preferable to AtomicLong when multiple threads update a common sum that is used for purposes such as collecting statistics, not for fine-grained synchronization control. Under low update contention, the two classes have similar characteristics. But under high contention, expected throughput of this class is significantly higher, at the expense of higher space consumption.</p>
</blockquote>
<p>在线程竞争不激烈的写（更新）操作，两者性能类似。在线程竞争激烈情况下，也即线程并发高时，LongAdder拥有更高的throughput（一般指吞吐量，在这里可以理解为并发计数量），但代价是占用更多内存（上面的例子代价就是多开几个窗口）</p>
<p>LongAdder是典型的空间换时间的设计！</p>
<h4 id="3、Striped64类分析"><a href="#3、Striped64类分析" class="headerlink" title="3、Striped64类分析"></a>3、Striped64类分析</h4><p>考虑到CHM的fullAddCount源代码设计其实就是LongAdder里面的longAccumulate方法而该方法来自Striped64类，因此本文不再给出longAccumulate的源代码解析，具体可参考之前的文章：</p>
<p>关于LongAdder类，因为其源代码设计简单，也不再作为详细说明，本文重点讲解Striped64的设计:</p>
<p>LongAdder继承自Striped64</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LongAdder</span> <span class="keyword">extends</span> <span class="title">Striped64</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">7249069246863182397L</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>Striped64继承自Number，源代码总共413行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Striped64</span> <span class="keyword">extends</span> <span class="title">Number</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-1-Cell内部类"><a href="#3-1-Cell内部类" class="headerlink" title="3.1 Cell内部类"></a>3.1 Cell内部类</h5><p>在线程对base cas发生激烈冲突时，线程通过创建Cell数据并对桶位上的Cell进行cas加值操作，可以看到其内部独立使用了Unsafe机制，并且定义了一个更新值的cas方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Padded variant of AtomicLong supporting only raw accesses plus CAS.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * JVM intrinsics note: It would be possible to use a release-only</span></span><br><span class="line"><span class="comment"> * form of CAS here, if it were provided.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@sun</span>.misc.Contended <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Cell</span> </span>&#123;</span><br><span class="line">    <span class="keyword">volatile</span> <span class="keyword">long</span> value;</span><br><span class="line">    Cell(<span class="keyword">long</span> x) &#123; value = x; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">cas</span><span class="params">(<span class="keyword">long</span> cmp, <span class="keyword">long</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> UNSAFE.compareAndSwapLong(<span class="keyword">this</span>, valueOffset, cmp, val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unsafe mechanics</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> sun.misc.Unsafe UNSAFE;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> valueOffset;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            UNSAFE = sun.misc.Unsafe.getUnsafe();</span><br><span class="line">            Class&lt;?&gt; ak = Cell.class;</span><br><span class="line">            valueOffset = UNSAFE.objectFieldOffset</span><br><span class="line">                (ak.getDeclaredField(<span class="string">&quot;value&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Error(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Striped64的Cell类对比CHM fullAddCount CounterCell计数类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* ---------------- Counter support -------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A padded cell for distributing counts.  Adapted from LongAdder</span></span><br><span class="line"><span class="comment"> * and Striped64.  See their internal docs for explanation.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@sun</span>.misc.Contended <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterCell</span> </span>&#123;</span><br><span class="line">    <span class="keyword">volatile</span> <span class="keyword">long</span> value;</span><br><span class="line">    CounterCell(<span class="keyword">long</span> x) &#123; value = x; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-2-其他成员变量"><a href="#3-2-其他成员变量" class="headerlink" title="3.2 其他成员变量"></a>3.2 其他成员变量</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/** Number of CPUS, to place bound on table size */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> NCPU = Runtime.getRuntime().availableProcessors();</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Table of cells. When non-null, size is a power of 2.也即fullAddCount里面的CounterCell[]数组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> Cell[] cells;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Base value, used mainly when there is no contention, but also as</span></span><br><span class="line"><span class="comment"> * a fallback during table initialization races. Updated via CAS.</span></span><br><span class="line"><span class="comment"> * 也即fullAddCount里面的baseCount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">long</span> base;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spinlock (locked via CAS) used when resizing and/or creating Cells.</span></span><br><span class="line"><span class="comment"> * 也即fullAddCount里面cellsBusy</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">int</span> cellsBusy;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Package-private default constructor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">Striped64() &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CASes the base field.</span></span><br><span class="line"><span class="comment"> * 更新base的操作已经被独立封装成一个内部方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">casBase</span><span class="params">(<span class="keyword">long</span> cmp, <span class="keyword">long</span> val)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> UNSAFE.compareAndSwapLong(<span class="keyword">this</span>, BASE, cmp, val);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CASes the cellsBusy field from 0 to 1 to acquire lock.</span></span><br><span class="line"><span class="comment"> 对cellsBusy操作已经被独立封装成一个内部方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">casCellsBusy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> UNSAFE.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the probe value for the current thread.</span></span><br><span class="line"><span class="comment"> * Duplicated from ThreadLocalRandom because of packaging restrictions.</span></span><br><span class="line"><span class="comment"> * probe范围已探针，在这里，可以理解为与当前线程一一对应的随机值，也即键值对：（当前线程，线程对应的随机值）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">getProbe</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> UNSAFE.getInt(Thread.currentThread(), PROBE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Pseudo-randomly advances and records the given probe value for the</span></span><br><span class="line"><span class="comment"> * given thread.</span></span><br><span class="line"><span class="comment"> * Duplicated from ThreadLocalRandom because of packaging restrictions.</span></span><br><span class="line"><span class="comment"> * 当线程未能成功对桶位Cell进行CAS加值时，就给该线程换一个probe值，使得线程hash定位能够尽量定到不同的桶位上</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">advanceProbe</span><span class="params">(<span class="keyword">int</span> probe)</span> </span>&#123;</span><br><span class="line">    probe ^= probe &lt;&lt; <span class="number">13</span>;   <span class="comment">// xorshift</span></span><br><span class="line">    probe ^= probe &gt;&gt;&gt; <span class="number">17</span>;</span><br><span class="line">    probe ^= probe &lt;&lt; <span class="number">5</span>;</span><br><span class="line">    UNSAFE.putInt(Thread.currentThread(), PROBE, probe); <span class="comment">// Unsafe操作对象是当前线程，更改的值为probe</span></span><br><span class="line">    <span class="keyword">return</span> probe;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至于这里为何这里键值对：（当前线程，线程对应的随机值）中线程对应的随机不是用<code>java.util.Random</code>生成？这是因为ThreadLocalRandom比Random更适合用在高并发情况，这里说的更适合是指“生成随机数性能更高而且与当前线程关联”，关键点为：ThreadLocalRandom，每个线程都有自己seed用于生成随机数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">localInit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> p = probeGenerator.addAndGet(PROBE_INCREMENT);</span><br><span class="line">    <span class="keyword">int</span> probe = (p == <span class="number">0</span>) ? <span class="number">1</span> : p; <span class="comment">// skip 0</span></span><br><span class="line">    <span class="keyword">long</span> seed = mix64(seeder.getAndAdd(SEEDER_INCREMENT));</span><br><span class="line">    Thread t = Thread.currentThread(); <span class="comment">// 当前线程</span></span><br><span class="line">    UNSAFE.putLong(t, SEED, seed); <span class="comment">// 将当前的seed与当前线程关联起来</span></span><br><span class="line">    UNSAFE.putInt(t, PROBE, probe); <span class="comment">// 将当前的probe与当前线程关联起来</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-3-longAccumulat"><a href="#3-3-longAccumulat" class="headerlink" title="3.3 longAccumulat"></a>3.3 longAccumulat</h5><p>longAccumulat设计跟CHM里面的fullAddCount设计一致，只不过这里多了参数： update function</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Handles cases of updates involving initialization, resizing,</span></span><br><span class="line"><span class="comment"> * creating new Cells, and/or contention. See above for</span></span><br><span class="line"><span class="comment"> * explanation. This method suffers the usual non-modularity</span></span><br><span class="line"><span class="comment"> * problems of optimistic retry code, relying on rechecked sets of</span></span><br><span class="line"><span class="comment"> * reads.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> x the value</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> fn the update function, or null for add (this convention</span></span><br><span class="line"><span class="comment"> * avoids the need for an extra field or function in LongAdder).</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> wasUncontended false if CAS failed before call</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">longAccumulate</span><span class="params">(<span class="keyword">long</span> x, LongBinaryOperator fn,</span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">boolean</span> wasUncontended)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((h = getProbe()) == <span class="number">0</span>) &#123;</span><br><span class="line">        ThreadLocalRandom.current(); <span class="comment">// force initialization</span></span><br><span class="line">        h = getProbe();</span><br><span class="line">        wasUncontended = <span class="keyword">true</span>;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>这个 update function有什么用呢？如下：</p>
<p>例如定义个匿名函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x -&gt; 2 * x +1</span><br></pre></td></tr></table></figure>
<p>那么每次cas就可以不只是<code>cas(this，value1，value2)</code>这种形式，而是<code>cas(this，value1，fn(value2)</code>能对value2进一步处理后再给到cas放入主存中。</p>
<p>调用点1：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (!wasUncontended)       <span class="comment">// CAS already known to fail</span></span><br><span class="line">            wasUncontended = <span class="keyword">true</span>;      <span class="comment">// Continue after rehash</span></span><br><span class="line"><span class="comment">// 当前线程对非空桶位Cell进行cas加值操作</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a.cas(v = a.value, ((fn == <span class="keyword">null</span>) ? v + x :</span><br><span class="line">                                     fn.applyAsLong(v, x))))</span><br></pre></td></tr></table></figure>
<p>调用点2：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        </span><br><span class="line"><span class="comment">// 当前线程又回到对base加值操作，若当前线程能在base身上加值成功就可以退出逻辑</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (casBase(v = base, ((fn == <span class="keyword">null</span>) ? v + x :</span><br><span class="line">                                    fn.applyAsLong(v, x))))</span><br><span class="line">            <span class="keyword">break</span>;                          <span class="comment">// Fall back on using base：这里是说当前线程因为进入longAccumulate没能竞争到对Cell加值操作，如果代码能执行到这个分支，那么线程又回到base变量去竞争cas加值操作</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>与longAccumulat类似逻辑的是double类型的doubleAccumulate高并发计数方法，这里不再累赘。</p>
<p>个人认为Striped64类里面表达最重要信息之一是：源码内部的详细代码功能注释说明，本文不再一一翻译，所有详细的设计和逻辑都在CHM文章中fullAddCount章节给出非常详细的解析，因此你可以基于源码的理解来对照以下官方源码注释：</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&#x2F;*</span><br><span class="line">* 1、首先介绍Striped64类数据结构：底层是可原子写操作的table+base变量</span><br><span class="line">* This class maintains a lazily-initialized table of atomically</span><br><span class="line">* updated variables, plus an extra &quot;base&quot; field. The table size</span><br><span class="line">* is a power of two. Indexing uses masked per-thread hash codes.</span><br><span class="line">* Nearly all declarations in this class are package-private,</span><br><span class="line">* accessed directly by subclasses.</span><br><span class="line">*</span><br><span class="line">* 2、Cell类的设计说明</span><br><span class="line">* Table entries are of class Cell; a variant of AtomicLong padded</span><br><span class="line">* (via @sun.misc.Contended) to reduce cache contention. Padding</span><br><span class="line">* is overkill for most Atomics because they are usually</span><br><span class="line">* irregularly scattered in memory and thus don&#39;t interfere much</span><br><span class="line">* with each other. But Atomic objects residing in arrays will</span><br><span class="line">* tend to be placed adjacent to each other, and so will most</span><br><span class="line">* often share cache lines (with a huge negative performance</span><br><span class="line">* impact) without this precaution.</span><br><span class="line">*</span><br><span class="line">* 3、Cells[] 数组的设计目的</span><br><span class="line">* In part because Cells are relatively large, we avoid creating</span><br><span class="line">* them until they are needed.  When there is no contention, all</span><br><span class="line">* updates are made to the base field.  Upon first contention (a</span><br><span class="line">* failed CAS on base update), the table is initialized to size 2.</span><br><span class="line">* The table size is doubled upon further contention until</span><br><span class="line">* reaching the nearest power of two greater than or equal to the</span><br><span class="line">* number of CPUS. Table slots remain empty (null) until they are</span><br><span class="line">* needed.</span><br><span class="line">*</span><br><span class="line">* 4、设计cellsBusy锁的目的</span><br><span class="line">* A single spinlock (&quot;cellsBusy&quot;) is used for initializing and</span><br><span class="line">* resizing the table, as well as populating slots with new Cells.</span><br><span class="line">* There is no need for a blocking lock; when the lock is not</span><br><span class="line">* available, threads try other slots (or the base).  During these</span><br><span class="line">* retries, there is increased contention and reduced locality,</span><br><span class="line">* which is still better than alternatives.</span><br><span class="line">*</span><br><span class="line">* 5、Thread probe用于定位在Cells数组哪个桶位</span><br><span class="line">* The Thread probe fields maintained via ThreadLocalRandom serve</span><br><span class="line">* as per-thread hash codes. We let them remain uninitialized as</span><br><span class="line">* zero (if they come in this way) until they contend at slot</span><br><span class="line">* 0. They are then initialized to values that typically do not</span><br><span class="line">* often conflict with others.  Contention and&#x2F;or table collisions</span><br><span class="line">* are indicated by failed CASes when performing an update</span><br><span class="line">* operation. Upon a collision, if the table size is less than</span><br><span class="line">* the capacity, it is doubled in size unless some other thread</span><br><span class="line">* holds the lock. If a hashed slot is empty, and lock is</span><br><span class="line">* available, a new Cell is created. Otherwise, if the slot</span><br><span class="line">* exists, a CAS is tried.  Retries proceed by &quot;double hashing&quot;,</span><br><span class="line">* using a secondary hash (Marsaglia XorShift) to try to find a</span><br><span class="line">* free slot.</span><br><span class="line">*</span><br><span class="line">* 6、capped翻译为：用...封顶（盖住） Cells数组长度最大只能扩容到和CPU数量相同</span><br><span class="line">* The table size is capped because, when there are more threads</span><br><span class="line">* than CPUs, supposing that each thread were bound to a CPU,</span><br><span class="line">* there would exist a perfect hash function mapping threads to</span><br><span class="line">* slots that eliminates collisions. When we reach capacity, we</span><br><span class="line">* search for this mapping by randomly varying the hash codes of</span><br><span class="line">* colliding threads.  Because search is random, and collisions</span><br><span class="line">* only become known via CAS failures, convergence can be slow,</span><br><span class="line">* and because threads are typically not bound to CPUS forever,</span><br><span class="line">* may not occur at all. However, despite these limitations,</span><br><span class="line">* observed contention rates are typically low in these cases.</span><br><span class="line">*</span><br><span class="line">* 7、在Cells数组有些桶位的Cell可能没被线程命中用于CAS加值计数，但没关系，不用理会它，也不需要去找到这的Cell桶位然后删除之（画蛇添足）。</span><br><span class="line">* It is possible for a Cell to become unused when threads that</span><br><span class="line">* once hashed to it terminate, as well as in the case where</span><br><span class="line">* doubling the table causes no thread to hash to it under</span><br><span class="line">* expanded mask.  We do not try to detect or remove such cells,</span><br><span class="line">* under the assumption that for long-running instances, observed</span><br><span class="line">* contention levels will recur, so the cells will eventually be</span><br><span class="line">* needed again; and for short-lived ones, it does not matter.</span><br><span class="line">*&#x2F;</span><br></pre></td></tr></table></figure>
</blockquote>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度讨论jdk1.8 ConcurrentHashMap并发环境下transfer方法桶位分配过程</title>
    <url>/2021/08/15/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%AE%A8%E8%AE%BAjdk1.8%20ConcurrentHashMap%E5%B9%B6%E5%8F%91%E7%8E%AF%E5%A2%83%E4%B8%8Btransfer%E6%96%B9%E6%B3%95%E6%A1%B6%E4%BD%8D%E5%88%86%E9%85%8D%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>在前面有多篇关于jdk1.8的ConcurrentHashMap研究是基于源代码给出的深度分析，要知道多线程环境下的ConcurrentHashMap内部运行机制是相对复杂的，好在IDEA提供的相关断点和Debug功能确实好用，使得多线程调试起来直观，通过这种方式能加深多线程操作CHM的执行流程。</p>
<h5 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h5><p>这部内容请参考文章中的小节部分，本文不再累赘。</p>
<h5 id="使用埋点打印法观测"><a href="#使用埋点打印法观测" class="headerlink" title="使用埋点打印法观测"></a>使用埋点打印法观测</h5><p>此方法相对繁琐，难度并不大，要求使用者对源代码设计足够理解，否则埋点位置不佳影响观测效果</p>
<p>1、测试代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResizeStampBugTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 设置64个线程并发put</span></span><br><span class="line">        <span class="keyword">int</span> maxThreads = <span class="number">64</span>;</span><br><span class="line">      	<span class="comment">// 初始容量为8，内部会被调整为16</span></span><br><span class="line">        ConcurrentHashMap&lt;Long, String&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; maxThreads; i++) &#123;</span><br><span class="line">            Thread t = <span class="keyword">new</span> Thread(() -&gt; map.put(Thread.currentThread().getId(),Thread.currentThread().getName()));</span><br><span class="line">            t.setName(<span class="string">&quot;Thread-&quot;</span>+i);</span><br><span class="line">            t.start();</span><br><span class="line">            <span class="comment">// 因为多个线程并发执行不方便查看打印结果，可以让前一个线程领先后面线程一丁点，以便观察打印结果，当然也可以不需要，多执行几次看看打印结果即可。</span></span><br><span class="line">            <span class="comment">// Thread.sleep(1);</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到这里ConcurrentHashMap类用的是项目concurrent.demo包下的ConcurrentHashMap.java 源码文件</p>
<p>2、更改桶位分配步长，将源码的16改为4，方便观察</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TRANSFER_STRIDE = <span class="number">4</span>;</span><br><span class="line"><span class="comment">// private static final int MIN_TRANSFER_STRIDE = 16;</span></span><br></pre></td></tr></table></figure>
<p>3、transfer方法加入打印每个线程分配的桶位区间</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                    (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                     nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                  nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">               bound = nextBound;</span><br><span class="line">               i = nextIndex - <span class="number">1</span>;</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line"><span class="comment">// 以下三行是新增代码</span></span><br><span class="line">               String s=String.format(<span class="string">&quot;%s分配的捅位区间为:[%d,%d]并挂起&quot;</span>,Thread.currentThread().getName(),bound,i);</span><br><span class="line">               System.out.println(s);</span><br><span class="line">               <span class="comment">// 在这里加一句挂起已经分配好桶位区间的线程，用于观察线程</span></span><br><span class="line">               LockSupport.park(<span class="keyword">this</span>);</span><br><span class="line">               </span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure>
<p>4、其他地方需要埋入打印语句</p>
<p>pulVal方法：当key定位到的桶位为空，直接放入key节点。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">         </span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; hash)) == <span class="keyword">null</span>) &#123;</span><br><span class="line">             <span class="keyword">if</span> (casTabAt(tab, i, <span class="keyword">null</span>,</span><br><span class="line">                          <span class="keyword">new</span> Node&lt;K,V&gt;(hash, key, value, <span class="keyword">null</span>))) &#123;</span><br><span class="line">         <span class="comment">// 埋入打印语句</span></span><br><span class="line">         System.out.println(Thread.currentThread().getName() + <span class="string">&quot;在桶位[&quot;</span>+i+<span class="string">&quot;]put入Node节点并退出&quot;</span>);</span><br><span class="line"></span><br><span class="line">                 <span class="keyword">break</span>;                   <span class="comment">// no lock when adding to empty bin</span></span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure>
<p>addCount方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))&#123;</span><br><span class="line">     <span class="comment">// 埋入打印语句</span></span><br><span class="line">        System.out.println(Thread.currentThread().getName()+<span class="string">&quot;后续进入扩容逻辑transfer方法&quot;</span>);</span><br><span class="line">        transfer(tab, nt);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                             (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))&#123;</span><br><span class="line">  	<span class="comment">// 埋入打印语句</span></span><br><span class="line">    System.out.println(Thread.currentThread().getName()+<span class="string">&quot;第1个进入扩容逻辑transfer方法&quot;</span>);</span><br><span class="line">    transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>5、预期执行</p>
<p>打印结果如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Thread-<span class="number">0</span>在桶位[<span class="number">9</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">1</span>在桶位[<span class="number">10</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">2</span>在桶位[<span class="number">11</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">3</span>在桶位[<span class="number">12</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">4</span>在桶位[<span class="number">13</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">5</span>在桶位[<span class="number">14</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">6</span>在桶位[<span class="number">15</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">7</span>在桶位[<span class="number">0</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">8</span>在桶位[<span class="number">1</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">9</span>在桶位[<span class="number">2</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">10</span>在桶位[<span class="number">3</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">11</span>在桶位[<span class="number">4</span>]put入Node节点并退出</span><br><span class="line">Thread-11第1个进入扩容逻辑transfer方法 # 注意此线程</span><br><span class="line">Thread-<span class="number">12</span>在桶位[<span class="number">5</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">12</span>进入扩容逻辑transfer方法</span><br><span class="line">Thread-<span class="number">13</span>在桶位[<span class="number">6</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">13</span>进入扩容逻辑transfer方法</span><br><span class="line">Thread-<span class="number">14</span>在桶位[<span class="number">7</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">14</span>进入扩容逻辑transfer方法</span><br><span class="line">Thread-<span class="number">15</span>在桶位[<span class="number">8</span>]put入Node节点并退出</span><br><span class="line">Thread-<span class="number">12</span>分配的捅位区间为:[<span class="number">12</span>,<span class="number">15</span>]并挂起</span><br><span class="line">Thread-<span class="number">13</span>分配的捅位区间为:[<span class="number">4</span>,<span class="number">7</span>]并挂起</span><br><span class="line">Thread-<span class="number">14</span>分配的捅位区间为:[<span class="number">0</span>,<span class="number">3</span>]并挂起</span><br><span class="line">Thread-<span class="number">11</span>分配的捅位区间为:[<span class="number">8</span>,<span class="number">11</span>]并挂起</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/img_convert/e2067f18b8c22bc1b20b494096de4fa3.png" alt="transfer埋点打印"></p>
<p>打印结果说明①</p>
<p>因为CHM的底层table容量为16也即有16个桶位，  此外使用线程ID作为节点的key，根据桶位定位算法<code>i = (n - 1) &amp; hash</code>，前面16个线程（第0号线程到第15号线程）并发将16个key恰好能放到table16个桶位上，这也是为何将打印点放在putVal对应位置，线程put完后break退出后进入addCount，因此才有以下类似的打印：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Thread-0在桶位[9]put入Node节点并退出</span><br><span class="line">Thread-1在桶位[10]put入Node节点并退出</span><br><span class="line">Thread-2在桶位[11]put入Node节点并退出</span><br><span class="line">Thread-3在桶位[12]put入Node节点并退出</span><br><span class="line">Thread-4在桶位[13]put入Node节点并退出</span><br><span class="line">Thread-5在桶位[14]put入Node节点并退出</span><br><span class="line">Thread-6在桶位[15]put入Node节点并退出</span><br><span class="line">Thread-7在桶位[0]put入Node节点并退出</span><br><span class="line">Thread-8在桶位[1]put入Node节点并退出</span><br><span class="line">Thread-9在桶位[2]put入Node节点并退出</span><br><span class="line">Thread-10在桶位[3]put入Node节点并退出</span><br><span class="line">Thread-11在桶位[4]put入Node节点并退出</span><br><span class="line"></span><br><span class="line">Thread-12在桶位[5]put入Node节点并退出</span><br><span class="line"></span><br><span class="line">Thread-13在桶位[6]put入Node节点并退出</span><br><span class="line"></span><br><span class="line">Thread-14在桶位[7]put入Node节点并退出</span><br><span class="line"></span><br><span class="line">Thread-15在桶位[8]put入Node节点并退出</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>打印结果说明②</p>
<p>在打印结果中你会发现Thread-11的特别之处，如下：这个线程在put节点后进入addCount，此时因为前面第0号到10号线程总共put 入了11个节点，当线程Thread-11去put节点完后发现此时CHM节点数量达到扩容阈值12（16*0.75），</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Thread-10在桶位[3]put入Node节点并退出</span><br><span class="line">Thread-11在桶位[4]put入Node节点并退出</span><br><span class="line"><span class="meta">#</span><span class="bash"> Thread-11 写入节点后发现s达到扩容阈值</span></span><br><span class="line">Thread-11第1个进入扩容逻辑transfer方法</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>因此线程Thread-11进入addCount方法的分支2且进入了扩容逻辑transfer，而且是作为第1个线程进入扩容逻辑</p>
<p>这就是为何要在addCount以下位置埋入打印点，捕获第1个进入扩容逻辑transfer方法的线程：线程Thread-11</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                             (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))&#123;</span><br><span class="line">  	<span class="comment">// 埋入打印语句</span></span><br><span class="line">    System.out.println(Thread.currentThread().getName()+<span class="string">&quot;第1个进入扩容逻辑transfer方法&quot;</span>);</span><br><span class="line">    transfer(tab, <span class="keyword">null</span>);</span><br></pre></td></tr></table></figure>
<p>打印结果说明③</p>
<p>因为继线程Thread-11put节点后使得s=12达到扩容阈值，因此后来的线程Thread-12、Thread-13、Thread-14在它们put完节点也发现需要扩容，因此也都进入的transfer方法，显然它们分别作为第2个、第3个、第4个进入transfer的线程。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Thread-11在桶位[4]put入Node节点并退出</span><br><span class="line">Thread-11第1个进入扩容逻辑transfer方法</span><br><span class="line">Thread-12在桶位[5]put入Node节点并退出</span><br><span class="line">Thread-12进入扩容逻辑transfer方法</span><br><span class="line">Thread-13在桶位[6]put入Node节点并退出</span><br><span class="line">Thread-13进入扩容逻辑transfer方法</span><br><span class="line">Thread-14在桶位[7]put入Node节点并退出</span><br><span class="line">Thread-14进入扩容逻辑transfer方法</span><br><span class="line">Thread-15在桶位[8]put入Node节点并退出</span><br><span class="line"><span class="meta">#</span><span class="bash"> 线程Thread-15没能进入扩容逻辑transfer方法</span></span><br><span class="line">Thread-12分配的捅位区间为:[12,15]并挂起</span><br><span class="line">Thread-13分配的捅位区间为:[4,7]并挂起</span><br><span class="line">Thread-14分配的捅位区间为:[0,3]并挂起</span><br><span class="line">Thread-11分配的捅位区间为:[8,11]并挂起</span><br></pre></td></tr></table></figure>
<p>这里为何线程Thread-15没能进入transfer方法？还记得上面将桶位分配步长设为4的说明吗</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TRANSFER_STRIDE = <span class="number">4</span>;</span><br><span class="line"><span class="comment">// private static final int MIN_TRANSFER_STRIDE = 16;</span></span><br></pre></td></tr></table></figure>
<p>因为一开始CHM容量为16，也即16个桶位，又因为桶位步长设定为4，因此只能有4个线程能成功cas分配到桶位区间。由于线程Thread-11第1个先进入transfer方法、线程Thread-12第2个进入、线程Thread-13第3个进入、线程Thread-14第4个进入，这4个线程恰好“瓜分”完16个桶位</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Thread-12分配的捅位区间为:[12,15]并挂起</span><br><span class="line">Thread-13分配的捅位区间为:[4,7]并挂起</span><br><span class="line">Thread-14分配的捅位区间为:[0,3]并挂起</span><br><span class="line">Thread-11分配的捅位区间为:[8,11]并挂起</span><br></pre></td></tr></table></figure>
<p>之后transferIndex=0，表示全部桶位已经分配出去，那么再来的线程Thread-15，恰好满足下面条件<code>transferIndex &lt;= 0)</code> ，因此Thread-15不能进入transfer并break结束<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">        sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">        transferIndex &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure></p>
<p>为什么线程Thread-11、Thread-12、Thread-13、Thread-14在transfer方法分配完桶位区间后没有退出？</p>
<p>这就是为何使用<code>LockSupport.park(this)</code>将线程挂起的原因，以便持续观察CHM的桶位分配机制，同时也能将CHM容量16扩容到32的过程暂停，使得CHM停留在第一轮扩容的过程中，而且暂停在桶位分配完之后，节点迁移之前。<br>“使得CHM扩容处理流程暂停在第一轮扩容的过程中”，这有什么用处呢，请看下面：</p>
<p>打印结果说明④</p>
<p>在AddCount方法下面位置埋入打印语句：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">    sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">    transferIndex &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+<span class="string">&quot;因无桶位可分配，此线程直接退出&quot;</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目的就是考察线程Thread-11、Thread-12、Thread-13、Thread-14在transfer方法分配完桶位区间后并挂起，后续的Thread-15到Thread-63是如何安排的</p>
<p>打印如下：其实无需打印也能知道，因为Thread-15进入AddCount分支2后，transferIndex已经是0，也即无桶位可分配，只好break退出，后面来更多的其他线程也同理。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Thread-15在桶位[8]put入Node节点并退出</span><br><span class="line">Thread-15因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-16因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-17因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-18因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-19因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-20因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-21因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-22因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-23因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-24因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-25因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-26因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-27因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-28因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-11分配的捅位区间为:[12,15]并挂起</span><br><span class="line">Thread-13分配的捅位区间为:[4,7]并挂起</span><br><span class="line">Thread-14分配的捅位区间为:[0,3]并挂起</span><br><span class="line">Thread-12分配的捅位区间为:[8,11]并挂起</span><br><span class="line">Thread-29因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-30因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-31因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-32因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-33因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-34因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-35因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-36因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-37因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-38因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-39因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-40因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-41因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-42因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-43因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-44因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-45因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-46因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-47因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-48因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-49因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-50因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-51因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-52因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-53因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-54因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-55因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-56因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-57因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-58因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-59因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-60因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-61因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-62因无桶位可分配，此线程直接退出</span><br><span class="line">Thread-63因无桶位可分配，此线程直接退出</span><br></pre></td></tr></table></figure>
<p>以上就是使用埋入打印点调试transfer机制，通过“print”来观察其运行机制先对入门，其实debug模式才是真正便捷的调试方式，继续查阅下面内容。</p>
<h5 id="使用IDEA-断点debug方式观测"><a href="#使用IDEA-断点debug方式观测" class="headerlink" title="使用IDEA 断点debug方式观测"></a>使用IDEA 断点debug方式观测</h5><p>1、只需在transfer以下位置打一个断点即可，也即在桶位区间cas分配的逻辑里的<code>advance=true</code>，如下，<br><img src="https://img-blog.csdnimg.cn/img_convert/b7c5eeb0d924c663a7d5c6750d53e527.png" alt="transfer break point位置"></p>
<p>并将断点的Suspend设为Thread，如下：<br><img src="https://img-blog.csdnimg.cn/img_convert/5235db69397ebffedd635cdeba214b7a.png" alt="Thread Option断点设定"></p>
<p>2、启动debug，IDEA进入debug操作，此处无需图。</p>
<p>3、在Watchs栏目加入要“观测的变量”，这里当然是关注每个线程分配到的桶位区间的左边界bound和右边界i<br><img src="https://img-blog.csdnimg.cn/img_convert/dcb029b159551701428cc0405b8af90a.png" alt="Watchs加入bound和i变量观测"><br>Frames栏目可以显示每个线程的方法调用栈，在点选Thread-11作为观察目标，你可以清晰看到Thread-11的方法调用栈：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">transfer <span class="comment">// 栈顶</span></span><br><span class="line">addCount</span><br><span class="line">putval</span><br><span class="line">put</span><br><span class="line">main主函数入口  <span class="comment">// 栈底</span></span><br></pre></td></tr></table></figure>
<p>点选“transfer”方法帧，然后右侧Variables栏目会展示该方法内部的局部变量值，由于我们想观测线程桶位区间的左边界bound和右边界i，因此在Watches加入这两个变量，图中可以非常直观看出Thread-11的桶位区间为[bound=12,i=15]</p>
<p>若想看其他线程分配的桶位，很简单，在下拉框选中其他线程即可，如Thread-12，可以在Watches栏目看到Thread-12的桶位区间为[bound=8,i=11]<br><img src="https://img-blog.csdnimg.cn/img_convert/4e7a0a685b1365bcff918cc5c3b444b2.png" alt="transfer Thread12桶位区间"><br>同理也可以看出Thread-13、Thread-14的界面，这里不再一一展示。</p>
<p>5、所观测的方法帧里面的变量和Watches对应<br><img src="https://img-blog.csdnimg.cn/img_convert/98a8833484149545a0b677c51e45cff1.png" alt="方法帧与watches变量对应"></p>
<p>如上图所示，Watches提示无法找到局部变量i和局部bound变量，这是因为当前观测的是在addCount这个方法帧，显然</p>
<p>addCount内部没有i和bound变量。此外，也可从addCount的局部变量表Variables栏目看Watches是否在里面。</p>
<p><strong>为何Frames只显示4个线程在RUNING状态？</strong></p>
<p>答案已经在第一小节的“使用埋点打印法观测”后面给出了详细的解释，这里也简要说明：</p>
<p>（1）第0到第15号线程put完节点后，其中第0到第10号线程结束退出，第11号线程发现put完后CHM节点数量达到扩容阈值12，因此进入AddCount分支2，并作为第1个进入扩容逻辑transfer的线程，故Thread-11分配到了桶位区间[12,15]</p>
<p>（2）线程Thread-12、Thread-13、Thread-14 put完节点也因为s节点数量达到扩容阈值，都进入到transfer方法，也分配到对应的桶位区间</p>
<p>（3）等线程Thread-15 put完节点后，发现已无桶位可分配，因此Thread-15 结束</p>
<p>（4）其他剩余的线程第16到第63号线程也同样因为“已无桶位可分配而结束”</p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>可以看出，IDEA调试并发环境的CHM确实小有难度，最好能在掌握源代码情况下debug，通过这种实践而非源代码分析的观测方式，你能任意控制多线程并发执行流以及观测其内部协调机制、竞争机制，从而能深入掌握JUC并发设计和源代码实现。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度解析jdk1.8的HashMap红黑树balanceDeletion节点删除平衡算法设计（核心文章）</title>
    <url>/2021/03/13/Java%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90jdk1.8%E7%9A%84HashMap%E7%BA%A2%E9%BB%91%E6%A0%91balanceDeletion%E8%8A%82%E7%82%B9%E5%88%A0%E9%99%A4%E5%B9%B3%E8%A1%A1%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%EF%BC%88%E6%A0%B8%E5%BF%83%E6%96%87%E7%AB%A0%EF%BC%89/</url>
    <content><![CDATA[<p>这可能是全网最期待的jdk1.8的红黑树balanceDeletion的源代码解析技术文章！</p>
<p>其实掌握HashMap红黑树的同学都知道，<code>balanceDeletion</code>方法的源代码是HashMap红黑树部分最复杂也是最难理解的部分，目前少有coder对<code>balanceDeletion</code>有足够深入且可理解的分析，绝大部分关于深入HashMap分析的文章都会跳过<code>balanceDeletion</code>源代码，有部分文章的coder他并不直接给出<code>balanceDeletion</code>的源代码解析，而是自行实现非HashMap的“红黑树删除平衡”代码，<a href="https://mp.weixin.qq.com/s/9ysi_wrjhmm2czTOui3SyA">如链接</a>，但显然不能跟jdk源码高质量功能相比（HashMap源码里面的<code>removeTreeNode</code>和<code>balanceDeletion</code>的代码设计是最完整的），因此要想真正掌握完整jdk级别的HashMap红黑树的balanceDeletion逻辑，那么源代码解析肯定要搬出来。</p>
<p>目前个人认可的文章是<a href="https://blog.csdn.net/anlian523/article/details/103649200#comments_14954397">这篇文章</a>，个人也给它留了评论和鼓励（该博客作者能深钻JUC源代码实现），但也发现该文在解析<code>balanceDeletion</code>源码、图示（少部分）不够直观、简约、清晰，因此亲自实现一篇相对高质量且尽量可理解的<code>removeTreeNode</code>和<code>balanceDeletion</code>源代码分析，本文不会跟类似文章图或者文章组织或者思路重复。</p>
<p><img src="https://img-blog.csdnimg.cn/82cde64db30a44919c1675305adc8605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>《gitee 博客文章封面》</p>
<a id="more"></a>
<h3 id="1、removeNode"><a href="#1、removeNode" class="headerlink" title="1、removeNode"></a>1、removeNode</h3><p>remove方法删除逻辑由内部的removeNode方法代理，如果能找到key对应的删除节点，那么removeNode返回这个节点的value，否则返回null</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = removeNode(hash(key), key, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">true</span>)) == <span class="keyword">null</span> ?</span><br><span class="line">        <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下是removeNode源码说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">removeNode</span><span class="params">(<span class="keyword">int</span> hash, Object key, Object value,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">boolean</span> matchValue, <span class="keyword">boolean</span> movable)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, index;</span><br><span class="line">    <span class="comment">// 显然如果table还未有节点或者key定位到桶位节点p为空，就返回null，否则进入主体逻辑</span></span><br><span class="line">    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (p = tab[index = (n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        Node&lt;K,V&gt; node = <span class="keyword">null</span>, e; K k; V v;</span><br><span class="line">      	<span class="comment">//① 桶位节点p恰好就是要删除的节点，先不执行删除，而是将p节点赋给node引用，统一在后面处理</span></span><br><span class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">            ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            node = p;</span><br><span class="line">      	<span class="comment">//② 桶位节点p不是目标删除节点，那么就只能从链表找到目标删除节点或者从红黑树找到目标删除节点</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((e = p.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">//③ 若桶位节点p是红黑树节点类型，则从红黑树找到目标删除节点。由getTreeNode负责找出目标删除节点，找到就赋给node引用</span></span><br><span class="line">            <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key);</span><br><span class="line">          	<span class="comment">//④ 若桶位节点p是链表头节点，遍历链表找到目标删除节点，找到就赋给node引用</span></span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">do</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                        ((k = e.key) == key ||</span><br><span class="line">                         (key != <span class="keyword">null</span> &amp;&amp; key.equals(k)))) &#123;</span><br><span class="line">                        node = e;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    p = e;</span><br><span class="line">                &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 对找到的目标删除节点node进行处理，分节点类型情况处理，这里首先需要判断value有无设定约束删除条件，即“key相等且value不等才可删除”或者“key相等且value也相等才可删除”</span></span><br><span class="line">        <span class="keyword">if</span> (node != <span class="keyword">null</span> &amp;&amp; (!matchValue || (v = node.value) == value ||</span><br><span class="line">                             (value != <span class="keyword">null</span> &amp;&amp; value.equals(v)))) &#123;</span><br><span class="line">            <span class="comment">//1、如果目标删除节点node是TreeNode类型，则调用removeTreeNode删除之，此方法是本文的核心和（特指内部的balanceDeletion）难点</span></span><br><span class="line">            <span class="keyword">if</span> (node <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                ((TreeNode&lt;K,V&gt;)node).removeTreeNode(<span class="keyword">this</span>, tab, movable);</span><br><span class="line">            <span class="comment">//2、如果目标删除节点node是桶位头节点，直接用删除节点的后继节点放在桶位即可。</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (node == p)</span><br><span class="line">                tab[index] = node.next;</span><br><span class="line">          	<span class="comment">//3、如果目标删除节点node在链表上，也即：p-&gt;node&gt;node.next，将其改为p-&gt;node.next即可完成node节点的删除</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                p.next = node.next;</span><br><span class="line">          	<span class="comment">// 删除节点属于改变HashMap结构的情况，当然需要计数 </span></span><br><span class="line">            ++modCount;</span><br><span class="line">            --size;</span><br><span class="line">            afterNodeRemoval(node);</span><br><span class="line">            <span class="keyword">return</span> node;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到removeNode内部逻辑清晰易懂，而最关键最难的逻辑是红黑树的删除节点逻辑（特指内部的balanceDeletion逻辑）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">((TreeNode&lt;K,V&gt;)node).removeTreeNode(<span class="keyword">this</span>, tab, movable);</span><br></pre></td></tr></table></figure>
<p>node是通过<code>((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key)</code> 在红黑树中找到的目标删除节点，removeTreeNode方法中入参this也就是指代这个node节点本身，具体分析见后面章节。</p>
<h3 id="2、removeTreeNode（难点）"><a href="#2、removeTreeNode（难点）" class="headerlink" title="2、removeTreeNode（难点）"></a>2、removeTreeNode（难点）</h3><p>removeTreeNode方法的官方注释：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Removes the given node, that must be present before this call. This is messier than typical red-black deletion code because we cannot swap the contents of an interior node with a leaf successor that is pinned by &quot;next&quot; pointers that are accessible independently during traversal. So instead we swap the tree linkages. If the current tree appears to have too few nodes, the bin is converted back to a plain bin. (The test triggers somewhere between 2 and 6 nodes, depending on tree structure).</span><br></pre></td></tr></table></figure>
<p>node with a leaf successor: 当前节点有后继节点</p>
<p>在理解以下复杂且难以理解的删除逻辑前，首先要记着红黑树是“具有两种数据结构”特征的，因为对于TreeNode类来说，</p>
<font color=red>TreeNode的parent、left、right、red属性构成红黑树结构，而TreeNode的prev、next属性构成双向链表结构，因此在删除一个指定节点node时，即要在“双向链表结构”中删除这个node，也要在“红黑树结构”删除这个node，这样才能正确的实现“红黑树已删除该node节点”的逻辑，理解这一点很重要 。HashMap的红黑树结构不像其他普通的二叉树设计，而普通版本的二叉树删除节点只需在树中删除该节点即可，因为普通的二叉树节点并没有prev和next属性，因此也无“双向链表”这一说。</font>

<h4 id="2-1-removeTreeNode（前部分逻辑）"><a href="#2-1-removeTreeNode（前部分逻辑）" class="headerlink" title="2.1 removeTreeNode（前部分逻辑）"></a>2.1 removeTreeNode（前部分逻辑）</h4><p>removeTreeNode主体逻辑分为两部分，第一部分：</p>
<p>1、在TreeNode构成的双向链表中删除node节点，这部分逻辑相对简单，调整前后驱关系即可完成</p>
<p>2、在TreeNode构成的红黑树中删除node节点，这部分逻辑设计最为复杂也是最难理解的，需要分开多个情况处理：</p>
<ul>
<li>2.1在TreeNode构成的红黑树中删除node节点过程，如果红黑树本身节点就很少（2到6个），注意：此时不需要再附加”删除节点操作“，因为在1中的双向链表已经删除了节点，到了这里，只需将这个“小树”调整为链表即可。</li>
</ul>
<p>以上逻辑被归到<font color=red>removeTreeNode（前部分逻辑）</font>，下面是属于第二部分：</p>
<ul>
<li>2.2 在TreeNode构成的红黑树中删除node节点过程，如果红黑树本身节点多，那么其节点删除的方式其实就是通过调整树代际关系实现删除，并在此之后实施红黑树删除平衡逻辑。</li>
</ul>
<p>第二部分部分的源代码设计参考<font color=red>removeTreeNode（后部分逻辑）</font>章节。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable);</span></span><br><span class="line"><span class="comment">	 这个node是从红黑树找出的符合删除条件的节点，然后调用其内部removeTreeNode方法</span></span><br><span class="line"><span class="comment">	 以下的当前节点指代this节点，也指代调用removeTreeNode方法的这个“node”节点</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">removeTreeNode</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab,</span></span></span><br><span class="line"><span class="function"><span class="params">                                <span class="keyword">boolean</span> movable)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">int</span> n;</span><br><span class="line">    			<span class="comment">//removeTreeNode（前部分逻辑）</span></span><br><span class="line">    			<span class="comment">//① 如果tab都为空，也即无节点可删除，直接返回即可</span></span><br><span class="line">          <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">    			<span class="comment">// 当前节点所在的桶位</span></span><br><span class="line">          <span class="keyword">int</span> index = (n - <span class="number">1</span>) &amp; hash;</span><br><span class="line">    			<span class="comment">// 先取出index桶位的头节点first，同时first节点也是红黑树的root根节点，因此也有root=first,rl是root节点的左子节点</span></span><br><span class="line">          TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index], root = first, rl;</span><br><span class="line">    			</span><br><span class="line">    			<span class="comment">//由于调用removeTreeNode的节点就是一个TreeNode，因此其next节点就是后继节点赋给succ变量，prev节点为前驱节点赋给pred变量</span></span><br><span class="line">          TreeNode&lt;K,V&gt; succ = (TreeNode&lt;K,V&gt;)next, pred = prev;</span><br><span class="line">    			<span class="comment">// 如果当前节点node的前驱节点为空,说明前节点node就位于桶位头节点上，因为要删除当前节点node，故只需将将first指向succ，并将当前节点node的后继节点succ放入桶位上，就可完成“删除当前节点node”的操作</span></span><br><span class="line">    			<span class="comment">//  node(头节点) &lt;=&gt; succ &lt;=&gt; succ.next  变成 succ(头节点) &lt;=&gt; succ.next</span></span><br><span class="line">          <span class="comment">// ②</span></span><br><span class="line">    			<span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">              tab[index] = first = succ;</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">          <span class="comment">/* 如果前驱节点不为空，说明当前要删除的node位于双向链表的其他位置，使用链表删除节点的方式即可完成对node节点的删除，如下： </span></span><br><span class="line"><span class="comment">          &lt;=&gt;符号:  前后指向关系已经建立</span></span><br><span class="line"><span class="comment">          -&gt; 符号:  前驱节点指向后面节点关系建立了，还差后面节点指向前驱节点的关系</span></span><br><span class="line"><span class="comment">           将 pred&lt;=&gt; node &lt;=&gt; succ &lt;=&gt; succ.next 变成 pred -&gt; succ &lt;=&gt; succ.next</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">              pred.next = succ;</span><br><span class="line">    			<span class="comment">// 如果后驱节点不为空，因为那么需要将后继节点的前驱指针指向前一个节点，以保证形成双向链关系。</span></span><br><span class="line">    			<span class="comment">// 例如将pred -&gt; succ &lt;=&gt; succ.next 变成 pred &lt;=&gt; succ &lt;=&gt; succ.next，这里pred节点可以是null也可以是非null</span></span><br><span class="line">    			<span class="comment">// ③</span></span><br><span class="line">          <span class="keyword">if</span> (succ != <span class="keyword">null</span>)</span><br><span class="line">              succ.prev = pred;</span><br><span class="line">    </span><br><span class="line">    			<span class="comment">// 前面可知tab[index] = first = succ，如果first为空，也即succ为空，说明本次删除节点已经完成，对于这种情况，删除当前节点node，其实tab[index]=null，也即该桶位为空了，就不需要做删除之后的平衡操作或者树转链表从中，可直接返回。</span></span><br><span class="line">          <span class="comment">// ④</span></span><br><span class="line">    			<span class="keyword">if</span> (first == <span class="keyword">null</span>)</span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    			<span class="comment">// ⑤ 代码执行到这里，说明first节点不为空，由前面root=first有：先判断root节点是否为树的根节点，如果root不是根节点，就调用内部root()找到真正的红黑树根节点</span></span><br><span class="line">          <span class="keyword">if</span> (root.parent != <span class="keyword">null</span>)</span><br><span class="line">              root = root.root();</span><br><span class="line">    			<span class="comment">// ⑥ 在前面的②和③中所要删除的节点已经在双向链表中删除了，因此当遇到红黑树结构节点很少时，直接将该树转成链表即可，不需要再实施树平衡操作。</span></span><br><span class="line">          <span class="keyword">if</span> (root == <span class="keyword">null</span></span><br><span class="line">              || (movable</span><br><span class="line">                  &amp;&amp; (root.right == <span class="keyword">null</span></span><br><span class="line">                      || (rl = root.left) == <span class="keyword">null</span></span><br><span class="line">                      || rl.left == <span class="keyword">null</span>))) &#123;</span><br><span class="line">              tab[index] = first.untreeify(map);  <span class="comment">// too small</span></span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>
<p>针对第⑥点的分析：TreeNode构成的红黑树中删除node节点过程，如果红黑树本身节点就很少（2到6个），注意：此时不需要再附加”删除节点操作“，因为在1中的双向链表已经删除了节点，到了这里，只需将这个“小树”调整为链表即可。</p>
<p>节点数量不多的红黑树具有的特征：</p>
<p>1、只有一个root节点，且空节点</p>
<p>2、root.right == null，说明只有一个左子节点，因此从红黑树性质可知：此时树只有两个节点：根节点root（黑色）、左子节点（红色）</p>
<p>3、若root.right == null不成立，则来到条件：(rl = root.left) == null，它成立说明左子节点为空，且只有一个右子节点，由于红黑树性质可推导出：此时树只有两个节点：根节点root（黑色）、右子节点（红色）</p>
<p>4、若root.right == null不成立，(rl = root.left) == null不成立，也即根节点有左右子节点，则来到条件rl.left == null，它成立则说明此时红黑树也是一棵简单的红黑树且构成有多种形式，但红黑树约束性质可知：基本对应到有2到6个节点，可以通过枚举法画出2到6个节点对应的红黑树结构，这里不再一一列举。</p>
<p>这里也顺便解释了源码官方注释为何提到2到6个节点对应的红黑树就是一棵“tree appears to have too few nodes”的实际情况：</p>
<blockquote>
<p>If the current tree appears to have too few nodes, the bin is converted back to a plain bin. (The test triggers somewhere between 2 and 6 nodes, depending on tree structure).</p>
</blockquote>
<h4 id="2-2-removeTreeNode（后部分逻辑）"><a href="#2-2-removeTreeNode（后部分逻辑）" class="headerlink" title="2.2 removeTreeNode（后部分逻辑）"></a>2.2 removeTreeNode（后部分逻辑）</h4><p>红黑树节点超过6个对应的删除逻辑（难点）</p>
<p>若目标删除节点为p，删除它前，需要考察以下三种情况（注意：以下均不考虑删除平衡调整的内容，此部分放在后面一章）：</p>
<p>1、p恰好无左、右子节点，这种情况简单，直接删除</p>
<p>2、p仅有一个子节点（左子节点或者右子节点），那么可用子节点替换p节点，符合“独子继承”的观念</p>
<p>注意这里左子节点可能是p节点一个子节点也可能是p节点的左子树，右子节点同理。</p>
<p>3、p有两个子节点，这种情况最复杂，需要找到p的后继节点或者p的前驱节点来替换p，而不是简单的用p的左子节点或用p的右子节点替换p。p的后继节点或者p的前驱节点如何找出来？（Doug Lea在源代码中选中使用p的后继节点）</p>
<blockquote>
<p>首先HashMap的红黑树节点是符合二叉查找树值的排序规则，即：</p>
<ul>
<li>若任意节点左子树不为空，它的左子树上所有节点值均小于等于它的根节点的值</li>
<li>若任意节点的右子树不为空，它的右子树上所有节点的值均大于等于它的根节点的值</li>
</ul>
<p>根据以上特点，要删除p节点就等价于用左子树中的最大节点或者右子树中的最小节点来替换p节点。</p>
<p>也即：</p>
<p>p的前继节点是指：p的左子树中的最大节点</p>
<p>p的后继节点是指：p的右子树中的最小节点</p>
</blockquote>
<p>本节内容根据removeTreeNode方法（后部分）的源代码逻辑安排以下分析框架：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 代码片段1</span></span><br><span class="line"><span class="comment">// p的左子节点pl、右子节点pr均不为空的条件下，对p和s做位置交换以及关系调整、候选节点replacement的选取</span></span><br><span class="line"><span class="keyword">if</span> (pl != <span class="keyword">null</span> &amp;&amp; pr != <span class="keyword">null</span>) &#123;</span><br><span class="line">		<span class="comment">// ......</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 代码片段2</span></span><br><span class="line"><span class="comment">// p仅有左子节点pl的条件下，此时无需后继节点s作为辅助处理，只需完成候选节点replacement的选取</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (pl != <span class="keyword">null</span>)</span><br><span class="line">  	replacement = pl;</span><br><span class="line"><span class="comment">// 代码片段3，与代码片段2对称</span></span><br><span class="line"><span class="comment">// p仅有右子节点pr的条件下，此时无需后继节点s作为辅助处理，只需完成候选节点replacement的选取</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (pr != <span class="keyword">null</span>)</span><br><span class="line">  	replacement = pr;</span><br><span class="line"><span class="comment">// 代码片段4</span></span><br><span class="line"><span class="comment">// p无左和右子节点，此时无需后继节点s作为辅助处理，只需完成候选节点replacement的选取</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   replacement = p;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 代码片段5</span></span><br><span class="line"><span class="keyword">if</span> (replacement != p)&#123;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 代码片段6</span></span><br><span class="line">TreeNode&lt;K,V&gt; r = p.red ? root : balanceDeletion(root, replacement);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 代码片段7</span></span><br><span class="line"><span class="keyword">if</span> (replacement == p)&#123;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 代码片段8</span></span><br><span class="line"><span class="keyword">if</span> (movable)&#123;</span><br><span class="line">  moveRootToFront(tab, r)</span><br><span class="line">&#125;</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<p>具体分析过程如下：</p>
<font color=red>这里必须记着一个贯穿全文策略：p来到后继节点s的位置后，要删除p节点，就等效于将p节点的候选节点replacement放到p位置，然后再将p的三个指针设为null。此外针对“单纯的删除节点操作”，以下图示不需要给出颜色标记，因为还到执行到进行删除平衡操作的逻辑</font>

<h5 id="代码片段1"><a href="#代码片段1" class="headerlink" title="代码片段1"></a>代码片段1</h5><p>p的左子节点pl、右子节点pr均不为空的条件下，对p和s做位置交换以及关系调整、候选节点replacement的选取，该处理逻辑相对复杂，因此给出对应的图示辅助大家理解其设计意图：<br><img src="https://img-blog.csdnimg.cn/d977342630a84d148cf01ce0f6a4afea.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt=""></p>
<h5 id="代码片段2"><a href="#代码片段2" class="headerlink" title="代码片段2"></a>代码片段2</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="comment">//2.1 如果p节点仅有一个左子节点pl，那么左子节点pl就是作为替代父节点p的候选节点，逻辑无需图示。</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (pl != <span class="keyword">null</span>)</span><br><span class="line">          replacement = pl;</span><br></pre></td></tr></table></figure>
<h5 id="代码片段3"><a href="#代码片段3" class="headerlink" title="代码片段3"></a>代码片段3</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="comment">// 3.1 如果p节点仅有一个右子节点pr，那么右子节点pr就是作为替代父节点p的候选节点，逻辑无需图示。</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (pr != <span class="keyword">null</span>)</span><br><span class="line">          replacement = pr;</span><br></pre></td></tr></table></figure>
<h5 id="代码片段4"><a href="#代码片段4" class="headerlink" title="代码片段4"></a>代码片段4</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 4.1 如果p节点没有左、右子节点，此情况的处理相对简单，那么先将replacement指向p节点本身，方便后面删除操作，逻辑无需图示。         </span></span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">           replacement = p;</span><br></pre></td></tr></table></figure>
<h5 id="代码片段5-7"><a href="#代码片段5-7" class="headerlink" title="代码片段5~7"></a>代码片段5~7</h5><p>后面这三个代码片段逻辑相对复杂，也给出对应的图示说明：<br><img src="https://img-blog.csdnimg.cn/b6acaff5f34a4a758e2fef4692d8a706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="代码片段8"><a href="#代码片段8" class="headerlink" title="代码片段8"></a>代码片段8</h5><p>在上面的代码片段6，若p节点为黑色节点，p删除后，必然引起所在子树少了1个黑色节点，导致红黑树黑高不平衡，需要做平衡调整。做完红黑树结构的平衡调整后，还需要保证桶位上头节点<code>table[i]</code>即是红黑树根节点，也是双向链表的头节点，因此还需在双向链表结构这一侧执行moveRootToFront操作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">     <span class="comment">//  movable默认设为true</span></span><br><span class="line"><span class="keyword">if</span> (movable)</span><br><span class="line">           moveRootToFront(tab, r);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>以上分析完成removeTreeNode方法的整体设计解析，虽然有一定难度，但好在其调整思路比较明确，因此根据图解分析也能很好掌握其设计，而最复杂的部分当然是下面的balanceDeletion操作！</p>
<h3 id="3、balanceDeletion删除平衡调整"><a href="#3、balanceDeletion删除平衡调整" class="headerlink" title="3、balanceDeletion删除平衡调整"></a>3、balanceDeletion删除平衡调整</h3><p>首先一定要清楚balanceDeletion被调用的代码上下文环境：红黑树节点超过6个的情况下，删除了一个黑色的p节点，候选节点replacement顶替p节点，接下来做树平衡调整，首先认识两种“基于红色节点”的调整策略：</p>
<h4 id="红色节点对树平衡的贡献1"><a href="#红色节点对树平衡的贡献1" class="headerlink" title="红色节点对树平衡的贡献1"></a>红色节点对树平衡的贡献1</h4><p>如图3-1，pl的兄弟节点那一侧有红色节点，经过变色和左旋后，使得左子树增加了1个黑色节点，正好使得左右子树的黑色节点数量同为N，完成平衡调整。</p>
<p>注意是一个非常有启发意义的调整算法：</p>
<font color=red>左子树的新黑色节点是右子树“补充”过来的，也即通过“左旋”将一个节点“补充”到左子树中，这是因为利用的右子树“多出1个”红色节点调整而来。我们知道红黑树的红色节点不计入黑高平衡约束，但在关键时刻我们可以将它变为黑色节点补充到缺少节点的一方，从而实现当前左右子树黑高平衡。这就是balanceDeletion的核心设计思路，理解这一点才能真正掌握红黑树的删除平衡设计逻辑。</font>

<p><img src="https://img-blog.csdnimg.cn/img_convert/09c7bbc67a79735f706e9dec7e7088ec.png" alt="balDel_1"></p>
<p>图中的N（或者N-1）标记是指当前节点到叶子节点路径所包含的黑色节点数量，便于随时观察每个小图中黑高平衡动态变化的过程。</p>
<h4 id="红色节点对树平衡的贡献2"><a href="#红色节点对树平衡的贡献2" class="headerlink" title="红色节点对树平衡的贡献2"></a>红色节点对树平衡的贡献2</h4><p>图3-4到6再次展示“红色节点对调整树平衡所起的关键作用”，这也是balanceDeletion用的调整算法之一，显然这是最简单的方式。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6f6fd3581249efa0e3be797a2483ea6e.png" alt="balDel_2"></p>
<h4 id="核心代码分析"><a href="#核心代码分析" class="headerlink" title="核心代码分析"></a>核心代码分析</h4><h5 id="代码主要框架"><a href="#代码主要框架" class="headerlink" title="代码主要框架"></a>代码主要框架</h5><p>首先其主体代码框架主要分为5部分，对应5个条件，其中第5部分的逻辑是第4部分逻辑的对称实现。如果想掌握balanceDeletion的代码实现（可能需要多次研究本文），你会发现，Doug Lea 的思路就是在第4部分经过各种调整后，让执行流来到第1或者第2或者第3部分的“循环出口”逻辑，从而结束删除平衡调整，可见第4部分的调整算法是最关键的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (TreeNode&lt;K,V&gt; xp, xpl, xpr;;) &#123;</span><br><span class="line">    <span class="comment">// 1</span></span><br><span class="line">    <span class="keyword">if</span> (x == <span class="keyword">null</span> || x == root)</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">  	<span class="comment">// 2</span></span><br><span class="line">  	<span class="keyword">else</span> <span class="keyword">if</span> ((xp = x.parent) == <span class="keyword">null</span>)&#123;</span><br><span class="line">        x.red = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">// 3</span></span><br><span class="line">   <span class="keyword">else</span> <span class="keyword">if</span> (x.red)&#123;</span><br><span class="line">        x.red = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// 4</span></span><br><span class="line">  	<span class="keyword">else</span> <span class="keyword">if</span> ((xpl = xp.left) == x)&#123;</span><br><span class="line">      <span class="comment">//......（核心逻辑也是最难的逻辑）</span></span><br><span class="line">    &#125;</span><br><span class="line">   <span class="comment">// 5 此部分逻辑和4的逻辑是对称的</span></span><br><span class="line">    <span class="keyword">else</span>&#123;<span class="comment">// symmetric</span></span><br><span class="line">      <span class="comment">//......</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="源码解析1"><a href="#源码解析1" class="headerlink" title="源码解析1"></a>源码解析1</h5><p>前面3部分源码说明，此部分的代码作用是用于循环出口，也分别称为循环出口1、循环出口2、循环出口3</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TreeNode&lt;K,V&gt; r = p.red ? root : balanceDeletion(root, replacement),这里的x节点就是replacement节点</span></span><br><span class="line">      <span class="keyword">static</span> &lt;K,V&gt; <span class="function">TreeNode&lt;K,V&gt; <span class="title">balanceDeletion</span><span class="params">(TreeNode&lt;K,V&gt; root,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                 TreeNode&lt;K,V&gt; x)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">for</span> (TreeNode&lt;K,V&gt; xp, xpl, xpr;;) &#123;</span><br><span class="line">              <span class="keyword">if</span> (x == <span class="keyword">null</span> || x == root)</span><br><span class="line">              <span class="comment">// 1、循环出口1</span></span><br><span class="line">              <span class="comment">// (1)这里是for循环的出口之一，情况4.3.2调整完后会走这个出口。</span></span><br><span class="line">              <span class="comment">// (2)当然候选节点replacement本身就是root节点，那么也不需要平衡调整，返回根节点即可。</span></span><br><span class="line">                  <span class="keyword">return</span> root;</span><br><span class="line">              <span class="comment">// 2、循环出口2</span></span><br><span class="line">              <span class="comment">// (1)这里是for循环的出口之一,表示：经过多次平衡调整后，当前调整节点x的父节点为空时，说明此时调整来到根节点位置，也即x就是根节点，直接将其变黑色即可结束调整。</span></span><br><span class="line">              <span class="comment">// (2)当然此情况也可能出现在首次循环时即满足该分支条件。</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> ((xp = x.parent) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                  x.red = <span class="keyword">false</span>;</span><br><span class="line">                  <span class="keyword">return</span> x;</span><br><span class="line">              &#125;</span><br><span class="line">              <span class="comment">// 3、非循环出口，适用最简单的调整情况。代码执行到这里，说明候选节点x的位置不是在树根位置，如果候选节点x为红色，因为删除的p是黑色，x放在p位置后，只需将x设为黑色，即可补回“刚刚被删除的黑节点p”，显然马上达到平衡，返回root节点即可。参考上面的“红色节点对树平衡的贡献2”图示。</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (x.red) &#123;</span><br><span class="line">                  x.red = <span class="keyword">false</span>;</span><br><span class="line">                  <span class="keyword">return</span> root;</span><br><span class="line">              &#125;</span><br></pre></td></tr></table></figure>
<h5 id="源码解析2"><a href="#源码解析2" class="headerlink" title="源码解析2"></a>源码解析2</h5><p>第4部分的逻辑是balanceDeletion最复杂也是最精华的设计，如果执行流能走到第4部分，说明候选节点x是黑色节点，并针对x位于xp左或者右子节点位置进行删除平衡调整。本章选择从x位于xp左子节点位置前置条件进行分析，而右边是对称情况，本章不再给出说明，调整算法也主要分为以下4种情况：</p>
<ul>
<li>4.1 候选节点x有兄弟节点xpr，且xpr为红色节点</li>
<li>4.2 候选节点x没有兄弟节点xpr</li>
<li>4.3.1 候选节点x有兄弟节点xpr，xpr为黑色节点，且xpr仅有黑色子节点或者都为空子节点（包含两个黑色子节点或者其中一个子节点为黑色）</li>
<li>4.3.2 候选节点x有兄弟节点xpr，xpr为黑色节点，且xpr存在红色子节点（包含两个红色子节点或者其中一个为红色子节点），xp节点是红色或者黑色都适用，对应图示给出这两种情况的调整过程。</li>
</ul>
<p>进一步解释：</p>
<ul>
<li>4.1的情况经过调整后，会变成4.3.1情况或者4.3.2情况，如果转为4.3.1情况，则走循环出口2结束；如果转为4.3.2情况，则走循环出口1结束。</li>
<li>4.2的情况最终可能走循环出口1或者循环出口2</li>
<li>4.3.1的情况最终走循环出口2结束。</li>
<li>4.3.2的情况，最终走循环出口1结束。</li>
</ul>
<p>关于4.1、4.2、4.3.1情况的源代码分析如下：</p>
<p>对比<a href="https://blog.csdn.net/anlian523/article/details/103649200#comments_14954397">该文</a>你会发现，它是从x位于右子树方向着手分析，而本文则按照“coder阅读代码思路（从上到下）”，源代码是从x位于左子树位置开始，因此本文也从“x在左”作为分析入口。此外，需要点赞这篇文章的一个地方——作者在结构图中的节点旁边标注黑高N，这一点对阅读者是非常友好和易于理解的，因此本文也采用了在节点旁边标注黑高，方便观察调整后黑高动态变化过程，以及两边黑高平衡情况。而其他内容包括作图等细节则无需参考该文，否则文章就没有异于他人、独立的、相对好理解的内容。</p>
<p><img src="https://img-blog.csdnimg.cn/8918d35c61bb4db1be89f87467528faf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>关于4.3.2情况的源代码分析如下：<br><img src="https://img-blog.csdnimg.cn/82cde64db30a44919c1675305adc8605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>要想写出如此精密、严谨的红黑树结构删除节点的调整逻辑确实不容易，看看jdk1.8源代码文件HashMap文件的作者，尤其是前两位。（HashMap红黑树removTreeNode和balanceDeletion的源代码非常值得多次研究或者复现）</p>
<blockquote>
<p>Since:1.2<br>Author:<br>Doug Lea, Josh Bloch, Arthur van Hoff, Neal Gafter</p>
</blockquote>
<p>这里也提供另外一个解析balanceDeletion很好的思路：使用逆向思维去考察。既然左旋是将一个黑色节点“补充到”x左子树这边，那么不妨从这个新补充的黑色节点开始逆向推导，这样你自然会思考左旋前，x节点的兄弟这一边树结构满足什么条件才能创造“左旋”的机会。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>Java高级主题</tag>
      </tags>
  </entry>
  <entry>
    <title>LinkedList数据结构设计原理的简要深入分析</title>
    <url>/2021/01/20/LinkedList%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E7%9A%84%E7%AE%80%E8%A6%81%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h5 id="成员变量以及节点定义"><a href="#成员变量以及节点定义" class="headerlink" title="成员变量以及节点定义"></a>成员变量以及节点定义</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Pointer to first node.</span></span><br><span class="line"><span class="comment"> 等效判断一定是头节点的类型： first为空且last为空，那么肯定是头节点，或者first的前驱节点为空且first节点内容不为空</span></span><br><span class="line"><span class="comment"> * Invariant: (first == null &amp;&amp; last == null) || // 空头节点</span></span><br><span class="line"><span class="comment"> *            (first.prev == null &amp;&amp; first.item != null) // 非空头节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; first;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Pointer to last node.</span></span><br><span class="line"><span class="comment"> //同上</span></span><br><span class="line"><span class="comment"> * Invariant: (first == null &amp;&amp; last == null) ||</span></span><br><span class="line"><span class="comment"> *            (last.next == null &amp;&amp; last.item != null)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; last;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>节点定义</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">    E item;</span><br><span class="line">    Node&lt;E&gt; next;</span><br><span class="line">    Node&lt;E&gt; prev;</span><br><span class="line"></span><br><span class="line">    Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.item = element;</span><br><span class="line">        <span class="keyword">this</span>.next = next; <span class="comment">// 指向后继节点</span></span><br><span class="line">        <span class="keyword">this</span>.prev = prev; <span class="comment">// 指向前驱节点</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="linkFirst"><a href="#linkFirst" class="headerlink" title="linkFirst"></a>linkFirst</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Links e as first element.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">linkFirst</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; f = first; <span class="comment">// 暂存链表</span></span><br><span class="line">  	<span class="comment">//① 头插法，新建节点时，节点的next节点直接指向链表头节点</span></span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(<span class="keyword">null</span>, e, f);</span><br><span class="line">    first = newNode; <span class="comment">// first指针指向newNode</span></span><br><span class="line">    <span class="keyword">if</span> (f == <span class="keyword">null</span>) <span class="comment">// 如果原头节点是空，说明此时newNode就是作为链表的第一个节点，显然last也是要指向它</span></span><br><span class="line">        last = newNode;</span><br><span class="line">    <span class="keyword">else</span><span class="comment">// 原头节点不为空，那么在前面①头插法后，那么newNode就可以作为原头节点前驱节点</span></span><br><span class="line">        f.prev = newNode;</span><br><span class="line">    size++;</span><br><span class="line">    modCount++; <span class="comment">// 新增节点肯定属于链表结构变化</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="linkLast"><a href="#linkLast" class="headerlink" title="linkLast"></a>linkLast</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 尾插法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">linkLast</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> Node&lt;E&gt; l = last; <span class="comment">// 暂存尾节点</span></span><br><span class="line">      <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(l, e, <span class="keyword">null</span>); <span class="comment">// 够建新节点，前驱节点指向尾部节点</span></span><br><span class="line">      last = newNode; <span class="comment">// 原尾部指针更新，指向新的尾部节点</span></span><br><span class="line">      <span class="keyword">if</span> (l == <span class="keyword">null</span>) <span class="comment">// 如果l为空，那么将first指针指向新增节点</span></span><br><span class="line">          first = newNode;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          l.next = newNode; <span class="comment">// 尾节点不为空时插入，原为节点的next指针当然要指向新增的节点</span></span><br><span class="line">      size++;</span><br><span class="line">      modCount++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="linkBefore"><a href="#linkBefore" class="headerlink" title="linkBefore"></a>linkBefore</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Inserts element e before non-null Node succ.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 在一个为非空的后继节点插入新节点</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">linkBefore</span><span class="params">(E e, Node&lt;E&gt; succ)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// assert succ != null;</span></span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; pred = succ.prev; <span class="comment">// 暂存后继节点的前节点</span></span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, succ);<span class="comment">// p&lt;=&gt;e&lt;=&gt;succ</span></span><br><span class="line">    succ.prev = newNode; <span class="comment">// 后继节点的前驱指针指向新节点</span></span><br><span class="line">    <span class="keyword">if</span> (pred == <span class="keyword">null</span>) <span class="comment">// 如果pred为空，说明此时succ其实first节点，那么在first节点前插入新节点，可知插入结束后，将first更新指向新节点</span></span><br><span class="line">        first = newNode;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        pred.next = newNode; <span class="comment">// 原pred.next是指向succ，现在指向newNode，而newNode的next指向succ</span></span><br><span class="line">    size++;</span><br><span class="line">    modCount++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="unlinkFirst"><a href="#unlinkFirst" class="headerlink" title="unlinkFirst"></a>unlinkFirst</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Unlinks non-null first node f.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 将非空头节点脱离链表</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> E <span class="title">unlinkFirst</span><span class="params">(Node&lt;E&gt; f)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// assert f == first &amp;&amp; f != null;</span></span><br><span class="line">      <span class="keyword">final</span> E element = f.item;</span><br><span class="line">      <span class="keyword">final</span> Node&lt;E&gt; next = f.next; <span class="comment">// 暂存除去头节点之后的子链表</span></span><br><span class="line">      f.item = <span class="keyword">null</span>;</span><br><span class="line">      f.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">      first = next; <span class="comment">// 更新first指向next节点，也即此时next节就是头节点角色</span></span><br><span class="line">      <span class="keyword">if</span> (next == <span class="keyword">null</span>) <span class="comment">// 如果next为空，说明处理f时，链表仅有一个头节点，删除后，last指针也要指向null</span></span><br><span class="line">          last = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          next.prev = <span class="keyword">null</span>; <span class="comment">// 若还未执行next.prev = null时，next.prev是指向原f节点，上面完成f节点删除后，next作为头节点角色，当然pred指针需要指向null</span></span><br><span class="line">      size--;</span><br><span class="line">      modCount++;</span><br><span class="line">      <span class="keyword">return</span> element;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>为何要先对xx节点判断为空时，就指向A操作，不为空时再执行B操作，</p>
<p>因为当xx节点不为空时，才能对xx节点的next或者pred指针进行操作</p>
<h5 id="unlinkLast"><a href="#unlinkLast" class="headerlink" title="unlinkLast"></a>unlinkLast</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Unlinks non-null last node l.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 将尾节点脱离链表</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> E <span class="title">unlinkLast</span><span class="params">(Node&lt;E&gt; l)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// assert l == last &amp;&amp; l != null;</span></span><br><span class="line">      <span class="keyword">final</span> E element = l.item; </span><br><span class="line">      <span class="keyword">final</span> Node&lt;E&gt; prev = l.prev; <span class="comment">// 暂存last节点之前的子链</span></span><br><span class="line">      l.item = <span class="keyword">null</span>;</span><br><span class="line">      l.prev = <span class="keyword">null</span>; <span class="comment">// help GC，此时last节点脱离原链表</span></span><br><span class="line">      last = prev;  <span class="comment">// 将last指针更新指向prev节点</span></span><br><span class="line">      <span class="keyword">if</span> (prev == <span class="keyword">null</span>) <span class="comment">// 如果pred节点为空，说明此时pred就是头节点</span></span><br><span class="line">          first = <span class="keyword">null</span>; </span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          prev.next = <span class="keyword">null</span>; <span class="comment">// 如果pred节点不为空，那么此时pred作为子链的最后一个节点，next指针自然要指向null</span></span><br><span class="line">      size--;</span><br><span class="line">      modCount++;</span><br><span class="line">      <span class="keyword">return</span> element;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="unlink"><a href="#unlink" class="headerlink" title="unlink"></a>unlink</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Unlinks non-null node x.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 将一个非空节点x脱离链表</span></span><br><span class="line">  <span class="function">E <span class="title">unlink</span><span class="params">(Node&lt;E&gt; x)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// assert x != null;</span></span><br><span class="line">      <span class="keyword">final</span> E element = x.item;   </span><br><span class="line">      <span class="keyword">final</span> Node&lt;E&gt; next = x.next;  <span class="comment">// 前子链&lt;=&gt;x&lt;=&gt;后子链 中的 x&lt;=&gt;后子链</span></span><br><span class="line">      <span class="keyword">final</span> Node&lt;E&gt; prev = x.prev;  <span class="comment">// 前子链&lt;=&gt;x&lt;=&gt;后子链 中的 前子链&lt;=&gt;x</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (prev == <span class="keyword">null</span>) &#123;   <span class="comment">// 如果x的前驱节的为空，说明此时删除x就是删除头节点，那么删除节后，first更新指向为next节点，此时next节点作为新的头节点</span></span><br><span class="line">          first = next;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          prev.next = next; <span class="comment">// 将pred.next=&gt;x=&gt;next 改为pred.next =&gt; next</span></span><br><span class="line">          x.prev = <span class="keyword">null</span>; <span class="comment">// 将x节点脱离前驱节点</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (next == <span class="keyword">null</span>) &#123; <span class="comment">// pred&lt;=&gt;x-&gt;null  变为 pred-&gt;null，那么删除x后，last更新指向新的尾节点pred</span></span><br><span class="line">          last = prev;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          next.prev = prev;  <span class="comment">// 原来next的前驱指针是指向x，那么x删除后，next.pred指向x节点的前驱节点即可：pred.next &lt;= next</span></span><br><span class="line">          x.next = <span class="keyword">null</span>; <span class="comment">// 将x节点脱离后继节点</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      x.item = <span class="keyword">null</span>; <span class="comment">// 前面已经完成pred、next引用指向为null，那么再将x节点的数据域置为null，help GC</span></span><br><span class="line">      size--;</span><br><span class="line">      modCount++;</span><br><span class="line">      <span class="keyword">return</span> element;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="getFirst"><a href="#getFirst" class="headerlink" title="getFirst"></a>getFirst</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the first element in this list.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the first element in this list</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> NoSuchElementException if this list is empty</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 如果是空链表调用getFirst()方法就会抛出NSEE异常</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">getFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; f = first; <span class="comment">// </span></span><br><span class="line">    <span class="keyword">if</span> (f == <span class="keyword">null</span>) <span class="comment">// 注意技巧：先取出节点，再判断节点是否为空</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">    <span class="keyword">return</span> f.item; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="removeFirst"><a href="#removeFirst" class="headerlink" title="removeFirst"></a>removeFirst</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//NoSuchElementException – if this list is empty</span></span><br><span class="line"><span class="comment">//如果是空链表调用removeFirst直接抛出异常</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">removeFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; f = first;  <span class="comment">// 先取出节点</span></span><br><span class="line">    <span class="keyword">if</span> (f == <span class="keyword">null</span>) <span class="comment">// 说明是空链表</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">    <span class="keyword">return</span> unlinkFirst(f); <span class="comment">// 非空链表情况下调用unlinkFirst将first节点脱离链表</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>removeLast同理</p>
<h5 id="addFirst"><a href="#addFirst" class="headerlink" title="addFirst"></a>addFirst</h5><p>addFirst内部调用linkFirst实现将e节点使用头插法插入链表</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Inserts the specified element at the beginning of this list.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> e the element to add</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    linkFirst(e);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="add"><a href="#add" class="headerlink" title="add"></a>add</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Appends the specified element to the end of this list.</span></span><br><span class="line"><span class="comment"> 默认使用尾插法插入节点</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;This method is equivalent to &#123;<span class="doctag">@link</span> #addLast&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> e element to be appended to this list</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; (as specified by &#123;<span class="doctag">@link</span> Collection#add&#125;)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    linkLast(e);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="remove"><a href="#remove" class="headerlink" title="remove"></a>remove</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Removes the first occurrence of the specified element from this list,</span></span><br><span class="line"><span class="comment"> * if it is present.  If this list does not contain the element, it is</span></span><br><span class="line"><span class="comment"> * unchanged.  More formally, removes the element with the lowest index</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@code</span> i&#125; such that</span></span><br><span class="line"><span class="comment"> * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;</span></span><br><span class="line"><span class="comment"> * (if such an element exists).  Returns &#123;<span class="doctag">@code</span> true&#125; if this list</span></span><br><span class="line"><span class="comment"> * contained the specified element (or equivalently, if this list</span></span><br><span class="line"><span class="comment"> * changed as a result of the call).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> o element to be removed from this list, if present</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if this list contained the specified element</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 分两种删除方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">      	<span class="comment">// 1、如果给定的对象为空，那么直接遍历链表，找出链表的item为null的节点x，并删除x节点</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">            <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) &#123;</span><br><span class="line">                unlink(x);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 2、如果给定的对象为空，那么就遍历链表节点的item和o.item相等的节点，并删除之</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">            <span class="keyword">if</span> (o.equals(x.item)) &#123;</span><br><span class="line">                unlink(x);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 找不到给定o，当然是返回false</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="clear"><a href="#clear" class="headerlink" title="clear"></a>clear</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Clearing all of the links between nodes is &quot;unnecessary&quot;, but:</span></span><br><span class="line">    <span class="comment">// - helps a generational GC if the discarded nodes inhabit</span></span><br><span class="line">    <span class="comment">//   more than one generation</span></span><br><span class="line">    <span class="comment">// - is sure to free memory even if there is a reachable Iterator</span></span><br><span class="line">    <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; ) &#123;</span><br><span class="line">        Node&lt;E&gt; next = x.next; <span class="comment">// 先暂存除去x节点的子类，next显然为子链头节点</span></span><br><span class="line">        x.item = <span class="keyword">null</span>;</span><br><span class="line">        x.next = <span class="keyword">null</span>;</span><br><span class="line">        x.prev = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">        x = next; <span class="comment">// 将x指针更新指向next节点</span></span><br><span class="line">    &#125;</span><br><span class="line">    first = last = <span class="keyword">null</span>; <span class="comment">// 头尾指针也要置空</span></span><br><span class="line">    size = <span class="number">0</span>; <span class="comment">// size肯定为0</span></span><br><span class="line">    modCount++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此外链表也提供索引方式来访问元素</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedListDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        LinkedList&lt;String&gt; list=<span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        list.add(<span class="string">&quot;foo&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;bar&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;foobar&quot;</span>);</span><br><span class="line">         System.out.println(list.indexOf(<span class="string">&quot;bar&quot;</span>)); <span class="comment">// 1</span></span><br><span class="line">         System.out.println(list.get(<span class="number">0</span>)); <span class="comment">// foo</span></span><br><span class="line">         System.out.println(list.get(<span class="number">10</span>)); <span class="comment">//IndexOutOfBoundsException</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Positional Access Operations</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the element at the specified position in this list.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> index index of the element to return</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the element at the specified position in this list</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IndexOutOfBoundsException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    checkElementIndex(index);</span><br><span class="line">    <span class="keyword">return</span> node(index).item;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="checkElementIndex"><a href="#checkElementIndex" class="headerlink" title="checkElementIndex"></a>checkElementIndex</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkElementIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (!isElementIndex(index))</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Tells if the argument is the index of an existing element.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="comment">//索引是从0开始到size-1,注意：这里是检查已存在节点的索引</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isElementIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> index &gt;= <span class="number">0</span> &amp;&amp; index &lt; size;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Tells if the argument is the index of a valid position for an</span></span><br><span class="line"><span class="comment">   * iterator or an add operation.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 为迭代器或者add操作判断给定索引是否有效位置</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> index &gt;= <span class="number">0</span> &amp;&amp; index &lt;= size;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="indexOf"><a href="#indexOf" class="headerlink" title="indexOf"></a>indexOf</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">//Returns the index of the first occurrence of the specified element in this list,</span></span><br><span class="line"><span class="comment">//从链表头节点位置开始遍历</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">indexOf</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> index = <span class="number">0</span>; <span class="comment">//从0开始计数</span></span><br><span class="line">       <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123; </span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) <span class="comment">// 如果first节点恰好为null，那么return的index就是0，因此头节点和索引0可对齐，那么其他节点和之后的索引也能对齐。</span></span><br><span class="line">                   <span class="keyword">return</span> index;</span><br><span class="line">               index++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="keyword">if</span> (o.equals(x.item))</span><br><span class="line">                   <span class="keyword">return</span> index;</span><br><span class="line">               index++;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>indexOf是从头节点开始遍历链表，而lastIndexOf是从尾部节点方向遍历</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lastIndexOf</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> index = size; <span class="comment">//取链表长度作为起始点</span></span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Node&lt;E&gt; x = last; x != <span class="keyword">null</span>; x = x.prev) &#123;</span><br><span class="line">            index--; </span><br><span class="line">            <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) </span><br><span class="line">                <span class="keyword">return</span> index;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (Node&lt;E&gt; x = last; x != <span class="keyword">null</span>; x = x.prev) &#123;</span><br><span class="line">            index--;<span class="comment">// 这里为何先自减1呢？ 因为链表的索引范围是0到size-1</span></span><br><span class="line">            <span class="keyword">if</span> (o.equals(x.item))<span class="comment">//假设尾节点恰好满足条件，那么返回的index就是size-1值，所以上面一行用index++的原因。</span></span><br><span class="line">                <span class="keyword">return</span> index;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="node"><a href="#node" class="headerlink" title="node"></a>node</h5><p>根据索引返回链表中的节点，这是一个关键方法</p>
<p>通过它可以直接复用链表遍历的所有方法，因为node返回index指向的节点，那么套用入参为节点类型的方法即可完成复用</p>
<p>注意索引指向的节点：例如index=2，表示指向第3个节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the (non-null) Node at the specified element index.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">Node&lt;E&gt; <span class="title">node</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// assert isElementIndex(index);</span></span><br><span class="line"><span class="comment">//如果索引小于链表长度的一半，则从链表头节点开始遍历</span></span><br><span class="line">    <span class="keyword">if</span> (index &lt; (size &gt;&gt; <span class="number">1</span>)) &#123;</span><br><span class="line">        Node&lt;E&gt; x = first; <span class="comment">//应对index=0的情况，此时无需进入for，直接返回x</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++) <span class="comment">//针对其他节点，注意是从0开始遍历，如果index=2，对应的节点位置是在第3个节点位置，因为x=first算是一次，接下来在for循环恰好可以遍历2次，总共三次。</span></span><br><span class="line">            x = x.next;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      	<span class="comment">//如果索引大于链表长度的一半，则从尾部节点开始遍历</span></span><br><span class="line">        Node&lt;E&gt; x = last; <span class="comment">//从尾部节点开始，注意是从size-1开始，考察index=size-1时，则i&gt;index不会满足，不进入for循环而直接返回尾节点</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = size - <span class="number">1</span>; i &gt; index; i--)</span><br><span class="line">            x = x.prev;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="add方法-添加对象"><a href="#add方法-添加对象" class="headerlink" title="add方法-添加对象"></a>add方法-添加对象</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Appends the specified element to the end of this list.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;This method is equivalent to &#123;<span class="doctag">@link</span> #addLast&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> e element to be appended to this list</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; (as specified by &#123;<span class="doctag">@link</span> Collection#add&#125;)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    linkLast(e);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="add方法-索引"><a href="#add方法-索引" class="headerlink" title="add方法-索引"></a>add方法-索引</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Inserts the specified element at the specified position in this list.</span></span><br><span class="line"><span class="comment"> * Shifts the element currently at that position (if any) and any</span></span><br><span class="line"><span class="comment"> * subsequent elements to the right (adds one to their indices).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> index index at which the specified element is to be inserted</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> element element to be inserted</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IndexOutOfBoundsException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">    checkPositionIndex(index); <span class="comment">// 这里的index可以取到size值：index&gt;=0 &amp;&amp; &lt;=size</span></span><br><span class="line">	</span><br><span class="line">    <span class="keyword">if</span> (index == size) <span class="comment">// 如果index就是size大小，那么说明是要在尾部插入新元素</span></span><br><span class="line">        linkLast(element);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="comment">// 注意：在这里是将新元素添加到node(index)这个节点的前面</span></span><br><span class="line">        linkBefore(element, node(index));  </span><br><span class="line">  		<span class="comment">// pred&lt;--&gt;ele&lt;--&gt;node(index)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    checkElementIndex(index); <span class="comment">// remove时的索引判断范围：index&gt;=0&amp;&amp;index&lt;size,如果取index&lt;=size，那么=号就会造成越界</span></span><br><span class="line">    <span class="keyword">return</span> unlink(node(index)); <span class="comment">// nodex(index)找到index指向的节点，然后复用unlink方法完成删除节点</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="peek"><a href="#peek" class="headerlink" title="peek"></a>peek</h5><p>记着：先取出头节点，然后判断是否为null在做出其他操作处理</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">peek</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node&lt;E&gt; f = first;  <span class="comment">//</span></span><br><span class="line">    <span class="keyword">return</span> (f == <span class="keyword">null</span>) ? <span class="keyword">null</span> : f.item;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>模拟stack的方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">push</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    addFirst(e);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> removeFirst();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="removeLastOccurrence"><a href="#removeLastOccurrence" class="headerlink" title="removeLastOccurrence"></a>removeLastOccurrence</h5><p>从尾部移除首次符合给定item的节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">removeLastOccurrence</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Node&lt;E&gt; x = last; x != <span class="keyword">null</span>; x = x.prev) &#123;</span><br><span class="line">            <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) &#123;</span><br><span class="line">                unlink(x);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      	<span class="comment">//从尾部遍历，删除首次符合条件的节点</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;E&gt; x = last; x != <span class="keyword">null</span>; x = x.prev) &#123;</span><br><span class="line">            <span class="keyword">if</span> (o.equals(x.item)) &#123;</span><br><span class="line">                unlink(x);<span class="comment">// 找到这个节点后，用unlink删除之</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="toArray"><a href="#toArray" class="headerlink" title="toArray"></a>toArray</h5><p>创建一个size同大的数组，然后从链表中，一个个拷贝到这个空数组中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Object[] toArray() &#123;</span><br><span class="line">    Object[] result = <span class="keyword">new</span> Object[size];</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>; <span class="comment">// 当然是从0开始拷贝</span></span><br><span class="line">    <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next)</span><br><span class="line">        result[i++] = x.item; <span class="comment">//如果有2个节点，那么此时i++=1,也即res[1]=第2个节点的item</span></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="链表迭代器创建"><a href="#链表迭代器创建" class="headerlink" title="链表迭代器创建"></a>链表迭代器创建</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> ListIterator&lt;E&gt; <span class="title">listIterator</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        checkPositionIndex(index);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ListItr(index);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 内部迭代器类</span></span><br><span class="line">    <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ListItr</span> <span class="keyword">implements</span> <span class="title">ListIterator</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Node&lt;E&gt; lastReturned;</span><br><span class="line">        <span class="keyword">private</span> Node&lt;E&gt; next;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> nextIndex;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> expectedModCount = modCount;</span><br><span class="line"></span><br><span class="line">        ListItr(<span class="keyword">int</span> index) &#123;</span><br><span class="line">            <span class="comment">// assert isPositionIndex(index);</span></span><br><span class="line">            next = (index == size) ? <span class="keyword">null</span> : node(index); <span class="comment">//next指向index对应的节点，注意这里不是</span></span><br><span class="line">          <span class="comment">/*x.next=node(index)的写法，next节点就是指代当前index节点</span></span><br><span class="line"><span class="comment">          foo-&gt;bar-&gt;foobar-&gt;null,node(index=1)就是指bar这个节点，next也是指向bar这个节点，而不是next指向node(index=1)的next节点——foobar</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">            nextIndex = index; </span><br><span class="line">          <span class="comment">// 其实next和nextIndex都是指向给定index的初始位置，然后不断移动指针</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> nextIndex &lt; size; <span class="comment">//只要遍历指针不超过size，就说明还有节点</span></span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">         ListIterator&lt;String&gt; listIterator=list.listIterator(1); </span></span><br><span class="line"><span class="comment">         System.out.println(listIterator.next());  //输出：bar节点，注意这里不是输出foobar节点！！！</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          <span class="comment">/*checkForComodification方法很实用，因为在创建链表迭代器时就默认将expectedModCount=modCount，</span></span><br><span class="line"><span class="comment">          因此在迭代过程中，如果没有删除或者写入节点操作，那么modCount计数将不会改变。</span></span><br><span class="line"><span class="comment">             final void checkForComodification() &#123;</span></span><br><span class="line"><span class="comment">                  if (modCount != expectedModCount)</span></span><br><span class="line"><span class="comment">                      throw new ConcurrentModificationException();</span></span><br><span class="line"><span class="comment">              &#125;</span></span><br><span class="line"><span class="comment">              //实际场景</span></span><br><span class="line"><span class="comment">              while (listIterator.hasNext())&#123;</span></span><br><span class="line"><span class="comment">                   System.out.println(listIterator.next());</span></span><br><span class="line"><span class="comment">                   list.remove(&quot;bar&quot;);</span></span><br><span class="line"><span class="comment">               &#125;</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">          <span class="comment">// 1、查看是否迭代过程有写操作</span></span><br><span class="line">            checkForComodification();</span><br><span class="line">          <span class="comment">// 2、没有节点可迭代，抛出异常</span></span><br><span class="line">            <span class="keyword">if</span> (!hasNext())</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line"></span><br><span class="line">            lastReturned = next; <span class="comment">//暂存当前节点结果，用于返回</span></span><br><span class="line">            next = next.next; <span class="comment">//next指针指向下一个节点</span></span><br><span class="line">            nextIndex++;<span class="comment">// nextIndex指针自增</span></span><br><span class="line">            <span class="keyword">return</span> lastReturned.item; <span class="comment">// 第一次调用next()方法不要以为是调用下一个节点，而是指向当前首个节点</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasPrevious</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          <span class="comment">//因为是从index开始反向迭代，nextIndex=index,然后nextIndex自减，只要自减到不等于0就说明还有节点，为何不是=0？</span></span><br><span class="line">            <span class="keyword">return</span> nextIndex &gt; <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//反向迭代</span></span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">         ListIterator&lt;String&gt; listIterator=list.listIterator(1); </span></span><br><span class="line"><span class="comment">         System.out.println(listIterator.previous()); //输出：虽然index=1指向bar这个节点，但输出时foo节点</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> E <span class="title">previous</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          	<span class="comment">//前面两个条件同next()</span></span><br><span class="line">            checkForComodification();</span><br><span class="line">            <span class="keyword">if</span> (!hasPrevious())</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line"></span><br><span class="line">            lastReturned = next = (next == <span class="keyword">null</span>) ? last : next.prev; <span class="comment">//当前节点的上一个节点</span></span><br><span class="line">            nextIndex--;</span><br><span class="line">            <span class="keyword">return</span> lastReturned.item;<span class="comment">// 返回next（index指向的节点）的前一个节点</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">nextIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> nextIndex; <span class="comment">// 在new阶段， nextIndex = index，绝不是index的下一个索引值</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">previousIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> nextIndex - <span class="number">1</span>; <span class="comment">//因为index作为nextIndex，那么index的前一个指针值显然就是nextIndex-1</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">      remove方法的使用：必须有next()迭代器调用后，才能执行一次remove，每次remove结束后，lastReturned会置为null，说明每次next()对应一次remove，否则会抛出错误</span></span><br><span class="line"><span class="comment">      </span></span><br><span class="line"><span class="comment">      （1） 未启用过next()</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator(1);</span></span><br><span class="line"><span class="comment">        listIterator.remove(); //lastReturned表示上次返回的节点，显然这里还没开始调用next()就直接调用remove，因此抛出错误</span></span><br><span class="line"><span class="comment">        System.out.println(list);   </span></span><br><span class="line"><span class="comment">        </span></span><br><span class="line"><span class="comment">        （2）启用过next()</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator(1);</span></span><br><span class="line"><span class="comment">        listIterator.next(); // bar</span></span><br><span class="line"><span class="comment">        listIterator.remove();// 删除的bar节点，不是 forbar！</span></span><br><span class="line"><span class="comment">        System.out.println(list); // [foo, foobar]</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      </span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            checkForComodification();</span><br><span class="line">           <span class="comment">// 若还没使用next()迭代一次，抛出非法状态</span></span><br><span class="line">            <span class="keyword">if</span> (lastReturned == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">						<span class="comment">// 调用next()后，next指向索引index的节点，且lastReturned也是指向next</span></span><br><span class="line">            Node&lt;E&gt; lastNext = lastReturned.next;</span><br><span class="line">            unlink(lastReturned); <span class="comment">// 删除的是index节点</span></span><br><span class="line">            <span class="keyword">if</span> (next == lastReturned)</span><br><span class="line">                next = lastNext; <span class="comment">//将next指针指向lastReturned的下一个节点</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                nextIndex--;</span><br><span class="line">            lastReturned = <span class="keyword">null</span>; <span class="comment">// 这里lastReturned为null保证了每次next()才能remove</span></span><br><span class="line">            expectedModCount++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 同样，迭代器需要启用一次next()方法，调用set方法才不会抛出异常，这是因为next()方法调用会将lastReturned指向next首个遍历节点，否则就认为在无指向节点情况非法使用set方法</span></span><br><span class="line">			<span class="comment">/*</span></span><br><span class="line"><span class="comment">			  list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator(1);</span></span><br><span class="line"><span class="comment">        listIterator.next(); // bar</span></span><br><span class="line"><span class="comment">        listIterator.set(&quot;newbar&quot;);// 因为next指向bar，而lastReturned指向next，因此lastReturned执行bar节点，执行lastReturned.item=&quot;newbar&quot;覆盖了bar值，</span></span><br><span class="line"><span class="comment">        输出为：[foo, newbar, foobar]</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      </span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (lastReturned == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException();</span><br><span class="line">            checkForComodification();</span><br><span class="line">            lastReturned.item = e;<span class="comment">//其实这里的lastReturned就是next，而next指向当前index节点。          </span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	<span class="comment">/*首先，add是不需要next()来启动迭代器的</span></span><br><span class="line"><span class="comment">      	（1）next指向非空节点</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator(1);  next指向node(index=1)也即next指向bar这个节点</span></span><br><span class="line"><span class="comment">        listIterator.add(&quot;newbar&quot;); // next指向非空节点，则使用linkBefore(e,next)</span></span><br><span class="line"><span class="comment">        System.out.println(list); // [foo, newbar, bar, foobar]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  		 （2）next指向空链表</span></span><br><span class="line"><span class="comment">        LinkedList&lt;String&gt; list = new LinkedList&lt;&gt;();</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator();</span></span><br><span class="line"><span class="comment">        listIterator.add(&quot;newbar&quot;); //空链表，直接linkLast(e)</span></span><br><span class="line"><span class="comment">        System.out.println(list); </span></span><br><span class="line"><span class="comment">        </span></span><br><span class="line"><span class="comment">       （3）next指向非空链表的尾节点</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator(3); //checkPositionIndex(index)，可等于size</span></span><br><span class="line"><span class="comment">        listIterator.add(&quot;newbar&quot;); //因为index的值为size=3，因此index指向第4个节点，即为null，因此根据next = (index == size) ? null : node(index)；next指向null，故在add内部使用linkLast(e)</span></span><br><span class="line"><span class="comment">        System.out.println(list);</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">            checkForComodification();</span><br><span class="line">            lastReturned = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">if</span> (next == <span class="keyword">null</span>)</span><br><span class="line">                linkLast(e);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                linkBefore(e, next);</span><br><span class="line">            nextIndex++;</span><br><span class="line">            expectedModCount++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">        LinkedList&lt;String&gt; list = new LinkedList&lt;&gt;();</span></span><br><span class="line"><span class="comment">        list.add(&quot;foo&quot;);//0</span></span><br><span class="line"><span class="comment">        list.add(&quot;bar&quot;);//1</span></span><br><span class="line"><span class="comment">        list.add(&quot;foobar&quot;);//2</span></span><br><span class="line"><span class="comment">        ListIterator&lt;String&gt; listIterator = list.listIterator();</span></span><br><span class="line"><span class="comment">//        list.forEach(s-&gt;System.out.println(s+&quot;:&quot;+s.length()));</span></span><br><span class="line"><span class="comment">        listIterator.next(); // 已经消耗bar这个节点，next()方法内部，lastReturned指向next，然后next指针更新，将next=next.next，</span></span><br><span class="line"><span class="comment">        //剩下的两个节点将会被forEachRemaining输出</span></span><br><span class="line"><span class="comment">        listIterator.forEachRemaining(s-&gt;System.out.println(s+&quot;:&quot;+s.length()));</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">      </span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">forEachRemaining</span><span class="params">(Consumer&lt;? <span class="keyword">super</span> E&gt; action)</span> </span>&#123;</span><br><span class="line">          	<span class="comment">//  action：非空的lambda函数</span></span><br><span class="line">            Objects.requireNonNull(action);</span><br><span class="line">            <span class="keyword">while</span> (modCount == expectedModCount &amp;&amp; nextIndex &lt; size) &#123;</span><br><span class="line">              	<span class="comment">// 在next()方法内部，next = next.next，也即next已经更新到指向下一个节点，因此可以实现“Remaining”的效果</span></span><br><span class="line">              <span class="comment">// 当前节点处理时next</span></span><br><span class="line">                action.accept(next.item);</span><br><span class="line">                lastReturned = next; <span class="comment">// 更新lastReturned指针指向新节点</span></span><br><span class="line">                next = next.next; <span class="comment">//更新next指针指向新节点</span></span><br><span class="line">                nextIndex++;<span class="comment">// 索引加1</span></span><br><span class="line">            &#125;</span><br><span class="line">            checkForComodification();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (modCount != expectedModCount)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="反向迭代器"><a href="#反向迭代器" class="headerlink" title="反向迭代器"></a>反向迭代器</h5><p>实例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">LinkedList&lt;String&gt; list = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">list.add(<span class="string">&quot;foo&quot;</span>);<span class="comment">//0</span></span><br><span class="line">list.add(<span class="string">&quot;bar&quot;</span>);<span class="comment">//1</span></span><br><span class="line">list.add(<span class="string">&quot;foobar&quot;</span>);<span class="comment">//2</span></span><br><span class="line">Iterator&lt;String&gt;  itr=list.descendingIterator();</span><br><span class="line">System.out.println(itr.next());</span><br></pre></td></tr></table></figure>
<p>descendingIterator</p>
<p>设计很巧妙：使用一个私有方法将正向迭代器的相关方法封装以下，然后在一个公开方法内部new一个私有迭代器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Iterator&lt;E&gt; <span class="title">descendingIterator</span><span class="params">()</span> </span>&#123; <span class="comment">// 注意：公开方法是Iterator类型，不是ListIterator，因此在实例使用中使用Iterator&lt;String&gt;  itr=list.descendingIterator()  创建反向迭代器</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DescendingIterator();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Adapter to provide descending iterators via ListItr.previous</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">DescendingIterator</span> <span class="keyword">implements</span> <span class="title">Iterator</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ListItr itr = <span class="keyword">new</span> ListItr(size()); <span class="comment">// 这里的index取的是size值，因此在next()方法调用previous()方法时，previous()方法内部的next是指向null的，因此DescendingIterator的next()方法首次是返回last节点。</span></span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> itr.hasPrevious();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> itr.previous();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        itr.remove();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>参考previous方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> E <span class="title">previous</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      checkForComodification();</span><br><span class="line">      <span class="keyword">if</span> (!hasPrevious())</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line"><span class="comment">//如果ListItr itr = new ListItr(size()) ，那么next指针是指向null，因此根据下面可知，当previous()调用时，next要被更新指向last</span></span><br><span class="line">      lastReturned = next = (next == <span class="keyword">null</span>) ? last : next.prev;</span><br><span class="line">      nextIndex--;</span><br><span class="line">      <span class="keyword">return</span> lastReturned.item;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>Java高级主题</tag>
      </tags>
  </entry>
  <entry>
    <title>MariaDB+Keepalived 搭建双主HA数据库服务</title>
    <url>/2019/08/26/MariaDB+Keepalived%20%E6%90%AD%E5%BB%BA%E5%8F%8C%E4%B8%BBHA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<h5 id="1、安装mysql"><a href="#1、安装mysql" class="headerlink" title="1、安装mysql"></a>1、安装mysql</h5><p>&#8195;&#8195;在linux版本下，mysql称为mariadb，可以选择在线安装，或编译安装。MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可。开发这个分支的原因之一是：甲骨文公司收购了MySQL后，有将MySQL闭源的潜在风险，因此社区采用分支的方式来避开这个风险。MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。</p>
<a id="more"></a>
<p>&#8195;&#8195;用国内镜像源替换官方的MariaDB镜像源，目前MariaDB最新版本10.4，这里选10.3作为稳定版安装，本人项目或者测试环境，都是以MariaDB为主。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/yum.repos.d/MariaDB.repo</span><br><span class="line">[mariadb]</span><br><span class="line">name = MariaDB</span><br><span class="line">baseurl = http://mirrors.ustc.edu.cn/mariadb/yum/10.3/centos7-amd64/</span><br><span class="line">gpgkey=http://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDB</span><br><span class="line">gpgcheck=1</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install mariadb mariadb-server -y</span><br><span class="line">systemctl restart mariadb</span><br><span class="line">systemctl enable mariadb</span><br><span class="line">mysql_secure_installation</span><br><span class="line"><span class="meta">#</span><span class="bash"> 该命令为重置root密码，并做一些安全配置,若不做配置，后续将无法使用sell进入mysql</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="2、配置主备server的my-cnf"><a href="#2、配置主备server的my-cnf" class="headerlink" title="2、配置主备server的my.cnf"></a>2、配置主备server的my.cnf</h5><p>&#8195;&#8195;通过my.cnf可以优化或者定制更高级的mysql使用，具体参考另外一篇文章==todo==，这里只写简单主主同步配置</p>
<p>主服务器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 数据目录可以使用默认也可以自行定义，所有的binlog以及db物理文件都在datadir里面后期可以通过挂载存储扩容</span></span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/lib/mysql/mysql.sock</span><br><span class="line">server-id=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启日志模式</span></span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">relay-log=mysql-relay-bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 双主模式下，防止两边插入插入时，自增键冲突，主服务器自增规则：1，3，5奇数自增</span></span><br><span class="line">auto_increment_offset=1</span><br><span class="line">auto_increment_increment=2</span><br><span class="line">log_slave_updates =1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略一些测试表</span></span><br><span class="line">replicate-wild-ignore-table=test.%</span><br><span class="line">replicate-wild-ignore-table=mysql.%</span><br><span class="line">replicate-wild-ignore-table=performance_schema.%</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">出现错误后忽略，若不跳过，出现任何同步错误，slave-IO进程会终止</span></span><br><span class="line">slave-skip-errors=all</span><br></pre></td></tr></table></figure>
<p>备服务器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/lib/mysql/mysql.sock</span><br><span class="line"></span><br><span class="line">server-id=2</span><br><span class="line"></span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">relay-log=mysql-relay-bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 双主模式下，防止两边插入插入时，自增键冲突，备服务器自增规则：2，4，6奇数自增</span></span><br><span class="line">auto_increment_offset=2</span><br><span class="line">auto_increment_increment=2</span><br><span class="line">log_slave_updates =1</span><br><span class="line"></span><br><span class="line">replicate-wild-ignore-table=test.%</span><br><span class="line">replicate-wild-ignore-table=mysql.%</span><br><span class="line">replicate-wild-ignore-table=performance_schema.%</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">出现错误后忽略，若不跳过，出现任何同步错误，slave-IO进程会终止</span></span><br><span class="line">slave-skip-errors=all</span><br></pre></td></tr></table></figure>
<h5 id="3、创建新账户"><a href="#3、创建新账户" class="headerlink" title="3、创建新账户"></a>3、创建新账户</h5><p>&#8195;&#8195;在主服务器，备服务器分别创建用于管理主备的msyql帐号，请勿用root账户，以免出现安全问题！<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql -u root -psd123321sd</span><br><span class="line">create user &#x27;sync_acct&#x27;@&#x27;192.168.100.%&#x27; identified by &#x27;*&amp;@jall190&#x27;;</span><br><span class="line">grant replication slave on *.* to &#x27;sync_acct&#x27;@&#x27;192.168.100.%&#x27;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下两条配置可以实现在shell中mysql -usync_acct -p*&amp;@jall190 直接登录，否则会提示如下：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ERROR 1045 (28000): Access denied <span class="keyword">for</span> user <span class="string">&#x27;sync_acct&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> (using password: YES)，登录失败</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grant select,insert,update,delete on</span> </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO sync_acc@&quot;127.0.0.1&quot; IDENTIFIED BY &quot;*&amp;@jall190&quot; WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO sync_acc@&quot;localhost&quot; IDENTIFIED BY &quot;*&amp;@jall190&quot; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看可本地登录的记录</span></span><br><span class="line">MariaDB [(none)]&gt; select HOST,User from mysql.user;</span><br><span class="line">+-----------------------+-----------+</span><br><span class="line">| HOST                  | User      |</span><br><span class="line">+-----------------------+-----------+</span><br><span class="line">| 127.0.0.1             | root      |</span><br><span class="line">| 127.0.0.1             | sync_acc  |</span><br><span class="line">| 192.168.100.%         | sync_acct |</span><br><span class="line">| ::1                   | root      |</span><br><span class="line">| localhost             | root      |</span><br><span class="line">| localhost             | sync_acc  |</span><br><span class="line">| localhost.localdomain | root      |</span><br><span class="line">+-----------------------+-----------+</span><br></pre></td></tr></table></figure></p>
<h5 id="4、查看主服务器bin-log信息"><a href="#4、查看主服务器bin-log信息" class="headerlink" title="4、查看主服务器bin-log信息"></a>4、查看主服务器bin-log信息</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">对数据库进行只读锁定（防止查看二进制日志同时有人对数据库修改操作）</span></span><br><span class="line">MariaDB [(none)]&gt; flush tables with read lock;</span><br><span class="line">MariaDB [(none)]&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">| mysql-bin.000002 |      504 |              |                  |</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">MariaDB [(none)]&gt; unlock tables;</span><br></pre></td></tr></table></figure>
<p>以上的file名称：mysql-bin.000002跟position：504在一下主备需要用到</p>
<h5 id="5、分别在主备服务器上配置主-主模式"><a href="#5、分别在主备服务器上配置主-主模式" class="headerlink" title="5、分别在主备服务器上配置主-主模式"></a>5、分别在主备服务器上配置主-主模式</h5><p>（1） 在备服务器上，配置db1-192.168.100.5作为master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接在命令行上敲</span></span><br><span class="line">MariaDB [(none)]&gt; change master to </span><br><span class="line">master_host=&#x27;192.168.100.5&#x27;, </span><br><span class="line">master_port=41210,</span><br><span class="line">master_user=&#x27;sync_acct&#x27;, </span><br><span class="line">master_password=&#x27;*&amp;@jall190&#x27;, </span><br><span class="line"><span class="meta">#</span><span class="bash">指明初始复制时的mysql1中的binlog文件</span></span><br><span class="line">master_log_file=&#x27;mysql-bin.000003&#x27;,</span><br><span class="line"><span class="meta">#</span><span class="bash">指明初始复制时binlog文件的位置</span></span><br><span class="line">master_log_pos=655;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; start slave;</span><br></pre></td></tr></table></figure>
<p>​    从服务器开启slave线程后，通过show slave status\G可以看到一些信息，这里\G是表示按竖立显示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.100.5</span><br><span class="line">                  Master_User: sync_acct</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000002</span><br><span class="line">          Read_Master_Log_Pos: 4041005</span><br><span class="line">               Relay_Log_File: mysql-relay-bin.000003</span><br><span class="line">                Relay_Log_Pos: 4041030</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000002</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">  ......</span><br></pre></td></tr></table></figure>
<p>以上信息可知</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#这个是指Slave连接到Master的状态，当前IO线程的状态为等待master发送事件，该字段显示mysql不同状态的不同信息，而且信息简单易读</span><br><span class="line">Slave_IO_State: Waiting for master to send event：</span><br><span class="line"></span><br><span class="line">Slave_IO_Running: Yes，显示slave的I&#x2F;O线程是否被启动并成功地连接到主服务器上。</span><br><span class="line">Slave_SQL_Running: Yes，显示slave上用于读取Relay_Log的SQL线程是否被启动</span><br></pre></td></tr></table></figure>
<p> 另外，备份Slave_IO_State信息对于在主服务器上的状态，用<code>show processlist</code>查看主服务器显示master已经发送所有的binlog到salve，并等待主服务器更新这个binlog</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show processlist\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">      Id: 24</span><br><span class="line">    User: sync_acct</span><br><span class="line">    Host: 192.168.100.6:35474</span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Binlog Dump</span><br><span class="line">    Time: 1143</span><br><span class="line">   State: Master has sent all binlog to slave; waiting for binlog to be updated</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br></pre></td></tr></table></figure>
<p>以上说明，主-备模式成功配置，但还不是主-主模式</p>
<p>（2） 在(1)上，已经配置好主-从同步，且已经验证可正常写入同步，这里，将配置主-主模式，在主服务器db2-192.168.100.5上，配置db2-192.168.100.6作为master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接在命令行上敲</span></span><br><span class="line">MariaDB [(none)]&gt; change master to </span><br><span class="line">master_host=&#x27;192.168.100.6&#x27;, </span><br><span class="line">master_user=&#x27;sync_acct&#x27;, </span><br><span class="line">master_password=&#x27;*&amp;@jall190&#x27;, </span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下两个字段的值，务必在从服务器上，用show master status 查看相关值</span></span><br><span class="line">master_log_file=&#x27;mysql-bin.000003&#x27;,</span><br><span class="line">master_log_pos=587;</span><br><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.100.6</span><br><span class="line">                  Master_User: sync_acct</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000003</span><br><span class="line">          Read_Master_Log_Pos: 587</span><br><span class="line">               Relay_Log_File: mysql-relay-bin.000002</span><br><span class="line">                Relay_Log_Pos: 529</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000003</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>（3）在第（1）、（2）已经成功完成主-主模式，但也仅仅是说明msyql主主配置正常，但还未通过数据写入来测试其正确性，可通过以下简单的两个步骤测试主-主模式数据的同步</p>
<ul>
<li>主服务器上新建一个databases，创建一个简单表，观察slave是否同步新建情况</li>
<li>在刚新建的表里插入一条记录，观察slave对应的表是否也多一条记录</li>
<li>在备服务器上drop 掉刚建的数据库，然后在salve重复以上操作，观察master上数据同步情况</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里仅简单给出一些命令，不再具体展开</span></span><br><span class="line">create database erp_app;</span><br><span class="line">use erp_app;</span><br><span class="line">create table if not exists apps_name(</span><br><span class="line">id int(4) not null primary key auto_increment,</span><br><span class="line">app_log_name char(20) not null,</span><br><span class="line">log_path char(200) not null,</span><br><span class="line">log_date timestamp default current_timestamp</span><br><span class="line">);</span><br><span class="line">show tables</span><br><span class="line"><span class="meta">#</span><span class="bash">插入数据，因为id是自增，无需自行插入，使用null交给mysql自动插入相应id号</span></span><br><span class="line">insert into apps_name values(null,&#x27;BI-Access-Log&#x27;,&#x27;/opt/data/apps_log/&#x27;,null);</span><br></pre></td></tr></table></figure>
<p>使用python插入数据做测试使用，后面可作为验证主-主故障切换使用<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line">db_info=&#123;</span><br><span class="line">            <span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;port&#x27;</span>:<span class="number">3306</span>,</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;****&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;db&#x27;</span>:<span class="string">&#x27;erp_app&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;charset&#x27;</span>:<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    conn = pymysql.connect(**db_info)</span><br><span class="line">    insert_sql = (<span class="string">&quot;insert into apps_name &quot;</span></span><br><span class="line">                  <span class="string">&quot;(id,app_log_name,log_path,log_date) &quot;</span></span><br><span class="line">                  <span class="string">&quot;values(null,%(app_log_name)s,%(log_path)s,null)&quot;</span>)</span><br><span class="line">    insert_data=&#123;</span><br><span class="line">        <span class="string">&#x27;app_log_name&#x27;</span>:<span class="string">&#x27;BI-Access-Log&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;log_path&#x27;</span>:<span class="string">&#x27;/opt/data/apps_log/&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> conn.cursor() <span class="keyword">as</span> cur:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            resl=cur.execute(insert_sql,insert_data)</span><br><span class="line">            print(resl)</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">            conn.commit()</span><br><span class="line"><span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> err:</span><br><span class="line">    print(err)</span><br><span class="line">    conn.rollback()</span><br><span class="line"></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p><strong>在第2点配置my.cnf，设定了自增键规则，也可以验证主、从插入数据时，其自增键的情况</strong></p>
<p>在主服务器重新插入四条，id为奇数id：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; select * from apps_name;</span><br><span class="line">+----+---------------+</span><br><span class="line">| id | app_log_name  |</span><br><span class="line">+----+---------------+</span><br><span class="line">|  1 | BI-Access-Log |</span><br><span class="line">|  3 | BI-Access-Log |</span><br><span class="line">|  5 | BI-Access-Log |</span><br><span class="line">|  7 | BI-Access-Log |</span><br><span class="line">+----+---------------+</span><br></pre></td></tr></table></figure>
<p>清空之前测试的数据，在备服务器重新插入四条，id为偶数id：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt;  select * from apps_name;</span><br><span class="line">+----+---------------+</span><br><span class="line">| id | app_log_name  |</span><br><span class="line">+----+---------------+</span><br><span class="line">|  2 | BI-Access-Log |</span><br><span class="line">|  4 | BI-Access-Log |</span><br><span class="line">|  6 | BI-Access-Log |</span><br><span class="line">|  8 | BI-Access-Log |</span><br><span class="line">+----+---------------+</span><br></pre></td></tr></table></figure>
<p>但是这种配置自增算法有bug，只在双主模式下，假设现在要三台服务器都作为主，互相同步，那么现有的自增键策略无法扩展，其实有两种可行方案：</p>
<p>第一种：</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"> #</span><span class="bash"> 通过增大步长，例如有10台，步长值=主服务器数量，解决自增键冲突</span></span><br><span class="line"><span class="meta"> #</span><span class="bash"> 服务器1</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 1</span><br><span class="line"><span class="meta"> #</span><span class="bash"> 服务器2</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 2</span><br><span class="line"><span class="meta"> #</span><span class="bash"> 服务器3</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 3</span><br><span class="line"> ......</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">服务器10</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 10</span><br></pre></td></tr></table></figure>
<p>第二种：数据库表不设置自增字段，由程序逻辑用UUID实现id唯一值，个人推荐此方式，不受限制。</p>
<p><strong>注意</strong>：</p>
<p>在主从模式下，或者主-主模式下，测试数据过程中，例如在master上进行drop erp_app时，若slave不存在erp_app数据库（主从分离连接后，从已先删除），slave将无法进行同步删除操作，后续的同步操作也无法正常进行，show slave status\G，看到有报错提示db不存在无法drop的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Last_SQL_Errno: 1008</span><br><span class="line">Last_SQL_Error: Error &#39;Can&#39;t drop database &#39;erp_app&#39;; database doesn&#39;t exist&#39; on query. Default database: &#39;erp_app&#39;. Query: &#39;drop database erp_app&#39;</span><br></pre></td></tr></table></figure>
<p>需在两台服务器上进行如下设置，==如果已经在my.cnf配置文件配好slave-skip-errors=all可跳过一下设置==</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; stop slave;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"># 将同步指针向移动下一个位置，跳过异常，以便mysql可继续执行下一个同步操作</span><br><span class="line">MariaDB [(none)]&gt; set global sql_slave_skip_counter&#x3D;1;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>
<p>（4）查看binlog,relay-log,创建数据库对应的物理存储文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost mysql]# ls</span><br><span class="line">aria_log.00000001  ibtmp1             mysql-bin.000003  mysql-bin.000010  mysql-bin.index</span><br><span class="line">aria_log_control   localhost.pid      mysql-bin.000004  mysql-bin.000011  mysql-relay-bin.000050</span><br><span class="line">erp_app            master.info        mysql-bin.000005  mysql-bin.000012  mysql-relay-bin.000051</span><br><span class="line">ib_buffer_pool     multi-master.info  mysql-bin.000006  mysql-bin.000013  mysql-relay-bin.index</span><br><span class="line">ibdata1            mysql              mysql-bin.000007  mysql-bin.000014  mysql.sock</span><br><span class="line">ib_logfile0        mysql-bin.000001   mysql-bin.000008  mysql-bin.000015  performance_schema</span><br><span class="line">ib_logfile1        mysql-bin.000002   mysql-bin.000009  mysql-bin.000016  relay-log.info</span><br></pre></td></tr></table></figure>
<p>相关文件说明：</p>
<p>master.info：主库A的账户密码等同步基本配置信息</p>
<p>mysql-bin.index：记录主库A所有的binlog日志文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./mysql-bin.000001</span><br><span class="line">./mysql-bin.000002</span><br><span class="line">./mysql-bin.000003</span><br><span class="line">./mysql-bin.000004</span><br><span class="line">./mysql-bin.000005</span><br><span class="line">./mysql-bin.000006</span><br><span class="line">./mysql-bin.000007</span><br><span class="line">./mysql-bin.000008</span><br><span class="line">./mysql-bin.000009</span><br><span class="line">./mysql-bin.000010</span><br><span class="line">./mysql-bin.000011</span><br><span class="line">./mysql-bin.000012</span><br><span class="line">./mysql-bin.000013</span><br><span class="line">./mysql-bin.000014</span><br><span class="line">./mysql-bin.000015</span><br><span class="line">./mysql-bin.000016</span><br><span class="line">~</span><br></pre></td></tr></table></figure><br>mysql-bin.000**：主库A的binlog日志里的sql操作命令，vi该二进制文件进去可以看到sql命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">^@^@^@^@^@^@^@&lt;84&gt;芦盲6贸bZ]^B^B^@^@^@&#125;^@^@^@茫     ^@^@^@^@        ^@^@^@^A^@^@^@^G^@^@^_^@^@^@^@^@^@^A^@^@ T^@^@^F^Cstd^C^B^@^B^@^D!^@!^@^H^@erp_app^@insert into apps_name values(null,&#x27;BI-Access-Log&#x27;)陆@莽篓贸bZ]^P^B^@^@^@^@^@^B</span><br></pre></td></tr></table></figure>
<p>relay-log.info:记录最新使用日志信息</p>
<p>mysql-relay-bin.000**：主库A在主库B同步回来的中继日志文件，记录主库B执行过的SQL命令</p>
<p>mysql-relay-bin.index：中继日志的索引文件，记录所有relay日志文件</p>
<p>==从上面的相关物理文件，也可看出msyql主从同步过程==</p>
<p>从库的IO线程 把主库mysql-bin.000<strong>  日志append到本机的中继日志文件mysql-relay-bin.000</strong><br>从库SQL线程 执行本机中继日志文件里的sql命令，将数据写入进本机。</p>
<p>通过<code>show processlist</code>也可清晰看到相关线程的作用：</p>
<p>Slave_IO线程说：正在等待主库发送事件</p>
<p>Slave_SQL线程说：本从库已经读完所有中继日志，正在等待Slave_IO线程更新到中继日志文件（或新建一个中继日志文件）</p>
<p>Binlog Dump：（这个线程说明，本机也是主库角色）本机已经把所有binlog发送到了从库，正在等待本机更新binlog<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show processlist\G</span><br><span class="line">*************************** 6. row ***************************</span><br><span class="line">      Id: 10</span><br><span class="line">    User: system user</span><br><span class="line">    Host: </span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Slave_IO</span><br><span class="line">    Time: 812</span><br><span class="line">   State: Waiting for master to send event</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br><span class="line">*************************** 7. row ***************************</span><br><span class="line">      Id: 11</span><br><span class="line">    User: system user</span><br><span class="line">    Host: </span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Slave_SQL</span><br><span class="line">    Time: 812</span><br><span class="line">   State: Slave has read all relay log; waiting for the slave I/O thread to update it</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br><span class="line">*************************** 8. row ***************************</span><br><span class="line">      Id: 13</span><br><span class="line">    User: sync_acct</span><br><span class="line">    Host: 192.168.142.4:51432</span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Binlog Dump</span><br><span class="line">    Time: 772</span><br><span class="line">   State: Master has sent all binlog to slave; waiting for binlog to be updated</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br></pre></td></tr></table></figure></p>
<p>（5）查看mariaDB 存储引擎</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show variables like &#x27;storage_engine&#x27;;</span><br><span class="line">+----------------+--------+</span><br><span class="line">| Variable_name  | Value  |</span><br><span class="line">+----------------+--------+</span><br><span class="line">| storage_engine | InnoDB |</span><br><span class="line">+----------------+--------+</span><br></pre></td></tr></table></figure>
<p><strong>可在my.cnf配置文件下设置’default-storage-engine=?’更改默认引擎，这是全局修改</strong></p>
<p><strong>若修改表的存储引擎 alter table table_name engine=engine_name;</strong></p>
<h5 id="6、keepalived配置以及VIP漂移策略"><a href="#6、keepalived配置以及VIP漂移策略" class="headerlink" title="6、keepalived配置以及VIP漂移策略"></a>6、keepalived配置以及VIP漂移策略</h5><p>（1）keepalived配置<code>/etc/keepalived/keepalived.conf</code>，权限必须为644</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 邮箱预警设置简单，不再说明</span></span><br><span class="line">   notification_email &#123;</span><br><span class="line">     acassen@firewall.loc</span><br><span class="line">     failover@firewall.loc</span><br><span class="line">     sysadmin@firewall.loc</span><br><span class="line">   &#125;</span><br><span class="line"><span class="meta">   #</span><span class="bash">notification_email_from Alexandre.Cassen@firewall.loc</span></span><br><span class="line"><span class="meta">   #</span><span class="bash">smtp_server 192.168.200.1</span></span><br><span class="line"><span class="meta">   #</span><span class="bash">smtp_connect_timeout 30</span>   </span><br><span class="line"><span class="meta">   #</span><span class="bash">以下两行处理脚本执行权限问题</span></span><br><span class="line">   script_user root</span><br><span class="line">   enable_script_security </span><br><span class="line"></span><br><span class="line">   router_id hdA # 备服务器hdB，主、备的router_id必须不同，否则VRRP无法选举</span><br><span class="line">   vrrp_skip_check_adv_addr</span><br><span class="line">   vrrp_strict</span><br><span class="line">   vrrp_garp_interval 0</span><br><span class="line">   vrrp_gna_interval 0</span><br><span class="line">   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检测db主-主同步脚本</span></span><br><span class="line">vrrp_script chk_mariadb_sync &#123;</span><br><span class="line">    script &quot;/etc/keepalived/check_db_sync.sh&quot;</span><br><span class="line">    interval 2</span><br><span class="line">    weight 10</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 非抢占模式</span></span><br><span class="line">    state BACKUP</span><br><span class="line">    nopreempt</span><br><span class="line">    interface ens33</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 100 # 备服务器99</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 89287201</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.100.7</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">track_script &#123;</span><br><span class="line">     chk_mariadb_sync</span><br><span class="line">   &#125;  </span><br></pre></td></tr></table></figure>
<p>（2） mariaDB 同步状态检测脚本 <code>/etc/keepalived/check_db_sync.sh</code></p>
<p>注意，这里别用root账户，保证一定安全，<code>chmod 744 check_db_sync.sh</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">mysql_bin=/usr/bin/mysql</span><br><span class="line">user=sync_acc</span><br><span class="line">pw=passPass</span><br><span class="line">host=127.0.0.1</span><br><span class="line">port=3306</span><br><span class="line"><span class="meta">#</span><span class="bash"> Seconds_Behind_Master</span></span><br><span class="line">sbm=60</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">io_thread_state=`$mysql_bin -h $host -P $port -u$user -p$pw -e &#x27;show slave status\G&#x27;  2&gt;/dev/null|grep &#x27;Slave_IO_Running:&#x27;|awk &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $io_thread_state</span><br><span class="line"></span><br><span class="line">sql_thread_state=`$mysql_bin -h $host -P $port -u$user -p$pw -e &#x27;show slave status\G&#x27; 2&gt;/dev/null|grep &#x27;Slave_SQL_Running:&#x27;|awk &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $sql_thread_state</span><br><span class="line"></span><br><span class="line">SBM=`$mysql_bin -h $host -P $port -u$user -p$pw -e &#x27;show slave status\G&#x27; 2&gt;/dev/null|grep &#x27;Seconds_Behind_Master:&#x27;|awk &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $SBM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Check <span class="keyword">for</span> <span class="variable">$mysql_bin</span></span></span><br><span class="line">if [ ! -f $mysql_bin ];then</span><br><span class="line">        echo &#x27;the path of mysqlbin is incorrect,please check msyql path&#x27;</span><br><span class="line">        exit 2</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> check mysql status whether is dead</span></span><br><span class="line">service mariadb status &amp;&gt;/dev/null</span><br><span class="line">if [ $? -ne 0 ];then</span><br><span class="line">        pkill keepalived</span><br><span class="line">        echo &#x27;mysql is dead&#x27;</span><br><span class="line">        exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> -z 表示如果<span class="variable">$IOThread</span>变量为空，说明数据库服务不可用，已down</span></span><br><span class="line">if [[ -z &quot;$io_thread_state&quot; ]];then</span><br><span class="line">		pkill keepalived</span><br><span class="line">		echo &#x27;mysql is dead&#x27;</span><br><span class="line">        exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [[ &quot;$io_thread_state&quot; == &quot;Connecting&quot; &amp;&amp; &quot;$sql_thread_state&quot; == &quot;Yes&quot; ]];then</span><br><span class="line">		echo &#x27;master is down,but slave still works&#x27;</span><br><span class="line">        exit 0</span><br><span class="line">        </span><br><span class="line">        # Seconds_Behind_Master timeout</span><br><span class="line">        elif [[ $SBM -ge $sbm ]];then</span><br><span class="line">                pkill keepalived</span><br><span class="line">                exit 1</span><br><span class="line">        else</span><br><span class="line">                exit 0</span><br><span class="line">fi</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>VIP切换到正常的主服务逻辑：</p>
<p>1）主A，主B db数据库正常，IO线程同步正常，vip被主A占用</p>
<p>2）主A数据库down后，如检测脚本所示，脚本把主A 的keepalived服务kill掉，此时VIP飘移到主B服务器，实现故障转移，对于主B，它的线程连接主A超时，无法同步，但仍可对外提供db服务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Slave_IO_State: Reconnecting after a failed master event read</span><br><span class="line">Slave_IO_Running: Connecting</span><br><span class="line">Slave_SQL_Running: Yes</span><br></pre></td></tr></table></figure>
<p>3）当主A 恢复数据库后，主A重启 keepalived服务，因为配置为非抢占模式，故此时还是由主B对外提供db服务</p>
<p>4）如果主A、主B服务都down了？ 你应该在此之前准备好相关邮件或者短信监控并人工介入</p>
<p>补充：VIP漂移策略不一定按上述情况，可自行加入数据库相关的监控指标来确定脚本，也可用python作为脚本</p>
<h5 id="7、-在防火墙写入ACL，开放相关端口"><a href="#7、-在防火墙写入ACL，开放相关端口" class="headerlink" title="7、 在防火墙写入ACL，开放相关端口"></a>7、 在防火墙写入ACL，开放相关端口</h5><blockquote>
<p>个人发现csdn上绝对大部分博客教程，尤其在前期配置环境这一类文章，一律跳过防火墙设置，直接停掉防火墙，以便快速部署，部署成功后，大部分文章会忽略最后加入防火墙设置，事实上一旦形成这种“偷懒的”习惯，在生成环境很容易出现“服务器、数据、漏洞安全”，即使服务内网使用。而且当前网络安全形势突出，在参与2019护网行动以及等保工作中，对安全有了较深刻体会。</p>
</blockquote>
<p><strong>1)</strong> 端口加入防火墙，防火墙设置分为centos7.5 firewalld和centos6或者redhat6.5的iptables，因为本人长期使用centos发行版，这里给出的是firewalld的设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">firewall-cmd --state</span><br><span class="line">systemctl start firewalld</span><br><span class="line">systemctl enable firewalld</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试端口连通性，很多人会用telnet测试端口是否打开，但centos默认没有telnet服务，其实针对tcp层连通性测试，使用http协议也一样，而且系统自带wget命令，非常方便测试</span></span><br><span class="line">在备服务器上，关闭防火墙的两种情况</span><br><span class="line">[root@localhost ~]# wget http://192.168.100.5:3306</span><br><span class="line">--2019-08-16 10:50:59--  http://192.168.100.5:3306/</span><br><span class="line">Connecting to 192.168.100.5:3306... connected.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开防火墙，默认并未放通3306端口</span></span><br><span class="line">[root@localhost ~]# wget http://192.168.100.5:3306</span><br><span class="line">--2019-08-16 10:51:10--  http://192.168.100.5:3306/</span><br><span class="line">Connecting to 192.168.100.5:3306... failed: No route to host.</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 限制仅192.168.100.0/24网段能访问端口3306</span></span><br><span class="line">firewall-cmd --permanent --zone=public --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.100.0/24&quot; port protocol=&quot;tcp&quot; port=&quot;3306&quot; accept&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新防火墙规则</span></span><br><span class="line">firewall-cmd --reload或firewall-cmd --complete-reload</span><br><span class="line">(两者的区别就是第一个无需断开连接，就是firewalld特性之一动态添加规则，第二个需要断开连接，类似重启服务)</span><br></pre></td></tr></table></figure>
<p><strong>2)</strong>  开放VRRP协议，用于keepalived 主备状态检测</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 主备都运行下面的命令，从组播地址224.0.0.18，可以看出VRRP用了组播协议</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 入方向</span></span><br><span class="line">firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --in-interface ens33 --destination 224.0.0.18 --protocol vrrp -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 出方向</span></span><br><span class="line">firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --out-interface ens33 --destination 224.0.0.18 --protocol vrrp -j ACCEPT</span><br><span class="line">firewall-cmd --reload</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql HA</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas数据预处理的常用函数</title>
    <url>/2019/11/17/Pandas%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>&#8195;&#8195;以往项目中也有引入Pandas，用于有关数据处理和分析的环节，结合Python的Web开发，很容易开发出一款轻量BI系统。Pandas、Numpy、Scipy、matplotlib、scikit-learn和Jupyter Notebook结合使用，完全可以组成非常出色的数据分析与挖掘的生产环境工具，数据方面的应用，比matlab强不少，以至于本人也不断强化这方面的积累。单独拿出这方面技能，即可完成数据分析师的相关工作（又称提数工程师）。本文主要归档一些高频使用的预处理方面的函数，注意本文不涉及Pandas数据挖掘和数理统计方面的知识点（会在另外blog给出）。</p>
<a id="more"></a>
<h3 id="1、读取数据文件"><a href="#1、读取数据文件" class="headerlink" title="1、读取数据文件"></a>1、读取数据文件</h3><p>&#8195;&#8195;读取数据的相关接口使用在pandas官网的document有非常详细的说明:在<a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html">IO tools部分</a>。pandas 不仅能读取基本常用的Excel、csv、文本，还可以读取hadoop文件，或者直接读取数据库等</p>
<h4 id="1-1-读取excel数据文件"><a href="#1-1-读取excel数据文件" class="headerlink" title="1.1  读取excel数据文件"></a>1.1  读取excel数据文件</h4><ul>
<li><p>加载Excel表，使用skiprows=1跳过首行<br>并指定加载的列，注意数据文件的编码，默认utf-8，常用还有gb2312，根据自身数据而定。</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">raw_pd = pd.read_excel(data_file,,skiprows=<span class="number">1</span>,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br><span class="line"><span class="number">181</span> ms ± <span class="number">1.32</span> ms per loop (mean ± std. dev. of <span class="number">7</span> runs, <span class="number">1</span> loop each)</span><br></pre></td></tr></table></figure>
<p>  这里可以为每个执行单元之前加入<code>%%timeit</code>，观察其耗时情况。</p>
</li>
<li><p>加载Excel表，使用header=0跳过有列标题的首行<br>除了使用skiprows=1可跳过首行，header=0也可以实现同样效果</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd = pd.read_excel(data_file,header=<span class="number">0</span>,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，首行为数据，不是列标题<br>若该表第一行不是列标题行而是数据行，则需要指定header=None，否则读取后，第一行数据会被作为column name</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，读取前n行数据<br>若数据文件大小为几个G，显然若直接全量读取，内存会挤爆，因此可以先读取前n看看。使用nrows=500，表示只读取前500行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>, nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，跳过所有空白行<br>若有些表出现了不定数量的空白行，可以使用skip_blank_lines=True处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,skip_blank_lines = <span class="literal">True</span>, nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，通过自定规则，跳过满足规则的行<br>例如跳过有值为单数的行，定义更复杂的函数，用于跳过满足复杂规则的行。不过，除非这些行很多，否则可以在读取后，直接用正则drop掉来得更有效。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.read_csv(data_file, skiprows=<span class="keyword">lambda</span> x: x % <span class="number">2</span> != <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="1-2-读取csv文件"><a href="#1-2-读取csv文件" class="headerlink" title="1.2 读取csv文件"></a>1.2 读取csv文件</h4><p>&#8195;&#8195;读取csv文件跟读取Excel文件区别不大，这里简单给出示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_csv(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>读取文件，需要注意的地方一般是选择编码，数据文件的编码决定读取数据后，是否正常显示。</p>
<h4 id="1-3-读取数据时，跳过尾行"><a href="#1-3-读取数据时，跳过尾行" class="headerlink" title="1.3 读取数据时，跳过尾行"></a>1.3 读取数据时，跳过尾行</h4><p>有些报表一般会在表（例如财务系统导出）的后几行写上制表人、制表日期<br>这里要注意，若使用c engine，则无法使用从尾部跳过数据的功能：</p>
<blockquote>
<p>skipfooter : int, default <code>0</code></p>
<p>Number of lines at bottom of file to skip (unsupported with engine=’c’).</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_csv(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>, skipfooter=<span class="number">5</span>,encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-4-读取特定分割符的数据文件"><a href="#1-4-读取特定分割符的数据文件" class="headerlink" title="1.4 读取特定分割符的数据文件"></a>1.4 读取特定分割符的数据文件</h4><p>read_csv也可以读取任意文本文件，只需要指定列分割符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_csv(<span class="string">&#x27;data_file.txt&#x27;</span>,sep=<span class="string">&#x27;||&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-5-使用c或者python作为读取文件的引擎"><a href="#1-5-使用c或者python作为读取文件的引擎" class="headerlink" title="1.5 使用c或者python作为读取文件的引擎"></a>1.5 使用c或者python作为读取文件的引擎</h4><p>pd.read_<em>*</em> 方法默认使用python解释器作为读取文件engine，若数据文件大，可选择c engine</p>
<blockquote>
<p>engine : {<code>&#39;c&#39;</code>, <code>&#39;python&#39;</code>}</p>
<p>Parser engine to use. The C engine is faster while the Python engine is currently more feature-complete.</p>
</blockquote>
<h4 id="1-6-使用迭代器读取超大文件"><a href="#1-6-使用迭代器读取超大文件" class="headerlink" title="1.6 使用迭代器读取超大文件"></a>1.6 使用迭代器读取超大文件</h4><p>参考官网文档给出的示例，使用<code>iterator=True</code>， 或者chunksize=4读取超大文件，返回的是TextFileReader，是一个文件迭代器</p>
<p>chunksize方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">187</span>]: reader = pd.read_csv(<span class="string">&#x27;tmp.sv&#x27;</span>, sep=<span class="string">&#x27;|&#x27;</span>, chunksize=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">188</span>]: reader</span><br><span class="line">Out[<span class="number">188</span>]: &lt;pandas.io.parsers.TextFileReader at <span class="number">0x7f2b428c17f0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">189</span>]: <span class="keyword">for</span> chunk <span class="keyword">in</span> reader:</span><br><span class="line">   .....:     print(chunk)</span><br></pre></td></tr></table></figure>
<p>iterator=True方式：<br>使用iterator=True方式，值读取前面5行，放回的也是df对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">190</span>]: reader = pd.read_csv(<span class="string">&#x27;tmp.sv&#x27;</span>, sep=<span class="string">&#x27;|&#x27;</span>, iterator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">191</span>]: chunk_pd=reader.get_chunk(<span class="number">5</span>)</span><br><span class="line">chunk_pd.head()</span><br></pre></td></tr></table></figure>
<p>当然最佳的方式是两者结合使用：返回迭代器方式，并指定分块读取，例如分64k读取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter_df=pd.read_csv(large_file,iterator=<span class="literal">True</span>，chunksize=<span class="number">64</span>*<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2、查看数据的基本信息"><a href="#2、查看数据的基本信息" class="headerlink" title="2、查看数据的基本信息"></a>2、查看数据的基本信息</h3><p>读入数据后，一般需要对数据进行基本的观察</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd.head(<span class="number">5</span>) <span class="comment"># 查看数据基本信息（前5行）</span></span><br><span class="line">raw_pd.tail(<span class="number">5</span>) <span class="comment"># 查看末尾5行</span></span><br><span class="line">raw_pd.sample(<span class="number">5</span>) <span class="comment"># 随机抽取5行查看</span></span><br><span class="line">raw_pd.dtypes <span class="comment"># 查看每列数据类型</span></span><br><span class="line">raw_pd.columns    <span class="comment">#查看列名</span></span><br><span class="line">raw_pd.info()     <span class="comment">#查看各字段的信息</span></span><br><span class="line">raw_pd.shape      <span class="comment">#查看数据集行列分布，几行几列</span></span><br><span class="line">raw_pd.describe() <span class="comment"># 快速查看数据的基本统计信息</span></span><br></pre></td></tr></table></figure>
<h3 id="3、有关空值处理"><a href="#3、有关空值处理" class="headerlink" title="3、有关空值处理"></a>3、有关空值处理</h3><p>空值：在pandas中的空值是””<br>缺失值：在dataframe中为NaN或者NaT（缺失时间），在series中为none或者nan<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试数据</span></span><br><span class="line">raw_pd = pd.DataFrame(&#123;<span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;aoo&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;college&quot;</span>: [np.nan, <span class="string">&#x27;SACT&#x27;</span>, <span class="string">&#x27;AACT&#x27;</span>],</span><br><span class="line">                  <span class="string">&quot;birth_date&quot;</span>: [pd.NaT, pd.Timestamp(<span class="string">&quot;2000-10-01&quot;</span>),pd.NaT]&#125;)</span><br></pre></td></tr></table></figure></p>
<h4 id="3-1-行的空值处理"><a href="#3-1-行的空值处理" class="headerlink" title="3.1 行的空值处理"></a>3.1 行的空值处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">axis：0或者&#x27;index&#x27;代表行操作（默认）  1或者&#x27;column&#x27;：列操作</span></span><br><span class="line"><span class="string">how：any-只要有空值就删除（默认），all-全部为空值才删除</span></span><br><span class="line"><span class="string">inplace：False-返回新的数据集（默认），True-在愿数据集上操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用频率高：查看name列中，有多少行为空值行,value_counts其实是一个统计方法</span></span><br><span class="line">raw_pd[<span class="string">&#x27;name&#x27;</span>].isnull().value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用频率高：any表示行的任意一列有空值，则删除该行；all表示该行全部为空，则删除</span></span><br><span class="line">raw_pd.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 行的任意一列有空值,且出现2个空值才删除这些行。例如该行有3列，其中2列都是为空，那么可以删掉该行。</span></span><br><span class="line">使用频率低：raw_pd.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>,thresh=<span class="number">2</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="3-2-列的空值处理"><a href="#3-2-列的空值处理" class="headerlink" title="3.2 列的空值处理"></a>3.2 列的空值处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用频率高：指定某几列，若这些列中出现了空值，则直接删除所在行</span></span><br><span class="line">raw_pd.dropna(subset=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;birth_date&#x27;</span>],inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-3-空值的填充"><a href="#3-3-空值的填充" class="headerlink" title="3.3 空值的填充"></a>3.3 空值的填充</h4><p>最简单的用法，对全部数据记录里面的空值填充指定值</p>
<p>df.fillna(value=’bar’)</p>
<p>频繁使用：对指定列的空值进行填充</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd[<span class="string">&#x27;name&#x27;</span>]=raw_pd[<span class="string">&#x27;name&#x27;</span>].fillna(value=<span class="string">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>高级填充方式</strong><br>使用与空值单元相邻近的值来填充。该用法一般用在大量数据统计分析的场景或者图像的像素值填充、实验室的实验数据。相邻是指可以使用上下左右四个方向的值实现前向或者后向填充<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">DataFrame.fillna(value=<span class="literal">None</span>, method=<span class="literal">None</span>, axis=<span class="literal">None</span>, inplace=<span class="literal">False</span>, limit=<span class="literal">None</span>, downcast=<span class="literal">None</span>, **kwargs)</span><br><span class="line"></span><br><span class="line">method : &#123;‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, <span class="literal">None</span>&#125;, default <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">Method to use <span class="keyword">for</span> filling holes <span class="keyword">in</span> reindexed Series pad / ffill: </span><br><span class="line">propagate last valid observation forward to <span class="built_in">next</span> valid backfill / bfill: use NEXT valid observation to fill gap</span><br><span class="line"></span><br><span class="line">axis : &#123;<span class="number">0</span> <span class="keyword">or</span> ‘index’, <span class="number">1</span> <span class="keyword">or</span> ‘columns’&#125;</span><br><span class="line">limit:限制填充的个数</span><br></pre></td></tr></table></figure><br>这里使用比较频繁的是纵向填充，因为纵向代表的是列，从相邻样本的同一特征中填值，对每列的空值实施前项或者后项填充。</p>
<p><code>df.fillna(method=&#39;ffill&#39;)</code> or df.fillna(method=’bfill’)</p>
<h4 id="3-4-空值使用所在列或者所在行的均值、中位数来填补"><a href="#3-4-空值使用所在列或者所在行的均值、中位数来填补" class="headerlink" title="3.4  空值使用所在列或者所在行的均值、中位数来填补"></a>3.4  空值使用所在列或者所在行的均值、中位数来填补</h4><p>这里以均值填充为例，当然也可以用该列的预测值填充<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mean_value = df[<span class="string">&#x27;age&#x27;</span>].mean()</span><br><span class="line">df[<span class="string">&#x27;age&#x27;</span>].fillna(mean_value, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="4、dataframe-取（定位）数据的操作"><a href="#4、dataframe-取（定位）数据的操作" class="headerlink" title="4、dataframe 取（定位）数据的操作"></a>4、dataframe 取（定位）数据的操作</h3><h4 id="4-1-按给定列名取数，类似字典操作：df-‘列名’"><a href="#4-1-按给定列名取数，类似字典操作：df-‘列名’" class="headerlink" title="4.1 按给定列名取数，类似字典操作：df[‘列名’]"></a>4.1 按给定列名取数，类似字典操作：df[‘列名’]</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd= raw_pd[<span class="string">&#x27;name&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>取出多列数据，入参为包含多个字段的list：[‘name’,’college’]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;college&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="4-2-按行默认的行索引号选取数据：df-loc"><a href="#4-2-按行默认的行索引号选取数据：df-loc" class="headerlink" title="4.2  按行默认的行索引号选取数据：df.loc"></a>4.2  按行默认的行索引号选取数据：df.loc</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;aoo&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;college&quot;</span>: [np.nan, <span class="string">&#x27;SACT&#x27;</span>, <span class="string">&#x27;AACT&#x27;</span>],</span><br><span class="line">                  <span class="string">&quot;birth_date&quot;</span>: [pd.NaT, pd.Timestamp(<span class="string">&quot;2000-10-01&quot;</span>),pd.NaT]&#125;)</span><br><span class="line"><span class="comment"># 查看该df的行索引</span></span><br><span class="line">df.index</span><br><span class="line">RangeIndex(start=<span class="number">0</span>, stop=<span class="number">3</span>, step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印                  </span></span><br><span class="line">	birth_date 	college 	name</span><br><span class="line"><span class="number">0</span> 	NaT 	NaN 	aoo</span><br><span class="line"><span class="number">1</span> 	<span class="number">2000</span>-<span class="number">10</span>-01 	SACT 	boo</span><br><span class="line"><span class="number">2</span> 	NaT 	AACT 	coo </span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的0,1,2就是pandas默认给加载的数据提供的行索引号</span></span><br></pre></td></tr></table></figure>
<p>按索引取数据，跟列表使用slice切片获取数据的用法一致<br>df.loc[1] 获取行索引1的行数据，df.loc[1:2]获取1到2行数据</p>
<p>若行索引号不是int，例如将以上数据的默认index序列，改成字符索引序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.index=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"> 	birth_date 	college 	name</span><br><span class="line">a 	NaT 	NaN 	aoo</span><br><span class="line">b 	<span class="number">2000</span>-<span class="number">10</span>-01 	SACT 	boo</span><br><span class="line">c 	NaT 	AACT 	coo</span><br></pre></td></tr></table></figure>
<p>获取索引为b的数据：df.loc[‘b’]<br>或者索引为b、c的数据：df.loc[[‘b’,’c’]]</p>
<h4 id="4-3-按给定列名以及行索引取出数据"><a href="#4-3-按给定列名以及行索引取出数据" class="headerlink" title="4.3 按给定列名以及行索引取出数据"></a>4.3 按给定列名以及行索引取出数据</h4><p>例如取出college列的b、c行数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.loc[[<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>],<span class="string">&#x27;college&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>例如取出college列、birth_date列的b、c行数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.loc[[<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>],[<span class="string">&#x27;college&#x27;</span>,<span class="string">&#x27;birth_date&#x27;</span>]]</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line">	college 	birth_date</span><br><span class="line">b 	SACT 	<span class="number">2000</span>-<span class="number">10</span>-01</span><br><span class="line">c 	AACT 	NaT</span><br></pre></td></tr></table></figure>
<h4 id="4-4-df-iloc利用index获取行数据或者列数据"><a href="#4-4-df-iloc利用index获取行数据或者列数据" class="headerlink" title="4.4  df.iloc利用index获取行数据或者列数据"></a>4.4  df.iloc利用index获取行数据或者列数据</h4><p>df.iloc只能使用整型切片获取数据:例如df.iloc[0:10]<br>而df.loc可以使用字符型索引等来取数</p>
<h3 id="5、通过复杂规则取数"><a href="#5、通过复杂规则取数" class="headerlink" title="5、通过复杂规则取数"></a>5、通过复杂规则取数</h3><p>在sql中经常会在where子句使用筛选条件：<br>select * from emp e  where e.age&gt;20 and e.dep_name=’dev’ and e.city&lt;&gt;’foo’<br>在pandas里面则使用方式如下：<br>单个筛选条件<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">20</span>]</span><br><span class="line">或者</span><br><span class="line">df.loc[df[<span class="string">&#x27;age&#x27;</span>]&gt;<span class="number">20</span>]</span><br></pre></td></tr></table></figure></p>
<p>多个筛选条件<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[(df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">20</span>)&amp;(df[<span class="string">&#x27;dep_name&#x27;</span>]==<span class="string">&#x27;dev&#x27;</span>)&amp;~(df[<span class="string">&#x27;city&#x27;</span>]==<span class="string">&#x27;foo&#x27;</span>)]</span><br></pre></td></tr></table></figure></p>
<p>使用isin方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;city&#x27;</span>].isin([<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>])]</span><br></pre></td></tr></table></figure>
<p>根据时间范围取值<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Excel加载的日期，如果格式不对，有可能是object类型，需将其转为datetime64[ns]类型，否则无法进行日期筛选比较</span></span><br><span class="line">df[<span class="string">&#x27;date_col&#x27;</span>]= df.to_datetime(df[<span class="string">&#x27;date_col&#x27;</span>])</span><br><span class="line">start_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment">#或者pd.Timestamp(&#x27;2017-02-01&#x27;)</span></span><br><span class="line">end_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">14</span>) <span class="comment">#或者pd.Timestamp(&#x27;2017-02-14&#x27;)</span></span><br><span class="line"><span class="comment"># 注意以上的实际范围其实是 2017-02-01 00:00:00 ~2017-02-14 00:00:00</span></span><br><span class="line"><span class="comment"># 或者截止到当天最后一秒</span></span><br><span class="line">end_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">14</span>,<span class="number">23</span>,<span class="number">59</span>,<span class="number">59</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找指定时间范围内的数据行</span></span><br><span class="line">filter_df=df[(df[<span class="string">&#x27;start_time&#x27;</span>]&gt;=start_time) &amp; (df[<span class="string">&#x27;end_time&#x27;</span>]&lt;=end_time)]</span><br></pre></td></tr></table></figure><br>还有另外这一种方式是选择一个时间列，变将其设为当前df的时间索引，</p>
<p>常用：根据某个字段，取出获取删除其值出现频率排在前n的数据行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  对name字段进行group_by</span></span><br><span class="line">groupby_df=df.groupby(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line"><span class="comment"># groupby_df:&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000010190C18&gt;</span></span><br><span class="line"></span><br><span class="line">resl_df=groupby_df[<span class="string">&#x27;name&#x27;</span>].count()</span><br><span class="line"><span class="comment"># resl_df 就像透视表的数据形式</span></span><br><span class="line">name</span><br><span class="line">foo    <span class="number">10</span></span><br><span class="line">bar    <span class="number">5</span></span><br><span class="line">coo    <span class="number">4</span></span><br><span class="line">dee    <span class="number">2</span></span><br><span class="line">Name: bar, dtype: int64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找name字段里，字符出现频率排在前2位，例如上述的例子：foo，bar。按降序返回一个python的列表</span></span><br><span class="line">top_2_list=resl_df.sort_values(ascending=<span class="literal">False</span>).head(<span class="number">2</span>)</span><br><span class="line">print(top_2_list)</span><br><span class="line">[<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将出现频率排在前2的内容拼接成用于正则匹配的字符串</span></span><br><span class="line">pattern_str=<span class="string">&#x27;|&#x27;</span>.join(top_2_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pandas提供的正则方法，剔除name字段中出现频率排在前2的数据行</span></span><br><span class="line">filtered_df= df[~df[<span class="string">&#x27;name&#x27;</span>].<span class="built_in">str</span>.contains(pattern_str, case=<span class="literal">False</span>, na=<span class="literal">False</span>,regex=<span class="literal">True</span>)]</span><br></pre></td></tr></table></figure>
<p>==使用时间索引选取数据行==<br>个人认为，这种方式是时间选取数据场景最高效的手段<br>例如有数据df，其中create_date是该df唯一的日期字段，通常做法:<br>新增一列命名为back_up_date，用于备份create_date<br><code>df[&#39;back_up_date&#39;]=df[&#39;create_date&#39;]</code><br>将crete_date置为该df的时间索引<br><code>df=df.set_index(&#39;create_date&#39;)</code><br>当时间索引设置后，那么根据时间筛选数据将变得异常简单<br>取2000年的数据行<br><code>df[&#39;2000&#39;]</code><br>取2000年到2019年的数据行<br><code>df[&#39;2000&#39;:&#39;2019&#39;]</code><br>某天具体时间到某天具体时间的数据行<br><code>df[&#39;2015-03-15 11:11:10&#39;:&#39;2015-05-15 10:30:00&#39;]</code><br>有关pandas时间的专题，官方文档给出了非常详细的用法示例，这里不再赘述，<a href="https://pandas.pydata.org/pandas-docs/version/0.25/user_guide/timeseries.html">timeseries链接</a></p>
<h3 id="6、调整列位置、列的增、删"><a href="#6、调整列位置、列的增、删" class="headerlink" title="6、调整列位置、列的增、删"></a>6、调整列位置、列的增、删</h3><p>交换birth_date和college列的位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;birth_date&#x27;</span>,<span class="string">&#x27;college&#x27;</span>]]=df[[<span class="string">&#x27;college&#x27;</span>,<span class="string">&#x27;birth_date&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<p>删除指定列<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop(columns=[<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>])</span><br></pre></td></tr></table></figure><br>删除指定行，使用行索引</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop([<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>删除重复行:df.drop_duplicates</p>
<p>直接删除重复行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates()</span><br></pre></td></tr></table></figure>
<p>删除name列、age列存在重复的行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates([<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>],keep=<span class="string">&#x27;first&#x27;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>请注意：以上删除行的操作，会破坏df原有的0到n的连续索引，例如原行索引为：0，1，2，3…n，其中索引1，2为空行，删除空行后，df的索引变为0，3…n，显然不连续，因此需要重置索引：df.reset_index(drop=True)，重置后，索引变为0，1，2，3…n</p>
<h3 id="7、单元格的字符相关处理"><a href="#7、单元格的字符相关处理" class="headerlink" title="7、单元格的字符相关处理"></a>7、单元格的字符相关处理</h3><p>例如有字段app_id，有部分值字符串为数字：‘12331’，需转成int64<br>有部分值为字符加数字：‘TD12331’，去掉字符TD并转成int64<br>有些值为非id例如：‘ # llsd’，需对此类值用固定数值填充。因此需要对其统一处理成整型id<br>使用replace方法去掉值里面的TD字符串<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>].replace(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>,regex=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<p>使用apply方法通过定义简单的lambda去掉值里面的TD字符串</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].apply(<span class="keyword">lambda</span>:item:item.replace(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>其实apply才是终极方法，适用各种自定义的数据行或者列的处理，例如对同一列的值有多种判断需要处理，则可以在外部定义要处理的函数，再用apply广播到该列的每个cell中。例如上面的例子：如果单元格数值含有TD则去掉TD字符保留其数值部分，如果单元格出现非数值，则将其设为NaN空值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_id</span>(<span class="params">cell</span>):</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">&#x27;\d&#123;3&#125;&#x27;</span>,cell):</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    <span class="keyword">elif</span> re.match(<span class="string">&#x27;TD&#x27;</span>,cell):</span><br><span class="line">        <span class="keyword">return</span> re.sub(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>,cell)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.nan</span><br><span class="line">        </span><br><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].apply(filter_id)</span><br></pre></td></tr></table></figure>
<p>apply方法另外一种常用的方式:对数值进行分级，例如10&lt;item&lt;30为D，30&lt;=item&lt;60为C，60&lt;=item&lt;90为B。此外，货币进行转化、时间转换也是常用的场景</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">10</span>&lt;=item&lt;<span class="number">30</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;D&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">30</span>&lt;=item&lt;<span class="number">60</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;C&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">60</span>&lt;=item&lt;<span class="number">90</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;B&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;A&#x27;</span></span><br><span class="line">    </span><br><span class="line">df[<span class="string">&#x27;level&#x27;</span>]=df[<span class="string">&#x27;level&#x27;</span>].apply(level)    </span><br></pre></td></tr></table></figure>
<p>使用astype转成整型id号，具体其他数据类型不再列出。astype要求整列数据是完整的同一数据类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].astype(<span class="string">&#x27;int64&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>使用频繁：使用pandas.Series.str.contains方法处理列的值</p>
<blockquote>
<p>Series.str.contains(pat, case=True, flags=0, na=nan, regex=True)<br>pat : str<br>    Character sequence or regular expression.<br>case : bool, default True<br>    If True, case sensitive.<br>flags : int, default 0 (no flags)<br>    Flags to pass through to the re module, e.g. re.IGNORECASE.<br>na : default NaN<br>    Fill value for missing values.<br>regex : bool, default True<br>    If True, assumes the pat is a regular expression.<br>    If False, treats the pat as a literal string.</p>
</blockquote>
<p>删除name列中含有foo字符串的行，默认使用正则匹配<br>df=df[~df[‘name’].str.contains(‘foo’, case=false, flags=re.IGNORECASE, na=False)]</p>
<p>或者使用正则匹配<br>df=df[~df[‘name’].str.contains(‘^foo’, case=false, flags=re.IGNORECASE, na=False)]</p>
<h3 id="8、有关遍历行的处理"><a href="#8、有关遍历行的处理" class="headerlink" title="8、有关遍历行的处理"></a>8、有关遍历行的处理</h3><p>单表处理，请勿使用循环！！ 效率很低！有apply方法足以，底层是矩阵操作。</p>
<p>遍历行，一般用在两个表之间，表A字段’date’与表B字段’date‘的比较<br>使用iterrows遍历行<br>iterate over DataFrame rows as (index, Series) pairs.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这种方式可以把索引和行数据遍历出，其中row的数据结构为nametuple</span></span><br><span class="line"><span class="keyword">for</span> index,row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    print(index,row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这种方式其实就是itertuples(index=False)的遍历</span></span><br><span class="line"><span class="keyword">for</span> _,row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    print(row.A,row.B)</span><br></pre></td></tr></table></figure></p>
<p>使用itertuples遍历行<br>Iterate over DataFrame rows as namedtuples of the values.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.Series(pd.date_range(<span class="string">&#x27;2012-1-1&#x27;</span>, periods=<span class="number">10</span>, freq=<span class="string">&#x27;D&#x27;</span>))</span><br><span class="line">td = pd.Series([pd.Timedelta(days=i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: s, <span class="string">&#x27;B&#x27;</span>: td&#125;)</span><br><span class="line"><span class="comment">#这种方式，取出的每行为nametuple</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">    print(row.A,row.B)</span><br></pre></td></tr></table></figure></p>
<p>使用iteritems遍历列<br>这种方式以横向遍历列数据，每次返回该列名和该列Series</p>
<h3 id="9、DataFrame和DataFrame合并、关联查询等"><a href="#9、DataFrame和DataFrame合并、关联查询等" class="headerlink" title="9、DataFrame和DataFrame合并、关联查询等"></a>9、DataFrame和DataFrame合并、关联查询等</h3><h4 id="9-1-DataFrame和DataFrame合并"><a href="#9-1-DataFrame和DataFrame合并" class="headerlink" title="9.1 DataFrame和DataFrame合并"></a>9.1 DataFrame和DataFrame合并</h4><p>合并具有相同结构的df<br>将多个DataFrame按垂直方向或者水平方向合并：这种场合使用批量处理具有相同字段结构的多份报表或数据源</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 默认是按垂直方向合并三个子df</span></span><br><span class="line">frames = [df1, df2, df3]</span><br><span class="line">result = pd.concat(frames)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在合并后，还可以为每个子df设定相应key</span></span><br><span class="line">result = pd.concat(frames, keys=[<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;cee&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用上面key，可以一次性取回合并前的df1</span></span><br><span class="line">df1=result.loc[<span class="string">&#x27;foo&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>合并字段不同的df</p>
<h4 id="9-2-DataFrame和DataFrame之间的关联查询"><a href="#9-2-DataFrame和DataFrame之间的关联查询" class="headerlink" title="9.2 DataFrame和DataFrame之间的关联查询"></a>9.2 DataFrame和DataFrame之间的关联查询</h4><p>因为关联查询基本是数据分析里面重要的、使用频繁的需求，例如实现报表1和报表的用vlookup关联查询、sql中多个表的关联查询（内连接、左连接、右连接、全连接）。pandas的doc官方文档在这部分的内容已经非常详细，并且有相应的关联前后的图文说明，本文不再一一赘述，仅给出简单的关联用法。<br>以内连接为例：<br>实现类似sql使用两表的多个外键关联：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select t1.*,t2.* from t1,t2 where t1.a&#x3D;t2.a</span><br><span class="line">and t1.b&#x3D;t2.b</span><br><span class="line">and t1.c&#x3D;t2.c</span><br></pre></td></tr></table></figure><br>pandas的方式<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.merge(df2, on=[ key1 ,  key2 ,  key ])</span><br></pre></td></tr></table></figure><br>使用单个字段(外键)关联两表<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.merge(df2, on=<span class="string">&#x27;dept_id&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="10、groupby基本用法"><a href="#10、groupby基本用法" class="headerlink" title="10、groupby基本用法"></a>10、groupby基本用法</h3><p>groupby可以说面对不同的数据需求，有不同用法，对sql熟悉的人应该无需多说。这里仅给出一些简单用法。</p>
<p>按季度分组，提取每个分组前n个数据行<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_n</span>(<span class="params">df,n=<span class="number">3</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> df[<span class="number">0</span>:n]</span><br><span class="line"><span class="comment"># 这里的n是top_n自定义的关键字参数n，不是apply的参数    </span></span><br><span class="line">df.groupby(<span class="string">&#x27;quarter&#x27;</span>).apply(top_n,n=<span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>
<p>按产品种类分组，提取每个分组里最大值和最小值之差<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每个产品种类的数值跨度范围，也即最大值减去最小值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_min</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">return</span> item.<span class="built_in">max</span>() - item.<span class="built_in">min</span>()</span><br><span class="line">df.groupby(<span class="string">&#x27;prod&#x27;</span>).agg(max_min)</span><br></pre></td></tr></table></figure></p>
<p>按产品种类分组，一次性取出每组的最值、均值、数值跨度范围，这里需要注意agg的入参为方法的列表，内置方法使用其字符名，自定义方使用其函数名<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;prod&#x27;</span>).agg([<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,max_min])</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>数据分析与挖掘</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark DataFrame、Spark SQL、Spark Streaming入门教程</title>
    <url>/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark DataFrame</a>、<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a>、<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a></p>
<h4 id="1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming"><a href="#1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming" class="headerlink" title="1、RDD、Spark DataFrame、Spark SQL、Spark Streaming"></a>1、RDD、Spark DataFrame、Spark SQL、Spark Streaming</h4><p>&#8195;&#8195;RDD：大家最熟悉的数据结构，主要使用transmissions和actions 相关函数式算子完成数据处理和数理统计，例如map、reduceByKey，rdd没有定义 Schema（一般指未定义字段名及其数据类型）， 所以一般用列表索引号来指定每一个字段。 例如， 在电影数据的推荐例子中:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">move_rdd.map(lambda line:line.split(&#39;|&#39;)).map(lambda a_list:(alist[0],a_list[1],a_list[2]))</span><br></pre></td></tr></table></figure><br><a id="more"></a></p>
<p>每行有15个字段的数据，因此只能通过索引号获取前3个字段的数据，这要求开发者必须掌握 Map/Reduce 编程模式，不过， RDD 功能也最强， 能完成所有 Spark 数据处理与分析需求。</p>
<p>&#8195;&#8195;Spark DataFrame：创建DataFrame时，可以定义 Schema，通过定义每一个字段名与数据类型，以便之后直接用字段名进行数据索引，用法跟Pandas的DataFrame差别不大。Spark DataFrame是一种更高层的API，而且基于PySpark，用起来像Pandas的”手感“，很容易上手。</p>
<p>&#8195;&#8195;Spark SQL 底层是封装了DataFrame（DataFrame封装了底层的RDD） ，让使用者直接用sql的方式操作rdd，进一步降低Spark作为分布式计算框架的使用门槛。<br>&#8195;&#8195;Spark Streaming是本博客重点要解析的数据结构，实际项目将使用它实现流式计算，相关定义参考原文：</p>
<blockquote>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.<br>Spark Streaming具有扩展性、数据吞吐量高、容错的特点，底层基于core Spark API 实现，用于流数据处理。Spark Streaming注入的实时数据源可来自Kafka、Flume、Kinesis或者TCP流等，park Streaming可借助Map、reduce、join和window等高级函数接口搭建复杂的算法用于数据处理。Spark Streaming实时处理后数据可存储到文件系统、数据库或者实时数据展示仪表。</p>
</blockquote>
<h4 id="2、Spark-DataFrame"><a href="#2、Spark-DataFrame" class="headerlink" title="2、Spark DataFrame"></a>2、Spark DataFrame</h4><p>&#8195;&#8195;Spark DataFrame API比较多，既然用于数据处理和计算，当然会有预处理接口以及各统计函数、各种方法，详细参考<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">官网:pyspark.sql.DataFrame</a>以及<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions">pyspark.sql.functions module模块</a></p>
<p>&#8195;&#8195;目前版本中，创建Spark DataFrame的Context接口可以直接用SparkSession接口，无需像RDD创建上下文时用Spark Context。<br>SparkSession：pyspark.sql.SparkSession:Main entry point for DataFrame and SQL functionality.</p>
<h5 id="2-1-创建基本的Spark-DataFrame"><a href="#2-1-创建基本的Spark-DataFrame" class="headerlink" title="2.1 创建基本的Spark DataFrame"></a>2.1 创建基本的Spark DataFrame</h5><p>&#8195;&#8195;创建 Spark DataFrame有多种方式，先回顾Pandas的DataFrame，Pandas可从各类文件、流以及集合中创建df对象，同样 Spark DataFrame也有类似的逻辑<br>首先需加载spark的上下文：SparkSession</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession <span class="comment">#  用于Spark DataFrame的上下文</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,StructType,StructField, LongType,DateType <span class="comment"># 用于定义df字段类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row,Column</span><br><span class="line"></span><br><span class="line"><span class="comment">#本地spark单机模式</span></span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br><span class="line">print(spark)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出spark上下文信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparkSession - in-memory</span><br><span class="line">SparkContext</span><br><span class="line">Spark UI</span><br><span class="line">Version v2.4.4</span><br><span class="line">Master</span><br><span class="line">    local[*]</span><br><span class="line">AppName</span><br><span class="line">    spark_dataframe</span><br></pre></td></tr></table></figure></p>
<p>from pyspark.sql.types：df目前支持定义的字段类型（参考源码），用于定义schema，类似关系型数据库建表时，定义表的字段类型<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__all__ = [</span><br><span class="line">    <span class="string">&quot;DataType&quot;</span>, <span class="string">&quot;NullType&quot;</span>, <span class="string">&quot;StringType&quot;</span>, <span class="string">&quot;BinaryType&quot;</span>, <span class="string">&quot;BooleanType&quot;</span>, <span class="string">&quot;DateType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;TimestampType&quot;</span>, <span class="string">&quot;DecimalType&quot;</span>, <span class="string">&quot;DoubleType&quot;</span>, <span class="string">&quot;FloatType&quot;</span>, <span class="string">&quot;ByteType&quot;</span>, <span class="string">&quot;IntegerType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;LongType&quot;</span>, <span class="string">&quot;ShortType&quot;</span>, <span class="string">&quot;ArrayType&quot;</span>, <span class="string">&quot;MapType&quot;</span>, <span class="string">&quot;StructField&quot;</span>, <span class="string">&quot;StructType&quot;</span>]</span><br></pre></td></tr></table></figure></p>
<p>直接用从RDD创建dataframe对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_rdd = spark.sparkContext.parallelize([</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义schema，就像数据库建表的定义：数据模型，定义列名，类型和是否为能为空</span></span><br><span class="line">schema = StructType([StructField(<span class="string">&quot;id&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;item&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;price&quot;</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;pub_date&quot;</span>, DateType(), <span class="literal">True</span>)])</span><br><span class="line"><span class="comment"># 创建Spark DataFrame</span></span><br><span class="line">spark_df= spark.createDataFrame(spark_rdd, schema)</span><br><span class="line"><span class="comment"># 创建一个零时表，用于映射到rdd上</span></span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;iPhone&quot;</span>)</span><br><span class="line"><span class="comment"># 使用Sql语句,语法完全跟sql一致</span></span><br><span class="line">data = spark.sql(<span class="string">&quot;select a.item,a.price from iPhone a&quot;</span>)</span><br><span class="line"><span class="comment"># 查看dataframe的数据</span></span><br><span class="line">print(data.collect())</span><br><span class="line"><span class="comment"># 以表格形式展示数据</span></span><br><span class="line">data.show()</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Row(item&#x3D;&#39;iPhoneX&#39;, price&#x3D;6000), Row(item&#x3D;&#39;iPhone7&#39;, price&#x3D;4000), Row(item&#x3D;&#39;iPhone4&#39;, price&#x3D;1000)]</span><br><span class="line">+-------+-----+</span><br><span class="line">|   item|price|</span><br><span class="line">+-------+-----+</span><br><span class="line">|iPhoneX| 6000|</span><br><span class="line">|iPhone7| 4000|</span><br><span class="line">|iPhone4| 1000|</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure></p>
<p>通过该例子，可了解df基本用法，只要从spark上下文加载完数据并转为dataframe类型后，之后调用df的api跟pandas的api大同小异，而且可将dataframe转为Spark SQL，直接使用sql语句操作数据。</p>
<p>上面的例子用了显示定义schema字段类型，pyspark支持自动推理创建df，也即无需原数据定义为rdd，和自动类型，直接传入Python列表的数据，以及定义字段名称即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_list = [</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line"><span class="comment"># 自动推理创建df,代码内部通过各类if 判断类型实现。</span></span><br><span class="line">spark_df= spark.createDataFrame(a_list, schema=[<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;item&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;pub_date&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h5 id="2-2-从各类数据源创建Spark-DataFrame"><a href="#2-2-从各类数据源创建Spark-DataFrame" class="headerlink" title="2.2  从各类数据源创建Spark DataFrame"></a>2.2  从各类数据源创建Spark DataFrame</h5><p>相关接口方法参考官网<a href="http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html">文档</a></p>
<p>==从csv文件创建Spark DataFrame==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file = <span class="string">&#x27;/opt/spark/data/train.csv&#x27;</span></span><br><span class="line">df = spark.read.csv(file,header=<span class="literal">True</span>,inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>==从pandas的DataFrame创建Spark DataFrame==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pandas_df = pd.DataFrame(np.random.random((<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line">spark_df = spark.createDataFrame(pandas_df, schema=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>==从json创建Spark DataFrame,json文件可以通过远程拉取，或者本地json，并设定json的字段schema==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_df = spark.read.json(<span class="string">&#x27;/opt/data/all-world-cup-players.json&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>==从各类数据库加载数据：==</p>
<p>pg数据库，使用option属性传入参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df = spark.read \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>) \</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure>
<p>pg数据库，用关键字参数传入连接参数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:postgresql://localhost:5432/&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;db_name.table_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure></p>
<p>mysql数据库，用关键字参数传入连接参数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:mysql://localhost:3306/db_name&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;table_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure></p>
<p>从hive里面读取数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果在SparkSession设置为连接hive，可以直接读取hive数据</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .enableHiveSupport() \      </span><br><span class="line">        .master(<span class="string">&quot;localhost:7077&quot;</span>) \</span><br><span class="line">        .appName(<span class="string">&quot;spark_hive&quot;</span>) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">spark_df=spark.sql(<span class="string">&quot;select * from hive_app_table&quot;</span>)</span><br><span class="line">spark_df.show()</span><br></pre></td></tr></table></figure><br>连接数据库需要相关的jar包，例如连接mysql，则需要将mysql-connector放在spark目录的jar目录下。</p>
<h5 id="2-3-Spark-DataFrame持久化数据"><a href="#2-3-Spark-DataFrame持久化数据" class="headerlink" title="2.3 Spark DataFrame持久化数据"></a>2.3 Spark DataFrame持久化数据</h5><p>以csv存储<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.write.csv(path=local_file_path, header=<span class="literal">True</span>, sep=<span class="string">&quot;,&quot;</span>, mode=<span class="string">&#x27;overwrite&#x27;</span>)</span><br></pre></td></tr></table></figure><br>注意：mode=‘overwrite’ 模式时，表示创建新表，若表名已存在则会被删除，整个表被重写。而 mode=‘append’ 模式就是普通的最加数据。</p>
<p>写入mysql：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url = <span class="string">&#x27;jdbc:mysql://localhost:3306/db_name?characterEncoding=utf-8&amp;autoReconnect=true&amp;useSSL=false&amp;serverTimezone=GMT&#x27;</span></span><br><span class="line">table = <span class="string">&#x27;table_name&#x27;</span></span><br><span class="line">properties = &#123;<span class="string">&quot;user&quot;</span>:<span class="string">&quot;test&quot;</span>,<span class="string">&quot;password&quot;</span>:<span class="string">&quot;test&quot;</span>&#125;</span><br><span class="line">spark_df.write.jdbc(url,table,mode=<span class="string">&#x27;append&#x27;</span>,properties=properties)</span><br></pre></td></tr></table></figure></p>
<p>写入hive</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开动态分区</span></span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition.mode = nonstrict&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition=true&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定分区写入表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).partitionBy(<span class="string">&quot;name&quot;</span>).insertInto(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用分区，直接将数据保存到Hive新表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from your_db.table_name&quot;</span>).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>默认的方式将会在hive分区表中保存大量的小文件，在保存之前对 DataFrame重新分区，从而控制保存的文件数量。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.repartition(<span class="number">5</span>).write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<p>写入redis：<br>这里需要自行实现redis的写入方法，其实也简单，定义入参为dataframe，函数内部连接redis后，从dataframe取出数据再将其插入redis即可。对于写入其他文件或者数据库，需自行实现相应的数据转存逻辑。</p>
<h5 id="2-4-Dataframe常见的API"><a href="#2-4-Dataframe常见的API" class="headerlink" title="2.4 Dataframe常见的API"></a>2.4 Dataframe常见的API</h5><p><a href="https://github.com/Apress/machine-learning-with-pyspark/blob/master/chapter_2_Data_Processing/sample_data.csv">样例数据参考</a></p>
<p>查看字段<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.columns </span><br><span class="line"><span class="comment"># [&#x27;ratings&#x27;, &#x27;age&#x27;, &#x27;experience&#x27;, &#x27;family&#x27;, &#x27;mobile&#x27;]</span></span><br></pre></td></tr></table></figure><br>spark_df.count() 统计行数<br>查看df的shape<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print((df.count(),<span class="built_in">len</span>(df.columns))) <span class="comment"># (33, 5)</span></span><br></pre></td></tr></table></figure></p>
<p>查看dataframe的schema字段定义<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.printSchema()</span><br><span class="line">输出：</span><br><span class="line">root</span><br><span class="line"> |-- ratings: integer (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- experience: double (nullable = true)</span><br><span class="line"> |-- family: integer (nullable = true)</span><br><span class="line"> |-- mobile: string (nullable = true)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>随机抽样探索数据集合：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark_df.sample(False,0.5,0).show(5)</span><br><span class="line">用法：</span><br><span class="line">Signature: spark_df.sample(withReplacement&#x3D;None, fraction&#x3D;None, seed&#x3D;None)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a sampled subset of this :class:&#96;DataFrame&#96;.</span><br><span class="line"></span><br><span class="line">:param withReplacement: Sample with replacement or not (default False).</span><br><span class="line">:param fraction: Fraction of rows to generate, range [0.0, 1.0].</span><br><span class="line">:param seed: Seed for sampling (default a random seed).</span><br></pre></td></tr></table></figure>
<p>查看行记录：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.show(<span class="number">3</span>) </span><br><span class="line">spark_df.head(<span class="number">3</span>) </span><br><span class="line">spark_df.take(<span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>
<p>以python列表返回记录，list中每个元素是Row类<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[Row(ratings=<span class="number">3</span>, age=<span class="number">32</span>, experience=<span class="number">9.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Vivo&#x27;</span>, age_2=<span class="number">32</span>),</span><br><span class="line"> Row(ratings=<span class="number">3</span>, age=<span class="number">27</span>, experience=<span class="number">13.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Apple&#x27;</span>, age_2=<span class="number">27</span>),</span><br><span class="line"> Row(ratings=<span class="number">4</span>, age=<span class="number">22</span>, experience=<span class="number">2.5</span>, family=<span class="number">0</span>, mobile=<span class="string">&#x27;Samsung&#x27;</span>, age_2=<span class="number">22</span>)]</span><br></pre></td></tr></table></figure></p>
<p>查看null的行，可以传入isnull函数，也可以自定义lambda函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">spark_df = spark_df.<span class="built_in">filter</span>(isnull(<span class="string">&quot;name&quot;</span>))</span><br></pre></td></tr></table></figure></p>
<p>选择列数据：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>).show(<span class="number">2</span>)<span class="comment"># select方法</span></span><br></pre></td></tr></table></figure></p>
<p>扩展dataframe的列数:withColumn可以在原df上新增列数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure>
<p>注意该方式不会更新到原df，如需替换原df，则更新spark_df即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure>
<p>新增一列数据，并将新增的数据转为double类型，需要用到cast方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,DoubleType,IntegerType</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;age_double&#x27;</span>,spark_df[<span class="string">&#x27;age&#x27;</span>].cast(DoubleType()))</span><br></pre></td></tr></table></figure>
<p>根据条件查询df，使用filter方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Apple&#x27;</span>)</span><br><span class="line"><span class="comment">#筛选记录后，再选出指定的字段记录</span></span><br><span class="line">spark_df.<span class="built_in">filter</span>(df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;ratings&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>选择mobile列值为‘Apple’的记录，多条件查询:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>)</span><br><span class="line">或者：</span><br><span class="line">spark_df.<span class="built_in">filter</span>((spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>)&amp;(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>distinct：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先用select取出要处理的字段，获取不同类型的mobile</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计不同类型mobile的数量</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct().count()</span><br></pre></td></tr></table></figure>
<p>groupBy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().show(<span class="number">5</span>,<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">+-------+-----+</span><br><span class="line">|mobile |count|</span><br><span class="line">+-------+-----+</span><br><span class="line">|MI     |<span class="number">8</span>    |</span><br><span class="line">|Oppo   |<span class="number">7</span>    |</span><br><span class="line">|Samsung|<span class="number">6</span>    |</span><br><span class="line">|Vivo   |<span class="number">5</span>    |</span><br><span class="line">|Apple  |<span class="number">7</span>    |</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure>
<p>groupBy之后，按统计字段进行降序排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().orderBy(<span class="string">&#x27;count&#x27;</span>,ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>groupBy之后，按mobile分组，求出每个字段在该分组的均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).mean().show(<span class="number">2</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|mobile|avg(ratings)     |avg(age)          |avg(experience)   |avg(family)       |avg(age_2)        |</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|MI    |3.5              |30.125            |10.1875           |1.375             |30.125            |</span><br><span class="line">|Oppo  |2.857142857142857|28.428571428571427|10.357142857142858|1.4285714285714286|28.428571428571427|</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">only showing top 2 rows</span><br></pre></td></tr></table></figure>
<p>同理还有<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code>、<code>df.groupBy(&#39;mobile&#39;).max()</code>、<code>df.groupBy(&#39;mobile&#39;).min()</code>等，或者用agg方法，然后传入相应的聚合函数</p>
<p><code>spark_df.groupBy(&#39;mobile&#39;).agg(&#123;&#39;experience&#39;:&#39;sum&#39;&#125;)</code>等同于<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code></p>
<p>用户定义函数UDF：</p>
<p>用户定义函数一般用于对列或者对行的数据进行定制化处理，就sql语句中，价格为数字的字段，根据不同判断条件，给字段加上美元符号或者指定字符等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costom_func</span>(<span class="params">brand</span>):</span></span><br><span class="line">    <span class="keyword">if</span> brand <span class="keyword">in</span> [<span class="string">&#x27;Samsung&#x27;</span>,<span class="string">&#x27;Apple&#x27;</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;High Price&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> brand ==<span class="string">&#x27;MI&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Mid Price&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Low Price&#x27;</span></span><br><span class="line">        </span><br><span class="line">brand_udf=udf(costom_func,StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;price_range&#x27;</span>,brand_udf(spark_df[<span class="string">&#x27;mobile&#x27;</span>])).show(<span class="number">5</span>,<span class="literal">False</span>) <span class="comment"># 使用spark_df[&#x27;mobile&#x27;]或者使用spark_df.mobile都可以</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|price_range|</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |Low Price  |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |High Price |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |High Price |</span><br><span class="line">|4      |37 |16.5      |4     |Apple  |37   |High Price |</span><br><span class="line">|5      |27 |9.0       |1     |MI     |27   |Mid Price  |</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用lambda定义udf</span></span><br><span class="line">age_udf = udf(<span class="keyword">lambda</span> age: <span class="string">&quot;young&quot;</span> <span class="keyword">if</span> age &lt;= <span class="number">30</span> <span class="keyword">else</span> <span class="string">&quot;senior&quot;</span>, StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&quot;age_group&quot;</span>, age_udf(df.age)).show(<span class="number">3</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|age_group|</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |senior   |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |young    |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |young    |</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>
<p>删除重复记录：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重复的行，将被删除</span></span><br><span class="line">spark_df=spark_df.dropDuplicates()</span><br></pre></td></tr></table></figure>
<p>删除一列数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_new=spark_df.drop(<span class="string">&#x27;mobile&#x27;</span>) <span class="comment"># 删除多列 spark_df.drop(&#x27;mobile&#x27;,&#x27;age&#x27;)</span></span><br></pre></td></tr></table></figure>
<h4 id="3、Spark-SQL"><a href="#3、Spark-SQL" class="headerlink" title="3、Spark SQL"></a>3、Spark SQL</h4><p>&#8195;&#8195;在第二部分内容给出了创建spark sql的方法，本章节给出更为详细的内容：这里重点介绍spark sql创建其上下文，完成相应的context设置后，剩下的就是熟悉的写SQL了。<br><strong>第一种方式：将本地文件加载为dataframe</strong><br>之后再使用createOrReplaceTempView方法转为<code>SQL模式</code>，流程如下</p>
<p>用第2节内容的数据做演示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.csv(<span class="string">&#x27;sample_data.csv&#x27;</span>,inferSchema=<span class="literal">True</span>,header=<span class="literal">True</span>)</span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;phone_sales&quot;</span>)</span><br><span class="line">df1 = spark.sql(<span class="string">&quot;select age,family,mobile from phone_sales &quot;</span>)</span><br><span class="line">df1.show(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+---+------+-------+</span><br><span class="line">|age|family| mobile|</span><br><span class="line">+---+------+-------+</span><br><span class="line">| 32|     3|   Vivo|</span><br><span class="line">| 27|     3|  Apple|</span><br><span class="line">| 22|     0|Samsung|</span><br><span class="line">+---+------+-------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>
<p>spark.sql用于传入sql语句，返回dataframe对象，故后续的数据处理将变得非常灵活，使用SQL确实能够降低数据处理门槛，再例如：</p>
<p><code>spark_df.groupBy(&#39;mobile&#39;).count().show(5,False)</code> 等同于</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">your_sql=(<span class="string">&quot;select mobile,count(mobile) as count from phone_sales group by mobile &quot;</span></span><br><span class="line">df1 = spark.sql(your_sql)</span><br></pre></td></tr></table></figure>
<p>如果df1集合较大，适合用迭代器方式输出记录（适合逐条处理的逻辑）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> each_record <span class="keyword">in</span> df1.collect(): </span><br><span class="line">	data_process(each_record)</span><br></pre></td></tr></table></figure>
<p><strong>第二种方式：直接连接诸如mysql或者hive的context，基于该context直接运行sql</strong></p>
<p>以mysql为例：<br>（1）配置mysql连接需要相关jar包和路径配置：<br> mysql-connector-java-5.1.48.jar 放入spark目录<code>/opt/spark-2.4.4-bin-hadoop2.7/jars/</code>目录下， mysql-connector包可在mysql自行下载。<br>在spark-env.sh 配置了EXTRA_SPARK_CLASSPATH=/opt/spark-2.4.4-bin-hadoop2.7/jars/（也可不配，spark按默认目录检索）</p>
<p>（2）连接mysql<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession </span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br></pre></td></tr></table></figure><br>连接数据库以及读取表<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apps_name_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).</span><br><span class="line">options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;erp_app.apps_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure><br>read方法详细使用可参考：<a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.format">spark.read.format</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pirnt(apps_name_df)</span><br><span class="line"><span class="comment"># DataFrame[id: int, app_log_name: string, log_path: string, log_date: timestamp]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apps_name_df.show(5)</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">| id| app_log_name|           log_path|           log_date|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">|  1|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  3|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  5|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  7|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  9|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure>
<p>上述连接mysql的erp_app后，直接读取apps_name全部字段的数据，如果想在连接时，指定sql，则需按以下方式进行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/erp_app&#x27;</span>,</span><br><span class="line">                                           dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span>,</span><br><span class="line">                                           user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">                                           password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">                                          ).load()</span><br></pre></td></tr></table></figure>
<p>dbtable这个值可以为一条sql语句，而且格式必须为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span></span><br></pre></td></tr></table></figure>
<p>如果写成以下格式，则提示解析出错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;select id,app_log_name,log_path from apps_name&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="4、Spark-Streaming"><a href="#4、Spark-Streaming" class="headerlink" title="4、Spark Streaming"></a>4、Spark Streaming</h4><p>&#8195;&#8195;以上在介绍dataframe和spark sql的相关用法，都用离线数据进行测试，本章节将给出spark的核心组件之一：spark streaming：实时流式计算（微批处理），该功能将应用于本bg其他项目。有关流式计算的相关概念，可以查看相关参考资料，这里不再累赘。此外，本bg也将给出一篇关于spark streaming的深度解析文章。</p>
<h5 id="4-1-实时计算TCP端口的数据"><a href="#4-1-实时计算TCP端口的数据" class="headerlink" title="4.1 实时计算TCP端口的数据"></a>4.1 实时计算TCP端口的数据</h5><p>&#8195;&#8195;以一个简单demo介绍pyspark实现streaming的流程：<br>在数据源输入端，使用netcat打开一个7070端口，手动持续向netcat shell输入句子；<br>在实时计算端：streaming连接7070端口，并实时计算word count，并将统计结果实时打印。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建本地的streaming context，并指定4个worker线程</span></span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少spark自生成的日志打印</span></span><br><span class="line"><span class="comment"># 每批处理间隔为1秒</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>) </span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 连接netcat的tcp端口，用于读取netcat持续输入的行字符串</span></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;192.100.0.10&quot;</span>, <span class="number">7070</span>)</span><br></pre></td></tr></table></figure>
<p>socketTextStream创建的对象称为：Discretized Streams（离散流） ，简称 DStream，是spark的核心概念</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计word的逻辑，这段代码再熟悉不过了。</span></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> value_1, value_2: value_1 + value_2)</span><br><span class="line">wordCounts.pprint() <span class="comment"># 这里wordCounts是&#x27;TransformedDStream&#x27; object，不再是普通的离线rdd</span></span><br></pre></td></tr></table></figure>
<p>启动流计算，并一直等到外部中断程序（相当于线程里面的jion）<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)    <span class="comment"># 默认无timeout，程序会一直阻塞在这里</span></span><br></pre></td></tr></table></figure></p>
<p>启动后，如果你使用jupyter开发，可以看到notebook的cell每隔1秒打印一次</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:50</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:51</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>在netstat shell输入字符串</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# nc -l 7070</span><br><span class="line">this is spark streaming</span><br><span class="line">streaming wordcount is awesome</span><br></pre></td></tr></table></figure>
<p>再查看notebook的cell的的实时打印出了wordcount统计结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;spark&#39;, 1)</span><br><span class="line">(&#39;this&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:58</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;wordcount&#39;, 1)</span><br><span class="line">(&#39;awesome&#39;, 1)</span><br></pre></td></tr></table></figure>
<p>以上实现了一个完整的实时流计算，虽然该streaming的demo较为简单，但却给了大家非常直观的流计算处理设计思路，只需改造相关逻辑，即可满足符合自己业务的需求，在这里给出一个可行的设计：</p>
<p>（1）实时数据源替换为Kafka等组件：启动一个进程，用于运行streaming。streaming的实时数据源来自kafka的topic<br>（2）定制MapReduce的计算逻辑，用于实时预处理流数据<br>（3）将（2）的实时结果保存到redis的list上<br>（4）启动另外一个进程，从结果队列里面取出并存到Hbase集群或者hdfs<br>或者无需使用队列，Spark Streaming实时预处理后直接写入Hbase。</p>
<h5 id="4-2-实时计算本地文件"><a href="#4-2-实时计算本地文件" class="headerlink" title="4.2 实时计算本地文件"></a>4.2 实时计算本地文件</h5><p>&#8195;&#8195;对于python接口，<code>streamingContext.textFileStream(dataDirectory)</code>方法可以实时监听并读取本地目录下的日志文件，但有几点需要指出，参考官方文档指引：</p>
<ul>
<li>能实时监听<code>dataDirectory</code>目录下创建的任意类型文件</li>
<li><p><code>dataDirectory</code>主要分为两种文件系统，第一种为本地文件系统，例如监听<code>/opt/appdata/</code>目录下的所有文件，格式为<code>file:///opt/appdata/</code>，第二种为hdfs文件系统：格式为<code>hdfs://namenode:8040/logs/</code></p>
</li>
<li><p>支持文件正则匹配，例如要监听本地文件目录下，所有以<code>.log</code>作为后缀类型的文件，<code>file:///opt/appdata/*.log</code></p>
</li>
<li><p>要求监听目录下的所有文件，里面的数据格式是一致的，例如1.log和2.log,里面都相同固定格式的日志记录。（每次新增的文件如果数据格式不一样，显然streaming处理逻辑无法完成）</p>
</li>
<li><p>目录下的文件数量越多，streaming扫描耗时将越长</p>
</li>
<li><p>若移动文件到这个监控目录，则无法触发streaming读取该新增文件，必须用流的形式写入到这个目录的文件才能被监听到</p>
</li>
<li><p>最后也是也是最重要的：streaming只处理在时间窗口内创建的新的数据文件，这里如何理解<code>新的数据文件</code>？</p>
<p>例如streaming流计算设为5秒，这个5秒就是时间窗口，若在5秒内目录产生了一个1.log，这个1.log会被读取，当5秒时间窗口已经过了，那么即使1.log有数据更新，streaming也不再reload该文件，为什么会这么设计呢？</p>
<p>流式处理的逻辑：一批一批的实时读取，已经读取过的数据文件，在下一轮时间窗口不再读取。假设在下一轮时间窗口，还读取已处理过的文件（该文件追加了新的数据行），那么该设计逻辑不再是流式处理了。例如<code>/opt/appdata/</code>目录下，有1.log,…100.log，并还会持续新增数据文件，101.log….等，如果streaming在每轮时间窗口还要对已处理过文件：<code>1.log,...100.log</code>再重新读取（读取新增加的数据行），那么spark streaming就无法完成微批的、实时的流式处理逻辑。在下面的实例会加以说明:</p>
</li>
</ul>
<p>spark streaming 监听文件夹实例：</p>
<p>时间窗口为5秒，实时监听<code>/opt/words/</code>目录下的文件，只要有新的文件创建（这里新的文件是指：每次创建的新文件，文件名必须唯一，streaming才会触发读取）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>)<span class="comment"># 时间窗口为5秒</span></span><br><span class="line">lines = ssc.textFileStream(<span class="string">&quot;file:///opt/words/&quot;</span>)</span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">wordCounts.pprint()</span><br><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)  </span><br></pre></td></tr></table></figure>
<p>模拟实时生成数据文件，每5秒生成一份数据文件，并在生成文件前，删除之前的文件（因为这个旧文件已经被spark streaming读取并流式计算过，下一轮时间窗口不再读取，所以可以删除旧文件）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">import</span> random,os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_text</span>(<span class="params">dir_path,file_name</span>):</span></span><br><span class="line">    words=[<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;streaming&#x27;</span>,<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>,<span class="string">&#x27;kafka&#x27;</span>,<span class="string">&#x27;yarn&#x27;</span>,<span class="string">&#x27;zookeeper&#x27;</span>]</span><br><span class="line">    line_num=random.randint(<span class="number">1000</span>,<span class="number">2000</span>)</span><br><span class="line">    text=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(line_num):</span><br><span class="line">        line=<span class="string">&#x27; &#x27;</span>.join([random.choice(words) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">        text=text+line+<span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    data_file_path=os.path.join(dir_path,file_name)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file_path,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(text)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">streaming_gen_data</span>(<span class="params">stream_dir_path</span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        tmp_file=os.listdir(stream_dir_path)</span><br><span class="line">        <span class="keyword">if</span>  tmp_file:<span class="comment"># 如果监听的目录下有旧文件，则直接删除</span></span><br><span class="line">            file_path=os.path.join(stream_dir_path,tmp_file[<span class="number">0</span>])</span><br><span class="line">            os.remove(file_path)</span><br><span class="line">        file_name=<span class="built_in">str</span>(uuid.uuid1())+<span class="string">&#x27;.log&#x27;</span> <span class="comment"># 保证每次新增的文件名唯一</span></span><br><span class="line">        save_text(stream_dir_path,file_name)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    streaming_gen_data(<span class="string">&#x27;/opt/words&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 933)</span><br><span class="line">(&#39;foo&#39;, 970)</span><br><span class="line">(&#39;yarn&#39;, 951)</span><br><span class="line">(&#39;zookeeper&#39;, 938)</span><br><span class="line">(&#39;bar&#39;, 1020)</span><br><span class="line">(&#39;spark&#39;, 990)</span><br><span class="line">(&#39;streaming&#39;, 1021)</span><br><span class="line">(&#39;kafka&#39;, 949)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:35</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 651)</span><br><span class="line">(&#39;zookeeper&#39;, 623)</span><br><span class="line">(&#39;bar&#39;, 593)</span><br><span class="line">(&#39;yarn&#39;, 584)</span><br><span class="line">(&#39;foo&#39;, 659)</span><br><span class="line">(&#39;spark&#39;, 623)</span><br><span class="line">(&#39;streaming&#39;, 592)</span><br><span class="line">(&#39;kafka&#39;, 571)</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>每隔5秒，spark streaming 完成微批计算：实时统计新创建文件的单词数</p>
<p>在这里重点说明 ：<code>file_name=str(uuid.uuid1())+&#39;.log&#39;</code>,这句保证了每次生成的文件使用了唯一文件名称，这样spark streaming才会瞬间监听到变化，及时读取到该文件。</p>
<p>有人会问：在生成文件前，已经删除了旧文件，那么每次创建文件使用固定文件名，对于spark streaming来说应该是唯一的、未加载过的文件才对吧？</p>
<p>解释：当使用os.remove一个文件后，如果等待间隔时长不长（例如几秒钟）又再创建同名文件，linux底层文件系统使用了缓存，用<code>ls -al</code>  查看该文件，会发现新创建文件的创建时间没有及时改变，导致spark streaming认为该文件还是原的旧文件，也就不再读取。</p>
<p>具体说明如下：<br>第一次创建文件的时间为<code>16 16:10</code>，接着下轮生成新文件，删除旧文件，等待5秒后，再创建同名新文件时，会发现创建时间没有改变还是<code>16 16:10</code>，而ssc.textFileStream读取时用的是创建时间去判断是否为新文件，所以才导致<code>明明已经创建新文件，但是</code>ssc.textFileStream却不读取的情况，这是pyspark textFileStream的bug，这个接口不应该只用创建时间判断。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 33847  16 16:10 streaming_data.log # 首次创建文件</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory #目录下的文件被删除</span><br><span class="line">[root@nn words]# ls -al streaming_data.log</span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">-rw-r--r-- 1 root root 31166  16 16:10 streaming_data.log # 5秒后，第二次创建同名的文件，创建时间未改变（如果等待时间去到十来秒，此时创建同名的文件的创建时间会发生变化）</span><br></pre></td></tr></table></figure>
<p>鉴于<code>textFileStream</code>接口使用场景受限，所以spark streaming实时数据源最适合的场景：接收kafka或者flume推来的流数据，这保证spark streaming能够立刻监听流数据的到来时间是已经发生变化，能触发streaming计算。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark DataFrame</tag>
        <tag>Spark SQL</tag>
        <tag>Spark Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title>redis-cluster原理及其部署测试</title>
    <url>/2019/09/14/redis-cluster%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E9%83%A8%E7%BD%B2%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h3 id="1、Part-I"><a href="#1、Part-I" class="headerlink" title="1、Part I"></a>1、Part I</h3><p>这里主要讨论redis集群数据分片的内容</p>
<h4 id="1-1-为何使用redis-cluster模式？"><a href="#1-1-为何使用redis-cluster模式？" class="headerlink" title="1.1 为何使用redis-cluster模式？"></a>1.1 为何使用redis-cluster模式？</h4><p>1）首先避免单点故障，本人项目中用了主从模式，但因并发量不高，而且在redis不可用条件下，可以直接去数据库拿数据，所以还未部署集群模式。</p>
<p>2）redis官方给出单服务最高可以达到每秒执行10万条命令的性能，其实这对于绝大部分项目都够用了，这里给出集群模式的讨论，一为了深入了解redis，二是如果有一种需求需要百万/s的操作，显然redis单服务无法满足需求</p>
<p>3）内存吃满，redis首先将数据写在内存中，同时不定时异步持久化存在硬盘，一般服务器32G、64G、128G内存，若redis接收的数据量高达1T，128G内存的高性能服务器也会崩溃，故需要做</p>
<p>数据分片（sharding），分别存储在多个redis服务器中，这就需要redis集群模式支持了，这不就是经典的分布式数据库思想吗！</p>
<a id="more"></a>
<h4 id="1-2-客户端分片"><a href="#1-2-客户端分片" class="headerlink" title="1.2 客户端分片"></a>1.2 客户端分片</h4><p>redis集群采用P2P模式，完全去中心化，将redis所有的key分成了16384个槽位，每个redis实例负责一部分slot，集群中的所有信息通过节点数据交换而更新。redis实例集群主要思想是将redis数据的key进行散列，通过hash函数特定的key会映射到指定的redis节点上</p>
<h4 id="1-3-数据分布基础"><a href="#1-3-数据分布基础" class="headerlink" title="1.3 数据分布基础"></a>1.3 数据分布基础</h4><p>分布式数据库首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集。常见的分区规则有哈希分区和顺序分区。<code>Redis Cluster</code>采用哈希分区规则，因此接下来会讨论哈希分区规则。</p>
<ul>
<li>节点取余分区</li>
<li>一致性哈希分区</li>
<li><strong>虚拟槽分区(redis-cluster采用的方式)</strong></li>
</ul>
<p>1)顺序分片</p>
<p>顺序分片按数据大小比较后进行分片</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">id1((数据1-100)) --&gt; id2((数据1-33))</span><br><span class="line">id1((数据1-100)) --&gt; id3((数据34-66))</span><br><span class="line">id1((数据1-100)) --&gt; id4((数据67-100))</span><br></pre></td></tr></table></figure>
<p>2）节点取余分区算法</p>
<p>以三个节点为例：1~100对3取余后有三种情况：余数为0，余数为1，余数2；（除数n，余数情况为n-1种）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">id1((数据1-100)) --&gt;|hashkey后取余%3|id2((3,6,..99))</span><br><span class="line">id1((数据1-100)) --&gt;|hashkey后取余%3| id3((1,4,..100))</span><br><span class="line">id1((数据1-100)) --&gt;|hashkey后取余%3| id4((2,5,..98))</span><br></pre></td></tr></table></figure>
<p>对于N个redis节点，那么数据分片为：hash(key)%N，节点取余分区算法显然是简单高效的</p>
<h4 id="1-4-虚拟槽分区"><a href="#1-4-虚拟槽分区" class="headerlink" title="1.4 虚拟槽分区"></a>1.4 虚拟槽分区</h4><p>虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有的数据映射(通过计算<code>CRC16(key)%16383</code>)到一个固定范围内的整数集合，redis将这种整数定义为槽（slot），Redis Cluster槽的范围是0～16383，总共有16384个哈希槽，槽是集群内数据管理和迁移的基本单位，每个节点负责一定数量的槽，例如有三个redis节点，那么槽位范围分配为：</p>
<p>redis node 1==&gt;slots:[0-5460]</p>
<p>redis node 2==&gt;slots:[5461-10922] </p>
<p>redis node 2==&gt;slots:[10923-16383]</p>
<p>文章后面在部署集群创建槽位分区会看到这个配置</p>
<p>当有某个key被映射到某个Master节点负责的槽，那么这个Master节点负责为这个key提供服务，只有Master节点才拥有槽的所有权，如果是某个Master的slave，这个slave节点只负责槽的使用，但是没有所有权。</p>
<h4 id="1-5-redis的槽位数量为什么是16384（2-14）个？"><a href="#1-5-redis的槽位数量为什么是16384（2-14）个？" class="headerlink" title="1.5 redis的槽位数量为什么是16384（2^14）个？"></a>1.5 redis的槽位数量为什么是16384（2^14）个？</h4><blockquote>
<p>参考redis的作者：在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用<code>char</code>进行bitmap压缩后是2k（<code>2 * 8 (8 bit) * 1024(1k) = 2K</code>），也就是说使用2k的空间创建了16k的槽数。</p>
<p>虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k（<code>8 * 8 (8 bit) * 1024(1k) = 8K</code>），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。</p>
</blockquote>
<h4 id="1-6-redis-cluster小结"><a href="#1-6-redis-cluster小结" class="headerlink" title="1.6 redis-cluster小结:"></a>1.6 redis-cluster小结:</h4><p>1）所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.</p>
<p>2）节点的fail是通过集群中超过半数的节点检测失效时才生效.通过投票机制</p>
<p>3）客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可</p>
<p>4）redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node&lt;-&gt;slot&lt;-&gt;value</p>
<p>5）redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis客户端先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点</p>
<h3 id="2、Part-2"><a href="#2、Part-2" class="headerlink" title="2、Part 2"></a>2、Part 2</h3><h4 id="2-1-redis单服务安装和配置"><a href="#2-1-redis单服务安装和配置" class="headerlink" title="2.1 redis单服务安装和配置"></a>2.1 redis单服务安装和配置</h4><p>这里先给出单个redis服务安装和部署过程，之后会基于多个实例集合redis自身的集群工具搭建redis集群</p>
<p>安装stable版本：目前未redis5.05版本，下载<a href="https://redis.io/download">地址</a></p>
<p>这里以安装在/usr/local目录下为例啊，安装和编译，当然需要服务器已经具备c环境yum install gcc-c++</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost local]# tar -xzf redis-5.0.5.tar.gz</span><br><span class="line">[root@localhost local]# cd redis-5.0.5  </span><br><span class="line">[root@localhost redis-5.0.5]# make    </span><br><span class="line">[root@localhostredis-5.0.5]# cd ./src   #进入到redis-5.0.5/src 文件目录下</span><br><span class="line">[root@localhost src]# make test #</span><br><span class="line">Hint: It&#x27;s a good idea to run &#x27;make test&#x27; ;)</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line"><span class="meta">#</span><span class="bash">redis 可执行脚本都在 utils目录下</span>    </span><br><span class="line">[root@localhost redis-5.0.5]# ls utils/</span><br><span class="line">build-static-symbols.tcl  hashtable          redis_init_script.tpl</span><br><span class="line">cluster_fail_time.tcl     hyperloglog        redis-sha1.rb</span><br><span class="line">corrupt_rdb.c             install_server.sh  releasetools</span><br><span class="line">create-cluster            lru                speed-regression.tcl</span><br><span class="line">generate-command-help.rb  redis-copy.rb      whatisdoing.sh</span><br><span class="line">graphs                    redis_init_script</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 为方便将集群中多个配置文件放在一块管理，可以新建一个redis-conf目录，放置多个redis实例的启动配置文件</span></span><br><span class="line">[root@localhost redis-conf]# ls</span><br><span class="line">redis.conf</span><br><span class="line">[root@localhost redis-conf]# vi redis.conf # 将redis改为后台运行</span><br><span class="line"><span class="meta">#</span><span class="bash"> Note that Redis will write a pid file <span class="keyword">in</span> /var/run/redis.pid when daemonized.</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash">容许远程访问（宿主机访问）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">bind</span> 127.0.0.1</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置密码</span></span><br><span class="line">requirepass foofoo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动一个实例</span></span><br><span class="line">[root@localhost redis-5.0.5]# redis-server redis-conf/redis.conf </span><br><span class="line">12797:C 12 Sep 2019 15:20:41.243 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">12797:C 12 Sep 2019 15:20:41.243 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=12797, just started</span><br><span class="line">12797:C 12 Sep 2019 15:20:41.243 # Configuration loaded</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 本地客户端登录测试</span></span><br><span class="line">[root@localhost redis-5.0.5]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt;</span><br><span class="line">127.0.0.1:6379&gt; set test foo</span><br><span class="line">(error) NOAUTH Authentication required # 提示需要密码认证</span><br><span class="line">127.0.0.1:6379&gt; AUTH foofoo #输入密码</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set test foo</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get test</span><br><span class="line">&quot;foo&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>设置redis开机启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-conf]# vi /etc/rc.d/rc.local </span><br><span class="line">redis-server /usr/local/redis-5.0.5/redis-conf</span><br></pre></td></tr></table></figure>
<p>以上完成redis单服务部署，显然部署过程相当简单，用redis-server 启动一个配置文件，则相应启动一个实例，==若需要在单服务器启动多个redis实例，则只需把整个redis5.0.5目录拷贝多份，修改配置文件redis.conf相应监听端口号等某几个设置项即可==，考虑到后面文章会实现高可用的redis分布式锁，这里会启动6个redis实例集群，以模拟真实集群服务。</p>
<p>为什么是6个实例？</p>
<p>因Redis的容错投票机制是集群中过半数的节点认为某个节点检测失效时才生效，因此最小集群模式至少需要三个节点，而为了高可用，还需要为这三个节点各增加一个slave节点，因此这里就需要6个。而在redis分布式锁算法中，需要在超过半数的redis实例中设置锁成功才能认为获得锁。</p>
<h4 id="2-2-启动多个redis实例的配置"><a href="#2-2-启动多个redis实例的配置" class="headerlink" title="2.2 启动多个redis实例的配置"></a>2.2 启动多个redis实例的配置</h4><p>创建放置集群redis目录，方便管理<code>/usr/local/redis-cluster</code></p>
<p>首先设置redis1，这里给出每个redis服务默认配置文件需要改的地方：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# vi redis1/redis-conf/redis.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 放通全网访问（当然是这内网访问）</span></span><br><span class="line">bind 0.0.0.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开远程访问</span></span><br><span class="line">protected-mode no</span><br><span class="line"><span class="meta">#</span><span class="bash">设置监听端口</span></span><br><span class="line">port 23451</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置redis进程pid路径</span></span><br><span class="line">pidfile /var/run/redis_23451.pid</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群密码</span></span><br><span class="line">masterauth foofoo</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置本服务密码</span></span><br><span class="line">requirepass foofoo</span><br><span class="line"><span class="meta">#</span><span class="bash">设置开启AOF模式</span>  </span><br><span class="line">appendonly yes  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定redis日志文件，方便查看启动日志或者出错日志，需自行创建,每个实例使用不同日志文件</span></span><br><span class="line">logfile &quot;/var/log/redis/redis1.log&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用集群模式</span></span><br><span class="line">cluster-enabled yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定集群节点配置文件路径，redis实例会创建该文件，每个实例使用不同nodes.conf</span></span><br><span class="line">cluster-config-file /usr/local/redis-cluster/nodes-conf/nodes-1.conf</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将redis1拷贝多份到 /usr/local/redis-cluster目录下，并命名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@localhost redis-cluster]# pwd</span><br><span class="line">/usr/local/redis-cluster</span><br><span class="line">[root@localhost redis-cluster]# mv redis-5.0.5/ redis1</span><br><span class="line">[root@localhost redis-cluster]# ls</span><br><span class="line">[root@localhost redis-cluster]# ls</span><br><span class="line">redis1  redis2  redis3  redis4  redis5  redis6</span><br></pre></td></tr></table></figure>
<p>在redis2-redis6目录，修改redis.conf文件，只需修改端口号、pid文件、logfile、cluster-config-file，其他设置项一致。端口也可以按业务规定划分，这里设为五位数端口23452~23456。</p>
<p>手工一个个去启动实例未免麻烦，可以在写个start_all_redis.sh，批量启动，作为测试环境，这里不再将其随机启动。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# ls</span><br><span class="line">appendonly.aof  nodes-6379.conf  redis1  redis2  redis3  redis4  redis5  redis6  start_all_redis.sh</span><br><span class="line"></span><br><span class="line">[root@localhost redis-cluster]# vi start_all_redis.sh </span><br><span class="line">redis-server /usr/local/redis-cluster/redis1/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis2/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis3/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis4/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis5/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis6/redis-conf/redis.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最加执行权限</span></span><br><span class="line">[root@localhost redis-cluster]# chmod +x start_all_redis.sh </span><br></pre></td></tr></table></figure>
<p>启动六个实例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# sh start_all_redis.sh </span><br><span class="line">[root@localhost redis-cluster]# ps -ef|grep redis</span><br><span class="line">root       7130      1  0 16:55 ?        00:00:00 redis-server *:23451 [cluster]</span><br><span class="line">root       7132      1  0 16:55 ?        00:00:00 redis-server *:23452 [cluster]</span><br><span class="line">root       7137      1  0 16:55 ?        00:00:00 redis-server *:23453 [cluster]</span><br><span class="line">root       7142      1  0 16:55 ?        00:00:00 redis-server *:23454 [cluster]</span><br><span class="line">root       7147      1  0 16:55 ?        00:00:00 redis-server *:23455 [cluster]</span><br><span class="line">root       7152      1  0 16:55 ?        00:00:00 redis-server *:23456 [cluster]</span><br><span class="line">root       7160   7091  0 16:55 pts/2    00:00:00 grep --color=auto redis</span><br></pre></td></tr></table></figure>
<p>去日志路径查看启动日志，这里截取了一部分日志内容，可以看到redis是集群模式，而且后面还有redis给出3项优化建议</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis]# pwd</span><br><span class="line">/var/log/redis</span><br><span class="line">[root@localhost redis]# cat redis4.log </span><br><span class="line">Redis 5.0.5 (00000000/0) 64 bit</span><br><span class="line">Running in cluster mode</span><br><span class="line">Port: 23454</span><br><span class="line">PID: 7142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is <span class="built_in">set</span> to the lower value of 128.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING overcommit_memory is <span class="built_in">set</span> to 0! Background save may fail under low memory condition. To fix this issue add <span class="string">&#x27;vm.overcommit_memory = 1&#x27;</span> to /etc/sysctl.conf and <span class="keyword">then</span> reboot or run the <span class="built_in">command</span> <span class="string">&#x27;sysctl vm.overcommit_memory=1&#x27;</span> <span class="keyword">for</span> this to take effect.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING you have Transparent Huge Pages (THP) support enabled <span class="keyword">in</span> your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the <span class="built_in">command</span> <span class="string">&#x27;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&#x27;</span> as root, and add it to your /etc/rc.local <span class="keyword">in</span> order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span></span><br><span class="line">* Ready to accept connections</span><br></pre></td></tr></table></figure>
<h4 id="2-3-测试redis集群情况"><a href="#2-3-测试redis集群情况" class="headerlink" title="2.3 测试redis集群情况"></a>2.3 测试redis集群情况</h4><p>登录其中一个redis实例并测试，提示没有slot</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli -p 23451 -c</span><br><span class="line">127.0.0.1:23451&gt; AUTH foofoo</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23451&gt; set test foo</span><br><span class="line">(error) CLUSTERDOWN Hash slot not served</span><br></pre></td></tr></table></figure>
<p>==以上只是启动六个redis，但redis集群的slot还未分配到每个redis上，还需最后一步分配slot！==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli  --cluster create 127.0.0.1:23451 127.0.0.1:23452 127.0.0.1:23453 127.0.0.1:23454 127.0.0.1:23455 127.0.0.1:23456  --cluster-replicas 1 -a foofoo</span><br><span class="line">Warning: Using a password with &#x27;-a&#x27; or &#x27;-u&#x27; option on the command line interface may not be safe.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Performing <span class="built_in">hash</span> slots allocation on 6 nodes...</span></span><br><span class="line">Master[0] -&gt; Slots 0 - 5460</span><br><span class="line">Master[1] -&gt; Slots 5461 - 10922</span><br><span class="line">Master[2] -&gt; Slots 10923 - 16383</span><br><span class="line">Adding replica 127.0.0.1:23455 to 127.0.0.1:23451</span><br><span class="line">Adding replica 127.0.0.1:23456 to 127.0.0.1:23452</span><br><span class="line">Adding replica 127.0.0.1:23454 to 127.0.0.1:23453</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Trying to optimize slaves allocation <span class="keyword">for</span> anti-affinity</span></span><br><span class="line">[WARNING] Some slaves are in the same host as their master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis1实例作为master角色</span></span><br><span class="line">M: 920db62a3c29e3cb28dcbb575d4a438761563feb 127.0.0.1:23451</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis2实例作为master角色</span></span><br><span class="line">M: 3c54734e178ca585477c8fb8b78d87d0d7a1e038 127.0.0.1:23452</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis3实例作为master角色</span></span><br><span class="line">M: beb60be98bdc8ce50825b9d42e5efd819b98dc34 127.0.0.1:23453</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span><span class="bash"> redis4实例作为slave角色</span></span><br><span class="line">S: 4d451a885c1974679fe7c193ecac0373bd5cd808 127.0.0.1:23454</span><br><span class="line">   replicates 3c54734e178ca585477c8fb8b78d87d0d7a1e038</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span><span class="bash"> redis5实例作为slave角色</span></span><br><span class="line">S: 6ea365783619da802cd917b10fafd6096b4166a5 127.0.0.1:23455</span><br><span class="line">   replicates beb60be98bdc8ce50825b9d42e5efd819b98dc34</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span><span class="bash"> redis6实例作为slave角色</span></span><br><span class="line">S: 120026cbf95be79f4a3b2ed32c6867e238b7f8ec 127.0.0.1:23456</span><br><span class="line">   replicates 920db62a3c29e3cb28dcbb575d4a438761563feb</span><br><span class="line">Can I set the above configuration? (type &#x27;yes&#x27; to accept): yes</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Nodes configuration updated</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Assign a different config epoch to each node</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span></span><br><span class="line">Waiting for the cluster to join</span><br><span class="line">.....</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 127.0.0.1:23451)</span></span><br><span class="line">M: 920db62a3c29e3cb28dcbb575d4a438761563feb 127.0.0.1:23451</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 4d451a885c1974679fe7c193ecac0373bd5cd808 127.0.0.1:23454</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 3c54734e178ca585477c8fb8b78d87d0d7a1e038</span><br><span class="line">M: 3c54734e178ca585477c8fb8b78d87d0d7a1e038 127.0.0.1:23452</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 120026cbf95be79f4a3b2ed32c6867e238b7f8ec 127.0.0.1:23456</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 920db62a3c29e3cb28dcbb575d4a438761563feb</span><br><span class="line">M: beb60be98bdc8ce50825b9d42e5efd819b98dc34 127.0.0.1:23453</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 6ea365783619da802cd917b10fafd6096b4166a5 127.0.0.1:23455</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates beb60be98bdc8ce50825b9d42e5efd819b98dc34</span><br><span class="line">[OK] All nodes agree about slots configuration.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="keyword">for</span> open slots...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 提示16384个槽位已经分区好</span></span><br><span class="line">[OK] All 16384 slots covered.</span><br></pre></td></tr></table></figure>
<p>登录其中一台，注意这是集群模式登录，如果不带密码-a，则在set值时，分配到其它节点槽号将提示认证失败</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli -p 23451 -c</span><br><span class="line">127.0.0.1:23451&gt; AUTH foofoo</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23451&gt; set test foo</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [6918] located at 127.0.0.1:23452</span></span><br><span class="line">(error) NOAUTH Authentication required</span><br></pre></td></tr></table></figure>
<p>启动客户端需要带上密码选项，可以看到成功set值，通过设置不同的key，可以看到数据被hash后重定向到不同节点上的槽号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli -p 23451 a foofoo -c</span><br><span class="line">127.0.0.1:23451&gt; set test foo</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [6918] located at 127.0.0.1:23452</span></span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23452&gt; set cluster-test 11</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [14783] located at 127.0.0.1:23453</span></span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23453&gt; get test</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [6918] located at 127.0.0.1:23452</span></span><br><span class="line">&quot;foo&quot;</span><br></pre></td></tr></table></figure>
<p>以上完成了cluster模式的部署和测试，接下将给出redis主从模式，以及使用docker部署redis服务，之后给出了基于redis集群实现的高可用分布式锁Redlock的过程，在此之前已经给出了zookeeper的分布式锁实现。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis-cluster</tag>
      </tags>
  </entry>
  <entry>
    <title>利用pandas将分组后的组内多行归并为一行</title>
    <url>/2020/11/22/%E5%88%A9%E7%94%A8pandas%E5%B0%86groupby%E5%88%86%E7%BB%84%E5%90%8E%E5%B0%86%E7%BB%84%E5%86%85%E5%A4%9A%E8%A1%8C%E4%B8%BA%E4%B8%80%E8%A1%8C/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文的使用场景相对常见，例如需要解析一份含几千上万行Excel表（或者长报告里需要插入表格数据），需要将某列的一个单元格长字符串对应展开为多行且索引号不变，或者groupby后将同组的多行值拼接为单个长字符串，使用pandas可以快速完成。（文章由jupyter notebook导出的Markdown文件生成）</p>
<h3 id="groupby后将同组的多行值连接为一个长字符串"><a href="#groupby后将同组的多行值连接为一个长字符串" class="headerlink" title="groupby后将同组的多行值连接为一个长字符串"></a>groupby后将同组的多行值连接为一个长字符串</h3><h4 id="使用df-groupby、apply、pd-merge"><a href="#使用df-groupby、apply、pd-merge" class="headerlink" title="使用df.groupby、apply、pd.merge"></a>使用df.groupby、apply、pd.merge</h4><p>（在mysql中可以直接使用GROUP_CONCAT(字段 SEPARATOR ‘、’)方法完成）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;省份&#x27;</span>:[<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;上海&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;大学名称&#x27;</span>:[<span class="string">&#x27;复旦大学&#x27;</span>,<span class="string">&#x27;中山大学&#x27;</span>,<span class="string">&#x27;同济大学&#x27;</span>,<span class="string">&#x27;华南理工大学&#x27;</span>,<span class="string">&#x27;上海交通大学&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df=pd.DataFrame(data)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>3</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>4</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flatten_rows=<span class="keyword">lambda</span> s:<span class="string">&#x27;、&#x27;</span>.join(s) <span class="comment"># 这就是分组后，将组内多行合并为一行的处理逻辑，这里用顿号连接行字符串</span></span><br><span class="line"><span class="comment"># 使用apply方法,as_index=False,表示不将省份作为groupby的行索引</span></span><br><span class="line">df1=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>)[<span class="string">&#x27;大学名称&#x27;</span>].apply(flatten_rows)</span><br><span class="line">df1</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>)[<span class="string">&#x27;大学名称&#x27;</span>].agg(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">df2</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数量统计一列合并到df1，用省份关联即可,指定合并后，非key列的后缀，以便区分列来源哪个dataframe</span></span><br><span class="line">df1.merge(df2,on=<span class="string">&#x27;省份&#x27;</span>,suffixes=[<span class="string">&#x27;_df1&#x27;</span>,<span class="string">&#x27;_df2&#x27;</span>]) </span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称_df1</th>
      <th>大学名称_df2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">merged_df=pd.merge(df1,df2,on=<span class="string">&#x27;省份&#x27;</span>,suffixes=[<span class="string">&#x27;_df1&#x27;</span>,<span class="string">&#x27;_df2&#x27;</span>]) <span class="comment"># 也可直接用pd.merge方法</span></span><br><span class="line">merged_df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称_df1</th>
      <th>大学名称_df2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">merged_df.columns=[<span class="string">&#x27;省份&#x27;</span>,<span class="string">&#x27;大学列表&#x27;</span>,<span class="string">&#x27;数量&#x27;</span>]</span><br><span class="line">merged_df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学列表</th>
      <th>数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<h4 id="df-groupby和agg方法"><a href="#df-groupby和agg方法" class="headerlink" title="df.groupby和agg方法"></a>df.groupby和agg方法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">groupby_df=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>).agg(&#123;<span class="string">&#x27;大学名称&#x27;</span>: [flatten_rows,<span class="string">&#x27;count&#x27;</span>]&#125;)</span><br><span class="line">groupby_df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th>省份</th>
      <th colspan="2" halign="left">大学名称</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>&lt;lambda_0&gt;</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">groupby_df.columns=[<span class="string">&#x27;省份&#x27;</span>,<span class="string">&#x27;大学列表&#x27;</span>,<span class="string">&#x27;数量&#x27;</span>]</span><br><span class="line">groupby_df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学列表</th>
      <th>数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="同组一行长字符串展开为多行"><a href="#同组一行长字符串展开为多行" class="headerlink" title="同组一行长字符串展开为多行"></a>同组一行长字符串展开为多行</h3><p>此内容为前面的逆向处理（若使用mysql，需较为复杂的sql语句实现）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data2=&#123;</span><br><span class="line">    <span class="string">&#x27;省&#x27;</span>:pd.Series([<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;广西&#x27;</span>]),</span><br><span class="line">    <span class="string">&#x27;大学列表&#x27;</span>:pd.Series([<span class="string">&#x27;复旦大学、同济大学、上海交通大学&#x27;</span>,<span class="string">&#x27;中山大学、华南理工大学&#x27;</span>,<span class="literal">None</span>])</span><br><span class="line">&#125;</span><br><span class="line">df=pd.DataFrame(data2)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df21=df.copy()</span><br><span class="line"><span class="comment"># 创建数据集要小心，用pd.Series创建，则每行数据为字符串类型</span></span><br><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h4 id="使用pd-explode方法快速将一行展开为多行"><a href="#使用pd-explode方法快速将一行展开为多行" class="headerlink" title="使用pd.explode方法快速将一行展开为多行"></a>使用pd.explode方法快速将一行展开为多行</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pd.explode要求单元格的值为一个列表，如[&#x27;复旦大学&#x27;, &#x27;同济大学&#x27;, &#x27;上海交通大学&#x27;]，因此需原单元格长字符串做处理</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_func</span>(<span class="params">cell</span>):</span> <span class="comment"># 每个cell的长字符串分割，注意对于空值的处理</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> cell:</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    <span class="keyword">return</span> cell.split(<span class="string">&#x27;、&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>]=df21[<span class="string">&#x27;大学列表&#x27;</span>].apply(split_func) <span class="comment"># 大学列表这一列的每行值都是字符串类型</span></span><br><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;复旦大学&#39;, &#39;同济大学&#39;, &#39;上海交通大学&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df21.explode(<span class="string">&#x27;大学列表&#x27;</span>,ignore_index=<span class="literal">False</span>) <span class="comment">#ignore_index=False使得DataFrame对象保持原一行对应的索引号</span></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>

<p>以上重点解析：对cell单元格的长字符串进行分割为一个列表，需自行设计一个分割函数<br>例如字符串：’复旦大学、同济大学、上海交通大学’<br>同理其他更为复杂的字符串:”黄小明201、李小明202|黄小明203%…”,采用正则分割即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关于pandas的explode几点说明</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">pandas.DataFrame.explode()内部调用pandas.Series.explode()：</span></span><br><span class="line"><span class="string">        result = df[column].explode()</span></span><br><span class="line"><span class="string">        # 删除原df指定展开列后，再与该列单独使用pandas.Series.explode得到的结果进行索引关联合并</span></span><br><span class="line"><span class="string">        result = df.drop([column], axis=1).join(result) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pandas.Series.explode()内部使用reshape.explode</span></span><br><span class="line"><span class="string">values, counts = reshape.explode(np.asarray(self.array))</span></span><br><span class="line"><span class="string"># 该方法来pandas._libs基础库目录的reshape.cpython-37m-darwin.so*，这里已经被编译为动态链接库</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># pandas.Series.explode()方法表示将一列中单元格为列表值展开为多行</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series([[1, 2, 3], &#x27;foo&#x27;, [], [3, 4]])</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s</span></span><br><span class="line"><span class="string">0    [1, 2, 3]</span></span><br><span class="line"><span class="string">1          foo</span></span><br><span class="line"><span class="string">2           []</span></span><br><span class="line"><span class="string">3       [3, 4]</span></span><br><span class="line"><span class="string">dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s.explode()</span></span><br><span class="line"><span class="string">0      1</span></span><br><span class="line"><span class="string">0      2</span></span><br><span class="line"><span class="string">0      3</span></span><br><span class="line"><span class="string">1    foo</span></span><br><span class="line"><span class="string">2    NaN</span></span><br><span class="line"><span class="string">3      3</span></span><br><span class="line"><span class="string">3      4</span></span><br><span class="line"><span class="string">dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 可以同一单元格展开多行后，索引号跟原单元格索引号相同，因此可用原df通过join展开多行的Series，就能快速完成全表展开。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="使用stack-join方法快速将一行展开为多行"><a href="#使用stack-join方法快速将一行展开为多行" class="headerlink" title="使用stack+join方法快速将一行展开为多行"></a>使用stack+join方法快速将一行展开为多行</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22=df.copy()</span><br><span class="line">df22</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_1=df22[<span class="string">&#x27;大学列表&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;、&#x27;</span>,expand=<span class="literal">True</span>) <span class="comment"># 将一个cell值扩展为多列的cell值，该方法能自动处理cell空值</span></span><br><span class="line">df22_1</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>复旦大学</td>
      <td>同济大学</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>中山大学</td>
      <td>华南理工大学</td>
      <td>None</td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#三、将扩展的列转成多行，有些列的单元格值为None，转为行记录需要去掉</span></span><br><span class="line">df22_2=df22_1.stack(dropna=<span class="literal">True</span>)</span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure>
<pre><code>0  0      复旦大学
   1      同济大学
   2    上海交通大学
1  0      中山大学
   1    华南理工大学
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_2.index</span><br></pre></td></tr></table></figure>
<pre><code>MultiIndex([(0, 0),
            (0, 1),
            (0, 2),
            (1, 0),
            (1, 1)],
           )
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 观察以上索引，若能把MultiIndex的第1列索引留下，则可关联到原表的0，1索引号</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_2.reset_index(level=<span class="number">1</span>,drop=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure>
<pre><code>0      复旦大学
0      同济大学
0    上海交通大学
1      中山大学
1    华南理工大学
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_2.name=<span class="string">&#x27;new_col&#x27;</span></span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure>
<pre><code>0      复旦大学
0      同济大学
0    上海交通大学
1      中山大学
1    华南理工大学
Name: new_col, dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span>(df22[<span class="string">&#x27;大学列表&#x27;</span>]) <span class="comment"># 删除原数据集大学列表这一列，用于后面合并</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_3=df22.join(df22_2) <span class="comment"># 默认使用两个df的索引号进行关联合并。最终完成将一行.注意df22是dataframe才有join方法，df22_2是Series，没有join方法</span></span><br><span class="line">df22_3</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>new_col</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 主要思路：将需要展开为多行的单元格值展开为多列，通过旋转操作将其变为一列Series，重置索引号，最终使用两个df索引号进行join关联合并,除了用pd.join，还可使用pd.concat两个省列与大学名称列关联合并为一张dataframe</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># df22[&#x27;省&#x27;]属于Series类型，df22_2也是Series类型，两者通过索引号进行关联后，按列方向合并</span></span><br><span class="line">df22_3=pd.concat([df22[<span class="string">&#x27;省&#x27;</span>],df22_2],join=<span class="string">&#x27;inner&#x27;</span>,axis=<span class="number">1</span>)</span><br><span class="line">df22_3</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>new_col</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_3.columns=[<span class="string">&#x27;省&#x27;</span>,<span class="string">&#x27;大学名称&#x27;</span>]</span><br><span class="line">df22_3 <span class="comment"># 注意到广西所在行已被删除，最终可将None行垂直合并到df22_3即可</span></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
  </tbody>
</table>
</div>

<h4 id="自行设计合并逻辑"><a href="#自行设计合并逻辑" class="headerlink" title="自行设计合并逻辑"></a>自行设计合并逻辑</h4><p>将以下两行数据行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上海	复旦大学、同济大学、上海交通大学</span><br><span class="line">广东	中山大学、华南理工大学</span><br></pre></td></tr></table></figure>
<p>对应生成两个Series列数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;上海&#39;,&#39;上海&#39;,&#39;上海&#39;,&#39;广东&#39;,&#39;广东&#39;]</span><br><span class="line">[&#39;复旦大学&#39;,&#39;同济大学&#39;,&#39;上海交通大学&#39;,&#39;中山大学&#39;,&#39;华南理工大学&#39;]</span><br></pre></td></tr></table></figure>
<p>再将这两个Series生成Dataframe即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df23=df.copy()</span><br><span class="line">df23</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">province_list=[] <span class="comment"># 存放省的Series的值</span></span><br><span class="line">college_list=[] <span class="comment"># 存放大学名称Series的值</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df23.itertuples():</span><br><span class="line">    prov_cell=row[<span class="number">1</span>]</span><br><span class="line">    split_cell=row[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">if</span> split_cell:</span><br><span class="line">        split_cell_list=split_cell.split(<span class="string">&#x27;、&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> split_cell_list:</span><br><span class="line">            province_list.append(prov_cell)</span><br><span class="line">            college_list.append(item)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 注意对cell为空值得处理</span></span><br><span class="line">        province_list.append(prov_cell)</span><br><span class="line">        college_list.append(split_cell)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">province_list</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;上海&#39;, &#39;上海&#39;, &#39;上海&#39;, &#39;广东&#39;, &#39;广东&#39;, &#39;广西&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">college_list</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;复旦大学&#39;, &#39;同济大学&#39;, &#39;上海交通大学&#39;, &#39;中山大学&#39;, &#39;华南理工大学&#39;, None]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data23=&#123;</span><br><span class="line">    <span class="string">&#x27;省&#x27;</span>:province_list,</span><br><span class="line">    <span class="string">&#x27;大学名称&#x27;</span>:college_list</span><br><span class="line">&#125;</span><br><span class="line">pd.DataFrame(data23) <span class="comment"># 将以上两列Series拼接为一个df</span></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>3</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>4</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>5</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>]]></content>
      <categories>
        <category>数据分析与挖掘</category>
      </categories>
  </entry>
  <entry>
    <title>在hadoopHA节点上部署flume高可用组件</title>
    <url>/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式</p>
<a id="more"></a>
<div class="table-container">
<table>
<thead>
<tr>
<th>hadoop节点</th>
<th>数据源节点</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>nn</td>
<td>nn</td>
<td>NameNode,DataNode数据源</td>
</tr>
<tr>
<td>dn2</td>
<td>dn2</td>
<td>NameNode,DateNode数据源</td>
</tr>
<tr>
<td>dn1</td>
<td>dn1</td>
<td>DataNode,数据源</td>
</tr>
</tbody>
</table>
</div>
<p>其他hbase、hive等不再此表给出，因为前面的文件已有相关表格。</p>
<h4 id="1、flume-的基本介绍"><a href="#1、flume-的基本介绍" class="headerlink" title="1、flume 的基本介绍"></a>1、flume 的基本介绍</h4><h5 id="1-1-基本介绍"><a href="#1-1-基本介绍" class="headerlink" title="1.1 基本介绍"></a>1.1 基本介绍</h5><blockquote>
<p>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.  It uses a simple extensible data model that allows for online analytic application.</p>
</blockquote>
<p>简单来说：flume 是一个分布式、高可靠、高可用的用来收集、聚合、转移不同来源的大量日志数据到中央数据仓库的工具</p>
<p>目前flume最新版本为今年1月发布的 Flume 1.9.0，具体优化的内容：</p>
<blockquote>
<ul>
<li>Better SSL/TLS support</li>
<li>Configuration Filters to provide a way to inject sensitive information like passwords into the configuration</li>
<li>Float and Double value support in Context</li>
<li>Kafka client upgraded to 2.0</li>
<li>HBase 2 support</li>
</ul>
</blockquote>
<p>环境要求：</p>
<p>Java Runtime Environment - Java 1.8 or later</p>
<h5 id="1-2-数据流模型"><a href="#1-2-数据流模型" class="headerlink" title="1.2 数据流模型"></a>1.2 数据流模型</h5><p>这里以Flume收取web的日志再将其写入到hdfs作为数据流模型示例说明。<br>（需要注意的flume支持多级配置、扇入、扇出，这里仅介绍单级、单输入、单输出的模式）<br><img src="https://img-blog.csdnimg.cn/20191123154225919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>==Event==<br>Event是Flume定义的一个数据流传输的最小单元。Agent就是一个Flume的实例，本质是一个JVM进程，该JVM进程控制Event数据流从外部日志生产者那里传输到目的地（或者是下一个Agent）。<br>在Flume中，event表示数据传输的一个最小单位，从上图可以看出Agent就是Flume的一个部署实例， 一个完整的Agent中包含了三个组件Source、Channel和Sink，Source是指数据的来源和方式，Channel是一个数据的缓冲池，Sink定义了数据输出的方式和目的地。</p>
<p>==Source组件==<br>Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。（对于实时大数据项目，这个source其实就是不断滚动的log数据文件。）<br>Flume提供了各种source的实现，具体参考官方文档：<br>Flume Sources、Avro Source、Thrift Source、Exec Source、JMS Source、Taildir Source、Kafka Source、NetCat TCP Source、NetCat UDP Source、Syslog Sources、HTTP Source、Custom Source等</p>
<p>==Channel组件==<br>Channel是连接Source和Sink的组件，可以看作一个数据的缓冲区，它可以将事件暂存到内存中也可以持久化到磁盘上， 直到Sink处理完该事件。flume提供多个channel提供对event数据的缓存：<br>Memory Channel、JDBC Channel、Kafka Channel、File Channel、Spillable Memory Channel、Pseudo Transaction Channel、Custom Channel<br>（在本大数据实时项目中，使用Memory Channel即可，生产环境需要根据具体需求而定）</p>
<p>==Sink组件==<br>Sink从Channel取出event数据，并将其写入到文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。<br>Flume也提供了各种sink的实现，具体参考官方说明：<br>HDFS Sink、Hive Sink、Logger Sink、Avro Sink、Thrift Sink、IRC Sink、File Roll Sink、Null Sink、HBaseSinks、AsyncHBaseSink、ElasticSearchSink、Kite Dataset Sink、Kafka Sink、HTTP Sink、Custom Sink<br>（在实时大数据项目中，用sink配置将数据写入kafka sink）</p>
<p>以上的数据模型的详细介绍可以参考中文翻译文档：<a href="https://flume.liyifeng.org/">地址</a><br>(注意：该翻译文档对应flume1.8版本的内容，若想查阅最新的flume，参考官网1.9原文)</p>
<h4 id="2、flume的配置文件说明"><a href="#2、flume的配置文件说明" class="headerlink" title="2、flume的配置文件说明"></a>2、flume的配置文件说明</h4><p>flume配置遵循Java properties文件格式的文本文件。一个或多个Agent配置可放在同一个配置文件里。配置文件包含Agent的source，sink和channel的各个属性以及他们的数据流连接。</p>
<p>完整的配置流程如下：<br>命名各个组件（定义流）—&gt;连接各个组件—&gt;配置各个组件的属性—&gt;启动Agent</p>
<h5 id="2-1-配置过程"><a href="#2-1-配置过程" class="headerlink" title="2.1 配置过程"></a>2.1 配置过程</h5><p>这里以后面文章——flume整合kafka的配置文件说明。<br>（有较多的blog文章会照搬a1.sources=r1,a1.sinks=k1,a1.channels=c1这样的写法，a1?r1?k1?c1?到底指代什么？所以建议要参考官方文档，否则一头雾水？当然熟悉flume后，可以使用短命名，设置配置属性字符串不会显得太长）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、列出Agent的所有Source、Channel、Sink</span></span><br><span class="line">&lt;Agent&gt;.sources = &lt;Source&gt;</span><br><span class="line">&lt;Agent&gt;.sinks = &lt;Sink&gt;</span><br><span class="line">&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接各个组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Source的关联</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Sink的关联</span></span><br><span class="line">&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、为每个组件配置属性，这些属性就是flume的性能参数，控制flume的各种工作方式，调优配置就在这部分了。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sources</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> channels</span></span><br><span class="line">&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sinks</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br></pre></td></tr></table></figure>
<p>对于本blog的实时大数据项目的配置，Agent名字为：agent_log从本地服务器读取log数据文件，使用内存channel缓存，然后通过kafka Sink从channel读取后发送到kafka集群，它的配置文件应该这样配：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、定义Agent的所有<span class="built_in">source</span>、sink和channel组件</span></span><br><span class="line">agent_log.sources = log-src</span><br><span class="line">agent_log.sinks = kafka-sink</span><br><span class="line">agent_log.channels = log-mem-channel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接三个组件</span></span><br><span class="line">agent_log.sources.log-src.channels =log-mem-channel    # 指定与source:log-src相连接的channel是log-mem-channel</span><br><span class="line">agent_foo.sinks.kafka-sink.channel = log-mem-channel   # 指定与sink:kafka-sink相连接的channel是log-mem-channel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、配置各个组件的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置avro-AppSrv-source的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> log-src 的类型是spooldir，官方建议不要使用tail -F抽取数据文件因会出现丢失</span></span><br><span class="line">agent_log.sources.log-src.type = spooldir         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置<span class="built_in">source</span>数据文件的路径</span></span><br><span class="line">agent_log.sources.log-src.spoolDir = /opt/flume_agent/web_log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置log-mem-channel的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> channel的类型是内存channel</span></span><br><span class="line">agent_log.channels.log-mem-channel.type = memory  </span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> channel的最大容量是1000</span></span><br><span class="line">agent_log.channels.log-mem-channel.capacity = 1000         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>和sink每次从channel写入和读取的Event数量</span></span><br><span class="line">agent_log.channels.log-mem-channel.transactionCapacity = 100    </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置kafka-sink的属性，将数据写入到kafka集群指定topic，实现Flume与Kafka集成</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接channel到kafkasink</span></span><br><span class="line">agent_log.sinks.kafka-sink.channel = log-mem-channel </span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定kafka sink</span></span><br><span class="line">agent_log.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka存放数据的topic</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.topic = webtopic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka sink 使用的 kafka 集群的实例列表</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每批要发送到kafka的消息数量</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.flumeBatchSize = 20</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在成功写入之前，要求有1个副本必须确认消息，保证数据一致性</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.producer.acks = 1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>通过上面的配置，就形成了[log-src]-&gt;[log-mem-channel]-&gt;[log-sink]的数据流，这将使Event通过内存channel（log-mem-channel）从log-src流向Kafka-sink，从而实现数据源-flume-kafka的实时数据流动。</p>
<h4 id="3、单点flume-agent测试"><a href="#3、单点flume-agent测试" class="headerlink" title="3、单点flume agent测试"></a>3、单点flume agent测试</h4><p>本节主要在name节点上部署单个flume agent，用于测试单agent的使用。<br>数据流向：<br>手动写日志内容—-&gt;flume spooldir抽取—-&gt;flume sink到hadoop集群文件系统上</p>
<h5 id="3-1-基本安装"><a href="#3-1-基本安装" class="headerlink" title="3.1 基本安装"></a>3.1 基本安装</h5><p>个人习惯将所有的hadoop组件都放置在同一个dir下，方便管理，如下所示<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hive-3.1.2       xcall.sh       </span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    scala-2.13.1         </span><br><span class="line">hbase-2.1.7   mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br></pre></td></tr></table></figure><br>配置flume-env.sh<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">[root@nn conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line">vi flume-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置java1.8的路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> JAVA_HOME=/opt/jdk1.8.0_16</span></span><br></pre></td></tr></table></figure><br>这里的配置要注意的点：如果已经在系统的环境变量配置JAVA_HOME，那么flume-env.sh可以不用再配置java路径</p>
<p>配置flume-conf.properties<br>这里就是用于配置flume agent的文件。有了第2章节的介绍后，这里有关source、channel、sink配置则相对简单，因此可以使用短字符进行配置<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>数据源为本地某个文件目录，flume监听这个目录下日志文件</span></span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里不需要写成web_log/</span></span><br><span class="line">a1.sources.r1.spoolDir = /opt/flume_log/web_log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录，hdapp为hadoop集群名称，不需要加入端口，否则flume无法写入，</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType=SequenceFile</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat=Writable</span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放在hdfs的文件文件命名方式，其实还有更详细的配置，这里仅给出简单示例，具体可参考官网。</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从flume过来的数据，每128M分割成一个文件</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128000000  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终在hdfs的文件名称为：%Y-%m-%d.TimeStamp.txt</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>这里在配置source.type要注意的是，配成spooldir类型后：允许把要收集的文件放入磁盘上的某个指定目录。它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来。数据解析逻辑是可配置的。<br>与Exec Source不同，Spooling Directory Source是可靠的，即使Flume重新启动或被kill，也不会丢失数据。<br>但这种可靠有一定的代价和限制：指定目录中的文件必须是不可变的、唯一命名的。Flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常：<br>如果文件在写入完成后又被再次写入新内容，Flume将向其日志文件（这是指Flume自己logs目录下的日志文件）打印错误并停止处理。如果在以后重新使用以前的文件名，Flume将向其日志文件打印错误并停止处理。<br>为了避免上述问题，最好在生成新文件的时候文件名加上时间戳，可以通过加入属性项实现：a1.sinks.k1.hdfs.useLocalTimeStamp = true</p>
<h5 id="3-2-启动flume-agent进程"><a href="#3-2-启动flume-agent进程" class="headerlink" title="3.2 启动flume agent进程"></a>3.2 启动flume agent进程</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 命令含义<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commands:</span><br><span class="line">  help                      display this help text</span><br><span class="line">  agent                     run a Flume agent</span><br><span class="line">  avro-client               run an avro Flume client</span><br><span class="line">  version                   show Flume version info</span><br><span class="line"> global options:</span><br><span class="line">    --conf,-c &lt;conf&gt;          use configs in &lt;conf&gt; directory</span><br><span class="line"> agent options:</span><br><span class="line">    --name,-n &lt;name&gt;          the name of this agent (required)</span><br><span class="line">  --conf-file,-f &lt;file&gt;     specify a config file (required if -z missing)</span><br></pre></td></tr></table></figure><br>创建数据文件，测试flume 能否成功把目录下的文件推到hdfs指定目录上<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn web_log]# pwd</span><br><span class="line">/opt/flume_log/web_log</span><br><span class="line">[root@nn web_log]#  vi log.txt</span><br><span class="line">aaa</span><br><span class="line">bbb</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当文件创建后，发现该log.txt被命名为log.txt.COMPLETED，说明已经被flume 读取过</span></span><br><span class="line">[root@nn web_log]# ls</span><br><span class="line">log.txt.COMPLETED</span><br></pre></td></tr></table></figure><br>hdfs上可看到数据文件已经上传到到/flume/web_log（这里打码了时间）<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn web_log]# hdfs dfs -ls /flume/web_log </span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup        161 **** /flume/web_log/2019-**-**.15**0.txt</span><br></pre></td></tr></table></figure></p>
<h5 id="3-3-将source-type配成tail-F"><a href="#3-3-将source-type配成tail-F" class="headerlink" title="3.3 将source.type配成tail F"></a>3.3 将source.type配成tail F</h5><p>Spooling Directory Source是可靠的，它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来，当此种方式不适合本blog后面开发的实时大数据项目需求。具体说明如下：<br>本blog后面开发的实时大数据项目需求：<br>实时抽取access.log的访问日志，access.log每插入一行，flume 就会把它实时sink到hdfs上（本文用于测试所以先sink到hdfs，若已经到开发阶段，这里会改为sink到kalka集群上）。<br>对于spooldir模式，当log.txt被sink后其文件名变为log.txt.COMM，若继续向log.txt.COMPLETED append数据行，flume不会再抽取该文件，也说明无法把新来的数据sink到hdfs上，显然不符合需求。<br>source需做以下调整，使用exec source：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改<span class="built_in">source</span> <span class="built_in">type</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><br>这里sink的文件滚动策略很重要，若配置不当，flume sink会在hdfs不断滚动生成多个小文件，例如access.log新增一行，触发flume sink在hdfs新增一个对应的文件。<br>以下的配置：access.log在hdfs存放的形式为：<br>/flume/web_log/2019-05-31.1579*.txt<br>每达到128M则开始滚动新建一个文件。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128M</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br></pre></td></tr></table></figure></p>
<p>但exec source方式也有缺点：会丢失数据，例如当flume 挂了重启，之前进来的日志行将不会被重启后flume抽取到，正官方的提示：<br>The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost.</p>
<p>这种数据丢失其实还可以接受，毕竟大部分日志收集应用场景还没到高级事务的严格标准，而且服务器集群运行以及进程运行稳定，即使宕机、断电再重启，也只是一小部分日志行丢失。</p>
<p>==测试结果：==<br>启动flume agent，并将日志实时打印在shell<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]#  bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将日志追加新数据行</span></span><br><span class="line">[root@nn web_log]# echo &#x27;test&#x27;&gt;&gt;access.log </span><br></pre></td></tr></table></figure><br>在hdfs上，存放的文件会以文件名.txt.tmp形式保持打开状态，供flume实时写入，若达到滚动条件，则会生成日期+时间戳.txt的数据文件，再新建另外一个日期+时间戳.txt.tmp文件。</p>
<p>至此，已完成flume的部署，下一步，在三个节点上配置高可用的flume集群。</p>
<h4 id="4、flume高可用配置"><a href="#4、flume高可用配置" class="headerlink" title="4、flume高可用配置"></a>4、flume高可用配置</h4><p>Flume高可用又称Flume NG高可用，NG：Next Generation。<br>flume高可用的实现思路比较清晰：多个节点flume agent  avro sink 到 多个flume collector avro source上，这些flume collector 会有优先级，优先级高的collector负责把数据sink到hdfs或者kafka上。因为agent和collector是多节点运行，在agent端：某个agent挂了，还有其他agent工作；在collecor端，某个collector挂了，还有其他collector继续工作。<br>架构图如下：<br><img src="https://img-blog.csdnimg.cn/20191124103515197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">各个节点规划，考虑到测试虚拟机资源有限，其中两个节点都分布运行agent和collector进程。<br>| 节点 |  flume 角色|<br>|—|—|<br>| nn | agent1，collector 1|<br>| dn1 | agen2 |<br>| dn2 | agent3，collector2 |</p>
<h5 id="4-1-三个agent的flume配置"><a href="#4-1-三个agent的flume配置" class="headerlink" title="4.1  三个agent的flume配置"></a>4.1  三个agent的flume配置</h5><p>三个agent的配置其实都一样，不同的部分：每个agent命名不同。<br>在nn节点的/opt/flume-1.9.0/conf新建一个avro-agent.properties<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出agent1的组件，sinks有两个，分别去到collector1和collector2</span></span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>属性</span></span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel</span></span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector1</span></span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = nn</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector2</span></span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = dn2</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建sink groups，将多个sinks绑定为一个组，agent会向这些组sink 数据，将k1和k2设置负载均衡模式，也可以设置为failover模式，本文使用load_balance</span></span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">agent1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agent1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">agent1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">agent1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> failover模式，只有collector1工作。仅当collector1挂了后，collector2才能启动服务。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> agent1.sinkgroups.g1.processor.type = failover</span></span><br><span class="line"><span class="meta">#</span><span class="bash">值越大，优先级越高，collector1优先级最高</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k1 = 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k2 = 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">发生异常的sink最大故障转移时间（毫秒），这里设为10秒</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.maxpenalty = 10000</span></span><br></pre></td></tr></table></figure><br>将avro-agent.properties拷贝到dn1和dn2节点，agent1这个名字可改，可不改。</p>
<h5 id="4-2-配置-collector"><a href="#4-2-配置-collector" class="headerlink" title="4.2  配置 collector"></a>4.2  配置 collector</h5><p>分别在nn和dn2节点的/opt/flume-1.9.0/conf新建一个avro-collector.properties<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在dn2节点上，则改为collector2，不改也没关系，这里只是为了区分两个collector</span></span><br><span class="line">collector1.sources = r1</span><br><span class="line">collector1.sinks = k1</span><br><span class="line">collector1.channels = c1</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义<span class="built_in">source</span>：这里的<span class="built_in">source</span>配成avro，从而连接agent端sink avro</span></span><br><span class="line">collector1.sources.r1.channels = c1</span><br><span class="line">collector1.sources.r1.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span>的属性：dn2节点需改为dn2</span></span><br><span class="line">collector1.sources.r1.bind = nn</span><br><span class="line">collector1.sources.r1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">定义channel</span></span><br><span class="line">collector1.channels.c1.type = memory</span><br><span class="line">collector1.channels.c1.capacity = 1000</span><br><span class="line">collector1.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">定义sinks：由collector将数据event推到hdfs上</span></span><br><span class="line">collector1.sinks.k1.channel=c1</span><br><span class="line">collector1.sinks.k1.type=hdfs</span><br><span class="line">collector1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">collector1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">collector1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">collector1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">collector1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">collector1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"> </span><br></pre></td></tr></table></figure></p>
<h5 id="4-3-测试flume高可用"><a href="#4-3-测试flume高可用" class="headerlink" title="4.3 测试flume高可用"></a>4.3 测试flume高可用</h5><p>在nn节点和dn2节点启动各自的collector<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">nn节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector1，所以这里启动的--name 为collector1</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">dn2节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector2，所以这里启动的--name 为collector2</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></p>
<p>在nn、dn1和dn2节点启动各自的agent，在shell可以看到以下agent 进程打印的信息，说明三个agent都可以连接到两个collector的source组件k1和k2<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k1 started.</span><br><span class="line"> ......</span><br><span class="line">Monitored counter group for type: SINK, name: k2: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k2 started.</span><br></pre></td></tr></table></figure><br>在nn节点上的access.log新增信息 echo ‘foo’ &gt;&gt;access.log后，在hdfs上可以看到<em>*</em>.txt.tmp文件可以相应的文件内容<br>停止collector1经常，此时测试collector2可以正常接替服务。</p>
<p>至此，已完成本文内容。下一篇文章为Hadoop引入Kafka组件，在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是本文测试的hdfs。</p>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume高可用</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：基于AQS条件队列实现的各种XXBlockingQueue分析</title>
    <url>/2021/07/10/%E5%9F%BA%E4%BA%8EAQS%E6%9D%A1%E4%BB%B6%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%90%84%E7%A7%8DXXBlockingQueue%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>有了《基于AQS实现的Condition底层原理解析》文章关于条件队列底层设计的讨论后，那么关于使用BlockingQueue接口和AQS设计各种阻塞队列实现——ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue的数据结构设计和源代码实现则非常好理解。注意本文的讨论的“阻塞队列”是指代BlockingQueue接口，不是指代AQS源码分析文章中所说的CLH的FIFO阻塞队列。</p>
<h4 id="BlockingQueue"><a href="#BlockingQueue" class="headerlink" title="BlockingQueue"></a>BlockingQueue</h4><p>BlockingQueue是一个接口，它有Queue接口常见的方法：add、offer，而put方法和take方法则是BlockingQueue特有的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">BlockingQueue</span>&lt;<span class="title">E</span>&gt; <span class="keyword">extends</span> <span class="title">Queue</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Inserts the specified element into this queue, waiting if necessary</span></span><br><span class="line"><span class="comment">     * for space to become available.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> e the element to add</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException if interrupted while waiting</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> ClassCastException if the class of the specified element</span></span><br><span class="line"><span class="comment">     *         prevents it from being added to this queue</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> NullPointerException if the specified element is null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IllegalArgumentException if some property of the specified</span></span><br><span class="line"><span class="comment">     *         element prevents it from being added to this queue</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(E e)</span> <span class="keyword">throws</span> InterruptedException</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Retrieves and removes the head of this queue, waiting if necessary</span></span><br><span class="line"><span class="comment">     * until an element becomes available.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the head of this queue</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException if interrupted while waiting</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">E <span class="title">take</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException</span>;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实put和take就是经典的生产者-消费者模型中的逻辑：生产者使用put操作不断向“容器”中放入元素，消费者使用take操作不断从“容器”中取出（消费）元素。只不过对于BlockingQueue接口来说，put和take都要求实现此BlockingQueue接口的子类需实现“阻塞机制”，也即：</p>
<p>1、对于put操作来说：生产者线程（1个或者多线程并发）向容器（该容器可以是基于数组实现或者基于链表实现）put一个元素（item），如果此容器已满，则所有生产者线程的put操作都会被阻塞直到“容器不满notFull”时。</p>
<p>结合AQS的条件队列的阻塞机制设计可推出：对于put的阻塞，其实就是让执行put操作的生产者线程在一个“特定的”条件队列中阻塞，这里的阻塞实现就是使用该条件队列的await方法达成。</p>
<p>2、对于take操作来说：消费者线程（1个或者多线程并发）从容器（该容器可以是基于数组实现或者基于链表实现）中take一个元素（item），如果此容器为空，则所有消费者线程的take操作都会被阻塞直到“容器不空notEmtpy”时。</p>
<p>结合AQS的条件队列的阻塞机制设计可推出：对于take的阻塞，其实就是让执行take操作的消费者线程在一个“特定的”条件队列中阻塞，这里的阻塞实现就是使用该条件队列的await方法达成。</p>
<a id="more"></a>
<h4 id="ArrayBlockingQueue基本使用"><a href="#ArrayBlockingQueue基本使用" class="headerlink" title="ArrayBlockingQueue基本使用"></a>ArrayBlockingQueue基本使用</h4><p>ArrayBlockingQueue的put阻塞demo</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BlockingQueue&lt;String&gt; q=<span class="keyword">new</span> ArrayBlockingQueue(<span class="number">3</span>,<span class="keyword">true</span>); <span class="comment">// 这里的true表示使用ArrayBlockingQueue内部ReentrantLock的公平锁模式，目的是为了使得多个线程有序抢锁资源，以便观察输出情况</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> ii=i;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            q.put(<span class="string">&quot;foo&quot;</span>);</span><br><span class="line">            System.out.println(Thread.currentThread().getName()+<span class="string">&quot;生产了foo:&quot;</span>+ii);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出以下三句后主线程进入阻塞状态，因为队列长度为3，此时有5个生产线程向q并发生产，因此当第3个线程完成后，q队列已经满了，当后面的第4个、第5个线程向q生产元素时，就会被阻塞：</p>
<p>Thread-0生产了foo:0<br>Thread-1生产了foo:1<br>Thread-2生产了foo:2</p>
<p>ArrayBlockingQueue的take操作阻塞demo</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BlockingQueue&lt;String&gt; q=<span class="keyword">new</span> ArrayBlockingQueue(<span class="number">3</span>,<span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> ii=i;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            q.take();</span><br><span class="line">            System.out.println(<span class="string">&quot;foo&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时没有输出：因为q队列没有元素，因此5个消费者线程只能被阻塞。</p>
<h4 id="ArrayBlockingQueue核心设计"><a href="#ArrayBlockingQueue核心设计" class="headerlink" title="ArrayBlockingQueue核心设计"></a>ArrayBlockingQueue核心设计</h4><p>本文并不定打算对ArrayBlockingQueue做全面的源代码的解析，本文仅挑出其核心的阻塞设计作为解析</p>
<h5 id="关键属性"><a href="#关键属性" class="headerlink" title="关键属性"></a>关键属性</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayBlockingQueue</span>&lt;<span class="title">E</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractQueue</span>&lt;<span class="title">E</span>&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">implements</span> <span class="title">BlockingQueue</span>&lt;<span class="title">E</span>&gt;, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Serialization ID. This class relies on default serialization</span></span><br><span class="line"><span class="comment">     * even for the items array, which is default-serialized, even if</span></span><br><span class="line"><span class="comment">     * it is empty. Otherwise it could not be declared final, which is</span></span><br><span class="line"><span class="comment">     * necessary here.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">817911632652898426L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The queued items ArrayBlockingQueue存放元素的容器是基于数组实现的 */</span></span><br><span class="line">    <span class="keyword">final</span> Object[] items;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** items index for next take, poll, peek or remove */</span></span><br><span class="line">    <span class="keyword">int</span> takeIndex;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** items index for next put, offer, or add */</span></span><br><span class="line">    <span class="keyword">int</span> putIndex;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Number of elements in the queue </span></span><br><span class="line"><span class="comment">    ArrayBlockingQueue当前含有元素的个数，每次向队列里面放入1个元素，则count++，每次向队列里面取走1个元素，则count--</span></span><br><span class="line"><span class="comment">    */</span>  </span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Concurrency control uses the classic two-condition algorithm</span></span><br><span class="line"><span class="comment">     * found in any textbook.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">		<span class="comment">/* 可以看到，ArrayBlockingQueue的相关put、take也即读写操作都是基于全局一把ReentrantLock，也即是互斥的，而非无锁实现，性能当然会比无锁队列要低。阻塞机制实现：使用一把锁和两个条件队列实现</span></span><br><span class="line"><span class="comment"> notEmpty:消费者线程调用take发现队列元素为空时，被阻塞所在的条件队列</span></span><br><span class="line"><span class="comment"> notFull:生产者线程调用put发现队列元素满时，被阻塞所在的条件队列</span></span><br><span class="line"><span class="comment">		*/</span></span><br><span class="line">    <span class="comment">/** Main lock guarding all access */</span></span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Condition for waiting takes */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Condition notEmpty;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Condition for waiting puts */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Condition notFull;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ArrayBlockingQueue</span><span class="params">(<span class="keyword">int</span> capacity, <span class="keyword">boolean</span> fair)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (capacity &lt;= <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        <span class="keyword">this</span>.items = <span class="keyword">new</span> Object[capacity]; <span class="comment">// 初始化时可以选择存放元素的个数，以及ReentrantLock的公平模式，默认采用性能相对较高的非公平模式</span></span><br><span class="line">        lock = <span class="keyword">new</span> ReentrantLock(fair);</span><br><span class="line">        notEmpty = lock.newCondition(); </span><br><span class="line">        notFull =  lock.newCondition();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="put方法"><a href="#put方法" class="headerlink" title="put方法"></a>put方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Inserts the specified element at the tail of this queue, waiting</span></span><br><span class="line"><span class="comment">  * for space to become available if the queue is full.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> InterruptedException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> NullPointerException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// 在队列尾部添加元素，且不支持添加null对象</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(E e)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">     checkNotNull(e);</span><br><span class="line">     <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">   	<span class="comment">// 1、如果有多个put线程，它们之间需要竞争互斥锁，因此没lock成功的写线程都会进入lock对象关联的CLH阻塞队列里面等着</span></span><br><span class="line">     lock.lockInterruptibly();</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 2、就算此put线程抢到独占锁资源，但因为条件中“队列的元素个数已经满了”，此时put线程会被notFull.await()放入 notFull对象关联的条件队列，这既是ArrayBlockingQueue的put的阻塞设计实现</span></span><br><span class="line">         <span class="keyword">while</span> (count == items.length)</span><br><span class="line">             notFull.await();</span><br><span class="line">       <span class="comment">// 3、执行流到这里，说明队列还没满，因此可以把元素放入队列，使用enqueue放入元素</span></span><br><span class="line">         enqueue(e);</span><br><span class="line">     &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">         lock.unlock();</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Inserts element at current put position, advances, and signals.</span></span><br><span class="line"><span class="comment">  * Call only when holding lock.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// 注意：将元素入队，是指将元素放在队尾，而取出元素也即出队，是从队列头取出</span></span><br><span class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">enqueue</span><span class="params">(E x)</span> </span>&#123;</span><br><span class="line">     <span class="comment">// assert lock.getHoldCount() == 1;</span></span><br><span class="line">     <span class="comment">// assert items[putIndex] == null;</span></span><br><span class="line">     <span class="keyword">final</span> Object[] items = <span class="keyword">this</span>.items;</span><br><span class="line">     <span class="comment">// 1、将元素放入putIndex下标指向的桶位，putIndex默认值为0，如果下一次要放入的下标已经达到数组长度，那么将putIndex重置为0，以便实现循环使用底层数组放置元素</span></span><br><span class="line">     items[putIndex] = x;</span><br><span class="line">     <span class="keyword">if</span> (++putIndex == items.length)</span><br><span class="line">         putIndex = <span class="number">0</span>;</span><br><span class="line">   	<span class="comment">// 2、入队一个元素，计数器加1</span></span><br><span class="line">     count++;</span><br><span class="line">     <span class="comment">// 3、既然队列已经有刚添加的元素，那么可以通知在notEmpty条件队列等待的“消费者线程们”</span></span><br><span class="line">     notEmpty.signal();</span><br><span class="line"> &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="take方法"><a href="#take方法" class="headerlink" title="take方法"></a>take方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">take</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">  	<span class="comment">// 1、显然take操作也是互斥锁操作，同理如果有多个消费者线程，未能抢到lock的消费者线程则进入lock关联的CLH阻塞队列，结合上面put方法，因此CLH阻塞队列的线程节点可能同时存在消费者线程节点和生产者线程节点</span></span><br><span class="line">    lock.lockInterruptibly();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      	<span class="comment">// 2、只要当前队列没有元素，就将当前消费者线程阻塞在notEmpty的条件队列中</span></span><br><span class="line">        <span class="keyword">while</span> (count == <span class="number">0</span>)</span><br><span class="line">            notEmpty.await();</span><br><span class="line">      	<span class="comment">// 3、说明此时队列里面还有元素可供取出，使用dequeue</span></span><br><span class="line">        <span class="keyword">return</span> dequeue();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Extracts element at current take position, advances, and signals.</span></span><br><span class="line"><span class="comment"> * Call only when holding lock.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> E <span class="title">dequeue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// assert lock.getHoldCount() == 1;</span></span><br><span class="line">    <span class="comment">// assert items[takeIndex] != null;</span></span><br><span class="line">    <span class="keyword">final</span> Object[] items = <span class="keyword">this</span>.items;</span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    E x = (E) items[takeIndex];</span><br><span class="line">  	<span class="comment">// 1、取出当前位置的元素后马上置为null</span></span><br><span class="line">    items[takeIndex] = <span class="keyword">null</span>;</span><br><span class="line">  	<span class="comment">// 2、如果下一个要取出的元素下标已经达到数组长度，则重置为0，以便重复利用items这个数组</span></span><br><span class="line">    <span class="keyword">if</span> (++takeIndex == items.length)</span><br><span class="line">        takeIndex = <span class="number">0</span>;</span><br><span class="line">  	<span class="comment">// 3、既然取出了一个元素，且items[takeIndex] 已经置为null，当然对元素个数的计数器减1</span></span><br><span class="line">    count--;</span><br><span class="line">  	<span class="comment">/*</span></span><br><span class="line"><span class="comment">    4、若使用了ArrayBlockingQueue的迭代器，在每次从队列取出一个元素时还需删除那些关联此队列的、“过时”的迭代器上对应的元素，以实现每个迭代器和底层容器数据数据的一致性：</span></span><br><span class="line"><span class="comment">    例如下面，同一个队列，开启了两个迭代器</span></span><br><span class="line"><span class="comment">    BlockingQueue&lt;String&gt; q=new ArrayBlockingQueue(3,true);</span></span><br><span class="line"><span class="comment">    Iterator iterator1=q.iterator();</span></span><br><span class="line"><span class="comment">    Iterator iterator2=q.iterator();</span></span><br><span class="line"><span class="comment">    由于ArrayBlockingQueue的迭代器设计比普通的ArrayList迭代器设计要复杂，本文在此不再展开。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> (itrs != <span class="keyword">null</span>)</span><br><span class="line">        itrs.elementDequeued();</span><br><span class="line">    <span class="comment">// 4、既然消费者线程已经从队列取出一个元素，说明队列肯定不满，因此可以通知阻塞在notFull条件队列的生产者线程。</span></span><br><span class="line">    notFull.signal();</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实关于put、take的设计思想，在<code>java.util.concurrent.locks.Condition</code>的源码设计中就有给出相应的demo说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BoundedBuffer</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// 设计一个有界的阻塞队列：一个独占锁关联两个条件队列</span></span><br><span class="line">    <span class="keyword">final</span> Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">    <span class="keyword">final</span> Condition notFull  = lock.newCondition();</span><br><span class="line">    <span class="keyword">final</span> Condition notEmpty = lock.newCondition();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Object[] items = <span class="keyword">new</span> Object[<span class="number">100</span>]; <span class="comment">// 内部指定此队列只能放置100个元素</span></span><br><span class="line">    <span class="keyword">int</span> putptr, takeptr, count; <span class="comment">// 这里的putptr 也即上面的putIndex，takeptr对应takeIndex</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(Object x)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (count == items.length)</span><br><span class="line">                notFull.await();</span><br><span class="line">            items[putptr] = x;</span><br><span class="line">            <span class="keyword">if</span> (++putptr == items.length) putptr = <span class="number">0</span>;</span><br><span class="line">            ++count;</span><br><span class="line">            notEmpty.signal();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">take</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (count == <span class="number">0</span>)</span><br><span class="line">                notEmpty.await();</span><br><span class="line">            Object x = items[takeptr];</span><br><span class="line">            <span class="keyword">if</span> (++takeptr == items.length) takeptr = <span class="number">0</span>;</span><br><span class="line">            --count;</span><br><span class="line">            notFull.signal();</span><br><span class="line">            <span class="keyword">return</span> x;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">BoundedBuffer buffer=<span class="keyword">new</span> BoundedBuffer();</span><br><span class="line"><span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        buffer.take();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        buffer.put(<span class="string">&quot;foo&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br></pre></td></tr></table></figure>
<h4 id="LinkedBlockingQueue基本使用"><a href="#LinkedBlockingQueue基本使用" class="headerlink" title="LinkedBlockingQueue基本使用"></a>LinkedBlockingQueue基本使用</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BlockingQueue&lt;String&gt; q=<span class="keyword">new</span> LinkedBlockingQueue&lt;String&gt;(<span class="number">3</span>);</span><br><span class="line">Iterator iterator1=q.iterator();</span><br><span class="line">Iterator iterator2=q.iterator();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> ii=i;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            q.put(<span class="string">&quot;foo&quot;</span>); <span class="comment">// 观察put的阻塞</span></span><br><span class="line">            System.out.println(Thread.currentThread().getName()+<span class="string">&quot;生产了foo:&quot;</span>+ii);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>LinkedBlockingQueue基本使用其实跟ArrayBlockingQueue一样的，只不过内部存放元素的容器是基于链表实现，当然也可以在初始化就指定可存放元素的数量，这样原本的链表阻塞队列即可变为有界阻塞队列。</p>
<h4 id="LinkedBlockingQueue核心设计"><a href="#LinkedBlockingQueue核心设计" class="headerlink" title="LinkedBlockingQueue核心设计"></a>LinkedBlockingQueue核心设计</h4><p>以下将存放元素的链表统称为容器链表，也即存放元素的队列</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Linked list node class</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">       E item;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * One of:</span></span><br><span class="line"><span class="comment">        * - the real successor Node</span></span><br><span class="line"><span class="comment">        * - this Node, meaning the successor is head.next</span></span><br><span class="line"><span class="comment">        * - null, meaning there is no successor (this is the last node)</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">     	<span class="comment">// 注意由于容器链表的head节点是一个辅助节点，真正的数据头节点是first=head.next</span></span><br><span class="line">       Node&lt;E&gt; next;</span><br><span class="line"></span><br><span class="line">       Node(E x) &#123; item = x; &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">   <span class="comment">/** The capacity bound, or Integer.MAX_VALUE if none */</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> capacity;  <span class="comment">// 可以指定默认存放元素个数，最多只能到Integer.MAX_VALUE个</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Current number of elements */</span></span><br><span class="line">  <span class="comment">// 当前容器列表的元素个数，这里为何使用线程安全的原子计数器，而不是像ArrayBlockingQueue那样使用普通的count？ 这里需要理解LinkedBlockingQueue双互斥锁设计才能明白其设计意图！</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger count = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Head of linked list.</span></span><br><span class="line"><span class="comment">    * Invariant: head.item == null</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">	<span class="comment">// 容器链表的头结点是head = new Node&lt;E&gt;(null)，因此只需要判断head.item==null，说明此时位于head就是位于头节点位置</span></span><br><span class="line">   <span class="keyword">transient</span> Node&lt;E&gt; head;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Tail of linked list.</span></span><br><span class="line"><span class="comment">    * Invariant: last.next == null</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">	<span class="comment">// 如果last.next=null（注意区别head.next=item），说明last执行的是容器链表的最后一个数据节点</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">transient</span> Node&lt;E&gt; last;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Lock held by take, poll, etc */</span></span><br><span class="line">	<span class="comment">// 这里就是体现了双锁设计：一把take锁，用于消费者线程，意味着消费者线程的take操作并不会阻塞生产者线程的put操作</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> ReentrantLock takeLock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Wait queue for waiting takes */</span></span><br><span class="line">	<span class="comment">// take锁关联的条件队列，用于存放被阻塞的消费者线程</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> Condition notEmpty = takeLock.newCondition();</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Lock held by put, offer, etc */</span></span><br><span class="line"><span class="comment">// 这里就是体现了双锁设计：一把put锁，用于生产者线程，意味着产者线程的put操作并不会阻塞消费者线程的take操作</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> ReentrantLock putLock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line">   <span class="comment">/** Wait queue for waiting puts */</span></span><br><span class="line"><span class="comment">// put锁关联的条件队列，用于存放被阻塞的生产者线程</span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> Condition notFull = putLock.newCondition();</span><br></pre></td></tr></table></figure>
<p>构造器</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 默认构造器的容器链表长度使用了最大正整数值</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">LinkedBlockingQueue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>(Integer.MAX_VALUE);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">LinkedBlockingQueue</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (capacity &lt;= <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">      <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">    	<span class="comment">/* 这里可以看到LinkedBlockingQueue存放元素的链表跟普通的链表的头节点不同，</span></span><br><span class="line"><span class="comment">    	 普通链表的头节点在初始化是head=null，意味着当head指向节点时，此节点就是真正的数据头节点,而LinkedBlockingQueue的头节点是一个辅助节点，在初始化时head指向new Node&lt;E&gt;(null)，显然head不是真正的数据头节点</span></span><br><span class="line"><span class="comment">    	 */</span></span><br><span class="line">      last = head = <span class="keyword">new</span> Node&lt;E&gt;(<span class="keyword">null</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>含有数据节点的链表结构示意如下：</p>
<p>head(null)-&gt;node0(item0)-&gt;node1(item1)-&gt;node2(item2)-&gt;node3(item3)-&gt;null</p>
<h5 id="put方法-1"><a href="#put方法-1" class="headerlink" title="put方法"></a>put方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  源码注释清楚说明：采用了尾插法，也即生产者线程将元素放入容器链表尾部</span></span><br><span class="line"><span class="comment">   * Inserts the specified element at the tail of this queue, waiting if</span></span><br><span class="line"><span class="comment">   * necessary for space to become available.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> InterruptedException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> NullPointerException &#123;<span class="doctag">@inheritDoc</span>&#125;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(E e)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    	<span class="comment">// 这里也说明LinkedBlockingQueue不支持放入null对象</span></span><br><span class="line">      <span class="keyword">if</span> (e == <span class="keyword">null</span>) <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">      <span class="comment">// Note: convention in all put/take/etc is to preset local var</span></span><br><span class="line">      <span class="comment">// holding count negative to indicate failure unless set.</span></span><br><span class="line">      <span class="keyword">int</span> c = -<span class="number">1</span>;</span><br><span class="line">      Node&lt;E&gt; node = <span class="keyword">new</span> Node&lt;E&gt;(e);</span><br><span class="line">    </span><br><span class="line">    	<span class="comment">// 当然如果有多个生产者线程使用put，那么那些没抢到独占锁的生产者线程将阻塞在putLock关联的CLH阻塞队列里面。</span></span><br><span class="line">      <span class="keyword">final</span> ReentrantLock putLock = <span class="keyword">this</span>.putLock;</span><br><span class="line">      <span class="comment">// 1、因为存在两把锁：putLock和takeLock，意味着生产者线程和消费者可以同一时刻操作计数器count，因此采用AtomicInteger才能计数操作的线程安全</span></span><br><span class="line">      <span class="keyword">final</span> AtomicInteger count = <span class="keyword">this</span>.count;</span><br><span class="line">      putLock.lockInterruptibly();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="comment">// 2、通过当前容器链表的元素数量达到指定长度，说明已经满了，则调用put生产者线程将被阻塞在notFull关联的条件队列</span></span><br><span class="line">          <span class="keyword">while</span> (count.get() == capacity) &#123;</span><br><span class="line">              notFull.await();</span><br><span class="line">          &#125;</span><br><span class="line">        	<span class="comment">// 3、执行流来到这里，说明此时容器链表可以放入元素，则在链表尾部追加即可</span></span><br><span class="line">          enqueue(node);</span><br><span class="line">        	<span class="comment">// 4、添加元素后，对计时器加1并返回上一次计数值</span></span><br><span class="line">          c = count.getAndIncrement();</span><br><span class="line">        	<span class="comment">// 5、如果计时器+1还未超过指定长度，则唤醒可能在notFull条件队列阻塞的的生产者线程：当前容器链表还没满，你们可以醒来向此容器put入元素。</span></span><br><span class="line">          <span class="keyword">if</span> (c + <span class="number">1</span> &lt; capacity)</span><br><span class="line">              notFull.signal();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          putLock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">    	<span class="comment">/* 6、首先c = count.getAndIncrement()，说明c是在容器链表添加元素之前的计数，如果c等于0，说明容器链表是空的，也说明可能在c=0时可能已经有take消费者线程阻塞在notEmpty的条件队列中，</span></span><br><span class="line"><span class="comment">    	而经过上面的3步骤后，既然已经入队了1个元素，那么就需要唤醒在c=0那段时刻被阻塞的消费者线程。</span></span><br><span class="line"><span class="comment">    	*/</span></span><br><span class="line">      <span class="keyword">if</span> (c == <span class="number">0</span>)</span><br><span class="line">          signalNotEmpty();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Links node at end of queue.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">enqueue</span><span class="params">(Node&lt;E&gt; node)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// assert putLock.isHeldByCurrentThread();</span></span><br><span class="line">      <span class="comment">// assert last.next == null;</span></span><br><span class="line">    	<span class="comment">// 可以看到如果是链表结构，入队操作比基于数组实现的效率快很多</span></span><br><span class="line">    	<span class="comment">// last.next=node 将当前链表尾指针的next指向新节点node，从而使得node入队</span></span><br><span class="line">      <span class="comment">// last=node 更新尾指针的指向最新的尾节点。</span></span><br><span class="line">      last = last.next = node;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Signals a waiting take. Called only from put/offer (which do not</span></span><br><span class="line"><span class="comment">   * otherwise ordinarily lock takeLock.)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 此方法也很简单，唤醒阻塞在takeLock对象下的条件队列的消费者线程节点</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">signalNotEmpty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> ReentrantLock takeLock = <span class="keyword">this</span>.takeLock;</span><br><span class="line">      takeLock.lock();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">          notEmpty.signal();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          takeLock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="take方法-1"><a href="#take方法-1" class="headerlink" title="take方法"></a>take方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 从容器链表头部的第一个数据节点取出元素（注意：这里说的是第一个数据节点，而不是head，因为head是一个辅助节点） </span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> E <span class="title">take</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        E x;</span><br><span class="line">        <span class="keyword">int</span> c = -<span class="number">1</span>;</span><br><span class="line"> <span class="comment">// 1、因为存在两把锁：putLock和takeLock，意味着生产者线程和消费者可以同一时刻操作计数器count，因此采用AtomicInteger才能计数操作的线程安全</span></span><br><span class="line">        <span class="keyword">final</span> AtomicInteger count = <span class="keyword">this</span>.count;</span><br><span class="line"><span class="comment">// 2、消费线程线程使用take锁，说明容器链表的put操作和take操作可以同时进行，而ArrayBlockingQueue是互斥的。当然如果有多个消费者线程使用take，那么那些没抢到独占锁的消费者线程将阻塞在takeLock关联的CLH阻塞队列里面。</span></span><br><span class="line">        <span class="keyword">final</span> ReentrantLock takeLock = <span class="keyword">this</span>.takeLock;</span><br><span class="line">        takeLock.lockInterruptibly();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//3、 如果当前容器链表元素为0，也即无元素可取，那么消费者线程只能去notEmpty的条件队列阻塞等待</span></span><br><span class="line">            <span class="keyword">while</span> (count.get() == <span class="number">0</span>) &#123;</span><br><span class="line">                notEmpty.await();</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// 4、来到这里说明容器是有元素的，从“队头”取出一个</span></span><br><span class="line">            x = dequeue();</span><br><span class="line">          	<span class="comment">// 5、从容器链表取出1个元素后，对计时器减1并返回上一次计数值</span></span><br><span class="line">            c = count.getAndDecrement();</span><br><span class="line">          	<span class="comment">/* 6、如果上一次计数值大于0，说明此刻容器队列还有元素可以去继续取，那么唤醒那些可能在notEmpty条件队列等待的消费线程：你们可以过来take元素。</span></span><br><span class="line"><span class="comment">          	注意：这里说的唤醒要求你确实掌握AQS对于signal的内部实现：signal只是将notEmpty条件队列中阻塞的第一个线程节点转移到takeLock关联的CLH阻塞队列，也即那个节点被转移到CLH阻塞队列还处于阻塞状态，直到当前线程执行完finally的unlock，此操作才会在AQS使用unparkSuccessor唤醒那个在takeLock关联的CLH阻塞队列的线程节点。</span></span><br><span class="line"><span class="comment">          	*/</span></span><br><span class="line">            <span class="keyword">if</span> (c &gt; <span class="number">1</span>)</span><br><span class="line">                notEmpty.signal();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          	<span class="comment">// 7、</span></span><br><span class="line">            takeLock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">/* 6、首先c = count.getAndIncrement()，说明c是在消费者取出元素之前的计数，如果c已经是指定容器长度，说明容器链表此时已满，也说明在c=capacity那一刻可能已经有put生产者线程阻塞在notFull的条件队列中，</span></span><br><span class="line"><span class="comment">      	而经过上面的4的出队操作后，既然已经容器已经扣减了1个元素，那么就需要唤醒c=capacity容器满的那段时刻被阻塞的生产者线程。</span></span><br><span class="line"><span class="comment">      	*/</span></span><br><span class="line">        <span class="keyword">if</span> (c == capacity)</span><br><span class="line">            signalNotFull();</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Removes a node from head of queue.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> the node</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> E <span class="title">dequeue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// assert takeLock.isHeldByCurrentThread();</span></span><br><span class="line">        <span class="comment">// assert head.item == null;</span></span><br><span class="line">        Node&lt;E&gt; h = head;</span><br><span class="line">        <span class="comment">// 取出第一个数据节点：h.next,注意：不是head，因为head=new Node(null)是一个辅助节点</span></span><br><span class="line">        Node&lt;E&gt; first = h.next;</span><br><span class="line">        h.next = h; <span class="comment">// help GC</span></span><br><span class="line">      	<span class="comment">// 将head指针指向第一个数据节点并取出该节点存放的数据，然后将节点的item置为null，那么此时head指向的first就是一个新的辅助节点：first(item=null,next=下一个数据节点)</span></span><br><span class="line">        head = first;</span><br><span class="line">        E x = first.item;</span><br><span class="line">        first.item = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：基于AQS驱动的CountDownLatch实现原理解析</title>
    <url>/2021/07/27/%E5%9F%BA%E4%BA%8EAQS%E9%A9%B1%E5%8A%A8%E7%9A%84CountDownLatch%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>countdown：倒计时，latch：门闩；插销</p>
<p>其实CountDownLatch是一种共享锁资源的同步器，用于协调多个线程并行执行（注意不是并发），也即控制多个线程完成后接着在同一时刻去完成某事。当然了，这种协调的底层实现是基于AQS的共享模式实现。</p>
<p>CountDownLatch一般用法如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">countdownLatch.countDown()  <span class="comment">// 使得内部的“计数器值”减1,一般由子线程去调用</span></span><br><span class="line">countdownLatch.await() <span class="comment">// 当内部的“计算器值”为0时，说明所有子线程都已经完成任务，那么阻塞的主线程就会被唤醒再去执行其他任务</span></span><br></pre></td></tr></table></figure>
<h4 id="第一种用法：主线程等所有子线程完成后再统一做某事"><a href="#第一种用法：主线程等所有子线程完成后再统一做某事" class="headerlink" title="第一种用法：主线程等所有子线程完成后再统一做某事"></a>第一种用法：主线程等所有子线程完成后再统一做某事</h4><a id="more"></a>
<p>这里用开会作为例子，假设有A、B、C三个人参与开会，主持人是AA，开会的规则：要求三个人到场后，主持人才可以开始开会。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountDownLatchDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        CountDownLatch countDownLatch =<span class="keyword">new</span> CountDownLatch(<span class="number">3</span>);</span><br><span class="line">        <span class="keyword">new</span> A(countDownLatch).start();</span><br><span class="line">        <span class="keyword">new</span> B(countDownLatch).start();</span><br><span class="line">        <span class="keyword">new</span> C(countDownLatch).start();</span><br><span class="line">        countDownLatch.await(); <span class="comment">// 主持人在等待三位到达会场</span></span><br><span class="line">        System.out.println(<span class="string">&quot;三位都到了会场，现在可以开会了&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch countDownLatch;</span><br><span class="line">    A (CountDownLatch countDownLatch)&#123;</span><br><span class="line">        <span class="keyword">this</span>.countDownLatch=countDownLatch;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;A已到达会场&quot;</span>);</span><br><span class="line">            countDownLatch.countDown();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch countDownLatch;</span><br><span class="line">    B (CountDownLatch countDownLatch)&#123;</span><br><span class="line">        <span class="keyword">this</span>.countDownLatch=countDownLatch;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;B已到达会场&quot;</span>);</span><br><span class="line">            countDownLatch.countDown();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch countDownLatch;</span><br><span class="line">    C (CountDownLatch countDownLatch)&#123;</span><br><span class="line">        <span class="keyword">this</span>.countDownLatch=countDownLatch;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;C已到达会场&quot;</span>);</span><br><span class="line">            countDownLatch.countDown();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>A已到达会场<br>B已到达会场<br>C已到达会场<br>三位都到了会场，现在可以开会了</p>
<h4 id="countDown的底层AQS解析"><a href="#countDown的底层AQS解析" class="headerlink" title="countDown的底层AQS解析"></a>countDown的底层AQS解析</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1、 用户代码（子线程调用）</span></span><br><span class="line">countDownLatch.countDown();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、 CountDownLatch同步器的countDown方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">countDown</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  sync.releaseShared(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//3、 AQS的releaseShared方法，</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">releaseShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tryReleaseShared(arg)) &#123; <span class="comment">// 减1成功后，如果state值不为0则不会进入唤醒传播操作。实际场景中一般由最后一个完成任务的子线程到这边一步就会tryReleaseShared==0，进入唤醒逻辑</span></span><br><span class="line">            doReleaseShared();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//4、 CountDownLatch同步器的tryReleaseShared方法,也即AQS定义的模板方法的子类实现，注意：该“释放方法”是将state值（计数器）减1，而在信号量Semaphore同步器的tryReleaseShared是对可用资源加1操作，注意区别。</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">tryReleaseShared</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Decrement count; signal when transition to zero</span></span><br><span class="line">  <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="keyword">int</span> c = getState();</span><br><span class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>; </span><br><span class="line">    <span class="keyword">int</span> nextc = c-<span class="number">1</span>; <span class="comment">// 这里就体现用户代码调用countDownLatch.countDown()使得内部计数器state减1的核心设计</span></span><br><span class="line">    <span class="keyword">if</span> (compareAndSetState(c, nextc))</span><br><span class="line">      <span class="keyword">return</span> nextc == <span class="number">0</span>; <span class="comment">// 如果计时器的值为0，那么调用此逻辑线程的就会接着执行doReleaseShared()唤醒传播操作：实际场景中一般由最后一个执行countDownLatch.countDown()的子线程来唤醒阻塞队列里面的线程节点</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="await-的底层AQS解析"><a href="#await-的底层AQS解析" class="headerlink" title="await()的底层AQS解析"></a>await()的底层AQS解析</h4><p>注意，调用countDownLatch.await()的线程可能会马上进入阻塞状态，已经掌握AQS的底层设计原理的同学应该可以看出所谓的“阻塞”其实就是进入了CLH阻塞队列后将自己park起来，并等待释放锁的线程来唤醒。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1、 主线程调用</span></span><br><span class="line">countDownLatch.await(); <span class="comment">// 主持人在等待三位到达会场</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*2、CountDownLatch的await方法</span></span><br><span class="line"><span class="comment">Causes the current thread to wait until the latch has counted down to zero, unless the thread is interrupted.</span></span><br><span class="line"><span class="comment">这里的官方注释：此方法会使得当前线程处于等待状态直到内部计数器的值为0或者当前线程被外界中断</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">await</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        sync.acquireSharedInterruptibly(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、AQS的获取资源</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquireSharedInterruptibly</span><span class="params">(<span class="keyword">int</span> arg)</span></span></span><br><span class="line"><span class="function">  <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 如果调用await前就已经被其他线程中断，那么会直接抛出异常，不再进入阻塞队列排队。</span></span><br><span class="line">  <span class="keyword">if</span> (Thread.interrupted())</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">  <span class="keyword">if</span> (tryAcquireShared(arg) &lt; <span class="number">0</span>) <span class="comment">// 注意：这里不是Semaphore对state可用资源进行减1操作，而是判断只要内部计时器不为0，tryAcquireShared就会返回-1，会导致调用await()的线程进入CLH阻塞队列：这里经常会这样使用，其他子线程做完任务后才能调用countDown(),而主线程作为和子线程接近同一时刻启动，因此对于主线程来说它会先读取到的state肯定不为0，使得tryAcquireShared(arg) &lt; 0成立，主线程就会优先执行阻塞队列的流程</span></span><br><span class="line">    doAcquireSharedInterruptibly(arg);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、CountDownLatch内部的tryAcquireShared，注意第3点所提，主线程先运行后await()这里返回getState()返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">int</span> <span class="title">tryAcquireShared</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> (getState() == <span class="number">0</span>) ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 5、AQS内部获取资源失败则进入阻塞队列进行阻塞</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doAcquireSharedInterruptibly</span><span class="params">(<span class="keyword">int</span> arg)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Node node = addWaiter(Node.SHARED); <span class="comment">// 调用await()的主线程作为节点进入阻塞队列的队尾</span></span><br><span class="line">        <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">               	<span class="comment">// 由于此刻阻塞队列里面仅有主线程在排队，因此它可以再去尝试获取锁资源</span></span><br><span class="line">                <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">                <span class="keyword">if</span> (p == head) &#123;</span><br><span class="line">                  	<span class="comment">// 根据上面的第3、第4说明，这里tryAcquireShared返回-1，因此主线程节点进入下面if阻塞自己</span></span><br><span class="line">                    <span class="keyword">int</span> r = tryAcquireShared(arg);</span><br><span class="line">                    <span class="keyword">if</span> (r &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                        setHeadAndPropagate(node, r);</span><br><span class="line">                        p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">                        failed = <span class="keyword">false</span>;</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">// 主线程在这里会阻塞自己（当然是了避免cpu空转），此时阻塞队列里面仅有一个正在等待外面线程唤醒的线程节点。</span></span><br><span class="line">              	<span class="comment">// 当然被唤醒后，继续在parkAndCheckInterrupt()内执行，进行自我中断检查：如果线程节点在阻塞队列的等待过程被外界中断过，则醒来后，直接抛出中断异常，然后就会进入finally的取消节点逻辑。</span></span><br><span class="line">                <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">                    parkAndCheckInterrupt())</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (failed)</span><br><span class="line">                cancelAcquire(node);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>总结以上“等三个人到齐后再开会”的例子：</p>
<p>虽然A、B、C以及主线程都是同时start，但因为A、B、C线程分别要执行一定时间后才能进行countDown减1，因此在主线程执行await()方法时马上有getState()==3，也即<code>tryAcquireShared</code>返回-1，故主线程进入了doAcquireSharedInterruptibly逻辑。</p>
<h4 id="谁去唤醒已经在阻塞队列里面“主线程节点”"><a href="#谁去唤醒已经在阻塞队列里面“主线程节点”" class="headerlink" title="谁去唤醒已经在阻塞队列里面“主线程节点”"></a>谁去唤醒已经在阻塞队列里面“主线程节点”</h4><p>上一小节已经分析了调用await()的主线程先被AQS“调度”进入了阻塞队列里面，那么谁去负责把这个位于阻塞队列里面的主线程唤醒呢？这里不妨给一个拟人化的演绎推理：</p>
<blockquote>
<p>假设A先到，他说我到了主持人可以开始开会，但显然后面还有B、C没到，因此A需要等齐人再去“唤醒主持人”，这样才显得“开会流程”是合理的，接着B到了，他说我到了主持人可以开始开会，但显然后面还有C没到，因此B也不适合去“唤醒主持人”，最后C到了，C看到前面的A、B都到了，这时符合三人都到齐了的条件，因此由最后一个到达会场的C来执行doReleaseShared唤醒逻辑才是合理的，也即让在“等候间睡眠的主持人醒来”开始主持会议。</p>
</blockquote>
<p>对应到CountDownLatch—AQS算法层面的流程如下：</p>
<p>A、B、C以及主线程同一时刻start</p>
<p>1、主线程先进入AQS的阻塞队列并park上（阻塞自己）。</p>
<p>2、接着线程A执行1秒结束后使用countDown，计数器从初始给定的3减为2，因为getState ！=0 ，此时不会执行<code>doReleaseShared</code> 唤醒操作</p>
<p>2、线程B执行2秒结束后使用countDown，计数器从上面的2减为1，因为getState ！=0 ，因此此时不会执行<code>doReleaseShared</code> 唤醒操作</p>
<p>3、线程C执行3秒结束后使用countDown，计数器从上面的1减为0，因为getState ==0成立 ，此时会进入<code>doReleaseShared</code>唤醒操作操作流程，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// C线程使用countDown对应内部以下流程：</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">tryReleaseShared</span><span class="params">(<span class="number">1</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Decrement count; signal when transition to zero</span></span><br><span class="line">  <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="keyword">int</span> c = getState(); <span class="comment">// C读取的计数器值为1</span></span><br><span class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>; </span><br><span class="line">    <span class="keyword">int</span> nextc = c-<span class="number">1</span>; <span class="comment">// nextc=1-1=0</span></span><br><span class="line">    <span class="keyword">if</span> (compareAndSetState(c, nextc)) <span class="comment">// 此时CAS后，state变为0</span></span><br><span class="line">      <span class="keyword">return</span> nextc == <span class="number">0</span>; <span class="comment">// nextc=0显然返回true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;  </span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">releaseShared</span><span class="params">(arg)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 显然上面的tryReleaseShared返回true，C线程可以去执行doReleaseShared唤醒传播操作，对应的逻辑就是让最后一个完成任务的C去唤醒在阻塞队列等待锁资源的主线程。</span></span><br><span class="line">        <span class="keyword">if</span> (tryReleaseShared(arg)) &#123; </span><br><span class="line">            doReleaseShared();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doReleaseShared</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            Node h = head;</span><br><span class="line">            <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h != tail) &#123;</span><br><span class="line">                <span class="keyword">int</span> ws = h.waitStatus;</span><br><span class="line">              <span class="comment">/*</span></span><br><span class="line"><span class="comment">              显然对于阻塞队列结构： head(waitStatus=-1) &lt;-&gt; 主线程(waitStatus=0)，满足</span></span><br><span class="line"><span class="comment">              h.waitStatus == Node.SIGNAL，因此进入unparkSuccessor唤醒主线程。</span></span><br><span class="line"><span class="comment">              */</span> </span><br><span class="line">                <span class="keyword">if</span> (ws == Node.SIGNAL) &#123; </span><br><span class="line">                    <span class="keyword">if</span> (!compareAndSetWaitStatus(h, Node.SIGNAL, <span class="number">0</span>))</span><br><span class="line">                        <span class="keyword">continue</span>;            <span class="comment">// loop to recheck cases</span></span><br><span class="line">                    unparkSuccessor(h);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (ws == <span class="number">0</span> &amp;&amp;</span><br><span class="line">                         !compareAndSetWaitStatus(h, <span class="number">0</span>, Node.PROPAGATE))</span><br><span class="line">                    <span class="keyword">continue</span>;                <span class="comment">// loop on failed CAS</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (h == head)                   <span class="comment">// loop if head changed</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上的唤醒流程其实可以得出这样的结论：虽然有多个线程使用的countDown，但最终是由最后一个完成自身任务的线程C（因为它可以使得state减为0）去唤醒“调用await被放入阻塞队列”的主线线程。</p>
<p>有了以上解析，那么针对CountDownLatch的第二种场景使用则可以很好理解其背后的工作机制。</p>
<h4 id="第二种用法：要求多个线程在同一时刻“开跑”"><a href="#第二种用法：要求多个线程在同一时刻“开跑”" class="headerlink" title="第二种用法：要求多个线程在同一时刻“开跑”"></a>第二种用法：要求多个线程在同一时刻“开跑”</h4><p>这里我们用了Doug Lea在源码给出的demo代码自行设计一个场景 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Doug Lea在源码给出的demo代码</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Driver</span> </span>&#123; <span class="comment">// ...</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    CountDownLatch startSignal = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    CountDownLatch doneSignal = <span class="keyword">new</span> CountDownLatch(N);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) <span class="comment">// create and start threads</span></span><br><span class="line">      <span class="keyword">new</span> Thread(<span class="keyword">new</span> Worker(startSignal, doneSignal)).start();</span><br><span class="line"></span><br><span class="line">    doSomethingElse();            <span class="comment">// don&#x27;t let run yet</span></span><br><span class="line">    startSignal.countDown();      <span class="comment">// let all threads proceed</span></span><br><span class="line">    doSomethingElse();</span><br><span class="line">    doneSignal.await();           <span class="comment">// wait for all to finish</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch startSignal;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch doneSignal;</span><br><span class="line">  Worker(CountDownLatch startSignal, CountDownLatch doneSignal) &#123;</span><br><span class="line">    <span class="keyword">this</span>.startSignal = startSignal;</span><br><span class="line">    <span class="keyword">this</span>.doneSignal = doneSignal;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      startSignal.await();</span><br><span class="line">      doWork();</span><br><span class="line">      doneSignal.countDown();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException ex) &#123;&#125; <span class="comment">// return;</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">doWork</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的Demo:</p>
<p>场景：三个跑者线程在同一起跑线上</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountDownLatchDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        CountDownLatch waitSignal=<span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">        CountDownLatch launchSignal=<span class="keyword">new</span> CountDownLatch(<span class="number">3</span>);</span><br><span class="line">        <span class="keyword">new</span> RunnerA(waitSignal,launchSignal).start();</span><br><span class="line">        <span class="keyword">new</span> RunnerB(waitSignal,launchSignal).start();</span><br><span class="line">        <span class="keyword">new</span> RunnerC(waitSignal,launchSignal).start();</span><br><span class="line">    </span><br><span class="line">        waitSignal.countDown(); </span><br><span class="line">        launchSignal.await();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RunnerA</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch waitSignal;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch launchSignal;</span><br><span class="line">    RunnerA (CountDownLatch startSignal,CountDownLatch doneSignal )&#123;</span><br><span class="line">        <span class="keyword">this</span>.waitSignal =startSignal;</span><br><span class="line">        <span class="keyword">this</span>.launchSignal =doneSignal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;A在起跑线等待裁判吹哨&quot;</span>);</span><br><span class="line">            waitSignal.await();</span><br><span class="line">            doRun();</span><br><span class="line">            launchSignal.countDown();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (InterruptedException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doRun</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;A 到达了终点&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RunnerB</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch startSignal;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch doneSignal;</span><br><span class="line">    RunnerB (CountDownLatch startSignal,CountDownLatch doneSignal )&#123;</span><br><span class="line">        <span class="keyword">this</span>.startSignal=startSignal;</span><br><span class="line">        <span class="keyword">this</span>.doneSignal=doneSignal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;B在起跑线等待裁判吹哨&quot;</span>);</span><br><span class="line">            startSignal.await();</span><br><span class="line">            doRun();</span><br><span class="line">            doneSignal.countDown();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (InterruptedException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doRun</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;B 到达了终点&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RunnerC</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch startSignal;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CountDownLatch doneSignal;</span><br><span class="line">    RunnerC (CountDownLatch startSignal,CountDownLatch doneSignal )&#123;</span><br><span class="line">        <span class="keyword">this</span>.startSignal=startSignal;</span><br><span class="line">        <span class="keyword">this</span>.doneSignal=doneSignal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;C在起跑线等待裁判吹哨&quot;</span>);</span><br><span class="line">            startSignal.await();</span><br><span class="line">            doRun();</span><br><span class="line">            doneSignal.countDown();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (InterruptedException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doRun</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;C 到达了终点&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">A在起跑线等待裁判吹哨<span class="comment">// 同一时间启动</span></span><br><span class="line">B在起跑线等待裁判吹哨<span class="comment">// 同一时间启动</span></span><br><span class="line">C在起跑线等待裁判吹哨<span class="comment">// 同一时间启动</span></span><br><span class="line"></span><br><span class="line">A 到达了终点 <span class="comment">// 1秒后</span></span><br><span class="line">B 到达了终点 <span class="comment">// 3秒后</span></span><br><span class="line">C 到达了终点 <span class="comment">// 5秒后</span></span><br></pre></td></tr></table></figure>
<p>以上线程调度涉及到两个AQS底层的阻塞队列：</p>
<h5 id="第一个阻塞队列的形成（waitSignal背后对应的阻塞队列）："><a href="#第一个阻塞队列的形成（waitSignal背后对应的阻塞队列）：" class="headerlink" title="第一个阻塞队列的形成（waitSignal背后对应的阻塞队列）："></a>第一个阻塞队列的形成（waitSignal背后对应的阻塞队列）：</h5><p>RunnerA、RunnerB、RunnerC这三个线程都在“run”里面先调用<code>waitSignal.await()</code>，且同一时刻启动，由于比主线程<code>waitSignal.countDown()</code>先执行，因此这三个线程的await的getSate !=0 会进入阻塞队列操作<code>doAcquireSharedInterruptibly</code>,也即在waitSignal对象上形成以下CLH阻塞队列：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head(null,-1) &lt;-&gt; node(RunnerA,-1)  &lt;-&gt; node(RunnerB,-1)  &lt;-&gt; node(RunnerC,0)-&gt;null </span><br></pre></td></tr></table></figure>
<p>(假设按A、B、C入队顺序做的阻塞队列示意图)</p>
<p>什么时刻由谁去唤醒他们呢？</p>
<p>这里就是为何在主线程下使用<code>waitSignal.countDown()</code>的原因：对应以下1、2执行流</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">//CountDownLatch内部</span></span><br><span class="line"> <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">tryReleaseShared</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">     <span class="comment">// Decrement count; signal when transition to zero</span></span><br><span class="line">     <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">         <span class="keyword">int</span> c = getState(); <span class="comment">// waitSignal=new CountDownLatch(1)，此时state值为1</span></span><br><span class="line">         <span class="keyword">if</span> (c == <span class="number">0</span>)</span><br><span class="line">             <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">         <span class="keyword">int</span> nextc = c-<span class="number">1</span>; <span class="comment">// 主线程waitSignal.countDown()先执行，因此这里nextc=1-1=0</span></span><br><span class="line">         <span class="keyword">if</span> (compareAndSetState(c, nextc))</span><br><span class="line">             <span class="keyword">return</span> nextc == <span class="number">0</span>; <span class="comment">// 显然返回true</span></span><br><span class="line">     &#125;</span><br><span class="line">     </span><br><span class="line"><span class="comment">// AQS     </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">releaseShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">if</span> (tryReleaseShared(arg)) &#123; <span class="comment">// 上面返回true后，当然进入唤醒操作</span></span><br><span class="line">     doReleaseShared(); <span class="comment">// 内部的unparkSuccessor(h)会先唤醒RunnerA线程节点</span></span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>从上面可以看出，在主线程执行<code>waitSignal.countDown()</code>后会在AQS内部先通过<code>doReleaseShared</code>里面的<code>unparkSuccessor</code>唤醒RunnerA，那么之后的RunnerB、RunnerC又是怎么被唤醒呢？这就需要使用AQS共享模式的唤醒传播设计去解释了，以下便是RunnerA唤醒之后的执行流程：</p>
<p>RunnerA通过<code>setHeadAndPropagate(node, r)</code> 唤醒了RunnerB，同时RunnerA也会出队</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doAcquireSharedInterruptibly</span><span class="params">(<span class="keyword">int</span> arg)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node node = addWaiter(Node.SHARED);</span><br><span class="line">    <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">          	<span class="comment">// ①RunnerA被唤醒后，会来到此位置继续循环</span></span><br><span class="line">            <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">            <span class="keyword">if</span> (p == head) &#123;</span><br><span class="line">                <span class="keyword">int</span> r = tryAcquireShared(arg); </span><br><span class="line">                <span class="keyword">if</span> (r &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">//② RunnerA通过此方法实现将唤醒传播到RunnerB</span></span><br><span class="line">                    setHeadAndPropagate(node, r);</span><br><span class="line">                    p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">                    failed = <span class="keyword">false</span>;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">               	<span class="comment">//省略部分</span></span><br></pre></td></tr></table></figure>
<p>以此类推，RunnerB通过<code>setHeadAndPropagate(node, r)</code> 唤醒了RunnerC，同时RunnerC也会出队</p>
<p>最终实现了以下唤醒流：</p>
<p>1、主线程唤醒RunnerA</p>
<p>2、RunnerA使用setHeadAndPropagate传播唤醒RunnerB</p>
<p>3、RunnerB使用setHeadAndPropagate传播唤醒RunnerC</p>
<p>如果没有看本博客之前关于AQS的共享模式设计的解析，则无法理解背后多个线程入队阻塞之后被唤醒的流程，以及所谓的传播唤醒设计原理。</p>
<h5 id="第二个阻塞队列的形成（launchSignal背后对应的阻塞队列）："><a href="#第二个阻塞队列的形成（launchSignal背后对应的阻塞队列）：" class="headerlink" title="第二个阻塞队列的形成（launchSignal背后对应的阻塞队列）："></a>第二个阻塞队列的形成（launchSignal背后对应的阻塞队列）：</h5><p>虽然三个线程和主线程都是同一时刻执行，但因为每个线程需要“跑步一段时间后”才执行<code>launchSignal.countDown()</code>，故主线程执行<code>launchSignal.await()</code>时，getState=3显然不是0，因此会执行<code>doAcquireSharedInterruptibly</code>进入另外一条CLH阻塞队列，队列结构如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head(null,-1) &lt;-&gt; node(主线程,0)-&gt;null</span><br></pre></td></tr></table></figure>
<p>当主线程进入阻塞队列且被阻塞后，此时对应的用户线程是这样的：RunnerA运行自己的任务、RunnerB运行自己的任务、RunnerC运行自己的任务，且主程序还未退出，那么在什么时间且最终由谁去唤醒位于<code>launchSignal对象内部的阻塞队列</code>的主线程呢？</p>
<p>由于RunnerA是第一个结束任务（执行1秒）、RunnerB是第二个结束任务（执行3秒），RunnerC是作为最后一个结束任务（运行5秒），因此容易推出：由RunnerC在众多线程中作为最后一个结束任务的线程去唤醒位于<code>launchSignal对象内部的阻塞队列</code>的主线程。这里就不再具体解释其唤醒过程了，参考第一种用法里面的部分解析即可。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：基于AQS驱动的CyclicBarrier实现原理解析</title>
    <url>/2021/08/02/%E5%9F%BA%E4%BA%8EAQS%E9%A9%B1%E5%8A%A8%E7%9A%84CyclicBarrier%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h4 id="CyclicBarrier的基本使用"><a href="#CyclicBarrier的基本使用" class="headerlink" title="CyclicBarrier的基本使用"></a>CyclicBarrier的基本使用</h4><p>这里的Barrier可以翻译为屏障、篱栅、栅栏、“关口”，本文统一称为屏障，因此CyclicBarrier如果非得翻译的话，可以理解为：一个可循环制造屏障的同步器。</p>
<blockquote>
<p><em>CyclicBarrier</em>是一个（用于多线程协调的）同步器，它允许一组线程实现互相等待，直到所有线程的执行流都来到一个公共屏障点 （然后再去做一个任务）</p>
<p>A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point</p>
</blockquote>
<p>要理解CyclicBarrier的使用或者其设计意图，可以通过以下简单的demo来了解：</p>
<a id="more"></a>
<p>场景： 三位同学A、B、C玩一个闯关游戏，总共有两关（每一关的关口称为屏障），每个同学在本关玩游戏花费时间不一定相同，但要求：当前仅当三位同学都到达关口位置时，由最晚到关口的同学去说：可以进行第二关的任务，然后三位同学继续下一关游戏…</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CyclicBarrierDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Player</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">        CyclicBarrier bar;</span><br><span class="line">        <span class="keyword">int</span> duration;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Player</span><span class="params">(CyclicBarrier bar,<span class="keyword">int</span> duration)</span></span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.bar=bar;</span><br><span class="line">            <span class="keyword">this</span>.duration=duration;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>*<span class="keyword">this</span>.duration);</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+<span class="string">&quot;同学到达关口1&quot;</span>);</span><br><span class="line">                bar.await();</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>*<span class="keyword">this</span>.duration+<span class="number">1000</span>);</span><br><span class="line">                System.out.println(Thread.currentThread().getName()+<span class="string">&quot;同学到达关口2&quot;</span>);</span><br><span class="line">                bar.await();</span><br><span class="line"></span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        CyclicBarrier bar=<span class="keyword">new</span> CyclicBarrier(<span class="number">3</span>, <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                String broadcast=<span class="string">&quot;最晚到关口的&quot;</span>+Thread.currentThread().getName()+<span class="string">&quot;说：A、B、C都齐了，可以开始下一关&quot;</span>;</span><br><span class="line">                System.out.println(broadcast);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Player(bar,<span class="number">1</span>).start(); <span class="comment">// A同学，对应Thread-0</span></span><br><span class="line">        <span class="keyword">new</span> Player(bar,<span class="number">2</span>).start(); <span class="comment">// B同学，对应Thread-1</span></span><br><span class="line">        <span class="keyword">new</span> Player(bar,<span class="number">3</span>).start(); <span class="comment">// C同学，对应Thread-2</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出： </p>
<p>Thread-0同学到达关口1<br>Thread-1同学到达关口1<br>Thread-2同学到达关口1<br>最晚到关口的Thread-2说：A、B、C都齐了，可以开始下一关<br>Thread-0同学到达关口2<br>Thread-1同学到达关口2<br>Thread-2同学到达关口2<br>最晚到关口的Thread-2说：A、B、C都齐了，可以开始下一关</p>
<p> 对输出的说明：</p>
<p>可以看到A、B、C三位依次到达关口1，最晚到关口的C同学来执行“通告工作”，接着三位同学又可以进行下一关游戏（第二关），同样A、B、C三位依次到达关口2，且由最晚到关口的C同学来执行“通告工作”。</p>
<p>可以结合以下示意图帮助理解<br><img src="https://img-blog.csdnimg.cn/f60f2823a81247779035a624abfc052c.png" alt="在这里插入图片描述"></p>
<h4 id="CyclicBarrier设计解析"><a href="#CyclicBarrier设计解析" class="headerlink" title="CyclicBarrier设计解析"></a>CyclicBarrier设计解析</h4><h5 id="重要的一些成员变量"><a href="#重要的一些成员变量" class="headerlink" title="重要的一些成员变量"></a>重要的一些成员变量</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Each use of the barrier is represented as a generation instance.</span></span><br><span class="line"><span class="comment">   * The generation changes whenever the barrier is tripped, or</span></span><br><span class="line"><span class="comment">   * is reset. There can be many generations associated with threads</span></span><br><span class="line"><span class="comment">   * using the barrier - due to the non-deterministic way the lock</span></span><br><span class="line"><span class="comment">   * may be allocated to waiting threads - but only one of these</span></span><br><span class="line"><span class="comment">   * can be active at a time (the one to which &#123;<span class="doctag">@code</span> count&#125; applies)</span></span><br><span class="line"><span class="comment">   * and all the rest are either broken or tripped.</span></span><br><span class="line"><span class="comment">   * There need not be an active generation if there has been a break</span></span><br><span class="line"><span class="comment">   * but no subsequent reset.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Generation</span> </span>&#123;</span><br><span class="line">      <span class="keyword">boolean</span> broken = <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** The lock for guarding barrier entry */</span></span><br><span class="line"><span class="comment">// 从这里可以看到，内部使用一把独占锁和一个条件队列实现阻塞机制</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="comment">/** Condition to wait on until tripped */</span></span><br><span class="line"><span class="comment">// 这里tripped可以理解为：如果所有线程还未到达屏障，那么那些先来到屏障的线程会被阻塞在trip条件队列中</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Condition trip = lock.newCondition();</span><br><span class="line">  <span class="comment">/** The number of parties */</span></span><br><span class="line"> <span class="comment">// 到达同一屏障才给放行的线程数量，例如上面demo参与游戏的人数，要求3位同学到达关口才能进入下一关</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> parties;</span><br><span class="line">  <span class="comment">/* The command to run when tripped */</span></span><br><span class="line"><span class="comment">// 当所有线程都到达了屏障，由最晚到的线程去执行barrierCommand定义的具体任务，例如demo的C同学：最晚到关口的Thread-2说：A、B、C都齐了，可以开始下一关</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Runnable barrierCommand;</span><br><span class="line">  <span class="comment">/** The current generation */</span></span><br><span class="line"><span class="comment">// 这里的generation可以理解“当前屏障”、“本关”、“本局”、“本阶段”，就像游戏过关一样，如果在本局就是当前generation，如果通过本局就来到下一局，也即nextGeneration</span></span><br><span class="line">  <span class="keyword">private</span> Generation generation = <span class="keyword">new</span> Generation();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Number of parties still waiting. Counts down from parties to 0</span></span><br><span class="line"><span class="comment">   * on each generation.  It is reset to parties on each new</span></span><br><span class="line"><span class="comment">   * generation or when broken.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">（1）用于统计目前还差多少个线程到达屏障才能进行下一关任务，由barrier.await()对其扣减。</span></span><br><span class="line"><span class="comment">	在new CyclicBarrier给定，count的初始值=parties</span></span><br><span class="line"><span class="comment">	例如上面的demo：</span></span><br><span class="line"><span class="comment">	Thread-0执行barrier.await()，此时count=2，表示当前还差2个线程到达屏障才能进行下一关任务</span></span><br><span class="line"><span class="comment">	Thread-1执行barrier.await()，此时count=1，表示当前还差1个线程到达屏障才能进行下一关任务</span></span><br><span class="line"><span class="comment">	Thread-2执行barrier.await()，此时count=0，表示3个线程都到齐了屏障，可以开始任务</span></span><br><span class="line"><span class="comment">	(2) 当“游戏”从本关进入到下一关(或者从本屏障通过，达到下一个屏障)，count又恢复到parties的数量，这里体现了Cyclic重复、循环的意思。</span></span><br><span class="line"><span class="comment">	(3) 当然还有其他情况，参考后面的解析。</span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  关于“when the barrier is tripped”，本文一律解释为“当屏障”</span></span><br><span class="line"><span class="comment">   * Creates a new &#123;<span class="doctag">@code</span> CyclicBarrier&#125; that will trip when the</span></span><br><span class="line"><span class="comment">   * given number of parties (threads) are waiting upon it, and which</span></span><br><span class="line"><span class="comment">   * will execute the given barrier action when the barrier is tripped,</span></span><br><span class="line"><span class="comment">   * performed by the last thread entering the barrier.</span></span><br><span class="line"><span class="comment">   注意这里已经很清楚说明：由最后到达barrier的线程来执行给定barrierAction（也即初始化传入一个Runnable的任务，类似demo里面“A、B、C都齐了，可以开始下一关”的打印任务），barrierAction也可以是null，意味着无任何操作。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> parties the number of threads that must invoke &#123;<span class="doctag">@link</span> #await&#125;</span></span><br><span class="line"><span class="comment">   *        before the barrier is tripped</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> barrierAction the command to execute when the barrier is</span></span><br><span class="line"><span class="comment">   *        tripped, or &#123;<span class="doctag">@code</span> null&#125; if there is no action</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IllegalArgumentException if &#123;<span class="doctag">@code</span> parties&#125; is less than 1</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// </span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CyclicBarrier</span><span class="params">(<span class="keyword">int</span> parties, Runnable barrierAction)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (parties &lt;= <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">      <span class="keyword">this</span>.parties = parties;</span><br><span class="line">      <span class="keyword">this</span>.count = parties;</span><br><span class="line">      <span class="keyword">this</span>.barrierCommand = barrierAction;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="核心方法await、dowait"><a href="#核心方法await、dowait" class="headerlink" title="核心方法await、dowait"></a>核心方法await、dowait</h5><p>对于await方法，它会响应中断异常和所谓的“打破屏障异常”</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*在源码该方法注释中,说明调用await的线程阻塞在trip条件队列中，除非发生以下5个情况（这些情况都会让条件队列中全部阻塞的线程们唤醒）：</span></span><br><span class="line"><span class="comment">1、最后一个到达屏障线程，此线程会signalAll条件队列中所有阻塞线程</span></span><br><span class="line"><span class="comment">1、The last thread arrives; or </span></span><br><span class="line"><span class="comment">2、其他线程中断当前执行await的线程</span></span><br><span class="line"><span class="comment">2、Some other thread interrupts the current thread; or </span></span><br><span class="line"><span class="comment">3、其他线程中断一个已经在条件队列阻塞的线程</span></span><br><span class="line"><span class="comment">3、Some other thread interrupts one of the other waiting threads; or</span></span><br><span class="line"><span class="comment">4、在条件队列等待状态的线程出现等待超时的情况</span></span><br><span class="line"><span class="comment">4、Some other thread times out while waiting for barrier; or</span></span><br><span class="line"><span class="comment">5、有些线程主动使用reset方法</span></span><br><span class="line"><span class="comment">5、Some other thread invokes reset on this barrier.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">await</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException, BrokenBarrierException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          	<span class="comment">//具体的阻塞机制和唤醒机制是在dowait实现。</span></span><br><span class="line">            <span class="keyword">return</span> dowait(<span class="keyword">false</span>, <span class="number">0L</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (TimeoutException toe) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Error(toe); <span class="comment">// cannot happen</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="dowait核心方法的设计"><a href="#dowait核心方法的设计" class="headerlink" title="dowait核心方法的设计"></a>dowait核心方法的设计</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Main barrier code, covering the various policies.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 注释说这里面有多种处理策略</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">dowait</span><span class="params">(<span class="keyword">boolean</span> timed, <span class="keyword">long</span> nanos)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> InterruptedException, BrokenBarrierException,</span></span><br><span class="line"><span class="function">             TimeoutException </span>&#123;</span><br><span class="line">      <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">      lock.lock(); <span class="comment">// 先获取独占锁的这种设计对于条件队列来说已经再熟悉不过</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        	<span class="comment">// 临时变量g指向当前代（或者本局游戏）</span></span><br><span class="line">          <span class="keyword">final</span> Generation g = generation;</span><br><span class="line">				<span class="comment">// 1、此线程执行await前，当前屏障就已经被本局中的其他线程提前打破，则此线程只能抛出BrokenBarrierException。</span></span><br><span class="line">          <span class="keyword">if</span> (g.broken)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> BrokenBarrierException();</span><br><span class="line">				<span class="comment">// 2、线程还未进入条件队列就已经被外界中断，那么“所有线程都到达当前屏障的情况一定不会满足了”， 那么采取需要这样的操作：Sets current barrier generation as broken and wakes up everyone，并抛出中断异常。</span></span><br><span class="line">          <span class="keyword">if</span> (Thread.interrupted()) &#123;</span><br><span class="line">              breakBarrier();              </span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">          &#125;</span><br><span class="line">				<span class="comment">// 3、线程调用await会在这里对计时器减1，或者说：每有一个线程到达屏障，计时器就减1</span></span><br><span class="line">          <span class="keyword">int</span> index = --count;</span><br><span class="line">          <span class="comment">// 4、这里是关键的逻辑：当计时器为0时，说明所有的线程都到达了屏障，那么最晚到的那个线程可以开始执行一个指定任务：barrierCommand，然后再唤醒所有线程</span></span><br><span class="line">          <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;  <span class="comment">// tripped</span></span><br><span class="line">              <span class="keyword">boolean</span> ranAction = <span class="keyword">false</span>; <span class="comment">// 字面意思也可以猜出：是否成功运行了任务</span></span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                  <span class="comment">// 5、这里就是demo里面的由最晚到的C同学来完成“通知可以进行下一关”的任务</span></span><br><span class="line">                  <span class="keyword">final</span> Runnable command = barrierCommand;</span><br><span class="line">                  <span class="keyword">if</span> (command != <span class="keyword">null</span>)</span><br><span class="line">                      command.run(); <span class="comment">// 由最晚到屏障的线程来执行给定的任务</span></span><br><span class="line">                  ranAction = <span class="keyword">true</span>;</span><br><span class="line">                  <span class="comment">// 6 &quot;开启下一关&quot;：这最晚的到屏障的线程（C同学）通知那些已经在条件队列阻塞的线程们“开始进行下一局游戏”</span></span><br><span class="line">                  nextGeneration(); </span><br><span class="line">                  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">              &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                	<span class="comment">// 7、如果最晚到达屏障的线程没能“成功执行给定的任务”，那么也要唤醒条件队列里面的线程，表明本局游戏中止</span></span><br><span class="line">                  <span class="keyword">if</span> (!ranAction)</span><br><span class="line">                      breakBarrier();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// loop until tripped, broken, interrupted, or timed out</span></span><br><span class="line">        	<span class="comment">// 8、来到这里，说明执行await的线程一定不是最晚到达屏障的，那就先进入条件队列里面阻塞等待着，直到triped、当前屏障被打翻、在条件队列阻塞过程中被中断、超时等待，只要其中一个发生，此线程都会被唤醒。</span></span><br><span class="line">          <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                	<span class="comment">// 9、如果使用普通的await，则调用AQS条件队列里面普通await，如果使用超时的await，那么就调用AQS条件队列里面有超时计时的awaitNanos方法</span></span><br><span class="line">                  <span class="keyword">if</span> (!timed)</span><br><span class="line">                      trip.await();</span><br><span class="line">                  <span class="keyword">else</span> <span class="keyword">if</span> (nanos &gt; <span class="number">0L</span>)</span><br><span class="line">                      nanos = trip.awaitNanos(nanos);</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">                	<span class="comment">// 10、执行流来到这里，说明第9点提到线程在条件队列等待中被外界中断唤醒，如果唤醒线程发现还在本局“游戏”进度中且屏障还未被打翻，那么自己就去做breakBarrier这个操作：因为CyclicBarrier的设计要求只要任一在条件队列阻塞的线程被中断唤醒，那么“本局游戏”就认为是“输了”，表示本局需要中止。</span></span><br><span class="line">                  <span class="keyword">if</span> (g == generation &amp;&amp; ! g.broken) &#123;</span><br><span class="line">                      breakBarrier();</span><br><span class="line">                      <span class="keyword">throw</span> ie;</span><br><span class="line">                  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 11 、来到这里，考察第10点提到的两个条件中，（1）如果g!=generation,说明这个醒来的线程发现自己处于在“下一局游戏环境中”，既然这样自己可以继续执行，不必打破已经处在“下一局”的环境，至多将自己标记为中断过</span></span><br><span class="line">                     <span class="comment">//（2）如果g == generation 且 g.broken=true，说明当前局的屏障被（其他醒来的线程）打翻过，自己只需补一个中断标记即可，不必重新又来一个breakBarrier()。</span></span><br><span class="line">                      <span class="comment">// We&#x27;re about to finish waiting even if we had not</span></span><br><span class="line">                      <span class="comment">// been interrupted, so this interrupt is deemed to</span></span><br><span class="line">                      <span class="comment">// &quot;belong&quot; to subsequent execution.</span></span><br><span class="line">                      Thread.currentThread().interrupt();</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line">						<span class="comment">/* </span></span><br><span class="line"><span class="comment">						11、由于g.broken要么被当前线程自己来标记，要么就是本局中其他线程比当前线程提早标记g.broken，因此分为以下几种情况。也即对应dowait的上面各种策略</span></span><br><span class="line"><span class="comment">              ① 此线程执行await前，当前屏障就已经被本局中的其他线程提前打破</span></span><br><span class="line"><span class="comment">              ② 如果本局中最晚到达屏障的线程没能“成功执行给定的任务”</span></span><br><span class="line"><span class="comment">              ③ 如果本局中已经在条件队列中阻塞过程中被外界中断或者超时中断唤醒</span></span><br><span class="line"><span class="comment">              ④ 本局中线程或者外界线程调用reset</span></span><br><span class="line"><span class="comment">              或者参考await方法的注释</span></span><br><span class="line"><span class="comment">              Some other thread interrupts the current thread; or</span></span><br><span class="line"><span class="comment">              Some other thread interrupts one of the other waiting threads; or</span></span><br><span class="line"><span class="comment">              Some other thread times out while waiting for barrier; or</span></span><br><span class="line"><span class="comment">              Some other thread invokes reset on this barrier.</span></span><br><span class="line"><span class="comment">            	*/</span></span><br><span class="line">              <span class="keyword">if</span> (g.broken)</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> BrokenBarrierException();</span><br><span class="line">						<span class="comment">// 12、说明当前线程是被最晚到达屏障的线程（例如demo的C同学）唤醒来的，那么说明“本局所有线程在玩游戏过程中都是正常执行，没有任何异常发生”，因为最晚到达屏障的线程它会在dowait第6点逻辑中执行nextGeneration，从而使得g指向下一局（指向下一代）</span></span><br><span class="line">              <span class="keyword">if</span> (g != generation)</span><br><span class="line">                  <span class="keyword">return</span> index;</span><br><span class="line">						<span class="comment">// 13、对于超时等待引起的中断从而唤醒正在条件队列等待的线程，那只能宣告“本局游戏中止，当前屏障要标记为打翻”然后抛出timeout异常</span></span><br><span class="line">              <span class="keyword">if</span> (timed &amp;&amp; nanos &lt;= <span class="number">0L</span>) &#123;</span><br><span class="line">                  breakBarrier();</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          lock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="nextGeneration方法"><a href="#nextGeneration方法" class="headerlink" title="nextGeneration方法"></a>nextGeneration方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Updates state on barrier trip and wakes up everyone.</span></span><br><span class="line"><span class="comment">   * Called only while holding lock.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 在dowait方法的第6个逻辑：唤醒那些已经到达屏障然后在条件队列中阻塞的线程们（wakes up everyone）：“可以开始开启下一局”</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">nextGeneration</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// signal completion of last generation</span></span><br><span class="line">      trip.signalAll();</span><br><span class="line">      <span class="comment">// set up next generation</span></span><br><span class="line">    	<span class="comment">// 计时器重置为指定的线程数量，以便在下一局又可以重新对到达屏障的线程计数</span></span><br><span class="line">      count = parties;</span><br><span class="line">    	<span class="comment">// “开始下一局”、“开始下一关”、“开始下一代” 这三种表达的意思都一样</span></span><br><span class="line">      generation = <span class="keyword">new</span> Generation();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="breakBarrier方法"><a href="#breakBarrier方法" class="headerlink" title="breakBarrier方法"></a>breakBarrier方法</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">broken有打翻、破碎的、中止了的意思，在这里可以理解本局中止了、本关中止了、当前代中止了。</span></span><br><span class="line"><span class="comment"> * Sets current barrier generation as broken and wakes up everyone.</span></span><br><span class="line"><span class="comment"> * Called only while holding lock.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 在dowait方法的第7个逻辑：最晚到达屏障的线程没能“成功执行给定的任务”，这种情况称为“当前屏障被打翻”（不建议直接翻译，还是按current barrier generation as broken去理解显然更为自然）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">breakBarrier</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    generation.broken = <span class="keyword">true</span>; <span class="comment">// 将当前局（当前代、当前关口）设为中止了。</span></span><br><span class="line">    count = parties;<span class="comment">// 重置计数器</span></span><br><span class="line">    trip.signalAll(); <span class="comment">// 即使最晚到达屏障的线程没能“成功执行给定的任务”，它也要将阻塞在trip条件队列里面的所有线程唤醒。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="reset方法"><a href="#reset方法" class="headerlink" title="reset方法"></a>reset方法</h5><p>首先reset方法是public的，因此如果有一组线程正在当前的barrier进行某任务中，假设此时外面有一个线程（或者本组的其他线程）主动调用reset，那么就导致“本局游戏中止”，所有正在trip条件队列阻塞的线程都要醒来，然后再重新开始“新一局游戏”</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Resets the barrier to its initial state.  If any parties are</span></span><br><span class="line"><span class="comment"> * currently waiting at the barrier, they will return with a</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> BrokenBarrierException&#125;. Note that resets &lt;em&gt;after&lt;/em&gt;</span></span><br><span class="line"><span class="comment"> * a breakage has occurred for other reasons can be complicated to</span></span><br><span class="line"><span class="comment"> * carry out; threads need to re-synchronize in some other way,</span></span><br><span class="line"><span class="comment"> * and choose one to perform the reset.  It may be preferable to</span></span><br><span class="line"><span class="comment"> * instead create a new barrier for subsequent use.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        breakBarrier();   <span class="comment">// break the current generation</span></span><br><span class="line">        nextGeneration(); <span class="comment">// start a new generation</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="关于“打破本局游戏”的说明"><a href="#关于“打破本局游戏”的说明" class="headerlink" title="关于“打破本局游戏”的说明"></a>关于“打破本局游戏”的说明</h5><p>理解了CyclicBarrier的await方法（dowait）核心设计思想后，可以看到“如果有一组线程想通过CyclicBarrier不中断的完成一局又一局游戏”，条件是严格的：</p>
<p>（1）先完成自己工作的线程先到达关口（屏障）后，都得乖乖的在屏障这里等待（也即在trip关联的条件队列中阻塞），外界或者本组线程最好不要出现中断它的操作，（如果给定了在屏障的等待时长，线程还不能等太长导致超时，或者组内的还未到达屏障的线程没有去中断已经在条件队列的其他线程）</p>
<p>（2）所有的线程都到达了屏障</p>
<p>（3）最晚到达关口（屏障）的线程能不出错地完成指定的任务（barrierCommand）</p>
<p>以上三点都能顺利的话，那么这一组线程们就可以顺利进入“下一关（下一个屏障）游戏”，也即使得线程们能继续执行下一阶段的工作任务。</p>
<h5 id="如何设计有多个关口的“游戏任务”？"><a href="#如何设计有多个关口的“游戏任务”？" class="headerlink" title="如何设计有多个关口的“游戏任务”？"></a>如何设计有多个关口的“游戏任务”？</h5><p>这里说的“游戏任务”是指一种类似流水线的工作任务。只需在每个线程定义自己的run方法时，每完成一个“特定工作”就调用barrier.await()，例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Player</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    CyclicBarrier bar;</span><br><span class="line">    <span class="keyword">int</span> duration;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Player</span><span class="params">(CyclicBarrier bar)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.bar=bar;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            doSomeWork1()</span><br><span class="line">            bar.await(); <span class="comment">// 此线程在第一局关口等待其他线程</span></span><br><span class="line">            doSomeWork2()</span><br><span class="line">            bar.await();<span class="comment">// 此线程在第二局关口等待其他线程</span></span><br><span class="line">            doSomeWork3()</span><br><span class="line">            bar.await();<span class="comment">// 此线程在第三局关口等待其他线程</span></span><br><span class="line">          	.... <span class="comment">// 以此类推</span></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSomeWork1</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSomeWork2</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSomeWork3</span><span class="params">()</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="CyclicBarrier和CountDownLatch的对比说明"><a href="#CyclicBarrier和CountDownLatch的对比说明" class="headerlink" title="CyclicBarrier和CountDownLatch的对比说明"></a>CyclicBarrier和CountDownLatch的对比说明</h4><p>1、首先回顾CountDownLatch的基本使用：</p>
<p>以下是“等A、B、C三个同学到齐后才可以开会”的伪代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.初始化一个CountDownLatch waitSignal&#x3D;new CountDownLatch(3)</span><br><span class="line"></span><br><span class="line">2.主线程先执行condition.await()，此时主线程在“CLH阻塞队列中阻塞”</span><br><span class="line"></span><br><span class="line">3.随后</span><br><span class="line">子线程A启动便调用condition.countDown()，AQS的内部state从3减到2</span><br><span class="line">子线程B启动便调用condition.countDown()，AQS的内部state从2减到1</span><br><span class="line">子线程C最晚启动调用condition.countDown()，AQS的内部state从1减到0，满足唤醒条件，子线程C使用unparkSuccessor唤醒CHL阻塞队列的主线程</span><br><span class="line"></span><br><span class="line">4.最后</span><br><span class="line">主线程打印：A、B、C三个同学已经到齐，现在可以开会</span><br></pre></td></tr></table></figure>
<p>可以看到CountDownLatch也可以完成CyclicBarrier的这种“等人到齐了再去做某任务”的场景，但你可以清楚看到，CountDownLatch并不能持续进行“下一阶段的开会”，但是使用CyclicBarrier可以实现“A、B、C三个同学到齐了可以进行第一次开会，接着再来一轮A、B、C三个同学到齐了可以开第二次会，依次类推”，这就是两者在使用上的区别。</p>
<p>2、AQS的底层工作机制不同</p>
<p>CyclicBarrier显然是基于一把独占锁（支持独占模式和共享模式）、一个条件队列和一个CLH阻塞队列去实现的，唤醒方法是使用条件队列的signalAll</p>
<p>而CountDownLatch是基于一把独占锁（仅共享模式）、一个CLH阻塞队列实现的，唤醒方法使用传播唤醒方式去唤醒，用到doReleaseShared（unparkSuccessor）去传播唤醒。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Centos+uWSGI+Nginx部署Django项目(详细版)</title>
    <url>/2019/08/24/%E5%9F%BA%E4%BA%8ECentos+uWSGI+Nginx%E9%83%A8%E7%BD%B2Django%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<p>&#8195;&#8195;部署背景：之前开发了一个本地的个人blog项目，还未部署云服务器，有些功能还未完善，先尝试在本地部署。本篇内容则是基于centos+uwsgi+nginx来部署项目，实现高性能web服务，其中还给出uwsgi和nginx分别部署在不同服务器上步骤，这一点，很多文章并未给出相关探讨。</p>
<a id="more"></a>
<h3 id="1、相关服务安装配置"><a href="#1、相关服务安装配置" class="headerlink" title="1、相关服务安装配置"></a>1、相关服务安装配置</h3><p>安装nginx。配置官方镜像源，这里baseurl里面有$basearch变量，用来检索yum命令安装所需要的包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/yum.repos.d/nginx.repo</span></span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=http://nginx.org/packages/centos/7/$basearch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"><span class="meta">$</span><span class="bash"> yum -y install nginx</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> nginx</span></span><br></pre></td></tr></table></figure>
<p>安装python3.6，采用python虚拟环境部署项目，跟系统不冲突</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> yum -y install python36 python36-devel</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置并载入 Python3 虚拟环境</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /opt</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在这里不需要将centos默认python2.7版本配置为默认python3.6</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 直接使用python3.6作为启动命令即可，可避免冲突</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python3.6 -m venv py36  <span class="comment"># py3 为虚拟环境名称, 可自定义</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出虚拟环境可以使用 deactivate 命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /opt/py36/bin/activate</span>  </span><br><span class="line">(py36) [root@nn py36]# ls</span><br><span class="line">bin  include  lib  lib64  pyvenv.cfg</span><br><span class="line"><span class="meta">#</span><span class="bash"> 载入环境后默认以下所有命令均在该虚拟环境中运行</span></span><br><span class="line">(py3) [root@localhost py3]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 Python 库依赖</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install --upgrade pip setuptools</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install -r requirements.txt</span></span><br></pre></td></tr></table></figure>
<p>安装uwsgi<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> uwsgi需要使用gcc环境编译否则无法安装成功</span></span><br><span class="line">yum install gcc -y</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 激活py36环境</span></span><br><span class="line">(py36) [root@nn mywebapp]# pip install uwsgi</span><br><span class="line">Successfully installed uwsgi-2.0.18</span><br></pre></td></tr></table></figure><br>数据库安装和配置可以参考本人<a href="https://blog.csdn.net/pysense/article/details/99892680">这篇文章</a></p>
<p>以上后台技术栈的相关版本总揽如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Centos 7.5</span><br><span class="line">MariaDB 10.3.17</span><br><span class="line">Python 3.6</span><br><span class="line">Django1.11</span><br><span class="line">uWSGI 2.0.18</span><br><span class="line">Nginx 1.12.2</span><br><span class="line">Redis 5.0.4</span><br></pre></td></tr></table></figure><br>这些服务都是部署同一台服务器上，适合单台的个人云服务器，毕竟一年几百元，服务器配置有限，而对于本地部署，可以通过多台linux虚拟机分别部署不同服务，并做HA，这一过程相信也会积累不少知识和经验，学有余力的同学一定要试试。</p>
<h3 id="2、配置uwsgi启动django项目"><a href="#2、配置uwsgi启动django项目" class="headerlink" title="2、配置uwsgi启动django项目"></a>2、配置uwsgi启动django项目</h3><h4 id="2-1-uwsgi-命令行启动项目"><a href="#2-1-uwsgi-命令行启动项目" class="headerlink" title="2.1 uwsgi 命令行启动项目"></a>2.1 uwsgi 命令行启动项目</h4><p>查看项目，对项目路径必须清楚那些文件在哪里路径下，否则使用uwsgi启动设置参数，容易出错，以本项目为例，项目根目录路径为：<code>/opt/mywebapp</code>，项目根目录内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mywebapp]# pwd</span><br><span class="line">/opt/mywebapp</span><br><span class="line">(py36) [root@nn mywebapp]# ls</span><br><span class="line">account  blog    db.sqlite3  __init__.py  media   script  templates</span><br><span class="line">article  course  image       manage.py    mysite  static</span><br></pre></td></tr></table></figure>
<p>==<strong>其中非常关键的wsgi入口</strong>==，在mysite目录下，也就是django项目总settings.py所在的目录，mysite目录下的wsgi.py，将在之后的uwsgi启动中使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mysite]# ls</span><br><span class="line">__init__.py  __pycache__  settings.py  urls.py  wsgi.py</span><br></pre></td></tr></table></figure>
<p>wsgi.py代码逻辑：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">WSGI config for mysite project.</span></span><br><span class="line"><span class="string">It exposes the WSGI callable as a module-level variable named ``application``.</span></span><br><span class="line"><span class="string">For more information on this file, see</span></span><br><span class="line"><span class="string">https://docs.djangoproject.com/en/1.10/howto/deployment/wsgi/</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> django.core.wsgi <span class="keyword">import</span> get_wsgi_application</span><br><span class="line">os.environ.setdefault(<span class="string">&quot;DJANGO_SETTINGS_MODULE&quot;</span>, <span class="string">&quot;mysite.settings&quot;</span>)</span><br><span class="line">application = get_wsgi_application()</span><br></pre></td></tr></table></figure>
<p>确认项目使用manage.py启动能正常运行<br><code>python manage.py runserver 0.0.0.0:9000</code></p>
<p>使用uwsgi启动项目并测试是否成功运行django项目，通过连接mysite/wsgi.py实现web server和application通信</p>
<p>这里有两种启动方式：</p>
<p>==<strong><em>A、不带静态文件启动，静态文件将无法加载，页面不正常显示</em></strong>==</p>
<p>—chdir /opt/mywebapp/     django项目根路径<br>wsgi-file mysite/wsgi.py    django项目settings.py所在目录下的 wsgi.py文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uwsgi --http 0.0.0.0:9000 --chdir /opt/mywebapp/ --wsgi-file mysite/wsgi.py --master --processes 4 --threads 2</span><br></pre></td></tr></table></figure>
<p>==<strong><em>B、带静态文件启动，也就是网页打开后，页面能正常显示</em></strong>==</p>
<p>—chdir /opt/mywebapp/     django项目根路径<br>wsgi-file mysite/wsgi.py    django项目settings.py所在目录下的 wsgi.py文件<br>—static-map=/static=static  django项目web页面静态文件，所在根目录的’static’目录<br>—static-map=/static=media  django项目内容静态文件，所在根目录的’media’目录</p>
<p>==注意：这里仅是指static、media目录，根目录下还有其他blog，account,templates目录等，可能也有人会问，是不是都需要把这些目录都要一一加入—static-map里面，答案是不需要，因为这些都不是django application对应的“static”目录(已在settins设定，并可以让其他views索引到static目录)，如果使用-static-map=/static=templates，uwsgi将无法找到相关静态文件==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uwsgi --http 0.0.0.0:9000 --chdir /opt/mywebapp/ --wsgi-file mysite/wsgi.py --static-map=/static=static --master --processes 4 --threads 2</span><br></pre></td></tr></table></figure>
<p>==注意==，对于这种启动方式，动、静态资源都可以访问</p>
<h4 id="2-2-使用uwsgi-ini配置文件启动项目"><a href="#2-2-使用uwsgi-ini配置文件启动项目" class="headerlink" title="2.2 使用uwsgi.ini配置文件启动项目"></a>2.2 使用uwsgi.ini配置文件启动项目</h4><p>以上两种启动直接在命令加入环境参数，比较繁琐，可通过在配置文件中放置这些环境参数，方便启动，在manage.py 同目录下（项目根目录），新建一个目录uwsgi_conf用来放置uwsgi.ini配置文件，目录路径：<code>/opt/mywebapp/uwsgi_conf</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mywebapp]# ls</span><br><span class="line">account  blog    db.sqlite3  __init__.py  media   uwsgi_conf  templates</span><br><span class="line">article  course  image       manage.py    mysite  static</span><br></pre></td></tr></table></figure>
<p>在uwsgi_conf目录下新建uwsgi.ini文件，配置如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> uwsig使用配置文件启动</span></span><br><span class="line">[uwsgi]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 项目所在的根目录</span></span><br><span class="line">chdir=/opt/mywebapp/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定项目的application,区别于启动命令--wsgi-filemysite/wsgi.py</span></span><br><span class="line">module=mysite.wsgi:application</span><br><span class="line"><span class="meta">#</span><span class="bash">the <span class="built_in">local</span> unix socket file than commnuincate to Nginx</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定sock的文件路径，这个sock文件会在nginx的uwsgi_pass配置，用来nginx与uwsgi通信</span>       </span><br><span class="line"><span class="meta">#</span><span class="bash"> 支持ip+port模式以及socket file模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash">socket=%(<span class="built_in">chdir</span>)/uwsgi_conf/uwsgi.sock</span></span><br><span class="line">socket=127.0.0.1:9001</span><br><span class="line"><span class="meta">#</span><span class="bash"> 进程个数</span>       </span><br><span class="line">processes = 8</span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个进程worker数</span></span><br><span class="line">workers=5</span><br><span class="line">procname-prefix-spaced=mywebapp                # uwsgi的进程名称前缀</span><br><span class="line">py-autoreload=1                              # py文件修改，自动加载</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定IP端口，web访问入口</span></span><br><span class="line">http=0.0.0.0:9000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定多个静态文件：static目录和media目录,也可以不用指定该静态文件，在nginx中配置静态文件目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> uwsgi有自己的配置语法，详细可参考官网，无需写绝对路径，可以用循环、判断等高级配置语法</span></span><br><span class="line">for =static media</span><br><span class="line">static-map=/static=%(chdir)/%(_)</span><br><span class="line">endfor =</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动uwsgi的用户名和用户组</span></span><br><span class="line">uid=root</span><br><span class="line">gid=root</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用主进程</span></span><br><span class="line">master=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自动移除unix Socket和pid文件当服务停止的时候</span></span><br><span class="line">vacuum=true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 序列化接受的内容，如果可能的话</span></span><br><span class="line">thunder-lock=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用线程</span></span><br><span class="line">enable-threads=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置一个超时，用于中断那些超过服务器请求上限的额外请求</span></span><br><span class="line">harakiri=30</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置缓冲</span></span><br><span class="line">post-buffering=4096</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置日志目录</span></span><br><span class="line">daemonize=%(chdir)/uwsgi_conf/uwsgi.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> uWSGI进程号存放</span></span><br><span class="line">pidfile=%(chdir)/uwsgi_conf/uwsgi.pid</span><br><span class="line"><span class="meta">#</span><span class="bash">monitor uwsgi status  通过该端口可以监控 uwsgi 的负载情况</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 支持ip+port模式以及socket file模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> stats=%(<span class="built_in">chdir</span>)/uwsgi_conf/uwsgi.status</span> </span><br><span class="line">stats = 127.0.0.1:9001</span><br></pre></td></tr></table></figure>
<p>之所以要新建一个uwsgi_conf目录，是为了集中放置uWSGI配置以及日志、进程等文件，方便管理，配置语法可参考<a href="https://uwsgi-docs.readthedocs.io/en/latest/ConfigLogic.html">官方配置文档说明</a></p>
<p>基于配置文件uwsgi启动django项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn uwsgi_conf]# uwsgi --ini uwsgi.ini </span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动后打印的信息，可以看到static静态文件和media媒体资源目录被uWSGI索引</span></span><br><span class="line">[uWSGI] getting INI configuration from uwsgi.ini</span><br><span class="line">[uwsgi-static] added mapping for /static =&gt; /opt/mywebapp/static</span><br><span class="line">[uwsgi-static] added mapping for /static =&gt; /opt/mywebapp/media</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行后，自动参数日志、进程,建议自行查看日志文件内容，了解更多uwsgi</span></span><br><span class="line">(py36) [root@nn uwsgi_conf]# ls</span><br><span class="line">uwsgi.ini  uwsgi.log  uwsgi.pid </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 停止uwsgi服务</span></span><br><span class="line">(py36) [root@nn uwsgi_conf]# uwsgi --stop uwsgi.pid</span><br></pre></td></tr></table></figure>
<p>以上说明使用uWSGI配置启动django项目成功运行，因uWSGI的配置文件里已加入静态文件static-map，因此在不需要nginx的配置下，也可以支撑服务（只是性能未到完美级别），此部分的流程如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190826235642727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>下面将使用nginx配置静态文件请求，uwsgi只负责动态请求部分的请求，各司其职，以进一步压榨服务性能。如果确定使用nginx代理django项目静态文件服务，那么配置之前，先把uwsgi.ini里面的—static-map=/static部分注释掉。</p>
<h3 id="3、使用nginx启动uwsgi"><a href="#3、使用nginx启动uwsgi" class="headerlink" title="3、使用nginx启动uwsgi"></a>3、使用nginx启动uwsgi</h3><h4 id="3-1-配置nginx"><a href="#3-1-配置nginx" class="headerlink" title="3.1 配置nginx"></a>3.1 配置nginx</h4><p>在/etc/nginx/conf.d/，新建一个myblog.conf文件，配置为：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">upstream blog_app &#123;</span><br><span class="line"><span class="meta">   #</span><span class="bash"> nginx通过socket在环回接口地址的9001端口与本地的uWSGI进程通信</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"> 支持ip:port模式以及socket file模式</span></span><br><span class="line"><span class="meta">   #</span><span class="bash">server unix:/opt/mywebapp/uwsgi_conf/uwsgi.sock;</span></span><br><span class="line">   server 127.0.0.1:9001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">    listen 9090;</span><br><span class="line">    server_name 192.168.100.5;</span><br><span class="line">    </span><br><span class="line">    access_log /var/log/nginx/access.log;</span><br><span class="line">    charset utf-8;</span><br><span class="line">  </span><br><span class="line">    gzip_types text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;</span><br><span class="line">    error_page 404 /404.html;</span><br><span class="line">    error_page 500 502 503 504 /50x.html;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        # nginx转发动态请求到uWSGI</span><br><span class="line">        include uwsgi_params;</span><br><span class="line">        uwsgi_connect_timeout 20</span><br><span class="line">        uwsgi_pass blog_app;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 如果写成/static/,nginx无法找到项目静态文件路径</span><br><span class="line">    location /static &#123;</span><br><span class="line">        alias /opt/mywebapp/static;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 如果写成/media/,nginx无法找到项目媒体文件路径</span><br><span class="line">    location /media &#123;</span><br><span class="line">        alias /opt/mywebapp/media;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>重启nginx服务，server nginx restart，只要在第2部分uWSGI与django application正常连接，那么到这部分，nginx是能够正常代理uWSGI服务的，如有问题，请认真检查nignx的在/etc/nginx/conf.d/myblog.conf配置文件。<br>==这里要解释为何配置可以放在/etc/nginx/conf.d/目录下，网上不是有很多教程是在/etc/nginx/nginx.conf默认配置文件改动吗？==<br>其实nginx配置文件很灵活，可以从其他模块include根配置文件里面，查看主配置nginx.conf内容里面的http 模块，它可以是可以把/etc/nginx/conf.d/目录下所有的配置文件内容包含到主配置文件里面，注意如果使用这种方式，请把主配置文件server模块注释，其实就是关闭多余其他服务端口而已。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 要确保nginx用户对django项目根目录下静态文件具有读权限,否则会出现403 Forbidden</span></span><br><span class="line">user nginx;</span><br><span class="line">worker_processes auto;</span><br><span class="line">error_log /var/log/nginx/error.log;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line"><span class="meta">#</span><span class="bash"> Load dynamic modules. See /usr/share/nginx/README.dynamic.</span></span><br><span class="line">include /usr/share/nginx/modules/*.conf;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    ......省略</span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    ......省略</span><br><span class="line"><span class="meta">	#</span><span class="bash">server  &#123;</span></span><br><span class="line">    ......省略</span><br><span class="line">    #		&#125;    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h4 id="3-2-关于myblog-conf路径放置"><a href="#3-2-关于myblog-conf路径放置" class="headerlink" title="3.2 关于myblog.conf路径放置"></a>3.2 关于myblog.conf路径放置</h4><p>理解了nginx的include配置文件方式，那么我们可以不需要在/etc/nginx/conf.d/目录下创建myblog.conf，直接在django项目的根目录下mywebapp/，新建一个nginx_conf目录,在这里放置myblog.conf，<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn nginx_conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;mywebapp&#x2F;nginx_conf</span><br><span class="line">[root@nn nginx_conf]# ls</span><br><span class="line">myblog.conf</span><br></pre></td></tr></table></figure><br>然后把该配置路径<code>/opt/mywebapp/nginx_conf/myblog.conf</code> 加入到nginx默认的主配置文件里面<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    ......省略</span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    # 不再是 include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    include /opt/mywebapp/nginx_conf/myblog.conf;</span><br><span class="line">    ......省略</span><br><span class="line"><span class="meta">	#</span><span class="bash">server  &#123;</span></span><br><span class="line">    ......省略</span><br><span class="line">    #		&#125;    </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><br>server nginx restart重启服务，nginx首先解析主配置文件/etc/nginx/nginx.conf，发现主配置里面，include了在其他位置的配置文件，于是nginx找到myblog.conf并加载，接着完成一些列其他逻辑。以上两种nginx配置都可以连接uWSGI服务，至于选哪种方式，看个人需求或项目文件管理习惯。</p>
<h4 id="3-3-nginx-uWSGI-django启动后，访问media目录的图片、文件、视频出现403-forbidden提示"><a href="#3-3-nginx-uWSGI-django启动后，访问media目录的图片、文件、视频出现403-forbidden提示" class="headerlink" title="3.3  nginx+uWSGI+django启动后，访问media目录的图片、文件、视频出现403 forbidden提示"></a>3.3  nginx+uWSGI+django启动后，访问media目录的图片、文件、视频出现403 forbidden提示</h4><p>这个问题，在csdn绝大部分nginx+uWSGI+django部署文章都没提及如何处理，因为大部分文章没有测试到这部分内容，只是测试nginx可以正常获取static目录下的js、css、html文件，页面显示正常即结束，但如果项目的代码中，例如有视频课程这么一个功能，上传视频是放在media目录下的course目录里，那么当网页访问该视频时，就会提示该资源403 forbidden状态。<br>==<strong>原因：nginx worker对media整个目录没有读权限</strong>==<br>nginx默认使用名字为“nginx”用户，这一点可在其主配置文件找到，也可以通过进程详情看到<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# ps -ef|grep nginx</span><br><span class="line">root     28759     1  0 02:01 ?        00:00:00 nginx: master process /usr/sbin/nginx</span><br><span class="line">nginx    28760 28759  0 02:01 ?        00:00:00 nginx: worker process</span><br><span class="line">root     28784 28382  0 02:26 pts/0    00:00:00 grep --color=auto nginx</span><br></pre></td></tr></table></figure><br>再看mywebapp/media权限，指向root用户<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x.  1 root  root     804 ** manage.py</span><br><span class="line">drwxr-xr-x.  4 root  root     49 ** media</span><br><span class="line">drwxr-xr-x.  3 root  root      93 ** mysite</span><br><span class="line">drwxr-xr-x.  9 root  root     113 ** static</span><br></pre></td></tr></table></figure><br>故需要将相关目录的读权限给到nginx用户，对django项目根目录下的static、media两个目录赋权给nginx user<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# chown -R nginx:nginx /opt/mywebapp/static/</span><br><span class="line">[root@nn mywebapp]# chmod -R ug+r /opt/mywebapp/static/</span><br><span class="line">[root@nn mywebapp]# chown -R nginx:nginx /opt/mywebapp/media/</span><br><span class="line">[root@nn mywebapp]# chmod -R ug+r /opt/mywebapp/media/</span><br></pre></td></tr></table></figure><br>聪明的同学可能此时会立刻联想到：既然nginx访问django项目静态文件要赋权，那么前面第2部分的uWSGI进程也是否需要赋权呢？答案：需要看uwsgi.ini配置了什么用户。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uid=root</span><br><span class="line">gid=root</span><br></pre></td></tr></table></figure><br>这里配置了root用户，而且对于django整个项目文件的rwx权限，root用户本已具备，因此不需要再赋权，除非uwsgi.ini配置了一个非root用户，例如blog_user用户，那么就需要重启赋权，目录是整个django项目，例如以下可行的赋权：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# chown -R blog_user:blog_user /opt/mywebapp</span><br><span class="line">[root@nn mywebapp]# chmod -R ug+r /opt/mywebapp</span><br></pre></td></tr></table></figure></p>
<p>==第3部分的请求流程可以表示为：==<br><img src="https://img-blog.csdnimg.cn/20190825195924701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="4、关于nginx配置django的静态文件讨论"><a href="#4、关于nginx配置django的静态文件讨论" class="headerlink" title="4、关于nginx配置django的静态文件讨论"></a>4、关于nginx配置django的静态文件讨论</h3><p>关于静态文件的配置，其过程有些地方非常容易引起混淆，在这里一一指出。<br>首先，在本文中，nginx服务和uWSGI服务部署在同一台服务器，因此在nginx配置中，location的静态文件因为的是本地django项目里面的静态文件，个人把这种配置过程nginx代理本地静态文件配置，另外一种django项目里面的静态文件放置在远程服务器上，由远程的nginx来代理，这种称为nginx代理远程静态文件配置。<br>==为什么要做这样的部署测试？<br>大部分文章都是基于同一台服务器进行nginx服务和uWSGI服务部署，很少有讨论在不同服务器上部署，事实上，如果生产环境比较严格，nginx服务器本身要做冗余和负载均衡，uWSGI服务器也是要做冗余和负载均衡，数据库MariaDB本身主-主模式。==</p>
<h4 id="4-1-nginx代理本地静态文件配置"><a href="#4-1-nginx代理本地静态文件配置" class="headerlink" title="4.1 nginx代理本地静态文件配置"></a>4.1 nginx代理本地静态文件配置</h4><p>在第三部分的uWSGI的配置文件中，socket配置为loopback地址，说明uWSGI进程与nginx进行本地通信，nginx代理本地静态文件。在第3部分可测试过程中，除了<a href="http://192.168.100.5:9090/admin无法正常显示页面（所有关于admin管理页面后台的静态文件nginx无法找到），其他子路径例如http://192.168.100.5:9090/blog,http://192.168.100.5:9090/article,都可以正常显示页面，这种情况如何处理？">http://192.168.100.5:9090/admin无法正常显示页面（所有关于admin管理页面后台的静态文件nginx无法找到），其他子路径例如http://192.168.100.5:9090/blog,http://192.168.100.5:9090/article,都可以正常显示页面，这种情况如何处理？</a><br>从nginx配置的项目静态文件路径为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如果写成/static/,nginx无法找到项目静态文件路径,注意避免配置语法出错</span></span><br><span class="line">location /static &#123;</span><br><span class="line">    alias /opt/mywebapp/static;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看其目录文件，发现并没有django项目admin的后台所需的静态文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# ls</span><br><span class="line">account     blog        image        media   </span><br><span class="line">all_static  course      __init__.py  mysite  templates</span><br><span class="line">article     db.sqlite3  manage.py    static  uwsgi_conf</span><br><span class="line">[root@nn mywebapp]# ls static</span><br><span class="line">css  editor  fonts  images  ImgCrop  js</span><br></pre></td></tr></table></figure>
<p>通过<code>python manage.py collectstatic</code>将admin后台包含所有的静态文件都拷贝到mywebapp根目录下，在执行命令之前，需要在settings.py设置一个放置整个mywebapp项目静态文件目录</p>
<ul>
<li>原静态文件目录的设置：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">STATIC_URL = &#x27;/static/&#x27;</span><br><span class="line">STATICFILES_DIRS = (</span><br><span class="line">    os.path.join(BASE_DIR, &quot;static&quot;),</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>加入admin静态文件的设置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">STATIC_URL = &#x27;/static/&#x27;</span><br><span class="line">STATICFILES_DIRS = (</span><br><span class="line">    os.path.join(BASE_DIR, &quot;static&quot;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">STATIC_ROOT=&#x27;all_static&#x27;</span><br></pre></td></tr></table></figure>
<p>==注意：若STATIC_ROOT=’static’，collect无法拷贝admin静态文件到项目中，会有提示==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> django.core.exceptions.ImproperlyConfigured: The STATICFILES_DIRS setting should not contain the STATIC_ROOT setting</span></span><br></pre></td></tr></table></figure>
<p>==能否在settings.py不指定STATIC_ROOT，利用已指定的mywebapp/static目录放置collect static？不行==，看以下提示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># django.core.exceptions.ImproperlyConfigured: You&#39;re using the staticfiles app without having set the STATIC_ROOT setting to a filesystem path.</span><br></pre></td></tr></table></figure>
<p>当正确拷贝所有静态文件到mywebapp/all_static目录后，查看其内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mywebapp]# ls all_static/</span><br><span class="line">admin  css  editor  fonts  images  ImgCrop  js</span><br></pre></td></tr></table></figure>
<p>原来 <code>python manage.py collectstatic</code> 把项目下的static目录静态文件和django自带的后台admin静态文件打包一起放在/mywebapp/all_static目录里。<br>就个人对项目目录管理习惯而言，把all_static/下的admin目录整个拷贝到static目录下，并删除all_static目录，settings的静态文件路径注释掉<code>STATIC_ROOT=&#39;all_static&#39;</code>，==这样既可保持整个django项目仅有一个static目录，而且该目录已经包含项目所需的所有静态文件（js、css、html等）==，注意media目录路径不做改变，还是位于根项目路径下。</p>
<h4 id="4-2-远程静态目录配置"><a href="#4-2-远程静态目录配置" class="headerlink" title="4.2 远程静态目录配置"></a>4.2 远程静态目录配置</h4><h5 id="4-2-1-拷贝静态文件到远程nginx服务器"><a href="#4-2-1-拷贝静态文件到远程nginx服务器" class="headerlink" title="4.2.1 拷贝静态文件到远程nginx服务器"></a>4.2.1 拷贝静态文件到远程nginx服务器</h5><p>如果已经理解了nginx代理本地静态文件的配置，其实远程方式其实也简单，本文的远程nginx服务器地址为192.168.100.6，在创建与uWSGI相同的application目录（可以使用不同路径，这里是为方便管理）：<code>/opt/mywebapp/</code>，该目录只放置static和media目录，文件可以手工同步过来，注意要赋权给nginx用户<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 mywebapp]# ls -al</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 4 root  root   33 Aug 17 20:29 .</span><br><span class="line">drwxr-xr-x. 3 root  root   67 Aug 17 20:03 ..</span><br><span class="line">drwxr-xr-x. 4 nginx nginx  35 Aug 19  2019 media</span><br><span class="line">drwxr-xr-x. 9 nginx nginx 113 Aug 18  2019 static</span><br><span class="line"></span><br><span class="line">[root@dn1 mywebapp]# ls static/</span><br><span class="line">admin  css  editor  fonts  images  ImgCrop  js</span><br><span class="line">[root@dn1 mywebapp]# ls media/</span><br><span class="line">courses  images</span><br></pre></td></tr></table></figure></p>
<h5 id="4-2-2-更改-5uWSGI服务器配置文件socket"><a href="#4-2-2-更改-5uWSGI服务器配置文件socket" class="headerlink" title="4.2.2 更改.5uWSGI服务器配置文件socket"></a>4.2.2 更改.5uWSGI服务器配置文件socket</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[uwsgi]</span><br><span class="line"># 项目所在的根目录</span><br><span class="line">chdir&#x3D;&#x2F;opt&#x2F;mywebapp&#x2F;</span><br><span class="line"># 指定项目的application,区别于启动命令--wsgi-filemysite&#x2F;wsgi.py</span><br><span class="line">module&#x3D;mysite.wsgi:application</span><br><span class="line"># 不再使用loopback地址，对外其他服务器暴露uWSGI服务</span><br><span class="line">socket&#x3D;192.168.1005:9001</span><br></pre></td></tr></table></figure>
<h5 id="4-2-3-更改-6服务器的nginx配置文件，"><a href="#4-2-3-更改-6服务器的nginx配置文件，" class="headerlink" title="4.2.3 更改.6服务器的nginx配置文件，"></a>4.2.3 更改.6服务器的nginx配置文件，</h5><p>为了方便管理，路径与uWSGI一致：<br><code>/etc/nginx/conf.d/myblog.conf</code><br>远程nginx服务器配置文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">upstream blog_app &#123;</span><br><span class="line"><span class="meta">   #</span><span class="bash"> 连接远程uWSGI服务器的socket</span> </span><br><span class="line">   server 192.168.88.5:9001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 9090;</span><br><span class="line">    server_name 192.168.100.6;</span><br><span class="line">    access_log /var/log/nginx/access.log;</span><br><span class="line">    charset utf-8;</span><br><span class="line">    gzip_types text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;</span><br><span class="line">    error_page 404 /404.html;</span><br><span class="line">    error_page 500 502 503 504 /50x.html;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        include uwsgi_params;</span><br><span class="line">        uwsgi_connect_timeout 30;</span><br><span class="line">        uwsgi_pass ;</span><br><span class="line">    &#125;</span><br><span class="line">    # 这就是为何在远程nginx服务器上，保持与uWSGI静态文件路径一致的原因，方便管理和理解配置文件</span><br><span class="line">    location /static &#123;</span><br><span class="line">        alias /opt/mywebapp/static;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    location /media &#123;</span><br><span class="line">        alias /opt/mywebapp/media;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>以.6IP访问动态资源和静态，可以正常请求。</p>
<h3 id="5-、关于sock文件的理解"><a href="#5-、关于sock文件的理解" class="headerlink" title="5 、关于sock文件的理解"></a>5 、关于sock文件的理解</h3><p>在uwsgi.ini和nginx配置文件里面，我们需要配置socket，以便他们之间进行进程通信，这里socket的配置方式，可以选择一个以.sock作为后缀的文件，而大家更熟悉的方式是通过socket pair进行socket通信，这里如何理解sock文件？<br>这里整理收集的资料概括为：</p>
<p>在Unix/Linux系统里面，“一切皆文件”，这里文件是由什么组成的？这些文件其实是可以读取和写入的普通字节（字节流）的集合。如果你持有一个文件引用（也就是文件描述符），就可以使用“打开open –&gt; 读写write/read –&gt; 关闭close”模式（抽象成一组Linux 文件操作API）来进行 IO 操作，无论设备的类型和底层硬件是什么。</p>
<p>所以进程（进程本身也是文件）之间的通信。个人理解uwsgi.sock文件用到的就是该模式的一个实现，socket就是这么一种特殊的套接字文件，也即是说nginx通过uwsgi.sock作为载体，与本地的uWSGI进程进行通信。</p>
<p>这种基于特殊的套接字文件来保持通信是无法进行远程通信，这时需要用到TCP/IP协议，通过远程IP+端口号的socket pair，基于TCP连接进行远程通信（若用loopback的IP地址127.0.0.1，就变成本地通信），所以这启发我们可以将nginx部署在另外一台服务器上，本文中，uWSGI+application部署在192.168.100.5，另外一台服务器192.168.100.6 作为nginx服务器，要实现他们之间的远程通信，<br>只需uwsgi配置文件uwsgi.ini里面socket行改为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#the local unix socket file than commnuincate to Nginx</span><br><span class="line"># 指定sock的文件路径，则限制本地通信</span><br><span class="line"># 指定loopback 地址+端口号，则限制本地通信</span><br><span class="line"># 指定全网地址或者本机真实IP，则可以实现远程通信</span><br><span class="line">socket&#x3D;192.168.100.5:9001</span><br></pre></td></tr></table></figure><br>在.6服务器上，nginx的配置myblog.conf的upstream模块个改为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream blog_app &#123;</span><br><span class="line"> # 指向远程uWSGI</span><br><span class="line">   server 192.168.100.5:9001;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>通过以上设置，可以把static静态文件目录和media目录放置在远程.6的nginx服务器上，.5服务器则负责application业务逻辑，两服务器之间的静态文件可以通过rsync实时同步。rsync的配置不再给出，也可参考<a href="https://blog.csdn.net/pysense/article/details/99289090">本blog文章</a></p>
<p>若考虑对uWSGI做负载均衡，比如第二台uWSGI服务器192.168.100.4:9001;可以加入upstream并设定相关负载算法，若还考虑对nginx服务器进行负载均衡，则需要用keepalived，通过VIP对外透明服务，负载均衡的配置会更有趣。</p>
]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>uWSGI</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Centos7.5完整部署分布式Hadoop3.1.2</title>
    <url>/2019/10/10/%E5%9F%BA%E4%BA%8ECentos7.5%E5%AE%8C%E6%95%B4%E9%83%A8%E7%BD%B2%E5%88%86%E5%B8%83%E5%BC%8FHadoop3.1.2/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文基于虚拟机以及有限的计算资源搭建了非HA模式下分布式的hadoop集群，主要是为了后续开发基于大数据的实时计算项目提供hadoop服务。</p>
<h3 id="1、相关安装包以及规划"><a href="#1、相关安装包以及规划" class="headerlink" title="1、相关安装包以及规划"></a>1、相关安装包以及规划</h3><p>考虑本地测试使用，这里所使用的三台服务器均有虚拟机创建，每台配置：1个vCPU+1G内存+9G硬盘，基本组件版本</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Ip</th>
<th>角色</th>
<th>hadoop路径</th>
<th>Hostname</th>
<th>jdk路径</th>
<th>linux版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.188.0.4</td>
<td>NameNode,Datanode,NodeManager</td>
<td>/opt/hadoop-3.1.2</td>
<td>nn</td>
<td>/opt/jdk1.8.0_161</td>
<td>Centos7.5</td>
</tr>
<tr>
<td>192.188.0.5</td>
<td>DataNode,ResourceManager,NodeManager，JobHistoryServer</td>
<td>/opt/hadoop-3.1.2</td>
<td>dn1</td>
<td>/opt/jdk1.8.0_161</td>
<td>Centos7.5</td>
</tr>
<tr>
<td>192.188.0.6</td>
<td>DataNode,Secondarynode,NodeManager</td>
<td>/opt/hadoop-3.1.2</td>
<td>dn2</td>
<td>/opt/jdk1.8.0_161</td>
<td>Centos7.5</td>
</tr>
</tbody>
</table>
</div>
<a id="more"></a>
<p>这里列出节点服务的基础介绍：</p>
<p>hadoop平台相关：</p>
<p>NameNode：</p>
<blockquote>
<p>接收用户操作请求<br>维护文件系统的目录结构<br>管理文件与block之间关系，block与datanode之间关系</p>
</blockquote>
<p>DataNode：</p>
<blockquote>
<p>存储文件<br>文件被分成block存储在磁盘上<br>为保证数据安全，文件会有多个副本</p>
</blockquote>
<p>Secondary NameNode：</p>
<blockquote>
<p>合并来自namenode的fsimage和edits文件来更新namenode的metedata</p>
</blockquote>
<p>yarn平台相关：<br>ResourceManager:</p>
<blockquote>
<p>集群中所有资源的统一管理和分配，它接受来自各个节点的NodeManager的资源汇报信息，并把这些信息按照一定的策略分配给各个应用程序，是整个yarn集群中最重要的组件之一。</p>
</blockquote>
<p>JobHistoryServer：</p>
<blockquote>
<p>历史服务器，可以通过历史服务器查看已经运行完成的Mapreduce作业记录，比如用了多少个Map、多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。默认情况下，历史服务器是没有启动的，需要进行参数配置才能启动</p>
</blockquote>
<p>NodeManager：</p>
<blockquote>
<p>运行在单个节点上的代理，管理hadoop集群中单个计算节点，它需要与相应用程序ApplicationMaster和集群管理者ResourceManager交互<br>从ApplicationMaster上接收有关Contioner的命令并执行<br>向ResourceManager汇报各个Container运行状态和节点健康状况，并领取有关的Container的命令并执行</p>
</blockquote>
<h3 id="2、设置hostname"><a href="#2、设置hostname" class="headerlink" title="2、设置hostname"></a>2、设置hostname</h3><p>分别对三个节点更改对应的hostname</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# vi /etc/hostname </span><br><span class="line">nn</span><br><span class="line">[root@dn1 ~]# vi /etc/hostname </span><br><span class="line">dn1</span><br><span class="line">[root@dn2 ~]# vi /etc/hostname </span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>配置域名解析，三个节点都需要配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.188.0.4  nn</span><br><span class="line">192.188.0.5 dn1</span><br><span class="line">192.188.0.6 dn2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 无需重启，直接ping主机名称</span></span><br><span class="line">[root@dn2 ~]# ping nn</span><br><span class="line">PING nn (192.188.0.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from nn (192.188.0.4): icmp_seq=1 ttl=64 time=0.322 ms</span><br><span class="line">64 bytes from nn (192.188.0.4): icmp_seq=2 ttl=64 time=0.347 ms</span><br></pre></td></tr></table></figure>
<h3 id="3、配置免密ssh"><a href="#3、配置免密ssh" class="headerlink" title="3、配置免密ssh"></a>3、配置免密ssh</h3><h4 id="3-1-对三台服务器设置ssh公钥"><a href="#3-1-对三台服务器设置ssh公钥" class="headerlink" title="3.1 对三台服务器设置ssh公钥"></a>3.1 对三台服务器设置ssh公钥</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn &#x2F;]# ssh-keygen -t rsa</span><br><span class="line"># 手动创建 authorized_keys文件</span><br><span class="line">[root@nn .ssh]# ls</span><br><span class="line">id_rsa  id_rsa.pub</span><br><span class="line">[root@nn .ssh]# cp id_rsa.pub authorized_keys</span><br></pre></td></tr></table></figure>
<p>其他两个节点同样操作</p>
<h4 id="3-2-在nn节点将自己公钥拷贝到其他两个节"><a href="#3-2-在nn节点将自己公钥拷贝到其他两个节" class="headerlink" title="3.2  在nn节点将自己公钥拷贝到其他两个节"></a>3.2  在nn节点将自己公钥拷贝到其他两个节</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 三个节点都需要操作</span><br><span class="line">[root@nn ~]# ssh-copy-id nn</span><br><span class="line">[root@nn ~]# ssh-copy-id dn1</span><br><span class="line">[root@nn ~]# ssh-copy-id dn2</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 测试免密登录</span><br><span class="line">[root@nn ~]# ssh dn1</span><br><span class="line">[root@nn ~]# ssh dn2</span><br></pre></td></tr></table></figure>
<h3 id="4、配置Java环境"><a href="#4、配置Java环境" class="headerlink" title="4、配置Java环境"></a>4、配置Java环境</h3><p>本项目中，java包、hadoop包、spark包都放在/opt目录下，三个节点都需配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># java包路径</span><br><span class="line">[root@nn jdk1.8.0_161]# pwd</span><br><span class="line">&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"># 配置环境变量</span><br><span class="line"># vi &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line">export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line"># 生效配置</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 查看版本</span><br><span class="line">[root@nn jdk1.8.0_161]# java -version</span><br><span class="line">java version &quot;1.8.0_161&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_161-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)</span><br></pre></td></tr></table></figure>
<h3 id="5、配置Hadoop环境"><a href="#5、配置Hadoop环境" class="headerlink" title="5、配置Hadoop环境"></a>5、配置Hadoop环境</h3><p>Hadoop路径<code>/opt/hadoop-3.1.2</code>，以下为hadoop文件目录的简要说明</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Bin</th>
<th>Hadoop最基本的管理脚本和使用脚本的目录，这些脚本是sbin目录下管理脚本的基础实现 。用户可以直接使用这些脚本管理和使用Hadoop</th>
</tr>
</thead>
<tbody>
<tr>
<td>include</td>
<td>对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</td>
</tr>
<tr>
<td>etc</td>
<td>Hadoop的配置文件所在的目录，各类**.xml配置文件夹</td>
</tr>
<tr>
<td>lib</td>
<td>该目录下存放的是Hadoop运行时依赖的jar包，Hadoop在执行时会把lib目录下面的jar全部加到classpath中。</td>
</tr>
<tr>
<td>libexec</td>
<td>各个服务对用的shell配置文件所在的目录，可用于配置日志输出、启动参数（比如JVM参数）等基本信息。</td>
</tr>
<tr>
<td>sbin</td>
<td>Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的启动/关闭脚本，</td>
</tr>
<tr>
<td>share</td>
<td>Hadoop各个模块编译后的jar包所在的目录，也官方自带的doc手册</td>
</tr>
<tr>
<td>logs</td>
<td>（hadoop初始化之后才会自动生成）该目录存放的是Hadoop运行的日志，查看日志对寻找Hadoop运行错误非常有帮助。</td>
</tr>
<tr>
<td>namenode_dir</td>
<td>在hdfs-site.xml配置后，hadoop首次启动会创建该目录，目录下包含edit文件和fsimage</td>
</tr>
<tr>
<td>datanode_dir</td>
<td>在hdfs-site.xml配置后，hadoop首次启动会创建该目录：存放数据文件</td>
</tr>
</tbody>
</table>
</div>
<h4 id="5-1-配置hadoop-env-sh"><a href="#5-1-配置hadoop-env-sh" class="headerlink" title="5.1 配置hadoop-env.sh"></a>5.1 配置hadoop-env.sh</h4><p>给hadoop配置Java路径，三个节点都需要配置，但无需每台去设置，因为后面会把整个/opt/hadoop-3.1.2/etc/hadoop拷贝到另外两个dn节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line">vi hadoop-env.sh</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br></pre></td></tr></table></figure>
<h4 id="5-2-core-site-xml"><a href="#5-2-core-site-xml" class="headerlink" title="5.2 core-site.xml"></a>5.2 core-site.xml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- namenode用9000根datanode通信 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;nn:9000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--hadoop临时文件路径 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-2-hdfs-site-xml"><a href="#5-2-hdfs-site-xml" class="headerlink" title="5.2 hdfs-site.xml"></a>5.2 hdfs-site.xml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-把dn2 设为secondary namenode，端口不能缺少 --&gt;</span><br><span class="line">          &lt;property&gt;</span><br><span class="line">                  &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">                 &lt;value&gt;dn2:50090&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;    </span><br><span class="line">    &lt;!-- namenode 上存储 hdfs 名字空间元数据--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- datanode 上数据块的物理存储位置--&gt;  </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 设置 hdfs 副本数量 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-3-mapred-site-xml"><a href="#5-3-mapred-site-xml" class="headerlink" title="5.3 mapred-site.xml"></a>5.3 mapred-site.xml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定Yyarn运行--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;Yyarn&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">	&lt;value&gt;dn1:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定dn1作为jobhistory服务器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;dn1:19888&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 注意这里的路径不是Linux文件路径，而是hdfs文件系统上的路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;history&#x2F;done&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;history&#x2F;done_intermediate&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- mp所需要hadoop环境 --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;Yyarn.app.mapreduce.am.env&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-4-yarn-site-xm"><a href="#5-4-yarn-site-xm" class="headerlink" title="5.4 yarn-site.xm;"></a>5.4 yarn-site.xm;</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific yarn configuration properties --&gt;</span><br><span class="line">    &lt;!-- 指定ResourceManager的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;dn1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;  </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="5-5-workers"><a href="#5-5-workers" class="headerlink" title="5.5 workers"></a>5.5 workers</h4><p>三个节点都设为datanode，当然也生产环境中，负责数据物理文件存储DD不要跟DN放在同一台服务器<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi etc&#x2F;hadoop&#x2F;workers </span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure></p>
<h4 id="5-6-设置start-dfs-sh-和-stop-dfs-sh"><a href="#5-6-设置start-dfs-sh-和-stop-dfs-sh" class="headerlink" title="5.6 设置start-dfs.sh 和 stop-dfs.sh"></a>5.6 设置start-dfs.sh 和 stop-dfs.sh</h4><p>在/opt/hadoop-3.1.2/sbin/start-dfs.sh 文件开头</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi sbin/start-dfs.sh</span><br><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure>
<h4 id="5-7-设置start-yarn-sh-和-stop-yarn-sh"><a href="#5-7-设置start-yarn-sh-和-stop-yarn-sh" class="headerlink" title="5.7 设置start-yarn.sh 和 stop-yarn.sh"></a>5.7 设置start-yarn.sh 和 stop-yarn.sh</h4><p>都是在文件开头处添加</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi sbin/start-yarn.sh </span><br><span class="line">yarn_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">yarn_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>
<h4 id="5-8-将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置"><a href="#5-8-将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置" class="headerlink" title="5.8 将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置"></a>5.8 将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">export HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<p><strong>直接将以上的配置文件所在目录拷贝到另外两个节点上，避免繁琐配置</strong></p>
<p>[root@nn hadoop-3.1.2]# scp -r /opt/hadoop-3.1.2/etc/hadoop/   dn1:/opt/hadoop-3.1.2/etc/</p>
<p>[root@nn hadoop-3.1.2]# scp -r /opt/hadoop-3.1.2/sbin   dn1:/opt/hadoop-3.1.2/</p>
<h4 id="5-9-初始化hadoop文件系统"><a href="#5-9-初始化hadoop文件系统" class="headerlink" title="5.9 初始化hadoop文件系统"></a>5.9 初始化hadoop文件系统</h4><p>因为nn是作为namenode管理节点，因此只需在nn节点进行相应的格式化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn bin]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;bin</span><br><span class="line">[root@nn bin]# hdfs namenode -format</span><br><span class="line">****</span><br><span class="line">*** INFO common.Storage: Storage directory &#x2F;opt&#x2F;hadoop-3.1.2&#x2F;namenode has been successfully formatted.</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at nn&#x2F;192.188.0.4</span><br></pre></td></tr></table></figure>
<p>以上说明namenode格式化成功</p>
<h3 id="6-启动hadoop服务"><a href="#6-启动hadoop服务" class="headerlink" title="6 启动hadoop服务"></a>6 启动hadoop服务</h3><h4 id="6-1-在namenode上启动服务"><a href="#6-1-在namenode上启动服务" class="headerlink" title="6.1 在namenode上启动服务"></a>6.1 在namenode上启动服务</h4><p>一键启动所有：如果使用start-all.sh，表示把集群的所有配置的服务都启动，它会调用start-dfs.sh和start-yarn.sh</p>
<p>单个节点启动：使用start-dfs.sh和start-yarn.sh，这里要注意，比如nn节点是作为namenode节点，那么在nn节点执行start-dfs.sh，无需执行start-yarn.sh</p>
<p>网上绝大部分教程会教你用start-all.sh启用集群服务，但这不是官方的推荐方式，个人推荐在每个节点启动相应服务</p>
<p>nn节点：NameNode,Datanode,NodeManager，只需运行start-dfs.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>dn1节点：DataNode,ResourceManager,NodeManager，因为需要使用yarn服务，且作为ResourceManager节点（本身也是NodeManager）,需运行start-yarn.sh</p>
<p>此外：dn1节点还是作为yarn主节点的JobHistoryServer服务，还需通过命令<code>mapred --daemon start historyserver</code>启动之，启动JobHistoryServer后，可以在yarn的web服务直观查看每个job的运行历史，后面会给截图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# start-yarn.sh </span><br><span class="line">[root@dn1 sbin]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;sbin</span><br><span class="line">[root@dn1 sbin]# mapred --daemon start historyserver </span><br></pre></td></tr></table></figure>
<p>dn2节点：DataNode,Secondarynode,NodeManager，因为nn节点的hdfs-site.xml已经配置了dn2节点作为sn节点，那么nn节点启动服务时，就已经自动在dn2节点启动了Secondarynode进程。</p>
<p>查看各个节点服务进程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# jps</span><br><span class="line">9957 NameNode</span><br><span class="line">10553 Jps</span><br><span class="line">10092 DataNode</span><br><span class="line">10430 NodeManager</span><br><span class="line"></span><br><span class="line">[root@dn1 ~]# jps</span><br><span class="line">31792 DataNode</span><br><span class="line">32133 NodeManager</span><br><span class="line">32492 Jps</span><br><span class="line">31998 ResourceManager</span><br><span class="line">17428 JobHistoryServer</span><br><span class="line"></span><br><span class="line">[root@dn2 ~]# jps</span><br><span class="line">31105 NodeManager</span><br><span class="line">30898 DataNode</span><br><span class="line">31235 Jps</span><br><span class="line">31005 SecondaryNameNode</span><br></pre></td></tr></table></figure>
<p>也可通过查看web服务来确认NameNode服务和yarn服务<br>NN入口：<code>http://192.188.0.4:9870/</code><br><img src="https://img-blog.csdnimg.cn/20191010210825857.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>yarn入口：<code>http://192.188.0.5:8088</code><br><img src="https://img-blog.csdnimg.cn/20191010205253651.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>也可通过起一个python http服务查看hadoop自带的手册，手册html文件在</p>
<p><code>/opt/hadoop-3.1.2/share/doc/hadoop</code>，里面有index.html,故只需在该目录下，开启一个后台web 服务，即可在 浏览器打开其网页</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop]# ls</span><br><span class="line">api                                  hadoop-fs2img</span><br><span class="line">css                                  hadoop-gridmix</span><br><span class="line">dependency-analysis.html             hadoop-hdfs-httpfs</span><br><span class="line">***</span><br><span class="line">***</span><br><span class="line">hadoop-dist                          images</span><br><span class="line">hadoop-distcp                        index.html</span><br><span class="line">hadoop-extras                        project-reports.html</span><br><span class="line"></span><br><span class="line">[root@nn hadoop]# python -m SimpleHTTPServer 8000 &amp;</span><br></pre></td></tr></table></figure>
<h3 id="7、跑个wordcount-测试"><a href="#7、跑个wordcount-测试" class="headerlink" title="7、跑个wordcount 测试"></a>7、跑个wordcount 测试</h3><h4 id="7-1-在namenode节点上的hadoop文件系统的根目录路上创建一个目录"><a href="#7-1-在namenode节点上的hadoop文件系统的根目录路上创建一个目录" class="headerlink" title="7.1 在namenode节点上的hadoop文件系统的根目录路上创建一个目录"></a>7.1 在namenode节点上的hadoop文件系统的根目录路上创建一个目录</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建测试目录,可以使用</span><br><span class="line">[root@nn hadoop-3.1.2]# hadoop fs -mkdir &#x2F;app</span><br><span class="line"># 或者</span><br><span class="line">[root@nn hadoop-3.1.2]# hdfs dfs -mkdir &#x2F;app</span><br><span class="line"># 查看hadoop文件系统的根目录下的结构</span><br><span class="line">[root@nn hadoop-3.1.2]# hadoop fs -ls &#x2F;</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          ** &#x2F;app</span><br><span class="line">drwxr-xr-x   - root supergroup          ** &#x2F;word-count-app</span><br></pre></td></tr></table></figure>
<p>以上目录的创建，也会在datanode同步创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# hadoop fs -ls &#x2F;</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** &#x2F;app</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** &#x2F;word-count-app</span><br></pre></td></tr></table></figure>
<h4 id="7-2-hdfs常用命令"><a href="#7-2-hdfs常用命令" class="headerlink" title="7.2  hdfs常用命令"></a>7.2  hdfs常用命令</h4><p>具体详细命令用户，官网给出非常仔细的说明：</p>
<p><code>http://hadoop.apache.org/docs/r3.1.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">列出 hdfs 下的文件</span><br><span class="line">$ hdfs dfs -ls</span><br><span class="line">列出 hdfs &#x2F; 路径下的所有文件，文件夹  </span><br><span class="line">$ hdfs dfs -ls -R &#x2F;</span><br><span class="line">创建目录 &#x2F;app</span><br><span class="line">$ hdfs dfs -mkdir &#x2F;app</span><br><span class="line">列出 hsfs 名为 input 的文件夹中的文件</span><br><span class="line">$ hadoop dfs -ls app</span><br><span class="line">将 words.txt 上传到 hdfs 中</span><br><span class="line">$ hdfs dfs -put &#x2F;hadoop_test&#x2F;words.txt &#x2F;app</span><br><span class="line"></span><br><span class="line">将 hsdf 中的 words.txt 文件保存到本地</span><br><span class="line">$ hdfs dfs -get &#x2F;app&#x2F;words.txt &#x2F;hadoop_test&#x2F;words.txt</span><br><span class="line"></span><br><span class="line">删除 hdfs 上的 test.txt 文件</span><br><span class="line">$ hadoop dfs -rmr &#x2F;hadoop_test&#x2F;words.txt</span><br><span class="line"></span><br><span class="line">查看 hdfs 下 app 文件夹中的内容</span><br><span class="line">$ hadoop fs -cat app&#x2F;*</span><br><span class="line">进入安全模式</span><br><span class="line">$ hadoop dfsadmin –safemode enter</span><br><span class="line">退出安全模式</span><br><span class="line">$ hadoop dfsadmin -safemode leave</span><br><span class="line">报告 hdfs 的基本统计情况</span><br><span class="line">$ hadoop dfsadmin -report</span><br></pre></td></tr></table></figure>
<h4 id="7-1-将测试文件添加到hadoop系统的指定目录"><a href="#7-1-将测试文件添加到hadoop系统的指定目录" class="headerlink" title="7.1 将测试文件添加到hadoop系统的指定目录"></a>7.1 将测试文件添加到hadoop系统的指定目录</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# cat hadoop_example&#x2F;words.txt </span><br><span class="line">foo is foo</span><br><span class="line">bar is not bar</span><br><span class="line">hadoop file system is the infrastructure of bigdata </span><br><span class="line"></span><br><span class="line">[root@nn opt]# hdfs dfs -put hadoop_example&#x2F;words.txt &#x2F;app</span><br><span class="line"></span><br><span class="line">[root@nn opt]# hdfs dfs -ls &#x2F;app      </span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup         76 ** &#x2F;app&#x2F;words.txt</span><br></pre></td></tr></table></figure>
<p>也可以通过web端查看fs目录结构和文件内容，直观易用</p>
<p><img src="https://img-blog.csdnimg.cn/20191010204829139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="7-2-运行word-count-java示例程序"><a href="#7-2-运行word-count-java示例程序" class="headerlink" title="7.2 运行word count java示例程序"></a>7.2 运行word count java示例程序</h4><p>例程序在此路径：<code>/opt/hadoop-3.1.2/share/hadoop/mapreduce</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn mapreduce]# hadoop jar hadoop-mapreduce-examples-3.1.2.jar wordcount &#x2F;app &#x2F;result</span><br><span class="line"></span><br><span class="line">**,500 INFO mapreduce.Job:  map 50% reduce 0%</span><br><span class="line">**,653 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">**,685 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">**,715 INFO mapreduce.Job: Job job_1***** completed successfully</span><br><span class="line">**,837 INFO mapreduce.Job: Counters: 55</span><br></pre></td></tr></table></figure>
<p>以上表示成功运行一个计算实例</p>
<p>查看计算结果，计算放在hdfs文件系统/result目录下,其中 /result/part-r-00000为计算结果，可以查看其输出内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# hdfs dfs -cat &#x2F;result&#x2F;part-r-00000</span><br><span class="line">bar     2</span><br><span class="line">big     1</span><br><span class="line">data    1</span><br><span class="line">file    1</span><br><span class="line">foo     2</span><br><span class="line">hadoop  2</span><br><span class="line">infrastructure  1</span><br><span class="line">is      3</span><br><span class="line">not     1</span><br><span class="line">spark   2</span><br><span class="line">system  1</span><br><span class="line">the     1</span><br><span class="line">zookeeper       2</span><br></pre></td></tr></table></figure>
<p>当然，因为我们引入yarn调度框架，并且有dn1节点提供yarn服务，当然可以对此次map-reduce计算任务job在web端查看。<br><img src="https://img-blog.csdnimg.cn/20191010211317781.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>以上为完整的基于hadoop3.1.2真实集群并引入yarn管理的文章讨论，给出了完整部署流程和测试案例，保证本次部署过程的可行性。注意到本文的hadoop集群中还不是HA模式，生产环境需要部署HA模式，后面的文章中我们将引入Zookeeper，给出HA模式的部署过程（zk文章在本博客已经有深入的探讨，目的也是为了后面大数据架构部署）</p>
<h3 id="Trouble-shooting"><a href="#Trouble-shooting" class="headerlink" title="Trouble shooting"></a>Trouble shooting</h3><p>1、运行word count时，yarn提示任务虚拟运行内存不足</p>
<p>Container [pid=17786,containerID=container_<em>**</em>002_01_000003] is running 459045376B beyond the ‘VIRTUAL’ memory limit. Current usage: 61.8 MB of 1 GB physical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing container</p>
<p>回答这个问题前，首先要理解yarn集群中Container的概念</p>
<ul>
<li><p>在yarn的NodeManager节点上，会将集群中所有节点的CPU和内存的一定值抽离出来，组成一个“资源池”，例如资源池是100，这个资源池根据配置（例如设置大哥容器申请的资源最大值为10）可以分成多个Container（100/10=10个可供Job使用的容器），当Application（在MapReduce时期叫Job）提出申请时，就会分配相应的Container资源，因此Container其实是yarn中的一个动态资源分配的概念，其拥有一定的内存，核数，由RM分配给ApplicationMaster或者MapTask或者ReduceTask使用，这些task就在Container为基础的容器中运行起来。</p>
</li>
<li><p>通俗点说，Container就是“一组资源：内存+CPU”，它跟Linux Container没有任何关系，仅仅是yarn提出的一个概念，当有一个Application来想RM节点申请资源是，第一个Container用来跑ApplicationMaster，然后ApplicationMaster再申请一些Container来跑Mapper，之后再申请一些Container来跑Reducer。</p>
</li>
<li><p>当Mapper或者Reducer所需的“资源之一虚拟内存大于Container默认提供值时”，以上问题就会出现：beyond the ‘VIRTUAL’ memory limit.</p>
</li>
</ul>
<p>解决办法有两种</p>
<p>A、降低Mapper或者Reducer所需内存资源配置值，在mapred-site.xml 进行配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;每个Map任务的物理内存限制&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;200&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;每个Reduce任务的物理内存限制&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.java.opts&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx100m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.java.opts&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx200m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>B、配置RM针对单个Container能申请的最大资源或者RM本身能配置的最大内存</p>
<p>配置解释：单个容器可申请的最小与最大内存，Application在运行申请内存时不能超过最大值，小于最小值则分配最小值，例如在本文测试中，因计算任务较为简单，无需太多资源，故最小值设为50M，最大值设为100M</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;50&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>配置解释：NM的内存资源配置，主要是通过下面两个参数进行的</p>
<p>第一个参数：每个节点可用的最大内存，默认值为-1，代表着yarn的NodeManager占总内存的80%，本文中，物理内存为1G</p>
<p>第二个参数：NM的虚拟内存和物理内存的比率，默认为2.1倍</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>vmem-pmem-ratio的默认值为2.1，由于本机器中，每个节点的物理内存为1G，因此单个RM拿到最大虚拟内存为2.1G，从<code>2.5 GB of 2.1 GB virtual memory used. Killing container</code>,可知，Container申请的资源为2.5G，已经超过默认值2.1G，当改为3倍时，虚拟化够用，故解决了问题。</p>
<p>2、org.apache.hadoop.yarn.exceptions.yarnException:Unauthorized request to start container</p>
<p>出错原因：Hadoop集群（本文也包含yarn集群）中多个节点的时间不同步导致.</p>
<p>解决：修改多个节点的时间为相同的时间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> # 将硬件时间写到系统时间</span><br><span class="line">[root@dn1 ~]# hwclock -s </span><br><span class="line">保存时钟</span><br><span class="line">[root@dn1 ~]# clock -w</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Docker单机部署ZooKeeper集群</title>
    <url>/2019/09/04/%E5%9F%BA%E4%BA%8EDocker%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2ZooKeeper%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面的文章部署zk服务，直接在裸机上部署，较为不便，现在很多服务如果不做docker化，无论在故障恢复、运维都增加很大困难，无法做到自动化部署，这种低效率的IT运营模式是比较难接受的，对于我们开发而已，必须是一键式优雅部署，所以本篇文章采用docker方式部署zk集群，可以从中对比裸机部署过程的不同以及优势</p>
<h3 id="1、部署docker和docker-compose"><a href="#1、部署docker和docker-compose" class="headerlink" title="1、部署docker和docker-compose"></a>1、部署docker和docker-compose</h3><p>参考本博客文章：<a href="https://blog.csdn.net/pysense/article/details/100547816">链接</a></p>
<h3 id="2、部署zookeeper集群"><a href="#2、部署zookeeper集群" class="headerlink" title="2、部署zookeeper集群"></a>2、部署zookeeper集群</h3><p>&#8195;&#8195;拉取zk镜像，可以dockerhub上面看下目前的zk官方镜像的tag有什么版本，默认是latest，接着是3.5.5以及3.4.14，这里用的stable版本3.4.14</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 opt]# docker pull zookeeper:3.4.14</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在宿主机上新建一个存放docker集群zk服务器目录（仅为了方便管理），并在该目录下新建一个compose配置文件</p>
<a id="more"></a>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# pwd</span><br><span class="line">/opt/zk_docker_cluster</span><br><span class="line">[root@dn2 zk_docker_cluster]# vi docker-compose.yml</span><br><span class="line">version: &#x27;3.3&#x27;</span><br><span class="line">services:</span><br><span class="line">  zoo1:</span><br><span class="line">    # 使用zookeeper:3.4.14镜像，加上tag标签</span><br><span class="line">    image: zookeeper:3.4.14</span><br><span class="line">    restart: always</span><br><span class="line">    hostname: zoo1</span><br><span class="line">    container_name: zk1</span><br><span class="line">    ports:</span><br><span class="line">      - 2181:2181</span><br><span class="line">    volumes:</span><br><span class="line">    # 宿主机目录路径无需手工创建，docker-compose有权限进行自行创建挂载的目录路径    </span><br><span class="line">      - /opt/zk_docker_cluster/zoo1/data:/data</span><br><span class="line">      - /opt/zk_docker_cluster/zoo1/datalog:/datalog</span><br><span class="line">      - /opt/zk_docker_cluster/zoo1/logs:/logs</span><br><span class="line">      </span><br><span class="line">    environment:</span><br><span class="line">      ZOO_MY_ID: 1</span><br><span class="line">			ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class="line"></span><br><span class="line">  zoo2:</span><br><span class="line">    image: zookeeper</span><br><span class="line">    restart: always</span><br><span class="line">    hostname: zoo2</span><br><span class="line">    container_name: zk2</span><br><span class="line">    ports:</span><br><span class="line">      - 2182:2181</span><br><span class="line">    volumes:</span><br><span class="line">      - /opt/zk_docker_cluster/zoo2/data:/data</span><br><span class="line">      - /opt/zk_docker_cluster/zoo2/datalog:/datalog</span><br><span class="line">      - /opt/zk_docker_cluster/zoo2/logs:/logs      </span><br><span class="line">    environment:</span><br><span class="line">      ZOO_MY_ID: 2</span><br><span class="line">			ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class="line"></span><br><span class="line">  zoo3:</span><br><span class="line">    image: zookeeper</span><br><span class="line">    restart: always</span><br><span class="line">    hostname: zoo3</span><br><span class="line">    container_name: zk3</span><br><span class="line">    ports:</span><br><span class="line">      - 2183:2181</span><br><span class="line">    volumes:</span><br><span class="line">      - /opt/zk_docker_cluster/zoo3/data:/data</span><br><span class="line">      - /opt/zk_docker_cluster/zoo3/datalog:/datalog</span><br><span class="line">      - /opt/zk_docker_cluster/zoo3/logs:/logs      </span><br><span class="line">    environment:</span><br><span class="line">      ZOO_MY_ID: 3</span><br><span class="line">			ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>配置文件需要注意的地方</p>
<p>在运行前，可以用<code>docker-compose -f docker-compose.yml config</code>检查配置文件是否正确</p>
<p>1）version 版本号不能随便改，例如这里改为1.0，提示不支持</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# docker-compose -f docker-compose.yml config </span><br><span class="line">ERROR: Version in &quot;./docker-compose.yml&quot; is unsupported. You might be seeing this error because you&#x27;re using the wrong Compose file version. Either specify a supported version (e.g &quot;2.2&quot; or &quot;3.3&quot;) and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.</span><br></pre></td></tr></table></figure>
<p>2）注意yaml语法的层次表达</p>
<p>例如在这里，故意把zoo1放置在service同层次上，引起解析出错，所以在编排zk的配置时，要注意这些细节</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ERROR: yaml.parser.ParserError: while parsing a block mapping</span><br><span class="line">  in &quot;.&#x2F;docker-compose .yml&quot;, line 1, column 1</span><br><span class="line">expected &lt;block end&gt;, but found &#39;&lt;block mapping start&gt;&#39;</span><br><span class="line">  in &quot;.&#x2F;docker-compose .yml&quot;, line 17, column 3</span><br></pre></td></tr></table></figure>
<p>==3) 注意到docker-compose yml跟裸机部署集群的不同==</p>
<ul>
<li>例如hostname：</li>
</ul>
<p>在docker中，无需要指明具体的ip地址，因为docker使用其内部私网为zk服务自动分配私网ip，而且自动DNS解析主机名，因此配置文件可以直接用zoo1这样的主机名</p>
<p>而在裸机部署中，裸机自己的网络设置需要指定具体IP地址，如果zoo.cfg配置用了主机名代替服务器IP，那么要求裸机网卡设定的DNS需支持zk网段的主机名解析</p>
<ul>
<li>再例如设定集群的server.n：</li>
</ul>
<p>在docker-compose里，直接主机名，server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 ，前面已经提过，docker内部其实已经对三个zk容器都分配相应的私网地址，通过以下命令查看：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出所有docker容器IP</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# docker inspect --format=&#x27;&#123;&#123;.Name&#125;&#125; - &#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27; $(docker ps -aq)</span><br><span class="line">/zk1 - 172.18.0.4</span><br><span class="line">/zk3 - 172.18.0.2</span><br><span class="line">/zk2 - 172.18.0.3</span><br><span class="line"><span class="meta">#</span><span class="bash"> 也可以用docker inspect zk2 查看容器内部具体的信息，这里截取一部分</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# docker inspect zk2</span><br><span class="line">        &quot;HostConfig&quot;: &#123;</span><br><span class="line">            &quot;Binds&quot;: [</span><br><span class="line">               # datalog:rw，说明docker对宿挂载的宿主机有读写权限</span><br><span class="line">                &quot;/opt/zk_docker_cluster/zoo2/datalog:/datalog:rw&quot;,</span><br><span class="line">                &quot;/opt/zk_docker_cluster/zoo2/data:/data:rw&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;ContainerIDFile&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LogConfig&quot;: &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;json-file&quot;,</span><br><span class="line">                &quot;Config&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;NetworkMode&quot;: &quot;zk_docker_cluster_default&quot;,</span><br><span class="line">            &quot;PortBindings&quot;: &#123;</span><br><span class="line">                # 绑定宿主机的端口号</span><br><span class="line">                &quot;2181/tcp&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;HostIp&quot;: &quot;&quot;,</span><br><span class="line">                        # zoo2容器内部的zk服务监听端口号</span><br><span class="line">                        &quot;HostPort&quot;: &quot;2182&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            </span><br><span class="line">            .......</span><br><span class="line">            </span><br><span class="line">            &quot;Networks&quot;: &#123;</span><br><span class="line">                &quot;zk_docker_cluster_default&quot;: &#123;</span><br><span class="line">                    &quot;IPAMConfig&quot;: null,</span><br><span class="line">                    &quot;Links&quot;: null,</span><br><span class="line">                    &quot;Aliases&quot;: [</span><br><span class="line">                        &quot;0595457ea13d&quot;,</span><br><span class="line">                        # hostname主机名</span><br><span class="line">                        &quot;zoo2&quot;</span><br><span class="line">                    ],</span><br><span class="line">                    &quot;NetworkID&quot;: &quot;46f7dbb34f0eefb1181729aeaaf6a1080d64a46fdba935b21d5e37a3b1aea34e&quot;,</span><br><span class="line">                    &quot;EndpointID&quot;: &quot;9dd1d2726ba6850ab851f0227fb04aa1a027c35e84d36cd308208a4d4fb42f7d&quot;,</span><br><span class="line">                    &quot;Gateway&quot;: &quot;172.18.0.1&quot;,</span><br><span class="line">                    &quot;IPAddress&quot;: &quot;172.18.0.3&quot;,</span><br><span class="line">                    &quot;IPPrefixLen&quot;: 16,</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>docker内部的私网段可以在宿主机上<code>ip a</code> 命令查看到，这是个docker的网桥网络，</p>
<p>地址池：172.18.0.1/16</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">8: br-46f7dbb34f0e: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:82:51:27:a6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-46f7dbb34f0e</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:82ff:fe51:27a6/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>三个zk容器服务分别从这个地址池获取三个ip，网关为172.18.0.1，相当于三台独立服务器，因此在server.n设置端口都可以指定为2888:3888相同端口，也即</p>
<p>zoo1:2888:3888等于172.18.0.4:2888:3888<br>zoo2:2888:3888等于172.18.0.3:2888:3888</p>
<p>zoo3:2888:3888等于172.18.0.2:2888:3888</p>
<p>总之，docker内部出色的网络结构设计，使得管理员从相对繁琐的网络配置解放出来。</p>
<p>而在单台裸机部署集群的配置中，则要指明ip（若有dns或者配置主机名解析，可无须指定IP地址）以及不同的管理端口号（使用不同端口是防止在同一服务器上端口冲突）：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server.1=192.168.4.100:42182:42183</span><br><span class="line">server.2=192.168.4.100:42184:42185</span><br><span class="line">server.3=192.168.4.100:42186:42187</span><br></pre></td></tr></table></figure>
<p><strong>启动docker-compose</strong></p>
<p>注意：所有的操作都应该在对应的docker-compse项目下进行，这是因为命令docker-compose自动读取本目录下的docker-compose.yml配置，注意这里仅当配置文件名为默认值<code>docker-compose.yml</code>，运行docker-compose命令才无需传入配置文件，否则如果项目目录下，yml配置文件为其它名字，例如</p>
<p>zk_docker_cluster.yml，每次执行docker-compose命令都需要指定配置文件：</p>
<p><code>docker-compose -f zk_docker_cluster.yml up -d</code></p>
<p>改了默认文件名的配置文件，在执行命令没传人配置文件，docker-compose提示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# ls</span><br><span class="line">zk_docker_cluster.yml  zoo1  zoo2  zoo3</span><br><span class="line">[root@dn2 zk_docker_cluster]# docker-compose ps</span><br><span class="line">ERROR: </span><br><span class="line">        Can&#x27;t find a suitable configuration file in this directory or any</span><br><span class="line">        parent. Are you in the right directory?</span><br><span class="line"></span><br><span class="line">        Supported filenames: docker-compose.yml, docker-compose.yaml</span><br><span class="line"><span class="meta">#</span><span class="bash"> 它这里提示在当前目录或者docker-compose的默认目录，都没有砸到配合文件，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 支持两种使用默认值命名的文件：docker-compose.yml, docker-compose.yaml</span></span><br></pre></td></tr></table></figure>
<p>执行相关命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# docker-compose  up -d</span><br><span class="line">Starting zk2 ... done</span><br><span class="line">Creating zk1 ... done</span><br><span class="line">Creating zk3 ... done</span><br><span class="line"></span><br><span class="line">[root@dn2 zk_docker_cluster]# docker-compose stop</span><br><span class="line">Stopping zk1 ... done</span><br><span class="line">Stopping zk3 ... done</span><br><span class="line">Stopping zk2 ... done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看zk_docker_cluster目录结构，目录路径由docker根据compose配置自动创建，无需手工预先创建</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# tree </span><br><span class="line">.</span><br><span class="line">├── docker-compose.yml</span><br><span class="line">├── zoo1</span><br><span class="line">│   ├── data</span><br><span class="line">│   │   ├── myid</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   │       ├── acceptedEpoch</span><br><span class="line">│   │       ├── currentEpoch</span><br><span class="line">│   │       ├── snapshot.0</span><br><span class="line">│   │       └── snapshot.400000000</span><br><span class="line">│   ├── datalog</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   └── logs</span><br><span class="line">├── zoo2</span><br><span class="line">│   ├── data</span><br><span class="line">│   │   ├── myid</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   │       ├── acceptedEpoch</span><br><span class="line">│   │       ├── currentEpoch</span><br><span class="line">│   │       ├── snapshot.0</span><br><span class="line">│   │       └── snapshot.400000000</span><br><span class="line">│   ├── datalog</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   └── logs</span><br><span class="line">└── zoo3</span><br><span class="line">    ├── data</span><br><span class="line">    │   ├── myid</span><br><span class="line">    │   └── version-2</span><br><span class="line">    │       ├── acceptedEpoch</span><br><span class="line">    │       ├── currentEpoch</span><br><span class="line">    │       ├── snapshot.0</span><br><span class="line">    │       └── snapshot.400000000</span><br><span class="line">    ├── datalog</span><br><span class="line">    │   └── version-2</span><br><span class="line">    └── logs</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可以直接进入容器内部查看</span></span><br><span class="line">[root@dn2 ~]# docker exec -it zk1 /bin/bash</span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在zk容器内部使用Cli登录并创建节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create /foo 1</span><br><span class="line">Created /foo</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create /app_conf 1</span><br><span class="line">Created /app_conf</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] ls /</span><br><span class="line">[zookeeper,app_conf,foo]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建临时顺序节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] create -e -s /foo 1</span><br><span class="line">Created /foo0000000001</span><br><span class="line">[zk: localhost:2181(CONNECTED) 7] create -e -s /foo 1</span><br><span class="line">Created /foo0000000002</span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] create -e -s /foo 1</span><br><span class="line">Created /foo0000000003</span><br></pre></td></tr></table></figure>
<p>在zk的docker容器内部，指定zk容器ip进入相应的服务</p>
<p>在前面已经给出三个zk服务在docker内部分配的私网IP，若想进入指定的zk容器，则需要用到这些私网网IP</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 三个zk服务器在docker内部分配到的IP</span></span><br><span class="line">(docker ps -aq)</span><br><span class="line">/zk3 - 172.18.0.3</span><br><span class="line">/zk1 - 172.18.0.4</span><br><span class="line">/zk2 - 172.18.0.2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 先选一个容器进入其内部，例如其内部zk client环境连接到其他zk服务实例，</span></span><br><span class="line">[root@dn2 bin]# docker exec -it zk1 /bin/bash</span><br><span class="line">root@zoo1:/zookeeper-3.4.14# </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接zk1服务</span></span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkCli.sh -server 172.18.0.4</span><br><span class="line">[zk: 172.18.0.4:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper, app_conf, foo]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接zk2服务</span></span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkCli.sh -server 172.18.0.2</span><br><span class="line">[zk: 172.18.0.2:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper, app_conf, foo]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接zk1服务</span></span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkCli.sh -server 172.18.0.3</span><br><span class="line">[zk: 172.18.0.3:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper, app_conf, foo]</span><br></pre></td></tr></table></figure>
<h3 id="3、用zk的四字命令查看zk集群状态"><a href="#3、用zk的四字命令查看zk集群状态" class="headerlink" title="3、用zk的四字命令查看zk集群状态"></a>3、用zk的四字命令查看zk集群状态</h3><p>&#8195;&#8195;通过进入容器查看zk状态显然不优雅，zk中有快捷的命令可以查看服务器的运行状态，它们的长度为4个英文字母缩写，又叫“四字命令”，需要结合nc命令，服务器安装nmap-ncat.x86_64（可通过yum install nc）</p>
<h4 id="state"><a href="#state" class="headerlink" title="state"></a>state</h4><p>stat命令用于获取zk的运行时状态信息，包括基本的zk版本、打包信息、运行时角色、集群数据节点个数等信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 详细信息</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2181    </span><br><span class="line">Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">Clients:</span><br><span class="line"> /172.18.0.1:38660[0](queued=0,recved=1,sent=0)</span><br><span class="line"></span><br><span class="line">Latency min/avg/max: 2/18/49</span><br><span class="line">Received: 7</span><br><span class="line">Sent: 6</span><br><span class="line">Connections: 1</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x300000002</span><br><span class="line">Mode: follower</span><br><span class="line">Node count: 4</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个zk实例的角色</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2181|grep Mode </span><br><span class="line">Mode: follower</span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2182|grep Mode </span><br><span class="line">Mode: follower</span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2183|grep Mode</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="conf"><a href="#conf" class="headerlink" title="conf"></a>conf</h4><p>conf命令用于输出ZooKeeper服务器运行时使用的基本配置信息，包括clientPort、dataDir和tickTime等。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo conf | nc 127.0.0.1 2181</span><br><span class="line">clientPort=2181</span><br><span class="line">dataDir=/data/version-2</span><br><span class="line">dataLogDir=/datalog/version-2</span><br><span class="line">tickTime=2000</span><br><span class="line">maxClientCnxns=60</span><br><span class="line">minSessionTimeout=4000</span><br><span class="line">maxSessionTimeout=40000</span><br><span class="line">serverId=1</span><br><span class="line">initLimit=5</span><br><span class="line">syncLimit=2</span><br><span class="line">electionAlg=3</span><br><span class="line">electionPort=3888</span><br><span class="line">quorumPort=2888</span><br><span class="line">peerType=0</span><br></pre></td></tr></table></figure>
<h4 id="mntr"><a href="#mntr" class="headerlink" title="mntr"></a>mntr</h4><p>mntr命令用于输出比stat命令更为详尽的服务器统计信息，包括请求处理的延迟情况、服务器内存数据库大小和集群的数据同步情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo mntr | nc 127.0.0.1 2181    </span><br><span class="line">zk_version      3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">zk_avg_latency  18</span><br><span class="line">zk_max_latency  49</span><br><span class="line">zk_min_latency  2</span><br><span class="line">zk_packets_received     8</span><br><span class="line">zk_packets_sent 7</span><br><span class="line">zk_num_alive_connections        1</span><br><span class="line">zk_outstanding_requests 0</span><br><span class="line">zk_server_state follower</span><br><span class="line">zk_znode_count  4</span><br><span class="line">zk_watch_count  0</span><br><span class="line">zk_ephemerals_count     0</span><br><span class="line">zk_approximate_data_size        27</span><br><span class="line">zk_open_file_descriptor_count   31</span><br><span class="line">zk_max_file_descriptor_count    1048576</span><br><span class="line">zk_fsync_threshold_exceed_count 0</span><br></pre></td></tr></table></figure>
<h4 id="crst"><a href="#crst" class="headerlink" title="crst"></a>crst</h4><p>crst命令是一个功能性命令(client reset)，用于重置所有的客户端连接统计信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo crst | nc 127.0.0.1 2181    </span><br><span class="line">Connection stats reset.</span><br></pre></td></tr></table></figure>
<h4 id="srvr"><a href="#srvr" class="headerlink" title="srvr"></a>srvr</h4><p>srvr命令和stat命令的功能一致，唯一的区别是srvr不会将客户端的连接情况输出，仅仅输出服务器的自身信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">imok[root@dn2 zk_docker_cluster]# echo srvr | nc 127.0.0.1 2181    </span><br><span class="line">Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">Latency min/avg/max: 0/0/49</span><br><span class="line">Received: 327</span><br><span class="line">Sent: 326</span><br><span class="line">Connections: 2</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x300000004</span><br><span class="line">Mode: follower</span><br><span class="line">Node count: 5</span><br></pre></td></tr></table></figure>
<h4 id="dump"><a href="#dump" class="headerlink" title="dump"></a>dump</h4><p>dump命令用于输出当前集群的所有会话信息，包括这些会话的会话ID，以及每个会话创建的临时节点等信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo dump | nc 127.0.0.1 2181</span><br><span class="line">SessionTracker dump:</span><br><span class="line">org.apache.zookeeper.server.quorum.LearnerSessionTracker@77315813</span><br><span class="line">ephemeral nodes dump:</span><br><span class="line">Sessions with Ephemerals (0):</span><br></pre></td></tr></table></figure>
<h4 id="envi"><a href="#envi" class="headerlink" title="envi"></a>envi</h4><p>envi命令用于输出ZooKeeper所在服务器环境以及一些runtime环境，包括os.version、java.version和user.home等</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo envi | nc 127.0.0.1 2181    </span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">host.name=zoo1</span><br><span class="line">java.version=1.8.0_222</span><br><span class="line">java.vendor=Oracle Corporation</span><br><span class="line">java.home=/usr/local/openjdk-8</span><br><span class="line">java.class.path=/zookeeper-3.4.14/bin/../zookeeper-server/target/classes:/zookeeper-3.4.14/bin/../build/classes:/zookeeper-3.4.14/bin/../zookeeper-server/target/lib/*.jar:/zookeeper-3.4.14/bin/../build/lib/*.jar:/zookeeper-3.4.14/bin/../lib/slf4j-log4j12-1.7.25.jar:/zookeeper-3.4.14/bin/../lib/slf4j-api-1.7.25.jar:/zookeeper-3.4.14/bin/../lib/netty-3.10.6.Final.jar:/zookeeper-3.4.14/bin/../lib/log4j-1.2.17.jar:/zookeeper-3.4.14/bin/../lib/jline-0.9.94.jar:/zookeeper-3.4.14/bin/../lib/audience-annotations-0.5.0.jar:/zookeeper-3.4.14/bin/../zookeeper-3.4.14.jar:/zookeeper-3.4.14/bin/../zookeeper-server/src/main/resources/lib/*.jar:/conf:</span><br><span class="line">java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span><br><span class="line">java.io.tmpdir=/tmp</span><br><span class="line">java.compiler=&lt;NA&gt;</span><br><span class="line">os.name=Linux</span><br><span class="line">os.arch=amd64</span><br><span class="line">os.version=3.10.0-957.27.2.el7.x86_64</span><br><span class="line">user.name=zookeeper</span><br><span class="line">user.home=/home/zookeeper</span><br><span class="line">user.dir=/zookeeper-3.4.14</span><br></pre></td></tr></table></figure>
<h4 id="ruok"><a href="#ruok" class="headerlink" title="ruok"></a>ruok</h4><p>ruok命令用于输出当前ZooKeeper服务器是否正在运行，“Are you ok”的缩写？如果当前ZooKeeper服务器正在运行，那么返回“imok”(I am ok)，否则没有任何响应输出。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo ruok | nc 127.0.0.1 2181    </span><br><span class="line">imok</span><br></pre></td></tr></table></figure>
<h4 id="wchs"><a href="#wchs" class="headerlink" title="wchs"></a>wchs</h4><p>wchs命令用于输出当前服务器上管理的Watcher的概要信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo wchs | nc 127.0.0.1 2181     </span><br><span class="line"> connections watching 0 paths</span><br><span class="line">Total watches:0</span><br></pre></td></tr></table></figure>
<h4 id="wchc"><a href="#wchc" class="headerlink" title="wchc"></a>wchc</h4><p>wchc命令用于输出当前服务器上管理的Watcher的详细信息，以会话为单位进行归组，同时列出被该会话注册了Watcher的节点路径</p>
<h4 id="wchp"><a href="#wchp" class="headerlink" title="wchp"></a>wchp</h4><p>wchp命令和wchc命令非常类似，也是用于输出当前服务器上管理的Watcher的详细信息，不同点在于wchp命令的输出信息以节点路径为单位进行归组。</p>
]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
  </entry>
  <entry>
    <title>基于Gitee Pages和Hexo搭建个人开源博客</title>
    <url>/2020/03/22/%E5%9F%BA%E4%BA%8EGitee%20Pages%E5%92%8CHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%BC%80%E6%BA%90%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>&#8195;&#8195;本blog用于归档如何在GitHub搭建个人博客的过程，内容参考来自本篇文章<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog">《如何用 GitHub 从零开始搭建一个博客》</a> 以及hexo中文官网的<a href="https://hexo.io/zh-cn/docs/">文档</a>。</p>
<p>更新：这篇文章写于今年年初，个人博客在年初已使用GitHub Pages搭建开源博客主页，今年下半年GitHub Pages在国内出现了某一时期无法正常访问，因此将其切换到国内Gitee Pages以保证主页正常访问，确实也需支持国内开源平台。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112219074494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<a id="more"></a>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><h4 id="选用GitHub-Gitee发布开源博客的理由"><a href="#选用GitHub-Gitee发布开源博客的理由" class="headerlink" title="选用GitHub/Gitee发布开源博客的理由"></a>选用GitHub/Gitee发布开源博客的理由</h4><p>个人就使用CSDN相关看法：</p>
<ul>
<li>优势：<br>首先CSDN流量不愁，不同博主之间可以进行留言、私信等方式进行技术交流等，其次CSDN提供不错Markdown编辑器工具，例如在线草稿、离线草稿、图片拖曳与上传、定时保存等功能相对完善。</li>
<li>不足：<br>发布文章后，博客页面两栏窗口过多广告，对于推崇简洁主义的开发者来说（尤其已经习惯MacOS暗黑模式高度focus的UI）较为烦人，毕竟CSDN平台运营存在大量的商业行为。<br>而GitHub/Gitee搭建的博客为开源博客，以静态文件方式发布，通过整合一些博客框架提供的简洁模板，可以为技术文章撰写者和阅读者提供“无打扰”的简约而清爽的阅读环境。</li>
</ul>
<h4 id="GitHub-Gitee可构建个人开源博客主页"><a href="#GitHub-Gitee可构建个人开源博客主页" class="headerlink" title="GitHub/Gitee可构建个人开源博客主页"></a>GitHub/Gitee可构建个人开源博客主页</h4><p>GitHub/Gitee上除了最重要的代码仓库功能，还有GitHub Pages （国内为Gitee Pages）功能，可用于部署静态网页文件，只要发布者整合基本的工具，通过博客框架将本地Markdown文件编译为静态网页文件，再部署到GitHub/Gitee仓库，访问URL即可看到该静态网页。因此技术撰写者需要做的事情为：</p>
<ul>
<li>配置GitHub /GitHub Pages 本地博客开发环境</li>
<li>在本地完成Markdown文章</li>
<li>push 到GitHub/Gitee完成公网的博客发布</li>
</ul>
<p>目前GitHub Pages在国内已无法正常访问（VPN除外），因此开源博客主页在个人的Gitee仓库部署。</p>
<h3 id="Gitee-pages相关环境配置"><a href="#Gitee-pages相关环境配置" class="headerlink" title="Gitee pages相关环境配置"></a>Gitee pages相关环境配置</h3><h4 id="新建一个Repository用于存博客文件"><a href="#新建一个Repository用于存博客文件" class="headerlink" title="新建一个Repository用于存博客文件"></a>新建一个Repository用于存博客文件</h4><p>首先在个人Gitee 新建一个仓库，名称为blog，当然gitee开启pages功能后，主页url会被gitee自动设为</p>
<p><code>https://yield-bytes.gitee.io/blog</code></p>
<p>需注意：若基于GitHub Pages 部署博客，需要按以下规则建立仓库名称：</p>
<p>名称为 {yourblogname}.github.io，仓库名称必须以github.io 为后缀结尾，国内的gitee不需要此方式。</p>
<h4 id="本地git的配置"><a href="#本地git的配置" class="headerlink" title="本地git的配置"></a>本地git的配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name  &quot;用户名&quot; </span><br><span class="line"></span><br><span class="line">注意这里gitee的邮箱配置：如果在gitee设置了不公开个人邮箱，那么gitee会帮你设置一个隐私邮箱***@user.noreply.gitee.com，后续git命令提交都需要设置这个隐私邮箱，而不是设置注册邮箱</span><br><span class="line"></span><br><span class="line">git config --global user.email &quot;***@user.noreply.gitee.com&quot; </span><br><span class="line"></span><br><span class="line"># 避免git gui中的中文乱码</span><br><span class="line">git config --global gui.encoding utf-8</span><br><span class="line"></span><br><span class="line"># 避免git status显示的中文名乱码</span><br><span class="line">git config --global core.quotepath off</span><br></pre></td></tr></table></figure>
<h4 id="配置码云-ssh-key"><a href="#配置码云-ssh-key" class="headerlink" title="配置码云 ssh key"></a>配置码云 ssh key</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &#39;注册码云的邮箱&#39;</span><br></pre></td></tr></table></figure>
<p>其中-t指定密钥类型，这里设置rsa即可，-C是密钥的注释，这里设置成邮箱方便分辨</p>
<p>将公钥打印出来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat ~&#x2F;.ssh&#x2F;id_rsa.pub</span><br></pre></td></tr></table></figure>
<p> 在码云上添加ssh 公钥：在个人设置里面找到相应的ssh key添加界面处理即可</p>
<p>测试连接是否成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**** % ssh gitee@gitee.com</span><br><span class="line">Hi ***! You&#39;ve successfully authenticated, but GITEE.COM does not provide shell access.</span><br><span class="line">Connection to gitee.com closed.</span><br></pre></td></tr></table></figure>
<p>测试往仓库推一个新建文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir MyTecBlog</span><br><span class="line">cd MyTecBlog</span><br><span class="line">git init</span><br><span class="line">touch README.md</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;首次提交&quot;</span><br><span class="line">git remote add origin git@gitee.com:yield-bytes&#x2F;blog.git # 指定push的线上仓库，这里就是前面创建的blog仓库</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure>
<p>在web端的gitee仓库可看到README.md文件以及提交记录</p>
<h3 id="Hexo环境配置"><a href="#Hexo环境配置" class="headerlink" title="Hexo环境配置"></a>Hexo环境配置</h3><p>Hexo依赖node.js，借用相关npm包，使得搭建出来的GitHub博客不会过于简单。<br>MacOS直接用brew安装即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install node </span><br><span class="line">node -v # node.js的版本</span><br><span class="line">npm -v # 包管理版本</span><br></pre></td></tr></table></figure>
<p>安装 Hexo博客框架<br>借助Hexo博客框架，可以快速部署和设计博客，Hexo自带命令行命令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#拉取源设置为淘宝镜像</span><br><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<p>在macOS安装 hexo-cli可能会提示未安装xcode CommandLineTools</p>
<ul>
<li><p><code>xcode-select --print-path</code>查看 command-line tools 的安装路径，不出意外显示的结果应该是<code>/Library/Developer/CommandLineTools</code></p>
</li>
<li><p><code>sudo rm -r -f /Library/Developer/CommandLineTools，</code>卸载 command-line tools </p>
</li>
<li><code>xcode-select --install，重新安装</code>command-line tools </li>
</ul>
<h3 id="创建hexo博客项目"><a href="#创建hexo博客项目" class="headerlink" title="创建hexo博客项目"></a>创建hexo博客项目</h3><p>创建一个名为yield-bytes的博客项目（名字可以任意取）<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder hexo init yield-bytes</span><br><span class="line">yymac@wonder yield-bytes % ls</span><br><span class="line">_config.yml		package.json		themes</span><br><span class="line">node_modules		scaffolds</span><br><span class="line">package-lock.json	source</span><br></pre></td></tr></table></figure><br>可以看到yield-bytes为一个目录，该目录下有相应的node模块目录、配置文件、主题目录等。<br>将项目编译成静态文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo generate</span><br></pre></td></tr></table></figure>
<p>项目目录下多了一个public目录，可以看到这些就是静态网页内容：css、html、js等<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder public % ls</span><br><span class="line">2020		css		index.html</span><br><span class="line">archives	fancybox	js</span><br></pre></td></tr></table></figure><br>其中index.html就是hexo的demo博客展示页面，通过在public目录下运行<code>hexo server</code>将页面作为web服务跑起来<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder public % hexo server</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure><br>在浏览器可以看到hexo的demo博客主页</p>
<h4 id="将本地demo博客部署到gitee"><a href="#将本地demo博客部署到gitee" class="headerlink" title="将本地demo博客部署到gitee"></a>将本地demo博客部署到gitee</h4><p>只需要在hexo创建的项目目录下的<code>_config.yml</code>加入gitee.io仓库的相关配置，hexo可一键部署到gitee上。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p>修改<code>_config.yml</code>，需要修改两处地方<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line"># URL</span><br><span class="line"># If your site is put in a subdirectory, set url as &#x27;http://example.com/child&#x27; and root as &#x27;/child/&#x27;</span><br><span class="line">url: https://yield-bytes.gitee.io/blog</span><br><span class="line">root: /blog/</span><br><span class="line">  </span><br><span class="line"># Deployment</span><br><span class="line"># Docs: https://hexo.io/docs/one-command-deployment</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@gitee.com:yield-bytes/blog.git</span><br><span class="line">  branch: master</span><br><span class="line">  </span><br></pre></td></tr></table></figure></p>
<p>上述提示本地静态网页文件已经push到gitee的blog仓库<br>打开网址<code>https://yield-bytes.gitee.io/blog</code>，即可看到demo主页</p>
<h3 id="配置和完善个人博客"><a href="#配置和完善个人博客" class="headerlink" title="配置和完善个人博客"></a>配置和完善个人博客</h3><p>前面章节仅完成基本的搭建，从本章开始，将完善个人博客的配置以及UI。在<code>_config.yml</code>文件里面，分了很多部分，都可用于配置博客不同功能</p>
<h4 id="修改博客简介等基本内容："><a href="#修改博客简介等基本内容：" class="headerlink" title="修改博客简介等基本内容："></a>修改博客简介等基本内容：</h4> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  4 # Site</span><br><span class="line"> 5 title: Yield-Bytes</span><br><span class="line"> 6 subtitle: &#39;分享与沉淀&#39;</span><br><span class="line"> 7 description: &#39;这是一个非常专注于技术总结与分享的博客&#39;</span><br><span class="line"> 8 keywords: &quot;Python,BigData,Web开发,数据分析,深度学习&quot;</span><br><span class="line"> 9 author: yield-bytes</span><br><span class="line">10 language: zh-CN</span><br><span class="line">11 timezone: &#39;Asia&#x2F;Shanghai&#39;</span><br></pre></td></tr></table></figure>
<p> 本地运行刷新即可看到主页修改后的效果。</p>
<h4 id="为博客设置主题"><a href="#为博客设置主题" class="headerlink" title="为博客设置主题"></a>为博客设置主题</h4><p>博客当然可以设置为不同风格的UI，称为主题，在前十年那会，网易163博客还很流行，博客主可根据平台提供不同模板将自己的博客打造更加高级，有些模板还需要VIP或者付费。在当今开源时代，Hexo提供很多不错的博客模板，网址:<code>https://hexo.io/themes/</code>，可直接clone使用，以next主题为例子的配置过程：<br>在<code>yield-bytes/themes</code>目录下仅有一个默认的landscape主题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yymac@wonder themes % ls</span><br><span class="line">landscape</span><br></pre></td></tr></table></figure>
<p>将next主题拉到该目录下<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder themes % ls</span><br><span class="line">yymac@wonder themes % git clone https://github.com/theme-next/hexo-theme-next</span><br><span class="line">hexo-theme-next	landscape</span><br></pre></td></tr></table></figure></p>
<p>在yield-bytes根目录下修改_config.yml 文件，在 theme 配置部分，修改为 hexo-theme-next:<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">4 # Extensions</span><br><span class="line">3 ## Plugins: https://hexo.io/plugins/</span><br><span class="line">2 ## Themes: https://hexo.io/themes/ # 官方提供更多开源的主题以及插件</span><br><span class="line">1 theme:  hexo-theme-next  # 更换主题</span><br></pre></td></tr></table></figure><br>重启hexo server即可看到主页已经换成next主题</p>
<h4 id="对主题进行深度定制"><a href="#对主题进行深度定制" class="headerlink" title="对主题进行深度定制"></a>对主题进行深度定制</h4><ul>
<li>更换样式</li>
</ul>
<p>每个主题的目录下也有一个<code>_config.yml</code>博客样式配置文件，这里可进行深度定制<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder hexo-theme-next % ls</span><br><span class="line">LICENSE.md	crowdin.yml	languages	scripts</span><br><span class="line">README.md	docs		layout		source</span><br><span class="line">_config.yml	gulpfile.js	package.json</span><br></pre></td></tr></table></figure><br>next 默认有四个样式，这里设为Pisces样式，也可打开暗黑模式<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br><span class="line"></span><br><span class="line"># Dark Mode</span><br><span class="line">darkmode: true</span><br></pre></td></tr></table></figure></p>
<ul>
<li>更换title的logo</li>
</ul>
<p>由于logo需要出裁剪为一定尺寸的png图片，需自行设计。在next主题下的<code>_config.yml</code>的favicon部分可进行更换logo的配置。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">   small: &#x2F;images&#x2F;favicon-16x16-next.png</span><br><span class="line">   medium: &#x2F;images&#x2F;favicon-32x32-next.png</span><br><span class="line">   apple_touch_icon: &#x2F;images&#x2F;apple-touch-icon-next.png</span><br><span class="line">   safari_pinned_tab: &#x2F;images&#x2F;logo.svg</span><br></pre></td></tr></table></figure></p>
<ul>
<li>avatar 设置头像后者主页的标识图像</li>
</ul>
<p>只需将图片放在该路径：themes/hexo-theme-next/source/images 路径，在next主题下的<code>_config.yml</code>文件下配置为该图片路径，也可设为网络图片路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7 avatar:</span><br><span class="line">6   # Replace the default image and set the url here.</span><br><span class="line">5   url: #&#x2F;images&#x2F;avatar.gif</span><br><span class="line"># url: #&#x2F;images&#x2F;avatar.gif</span><br><span class="line">4   # If true, the avatar will be dispalyed in circle.</span><br><span class="line">3   rounded: true</span><br><span class="line">2   # If true, the avatar will be rotated with the cursor.</span><br><span class="line">1   rotated: false</span><br></pre></td></tr></table></figure>
<ul>
<li><p>为个人博客开启RSS(用处不大)<br>在hexo创建的博客项目目录下安装feed插件,博客项目安装的所有插件都放置在node_modules目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder  yield-bytes % npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure>
<p>安装完成之后不需要其他的配置，以后每次编译生成站点的时候就会自动生成 RSS Feed 文件</p>
</li>
<li><p>修改code代码的显示样式</p>
</li>
</ul>
<p>在文章中，经常需要贴上代码，为保证阅读效果，可将默认浅灰色样式设为Mac样式，看起还不错。<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">12 codeblock:</span><br><span class="line">11   # Code Highlight theme</span><br><span class="line">10   # Available values: normal | night | night eighties | night blue | night  bright | solarized | solarized dark | galactic</span><br><span class="line"> 9   # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line"> 8   highlight_theme: solarized dark</span><br><span class="line"> 7   # Add copy button on codeblock</span><br><span class="line"> 6   copy_button:</span><br><span class="line"> 5     enable: true</span><br><span class="line"> 4     # Show text copy result.</span><br><span class="line"> 3     show_result: true</span><br><span class="line"> 2     # Available values: default | flat | mac</span><br><span class="line"> 1     style: mac</span><br></pre></td></tr></table></figure></p>
<p>default样式：<br><img src="https://img-blog.csdnimg.cn/20200201140722675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>mac样式：<br><img src="https://img-blog.csdnimg.cn/2020020114043519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>back2top设置页面滚动逻辑、阅读进度</li>
</ul>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">14 back2top:</span><br><span class="line">13   enable: true</span><br><span class="line">12   # Back to top in sidebar.</span><br><span class="line">11   sidebar: true</span><br><span class="line">10   # Scroll percent label in b2t button.</span><br><span class="line"> 9   scrollpercent: true</span><br><span class="line"> 8</span><br><span class="line"> 7 # Reading progress bar</span><br><span class="line"> 6 reading_progress:</span><br><span class="line"> 5   enable: true</span><br><span class="line"> 4   # Available values: top | bottom</span><br><span class="line"> 3   position: top</span><br><span class="line"> 2   color: &quot;#37c6c0&quot;</span><br><span class="line"> 1   height: 3px</span><br><span class="line"></span><br><span class="line">10 # Bookmark Support</span><br><span class="line"> 9 bookmark:</span><br><span class="line"> 8   enable: true</span><br><span class="line"> 7   # Customize the color of the bookmark.</span><br><span class="line"> 6   color: &quot;#222&quot;</span><br><span class="line"> 5   # If auto, save the reading progress when closing the page or clicking th     e bookmark-icon.</span><br><span class="line"> 4   # If manual, only save it by clicking the bookmark-icon.</span><br><span class="line"> 3   save: auto</span><br><span class="line"> 2</span><br></pre></td></tr></table></figure>
<p>打开Bookmark Support功能后，可以提示阅读体验，例如关闭该文章后，再打开浏览，可以恢复到上次阅读位置，类似微信阅读文章的体验。</p>
<ul>
<li>为每篇文章关联GitHub Banner</li>
</ul>
<p>大家阅读一些技术文章应该经常看到文章右上角有GitHub的图标，该链接就是去GitHub Repository:<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">   4 # `Follow me on GitHub` banner in the top-right corner.</span><br><span class="line">   3 github_banner:</span><br><span class="line">   2   enable: true</span><br><span class="line">   1   permalink: https://github.com/yield-bytes/yield-bytes.github.io</span><br><span class="line">403    title: Follow me on GitHub</span><br></pre></td></tr></table></figure></p>
<ul>
<li>为博客正确显示math 的Markdown语法</li>
</ul>
<p>为了能让文章中能正常显示数学公式（常见数据挖掘、深度学习等文章），可通过hexo为next主题引入相关插件。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % npm install  hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p>
<p>注意：这里不要安装hexo-renderer-pandoc，该库的js有bug，无法正确解析Markdown文章，会导致hexo运行报错</p>
<p>在主题配置打开math配置即可：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">   1 math:</span><br><span class="line">512    enable: true</span><br><span class="line">   1   # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">   2   # That is it only render those page which has `mathjax: true` in Front-ma     tter.</span><br><span class="line">   3   # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">   4   per_page: true</span><br><span class="line">   5</span><br><span class="line">   6   # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJa     x support.</span><br><span class="line">   7   mathjax:</span><br><span class="line">   8     enable: true</span><br><span class="line">   9     # See: https://mhchem.github.io/MathJax-mhchem/</span><br><span class="line">  10     mhchem: true</span><br></pre></td></tr></table></figure>
<ul>
<li>开启无刷新加载页面</li>
</ul>
<p>为进一步提升博客体验，hexo支持页面实现无刷新加载，借助pjax插件即可。<br>首先在配置文件里开启pjax</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">1   # pjax: //cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js</span><br><span class="line">938    pjax: true</span><br></pre></td></tr></table></figure>
<p>在hexo-theme-next目录下，安装该插件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder hexo-theme-next % pwd</span><br><span class="line">***/yield-bytes/themes/hexo-theme-next</span><br><span class="line">yymac@wonder hexo-theme-next % git clone https://github.com/theme-next/theme-next-pjax source/lib/pjax</span><br></pre></td></tr></table></figure></p>
<p>到处，已经完成对搭建博客的较为深度的定制，其他定制可参考官网：<a href="https://theme-next.org/docs/getting-started/，这里不再累赘。">https://theme-next.org/docs/getting-started/，这里不再累赘。</a></p>
<h3 id="文章设置"><a href="#文章设置" class="headerlink" title="文章设置"></a>文章设置</h3><p>前置章节主要对博客主题及其UI做定制配置，本章节介绍如何在搭建的Gitee Pages上发布新文章等内容。</p>
<h4 id="增加文章"><a href="#增加文章" class="headerlink" title="增加文章"></a>增加文章</h4><p>新增一篇名为《深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）》的文章<br>在本地yield-bytes根目录创建文章，文章类型为Markdown格式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo new 深入理解异步IO的底层逻辑——IO多路复 用（select、poll、epoll）</span><br><span class="line">INFO  Created:***yield-bytes/source/_posts/深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）.md</span><br></pre></td></tr></table></figure>
<p>所有新建的文章的都拷贝到该目录下<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % cd source/_posts/</span><br><span class="line">yymac@wonder _posts % ls</span><br><span class="line">hello-world.md</span><br><span class="line">深入理解异步IO的底层逻辑——IO多路复用(select、poll、epoll).md</span><br></pre></td></tr></table></figure><br>每篇文章的头部为该文章的元数据，例如标题，创建时间，标签，文章分类，有关文章属性更为详细的配置，可以参考：<a href="https://hexo.io/zh-cn/docs/writing.html">https://hexo.io/zh-cn/docs/writing.html</a><br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">  ---</span><br><span class="line">  title: 深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）</span><br><span class="line">  date: 2020-02-01 21:50:46</span><br><span class="line">tags: </span><br><span class="line"><span class="bullet">-</span> IO多路复用</span><br><span class="line"><span class="bullet">-</span> epoll</span><br><span class="line">categories:</span><br><span class="line"><span class="bullet">-</span> Python进阶</span><br><span class="line">  ---</span><br></pre></td></tr></table></figure><br>头部元数据之后就是Markdown的正文</p>
<h4 id="修改文章字体"><a href="#修改文章字体" class="headerlink" title="修改文章字体"></a>修改文章字体</h4><p>在next主题的配置文件<code>_config.yml</code>里面，font 部分可以设置文章显示字体，默认博客站点的字体大小为1，个人在global设为0.8后，文字看起相对舒服<br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">16 font:</span><br><span class="line">15   enable: true</span><br><span class="line">14</span><br><span class="line">13   # Uri of fonts host, e.g. //fonts.googleapis.com (Default).</span><br><span class="line">12   host:</span><br><span class="line">11</span><br><span class="line">10   # Font options:</span><br><span class="line"> 9   # <span class="code">`external: true`</span> will load this font family from <span class="code">`host`</span> above.</span><br><span class="line"> 8   # <span class="code">`family: Times New Roman`</span>. Without any quotes.</span><br><span class="line"> 7   # <span class="code">`size: x.x`</span>. Use <span class="code">`em`</span> as unit. Default: 1 (16px)</span><br><span class="line"> 6</span><br><span class="line"> 5   # Global font settings used for all elements inside <span class="xml"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span>.</span><br><span class="line"> 4   global:</span><br><span class="line"> 3     external: true</span><br><span class="line"> 2     family: Lato</span><br><span class="line"> 1     size: 0.8</span><br></pre></td></tr></table></figure></p>
<h4 id="为博客左侧增加标签页、分类页等链接"><a href="#为博客左侧增加标签页、分类页等链接" class="headerlink" title="为博客左侧增加标签页、分类页等链接"></a>为博客左侧增加标签页、分类页等链接</h4><p>上面搭建的博客仅有少量栏目例如文章栏目、主页栏目，每次发布文章前，我们需要为其打个标签，以便当文章数量较多时，读者可在标签页查看相关的标签，从而快速找到相关文章，例如一篇streaming和kafka整合的文章，标签可以打为：streaming、kafka、实时流计算等。此外还需多文章进行分类，例如Python进阶分类、spark相关的分类、数据分析与挖掘的分类、数据结构与算法的分类、Linux相关的分类等<br>创建标签页和分类页：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo new page tags</span><br><span class="line">INFO  Created: ***/yield-bytes/source/tags/index.md</span><br><span class="line"></span><br><span class="line">yymac@wonder yield-bytes % hexo new page categories</span><br><span class="line">INFO  Created: ***/yield-bytes/source/categories/index.md</span><br></pre></td></tr></table></figure>
<p>创建的标签页以及分类页都是Markdown文件<br>标签页的md：<br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: tags</span><br><span class="line">  2 date: <span class="strong">****</span> 10:58:03</span><br><span class="line">  3 ---</span><br><span class="line">~</span><br></pre></td></tr></table></figure><br>分类页的md：<br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: categories</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 11:32:51</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 ---</span></span></span><br></pre></td></tr></table></figure></p>
<p>将tags的md指定为标签页：<br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: tags</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 10:58:03</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 type: tags</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  4 comments: false  # 这里表示关闭当前文章的评论</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  5 ---</span></span></span><br></pre></td></tr></table></figure><br>将categories制定为分类页：<br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: categories</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 11:32:51</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 type: categories</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  4 comments: false</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  5 ---</span></span></span><br></pre></td></tr></table></figure></p>
<p>在next主题的配置文章的menu部分新增tags访问路径和categories访问路径，这里就是配置页面路由的地方，可自行新增路径。</p>
<p>配置说明：<code>tags: /tags/ || fa fa-tags</code>表示tags的路径为/tags/，对应的icon图标为fa fa-tags</p>
<p>这里有一个图标与名称对应的地址：<code>https://v3.bootcss.com/components/#glyphicons</code>  ，将选中的icon图标名称如glyphicons glyphicons-th-large改为fa fa-th-large即可</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  #about: /about/ || fa fa-user</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th-large</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line">  #schedule: /schedule/ || fa fa-calendar</span><br><span class="line">  #sitemap: /sitemap.xml || fa fa-sitemap</span><br><span class="line">  #commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure>
<p>刷新后访问<code>http://localhost:4000/tags/</code>即可<br><img src="https://img-blog.csdnimg.cn/20200202132540606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200202132613168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">若需要对文章添加多个分类，用以下格式书写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">categories:</span><br><span class="line">	- [Spark,kafka]</span><br></pre></td></tr></table></figure>
<p>再增加一个404页面：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new 404</span><br></pre></td></tr></table></figure>
<p>在yield-bytes/source/404/路径下，更改index.md内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">   title: 404 Not Found</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">   &lt;center&gt;</span><br><span class="line">   对不起，您所访问的页面不存在或者已删除。</span><br><span class="line">   您可以&lt;a href&#x3D;&quot;https:&#x2F;&#x2F;yield-bytes.gitee.io&#x2F;blog&quot;&gt;点击此处&lt;&#x2F;a&gt;返回首页。</span><br><span class="line">   &lt;&#x2F;center&gt;</span><br><span class="line"></span><br><span class="line">  &lt;blockquote class&#x3D;&quot;blockquote-center&quot;&gt;</span><br><span class="line">      yield-bytes</span><br><span class="line">  &lt;&#x2F;blockquote&gt;</span><br></pre></td></tr></table></figure>
<h4 id="博客左侧menu菜单显示图片"><a href="#博客左侧menu菜单显示图片" class="headerlink" title="博客左侧menu菜单显示图片"></a>博客左侧menu菜单显示图片</h4><p>在next主题的配置文件_config.yml下加入Avatar地址即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar:</span><br><span class="line">  # Replace the default image and set the url here.</span><br><span class="line">  url: your avatar image url</span><br><span class="line">  # If true, the avatar will be dispalyed in circle.</span><br><span class="line">  rounded: true</span><br><span class="line">  # If true, the avatar will be rotated with the cursor.</span><br><span class="line">  rotated: false</span><br></pre></td></tr></table></figure>
<h4 id="多篇文章在首页的展示逻辑"><a href="#多篇文章在首页的展示逻辑" class="headerlink" title="多篇文章在首页的展示逻辑"></a>多篇文章在首页的展示逻辑</h4><p>若博客已发布多篇文章，next主题默认在首页里将在可视化区域窗口内展示所有文章的所有完整内容，而不是只显示每篇文章的摘要部分，这将导致无法预览多篇文章内容。处理方式很简单，hexo支持对每篇文章提供只显示摘要部分，文章的更多内容则用more提示来指引读者。<br>处理方式：<br>在每篇Markdown文章比较靠前的位置，加入<code>&lt;!--more--&gt;</code>标识即可，例如下面的两篇文章：<br>第一篇文章，在首页中，只给它显示前言的内容即可<br><img src="https://img-blog.csdnimg.cn/2020020213171413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>第二篇spark streaming的文章，在首页，将该文的第1章节前面几句作为文章摘要显示<br><img src="https://img-blog.csdnimg.cn/2020020213193456.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最后在查看首尔显示效果：<br><img src="https://img-blog.csdnimg.cn/20200202132150362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到首页的多篇文章只展示<code>&lt;!--more--&gt;</code> 前面的内容。</p>
<h4 id="为博客增加全站搜索功能"><a href="#为博客增加全站搜索功能" class="headerlink" title="为博客增加全站搜索功能"></a>为博客增加全站搜索功能</h4><p>开启博客全文搜索功能需要加入相关插件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></p>
<p>首先在yield-bytes项目的<code>_config.yml</code>新增搜索配置：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">2 search:</span><br><span class="line">3   path: search.xml</span><br><span class="line">4   field: post</span><br><span class="line">5   format: html</span><br><span class="line">6   limit: 1000</span><br></pre></td></tr></table></figure></p>
<p>再到next主题的<code>_config.yml</code>开启相关检索配置，这里启动 Local Search功能，这里的配置也提示了需要安装依赖hexo-generator-searchdb插件：<br><figure class="highlight md"><table><tr><td class="code"><pre><span class="line">   8 # Local Search</span><br><span class="line">   7 # Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span><br><span class="line">   6 local<span class="emphasis">_search:</span></span><br><span class="line"><span class="emphasis">   5   enable: true</span></span><br><span class="line"><span class="emphasis">   4   # If auto, trigger search by changing input.</span></span><br><span class="line"><span class="emphasis">   3   # If manual, trigger search by pressing enter key or search button.</span></span><br><span class="line"><span class="emphasis">   2   trigger: auto</span></span><br><span class="line"><span class="emphasis">   1   # Show top n results per article, show all results by setting to -1</span></span><br><span class="line"><span class="emphasis">761    top_</span>n<span class="emphasis">_per_</span>article: 5</span><br><span class="line">   1   # Unescape html strings to the readable one.</span><br><span class="line">   2   unescape: false</span><br><span class="line">   3   # Preload the search data when the page loads.</span><br><span class="line">   4   preload: false</span><br><span class="line">   5</span><br></pre></td></tr></table></figure><br>上述表示输入关键字后自动触发搜索，并只显示5条命中记录。从效果来看，不得不佩服hexo博客框架（结合各类第三方插件）确实强大。（在独立开发的博客项目中，若要启用全文检索，则需引入elasticsearch技术栈）<br><img src="https://img-blog.csdnimg.cn/20200202151834914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>至此，博客的文章板块已经定制完毕，重新编译部署到gitee即可看到效果</p>
<h4 id="为GitHub-Pages配置自定义域名-注意以下不是Gitee-Pages的配置"><a href="#为GitHub-Pages配置自定义域名-注意以下不是Gitee-Pages的配置" class="headerlink" title="为GitHub Pages配置自定义域名(注意以下不是Gitee Pages的配置)"></a>为GitHub Pages配置自定义域名(注意以下不是Gitee Pages的配置)</h4><p>GitHub pages为git用户提供免费的自定义域名服务，所搭建的博客站点可无需使用类似<code>https://yourrepo.github.io</code>作为访问地址</p>
<p>在github博客项目仓库的Settings里面的GitHub Pages 可进行自定义域名设置：</p>
<p><a href="https://io.yield-bytes.cn">https://io.yield-bytes.cn</a></p>
<h4 id="为文章添加字数统计和阅读时长"><a href="#为文章添加字数统计和阅读时长" class="headerlink" title="为文章添加字数统计和阅读时长"></a>为文章添加字数统计和阅读时长</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_wordcount:</span><br><span class="line">  item_text: true</span><br><span class="line">  wordcount: true         # 单篇 字数统计</span><br><span class="line">  min2read: true          # 单篇 阅读时长</span><br><span class="line">  totalcount: false       # 网站 字数统计</span><br><span class="line">  separated_meta: true</span><br><span class="line">  </span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<p>安装相关插件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % $ npm install eslint --save</span><br><span class="line">yymac@wonder yield-bytes % $ npm install hexo-wordcount --save</span><br><span class="line">yymac@wonder yield-bytes % $ npm install hexo-symbols-count-time --save</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在博客yield-bytes项目根目录的_config.yml文件的最后加入以下配置<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true                # 文章字数统计</span><br><span class="line">  time: true                   # 文章阅读时长</span><br><span class="line">  total_symbols: true          # 站点总字数统计</span><br><span class="line">  total_time: true             # 站点总阅读时长</span><br><span class="line">  exclude_codeblock: false     # 排除代码字数统计</span><br></pre></td></tr></table></figure><br>在next主题的配置文件已有相关文章计数配置<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true     # 是否另起一行（true的话不和发表时间等同一行）</span><br><span class="line">  item_text_post: true     # 首页文章统计数量前是否显示文字描述（本文字数、阅读时长）</span><br><span class="line">  item_text_total: false   # 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长）</span><br></pre></td></tr></table></figure><br>注意字体统计效果需重新hexo clean 和hexo generate才有效果，直接重启本地hexo server将无效</p>
<p><img src="https://img-blog.csdnimg.cn/20200204182425284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后重新部署一遍博客即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<h3 id="自动更新Gitee-Pages"><a href="#自动更新Gitee-Pages" class="headerlink" title="自动更新Gitee Pages"></a>自动更新Gitee Pages</h3><p>GitHub Pages只要 push上去，主页文章就会自动更新，但Gitee Pages的个人版无法实现自动更新，需要手动在仓库设置中点击更新按钮，若想自动化该过程，可以用selenium爬虫工具实现，npm有一个插件可以实现该过程——官网<a href="https://developer.aliyun.com/mirror/npm/package/gitee-publish/v/1.0.18">地址</a></p>
<p>当然使用Python也可以快速开发一个自动更新脚本</p>
<h3 id="本地git同时配置github和gitee-重要"><a href="#本地git同时配置github和gitee-重要" class="headerlink" title="本地git同时配置github和gitee(重要)"></a>本地git同时配置github和gitee(重要)</h3><p>由于在开发项目或者博客项目中，若只设置的github 公钥或者gitee的公钥，那么git push到仓库只能二选一，下面给出同一台电脑同时配置github和gitee</p>
<p>为github生成对应的公钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -f ~/.ssh/id_rsa.github -C &quot;****@qq.com&quot; # 备注用，可以用github注册邮箱或自定义</span><br></pre></td></tr></table></figure>
<p>为gitee生成对应的公钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -f ~/.ssh/id_rsa.gitee -C &quot;****@qq.com&quot; # 备注用，可以用gitee注册邮箱或自定义</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看结果</span></span><br><span class="line">**@MacBookPro .ssh % ls</span><br><span class="line">id_rsa			id_rsa.github	</span><br><span class="line">id_rsa.gitee		id_rsa.github.pub</span><br><span class="line">id_rsa.gitee.pub	id_rsa.pub</span><br></pre></td></tr></table></figure>
<p>因bash shell执行 ssh时默认读取id_rsa为名称的公钥，为了让ssh访问github.com和gitee.com时使用对应的id_rsa.github和id_rsa.gitee ，需将这两个公钥加入到 本地ssh agent 中：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">***@MacBookPro .ssh % ssh-agent bash</span><br><span class="line"></span><br><span class="line">bash-3.2$ ssh-add ~/.ssh/id_rsa.github</span><br><span class="line">Identity added: /***/.ssh/id_rsa.github (***@qq.com)</span><br><span class="line">bash-3.2$ ssh-add ~/.ssh/id_rsa.gitee</span><br><span class="line">Identity added: /***/.ssh/id_rsa.gitee (***@qq.com)</span><br></pre></td></tr></table></figure>
<p>在~/.ssh目录下创建一个config文件，无需后缀，其实就是告诉<code>ssh -T 域名</code>命令运行时路由到对应的公钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">***@MacBookPro .ssh % vi config</span><br></pre></td></tr></table></figure>
<p>文件内容:</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Host github.com</span><br><span class="line">    HostName github.com</span><br><span class="line">    User git</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa.github</span><br><span class="line"></span><br><span class="line">Host gitee.com</span><br><span class="line">    Port 22</span><br><span class="line">    HostName gitee.com</span><br><span class="line">    User git</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa.gitee</span><br></pre></td></tr></table></figure>
<p>最后分别将~/.ssh目录下的id_rsa.github.pub以及id_rsa.gitee.pub里面的公钥复制到平台ssh key设置，本地再测试是否通过:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">***@MacBookPro ~ % ssh -T git@github.com</span><br><span class="line">Hi yield-bytes! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br><span class="line">***@MacBookPro ~ % ssh -T git@gitee.com</span><br><span class="line">Hi yield-bytes! You&#x27;ve successfully authenticated, but GITEE.COM does not provide shell access.</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title>基于PySpark整合Spark Streaming与Kafka</title>
    <url>/2020/03/06/%E5%9F%BA%E4%BA%8EPySpark%E6%95%B4%E5%90%88Spark%20Streaming%E4%B8%8EKafka/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文内容主要给出基于PySpark程序，整合Spark Streaming和Kafka，实现实时消费和处理topic消息，为PySpark开发大数据实时计算项目提供基本参考。</p>
<h4 id="1-程序环境准备："><a href="#1-程序环境准备：" class="headerlink" title="1 程序环境准备："></a>1 程序环境准备：</h4><p>&#8195;&#8195;这里不再使用Spark的集群环境，因涉及的计算资源测试环境受限，目前两台虚拟机：1个vcore+2G内存，其中一台虚拟机启动Spark Streaming服务进程，另外一台虚拟机启动kafka进程。</p>
<ul>
<li>虚拟机A：启动单实例kafka服务</li>
<li>虚拟机B：运行PySpark程序</li>
</ul>
<p>&#8195;&#8195;在VM A，程序环境要求安装jdk1.8以上以及与kafka匹配版本的scala版本<br>版本兼容说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafka：kafka_2.11-2.4.0</span><br><span class="line">java：java version &quot;1.8.0_11&quot;</span><br><span class="line">scala： Scala 2.12.0</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;这里需要注意：如果使用kafka_2.12版本以上，需要使用jdk1.8.0_212以上；kafka_2.12与jdk1.8.0_11有不兼容地方，kafka启动报错提示<code>java.lang.VerifyError: Uninitialized object exists on backward branch 209</code>。</p>
<a id="more"></a>
<h5 id="1-1-基本配置"><a href="#1-1-基本配置" class="headerlink" title="1.1 基本配置"></a>1.1 基本配置</h5><p>（1）配置单机zk这里无需依赖ZooKeeper集群，只需使用kafka自带的zk服务即可<br>vim /opt/kafka_2.11-2.4.0/config/zookeeper.properties<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;zookeeper # zk的snapshot数据存储路径</span><br><span class="line">clientPort&#x3D;2181 # 按默认端口</span><br></pre></td></tr></table></figure></p>
<p>（2）配置kafka的，路径<code>/opt/kafka_2.11-2.4.0/config/ server.properties</code><br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">log.dirs=/opt/kafka-logs # 存放kafka数据目录</span><br><span class="line">zookeeper.connect=127.0.0.1:2181 # 按默认连接本机zk即可</span><br></pre></td></tr></table></figure></p>
<h5 id="1-2-启动zk和kafka"><a href="#1-2-启动zk和kafka" class="headerlink" title="1.2 启动zk和kafka"></a>1.2 启动zk和kafka</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# pwd</span><br><span class="line">/opt/kafka_2.12-2.4.0</span><br><span class="line"></span><br><span class="line">[root@nn kafka_2.11-2.4.0]#  nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>kafka server后台启动：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# nohup bin/kafka-server-start.sh config/server.properties 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<h5 id="1-3-测试单实例Kafka"><a href="#1-3-测试单实例Kafka" class="headerlink" title="1.3 测试单实例Kafka"></a>1.3 测试单实例Kafka</h5><p>&#8195;&#8195;对于kafka单节点而言，这里只能使用1个分区且1个replication-factor，topic名称为sparkapp<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sparkapp</span><br><span class="line">Created topic sparkapp.</span><br></pre></td></tr></table></figure></p>
<p>打开一个新的shell,用于启动producer<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sparkapp</span><br></pre></td></tr></table></figure></p>
<p>再打开一个新的shell,用于启动consumer<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic sparkapp</span><br></pre></td></tr></table></figure></p>
<p>&#8195;&#8195;在producer shell输入字符串，consumer端可以看到相应输出，说明单机的kafka可以正常运行，下面将使用Spark Streaming实时读取kafka的输入流</p>
<h4 id="2-整合streaming和kafka"><a href="#2-整合streaming和kafka" class="headerlink" title="2  整合streaming和kafka"></a>2  整合streaming和kafka</h4><h5 id="2-1-配置依赖包"><a href="#2-1-配置依赖包" class="headerlink" title="2.1 配置依赖包"></a>2.1 配置依赖包</h5><p>&#8195;&#8195;具体说明<a href="http://spark.apache.org/docs/2.4.4/streaming-kafka-integration.html">参考官方文档</a>spark streaming连接kafka需要依赖两个jar包（注意版本号）：<br>spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar： <a href="http://archiva-maven-storage-prod.oss-cn-beijing.aliyuncs.com/repository/central/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.3/spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar?Expires=1579170671&amp;OSSAccessKeyId=LTAIfU51SusnnfCC&amp;Signature=Yh6l7ZwWEfW0QPkwKlrDAdXrGxs%3D">下载链接</a><br>spark-streaming-kafka-0-8_2.11-2.4.4.jar：  <a href="https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.4/spark-streaming-kafka-0-8_2.11-2.4.4.jar">下载链接</a><br>&#8195;&#8195;将这两个jar包放在spark 的jars目录下，需要注意的是：这两个jar包缺一不可，如果是在Spark集群上做测试，那么每个Spark节点都需要放置这两个jars包：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn jars]# pwd</span><br><span class="line">/opt/spark-2.4.4-bin-hadoop2.7/jars</span><br><span class="line"></span><br><span class="line">[root@nn jars]# ls spark-streaming-kafka-0-8</span><br><span class="line">spark-streaming-kafka-0-8_2.11-2.4.4.jar</span><br><span class="line">spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;(关于spark-streaming-kafka的jar包依赖说明：就像python连接kafka，需要使用pip 安装kafka这个库）</p>
<h5 id="2-2-Spark-Streaming实时消费Kafka消息"><a href="#2-2-Spark-Streaming实时消费Kafka消息" class="headerlink" title="2.2 Spark Streaming实时消费Kafka消息"></a>2.2 Spark Streaming实时消费Kafka消息</h5><p>&#8195;&#8195;使用spark自带的直连kafka，实现实时计算wordcount，可以看到写普通的PySpark逻辑相对简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.kafka <span class="keyword">import</span> KafkaUtils</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(appName=<span class="string">&quot;streamingkafka&quot;</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少shell打印日志</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>) <span class="comment"># 5秒的计算窗口</span></span><br><span class="line">    brokers=<span class="string">&#x27;127.0.0.1:9092&#x27;</span></span><br><span class="line">    topic = <span class="string">&#x27;sparkapp&#x27;</span></span><br><span class="line">    <span class="comment"># 使用streaming使用直连模式消费kafka </span></span><br><span class="line">    kafka_streaming_rdd = KafkaUtils.createDirectStream(ssc, [topic], &#123;<span class="string">&quot;metadata.broker.list&quot;</span>: brokers&#125;)</span><br><span class="line">    lines_rdd = kafka_streaming_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    counts = lines_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">        .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    <span class="comment"># 将workcount结果打印到当前shell    </span></span><br><span class="line">    counts.pprint()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>spark streaming流默认接收的是utf-8编码的字符串</p>
<p>KafkaUtils接口<code>createDirectStream</code>说明：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Parameters:	</span><br><span class="line">    ssc – StreamingContext object.</span><br><span class="line">    topics – list of topic_name to consume.</span><br><span class="line">    kafkaParams – Additional params for Kafka.</span><br><span class="line">    fromOffsets – Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.</span><br><span class="line">    keyDecoder – A function used to decode key (default is utf8_decoder).</span><br><span class="line">    valueDecoder – A function used to decode value (default is utf8_decoder).</span><br><span class="line">    messageHandler – A function used to convert KafkaMessageAndMetadata. You can assess meta using messageHandler (default is None).</span><br><span class="line"></span><br><span class="line">Returns:	</span><br><span class="line">A DStream object</span><br></pre></td></tr></table></figure>
<p>spark streaming 从 kafka 接收数据，有两种方式<br>（1）使用Direct API，这是更底层的kafka API<br>（2）使用receivers方式，这是更为高层次的API</p>
<p>&#8195;&#8195;在本博客后面讨论streaming的原理同时也给出Direct模式的相关详细的解析。当前测试使用为Direct模式，在虚拟机B的Spark目录下，启动application，启动命令需要带上指定的jars包。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/spark-submit --jars spark-streaming-kafka-0-8_2.11-2.4.4.jar direct_stream.py </span><br></pre></td></tr></table></figure><br>&#8195;&#8195;在虚拟机A的producer shell端，输入字符串句子<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-producer.sh --broker-list localhost:9 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">welcome to pyspark kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">从这里开始  将开发一个 由sparkstreaming 完成的 实时计算的 大数据项目</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;在spark-submit窗口，可以看到spark streaming消费并处理kafka生成的实时流字符串结果：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:28</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;welcome&#x27;, 1)</span><br><span class="line">(&#x27;to&#x27;, 1)</span><br><span class="line">(&#x27;pyspark&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:34</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;从这里开始&#x27;, 1)</span><br><span class="line">(&#x27;&#x27;, 1)</span><br><span class="line">(&#x27;将开发一个&#x27;, 1)</span><br><span class="line">(&#x27;由sparkstreaming&#x27;, 1)</span><br><span class="line">(&#x27;完成的&#x27;, 1)</span><br><span class="line">(&#x27;实时计算的&#x27;, 1)</span><br><span class="line">(&#x27;大数据项目&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:36</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;以上完成基于PySpark整合Spark Streaming与Kafka的测试。</p>
<h5 id="2-3-关于以上测试过程有关offset简单说明"><a href="#2-3-关于以上测试过程有关offset简单说明" class="headerlink" title="2.3 关于以上测试过程有关offset简单说明"></a>2.3 关于以上测试过程有关offset简单说明</h5><p>&#8195;&#8195;该测试并没有给出consumer自己管理消息的offset，在上面测试中，例如，producer连续生产5条消息，那么消息体可以看出以下简单构成：<br>| offset| msg |<br>|—|—|<br>|  0|123@qq.com|<br>|  1|124@qq.com  |<br>|  2|125@qq.com |<br>|  3|126@qq.com  |<br>|  5|127@qq.com  |<br>&#8195;&#8195;上面的测试中，streaming 以Direct模式连接kafka，每消费一条消息，streaming默认自动commit offset到kafka，以期实现当下一批streaming去kafka取消息时，是按顺延下一条来取，保证没有重复处理消息，也不会漏了消息，这是什么意思呢？<br>&#8195;&#8195;例如当前streaming 消费offset=1的消息后，自动将消费位置offset=1告诉kafka：你记住我已经把第1个位置消息处理了，如果我下次找你kafka消费，请你找出offset=2的消息给我，但如果你将offset=0的消息给我，说明你让我重复消费消息，如果将offset=4消息给我，说明你让我漏了处理offset=3的消息。<br>&#8195;&#8195;根据以上说明，例如producer已经生产了offset=9共10消息，即使将当前spark streaming进程再消费offset=1的消息后，被退出，之后重启，spark streaming从kafka消费的消息将是offset=2的消息，而不是offset=10的消息。虽然默认配置有一定合理性，但也有这种情况，导致无法实现“仅消费一次而且保证业务正常”，参考以下场景：<br>&#8195;&#8195;spark streaming当前进程消费了offset=1的消息后，在业务处理过程中程序出错导致没有将办理业务详情发送到用户<code>124@qq.com</code>，因为spark streaming默认自动提交offset的位置给到kafka，因此spark streaming在一批处理中将消费offset=2的消息。若你想倒回去重新处理offset=1的消息，以保证邮件正确送到给用户，那么只能自己用外部数据库存放成功完成业务的offset，也即是自行管理offset，而不是被动的自动提交到kafka保存消费的offset。<br>&#8195;&#8195;kafka的offset消费位置的管理详解将在之后的文章给出，只有将offset的消费位置交由客户端自行管理，才能灵活实现各种需求：重新消费、只消费一次等</p>
<h4 id="3-Spark-Streaming与Kafka整合的两种方式"><a href="#3-Spark-Streaming与Kafka整合的两种方式" class="headerlink" title="3 Spark Streaming与Kafka整合的两种方式"></a>3 Spark Streaming与Kafka整合的两种方式</h4><p>&#8195;&#8195;在上面的整合测试里，用的streaming直连kafka进行消费消息。目前Spark Streaming 与 Kafka 的结合主要有两种方式：Receiver Dstream和Direct Dstream，目前企业实际项目主要采用 Direct Dstream 的模式，为何我这边可以断言企业主要使用Direct Dstream模式呢？因为在企业中，他们主力用Java和Scala，考虑企业需求方面，肯定使用spark-streaming-kafka-0-10版本的整合包，而这一版本不再支持Receiver模式。除非某些企业用了Pyspark作为spark应用开发，否则基本没人用Receiver模式。Spark官网也给出整合Kafka的指引<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html">链接</a><br><img src="https://img-blog.csdnimg.cn/20200205144614446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;因为基于PySpark开发实时流计算程序，这里只能选择spark-streaming-kafka-0-8开发包，从官方提示可知，spark-streaming-kafka-0-10是stable版本而且支持ssl安全传输，支持offset commit（支持手动提交，这个非常重要，自行控制消息位置从哪条开始处理，保证准确消费）和dynamic topic subscription，这就是为何要用Scala语言开发面向高级需求的Spark程序或者streaming程序，亲儿子！<br>&#8195;&#8195;对于两种连接连接方式，有必要给出讨论和对比，以便加深streaming消费kafka topic更深理论知识。</p>
<h5 id="3-1-基于Receiver消费消息方式"><a href="#3-1-基于Receiver消费消息方式" class="headerlink" title="3.1 基于Receiver消费消息方式"></a>3.1 基于Receiver消费消息方式</h5><p><strong>原理图（已启用WAL机制）</strong>：<br><img src="https://img-blog.csdnimg.cn/20200303193251128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   （原理图需要注意的地方：如果Receiver模式下，未开启WAL用于备份接收的消息，那么图中Save data to WAL是不存在的。）<br>&#8195;&#8195;早期版本的Spark Streaming与Kafka的整合方式为Receiver从Kafka消费消息，在提交Spark Streaming任务后，Spark会划出指定的Receiver来持续不断、异步读取kafka数据，这个Receiver其实是Executor（jvm进程）的一个常驻线程，跟task类似，为何它是常驻的？因为它需要不断监听Kafka的Producer生产的消息，从这点也可以看出，Receiver收到的消息是存放在Executor的内存中，换句话说，占用了Executor的内存。至于Receiver线程内部使用哪种数据结构存放接收的消息？对于先进先消费，后进后消费场景，显然使用queue最适合（通过队列实现多线程的生产-消费编程逻辑）。当Driver这边提交job后，Executors从Receiver拿到消息去交给task处理。在执行完之后，Receiver向Kafka的Zookeeper提交offset，告诉Kafka记主它当前已消费的位置。<br>&#8195;&#8195;早期的设计中，Spark Streaming为了零丢失地消费kafka消息，增加对接收到的消息进行预写日志处理（Write Ahead Log， WAL）这个WAL是放在hdfs的checkpoint 目录下，开启该功能后，Receiver除了将接收到消息存放到Executor内存中，还将其同步写入到hdfs上的WAL日志文件。因此，当一直运行的Spark Streaming任务突然挂了，后期启动时，Streaming也可以自动从hfds的checkpoint目录下的WAL日志找回丢失的消息。</p>
<h6 id="Receiver连接方式的缺点"><a href="#Receiver连接方式的缺点" class="headerlink" title="Receiver连接方式的缺点"></a>Receiver连接方式的缺点</h6><p>&#8195;&#8195;从上面receiver工作原理可以总结其缺点出将出现在内存方面、wal日志影响吞吐量等方面存在设计上的缺点：<br><strong>（1）占用cpu+内存</strong>：每个receiver需要单独占用一个vcore以及相应内存，如果Receiver并发数量多，占用Executor更多cpu和内存资源，这些资源本应用来跑tasks做计算用的，这就出现浪费资源的情况。</p>
<p><strong>（2）WAL拖累整体处理效率</strong>：为了不丢数据需要开启WAL，也即Receiver将接收到的数据写一份备份到文件系统上（hdfs的checkpoint目录），既然有落到磁盘自然会有IO，这降低了<code>kafka+streaming</code>这个组合实时处理消息的效率，换句话说：增加job的执行时间。此外，开启WAL，还有造成重复消费的可能。</p>
<p><strong>（3）接收数量大于处理速率</strong>： 若Receiver并发数量设置不合理，接受消息速率大于streaming处理消息的速率，就会出现数据积压在队列中，最终可能会导致程序异常退出。这里也是面试常见的问题：例如提高Receiver的并发数量，就可以提高streaming处理能力吗？首先，Receiver异步接收kafka消息，不参与计算，真正执行计算的是streaming，如果streaming并发性没有调高，整个计算能力也没有提高。一定要记着：kafka跟streaming是需要两边同时调优，才能打得计算能力的整体提升，不能只调优一边，这一个组合！！</p>
<p>（补充知识点：Receiver的并发数据量是怎么确定？<br>&#8195;&#8195;在KafkaUtils.createStream()中，可以指定topic的partition数量，该数量就是Receiver消费此topic的并发数（其实就是Executor 启动消费此topic的线程数量）但需要指出的是：Kafka中topic的partition与Spark中RDD的partition是两个不同的概念，两者没有关联关系。）</p>
<h5 id="3-2-基于Direct消费消息方式"><a href="#3-2-基于Direct消费消息方式" class="headerlink" title="3.2 基于Direct消费消息方式"></a>3.2 基于Direct消费消息方式</h5><p>原理图：<br><img src="https://img-blog.csdnimg.cn/20200305173531689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;当Receiver的工作原理及其缺点理解后，Direct模式将更容易理解。Driect模式下，Streaming定时主动查询Kafka，以获得指定topic的所有partition的最新offset，结合上一批次已保存的offset位置，Streaming就可以确定出每个批次拉取消息offset的范围，例如第1批次的消息（offset范围0-100）正在处理过程中，streaming指定特定的线程定时去Kafka查询第2批次最新的offset，发现最新值为300，那么如果streaming没有限制每批次的最大消费速率，在第2批次取消息时，会一次性取回offset=101到300的消息记录，这个就是所谓的offset ranges。当让如果streaming没有限制每批次的最大消费速率就是每批次100，那么即使最新的offset位置为300，第2批次消费的offset 访问只能是101~200共计100条消费记录。<br>&#8195;&#8195;当处理数据的job启动时，就会使用kafka的简单Consumer API来获取kafka中指定offset范围的数据。此外，Streaming已消费的offset不再交由Zookeeper来管理，而是手动采用外部存储数据库如mysql、redis等存放和管理已消费的offset。<br>以下为Scala代码演示从rdd拿到offset ranges属性的逻辑（rdd当然本身包含消息数据）<br>​```java<br>directKafkaStream.map {<br>           …<br> }.foreachRDD { batchRdd =&gt;<br>    // 获取当前rdd数据对应的offset<br>    val offsetRanges = batchRdd.asInstanceOf[HasOffsetRanges].offsetRanges<br>    // 运行计算任务<br>    doCompute(batchRdd)<br>    // 使用外部数据库自行保存和管理offsetRanges<br>    saveToRedis(offsetRanges)<br> }<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&amp;#8195;&amp;#8195;而Receiver方式下没有关于offset的处理逻辑，这是因为streaming在该模式下内部通过kafka consumer high level API 提交到zk保存。</span><br><span class="line">​&#96;&#96;&#96;java</span><br><span class="line">receiverkafkaStream.map &#123;</span><br><span class="line">           ...</span><br><span class="line"> &#125;.foreachRDD &#123; streamRdd &#x3D;&gt;</span><br><span class="line">    &#x2F;&#x2F; 运行计算任务</span><br><span class="line">    doCompute(rdd)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<h6 id="Direct连接方式的优点"><a href="#Direct连接方式的优点" class="headerlink" title="Direct连接方式的优点"></a>Direct连接方式的优点</h6><p><strong>（1）提高计算资源利率</strong>：不像Receiver那样还占用Executor的一部分内存和计算资源，Direct方式下的Executor的代码实现踢掉Receiver这块设计，因此可以实现计算和内存资源全部用在计算任务，因为streaming定时主动去kafka拉取batch 消息，拉过来直接计算，而不是像Receiver不断接收消息不断地存放在内存中。</p>
<p><strong>（2）无需开启WAL</strong>：Receiver方式需要开启WAL机制以保证不丢失消息，这种方式加大了集群的计算延迟和效率，而Direct的方式，无需开启WAL机制，因为Kafka集群有partition做了高可用，只要streaming消费方自己存放和管理好已经消费过的offset，那么即使程序异常退出等，也可利用已存储的offset去Kafka消费丢失的消息。</p>
<p><strong>（3）可保证exactly once的消费语义</strong>：基于Receiver的方式，使用kafka的高阶API来在Zookeeper中保存消费过的offset。这是消费kafka数据的传统方式。这种方式配合WAL机制，可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和Zookeeper之间可能是不同步的。基于Direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据时消费一次且仅消费一次。</p>
<p><strong>（4）计算程序更稳定</strong>：Receiver模式是通过异步持续不断的读取数据，当集群出现网络、计算负载跟不上等因素，导致streaming计算任务侧出现延迟和堆积，而Receiver却还在持续接收kafka消息，此种情况容易导致Executor内存溢出或者其他异常抛出，从而引起计算程序退出，换句话说，Receiver模式的streaming实时计算可靠性和稳定性欠缺。对于Direct模式，Driver在触发batch计算任务时，才会去kafka拉消息回来并计算，而且给streaming加入最大消费速率控制后，整个实时计算集群鲁棒性更强。</p>
<p><strong>（5）Dstream 的rdd分区数与kafka分区一致</strong>：<br>&#8195;&#8195;Direct模式下，Spark Streaming创建的rdd分区数跟Kafka的partition数量一致，也就是说Kafka partitions和streaming rdd partitions之间有一对一的映射关系，这样的好处是明显和直观的：只要增加kafka topic partition数量，就可以直接增大spark streaming的计算的并发数。<br>&#8195;&#8195;当然，Direct模式不足的地方就是需要自行实现可靠的offset管理逻辑，但对于开发方向来说，这点很容易实现，我个人若对offset管理，将优先选用redis，而且是集群！<br>&#8195;&#8195;以上有关Spark Streaming 整合Kafka的方式和原理分析必须要理解，否则在后面的实时计算平台的代码开发上，有些逻辑你不一定能处理好。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Streaming</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Sentinel模式部署高可用Redis</title>
    <url>/2019/12/20/%E5%9F%BA%E4%BA%8ESentinel%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8Redis/</url>
    <content><![CDATA[<p>&#8195;&#8195;在本博客前面的文章给出redis-cluster模式的配置和测试<a href="https://blog.csdn.net/pysense/article/details/100827689">《一篇文章掌握redis-cluster原理及其部署、测试》</a>，redis还有另外一种failover自动切换的部署方式，也即是本文给出的——Sentinel模式（哨兵模式），这两种方式部署的redis服务其实在普通的项目完全够用，例如个人在Django项目使用的Sentinel模式保证了”查询缓存服务以及一些频繁读取配置参数服务“的高可用。对于并发量大的需求，可以使用国内知名Codis——分布式Redis集群代理中间件，可配置规模更大的redis集群服务。</p>
<a id="more"></a>
<h4 id="1、安装redis"><a href="#1、安装redis" class="headerlink" title="1、安装redis"></a>1、安装redis</h4><p>&#8195;&#8195;为保持文章内容完整，这里给出redis的安装过程。两种方式，一种为yum 安装，另外一种下载包安装。这里选择bin包下载安装。目前redis稳定版为5.0.7，tar包为仅为1.7M，不愧为缓冲界的宠儿。安装包放在opt下，个人喜好将所有关开发的相关组件安装包放置于/opt目录，例如前面大数据各个组件的安装包，还是为了方便记忆、管理和查找。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# pwd</span><br><span class="line">/opt/redis-5.0.7</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget http://download.redis.io/releases/redis-5.0.7.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar xzf redis-5.0.7.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> redis-5.0.7</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make</span></span><br></pre></td></tr></table></figure></p>
<p>将redis启动命令所在的src路径加入系统变量<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# source ~/.bash_profile  </span><br><span class="line">PATH=$PATH:$HOME/bin:/opt/redis-5.0.7/src/  </span><br></pre></td></tr></table></figure></p>
<p>查看版本<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# redis-server -v</span><br><span class="line">Redis server v=5.0.7 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=864a7319aeb56c9b</span><br></pre></td></tr></table></figure><br>启动redis-server后台进程：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# redis-server &amp;</span><br><span class="line">[root@nn opt]# ps -ef |grep redis</span><br><span class="line">root      91054  60098  0 11:47 pts/0    00:00:00 redis-server *:6379</span><br><span class="line"></span><br><span class="line">[root@nn opt]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; set msg 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get msg</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure></p>
<h4 id="2、Sentinel-的配置说明"><a href="#2、Sentinel-的配置说明" class="headerlink" title="2、Sentinel 的配置说明"></a>2、Sentinel 的配置说明</h4><h5 id="2-1-官网有关Sentinel模式的基本信息"><a href="#2-1-官网有关Sentinel模式的基本信息" class="headerlink" title="2.1 官网有关Sentinel模式的基本信息"></a>2.1 官网有关Sentinel模式的基本信息</h5><blockquote>
<p>The current version of Sentinel is called <strong>Sentinel 2</strong>，A stable release of Redis Sentinel is shipped since Redis 2.8.<br>Sentinels by default run <strong>listening for connections to TCP port 26379</strong>,<br>If you are using the <code>redis-sentinel</code> executable， you can run Sentinel<br>with the following command line:<br><code>redis-sentinel /path/to/sentinel.conf</code></p>
</blockquote>
<p>redis要求启动Sentinel服务时必须带上其配置文件，否则直接返回启动失败。<br>启动Sentinel模式前的基本要求：</p>
<ul>
<li>You need at least three Sentinel instances for a robust deployment.（至少3个Sentinel实例，多数票选举）</li>
<li>最好在不同物理机上或者虚拟机上启动每个Sentinel 实例（在测试环境下，当然也可在同一台服务器里面，启动不同端口的多个实例也可完成测试。）</li>
<li>Sentinel + Redis distributed system does not guarantee that acknowledged writes are retained during failures，since Redis uses asynchronous replication.（Sentinel+redis分布式集群环境下，节点出现故障时，不保证写一致性，因redis异步复制方式实现集群数据同步）</li>
</ul>
<h5 id="2-2-redis官网Sentinel模式说明"><a href="#2-2-redis官网Sentinel模式说明" class="headerlink" title="2.2 redis官网Sentinel模式说明"></a>2.2 redis官网Sentinel模式说明</h5><p>有些缩写需要说明：</p>
<ul>
<li>Masters are called M1, M2, M3, …, Mn.</li>
<li>replicas are called R1, R2, R3, …, Rn (R stands for <em>replica</em>).（replica也就是slave角色，因为slave有歧视语义，很多中间件不再使用该词描述副角色，例如kafka备份分区的：replica）</li>
<li>Sentinels are called S1, S2, S3, …, Sn.</li>
<li>Clients are called C1, C2, C3, …, Cn.</li>
</ul>
<p>首先看官方首推的 basic setup with three boxes:It is based on three boxes, each box running both a Redis process and a Sentinel process. 每个box代表一个redis节点，确认master失败的选票数为2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">       +----+</span><br><span class="line">       | M1 |</span><br><span class="line">       | S1 |</span><br><span class="line">       +----+</span><br><span class="line">          |</span><br><span class="line">+----+    |    +----+</span><br><span class="line">| R2 |----+----| R3 |</span><br><span class="line">| S2 |         | S3 |</span><br><span class="line">+----+         +----+</span><br><span class="line"></span><br><span class="line">Configuration: quorum &#x3D; 2</span><br></pre></td></tr></table></figure>
<p>If the master M1 fails, S2 and S3 will agree about the failure and will<br>be able to authorize a failover, making clients able to continue.</p>
<p>如果主redis M1宕机（哨兵S1当然也会挂掉），那么其他节点上哨兵S2和哨兵S3发现与S1心跳失败，两者一致同意此时进入故障转移，选举R2为新的master M2。</p>
<p>redis sentinel实现高可用，但也会在某种程度下有丢失有些写数据。例如下面的情况：客户端C1原来与M1连接，写入M1，当M1挂了，到M2起来的这个过程，C1在这一过程写的部分数据会丢失。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">         +----+</span><br><span class="line">         | M1 |</span><br><span class="line">         | S1 | &lt;- C1 (writes will be lost)</span><br><span class="line">         +----+</span><br><span class="line">            |</span><br><span class="line">            &#x2F;</span><br><span class="line">            &#x2F;</span><br><span class="line">+------+    |    +----+</span><br><span class="line">| [M2] |----+----| R3 |</span><br><span class="line">| S2   |         | S3 |</span><br><span class="line">+------+         +----+</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;以上情况可通过以下两个配置实现数据丢失最小化。that allows to stop accepting writes if a master detects thatit is no longer able to transfer its writes to the specified number of replicas。<br>这里用到replica关键字单词，在本博客前面kafka文章里面，kafka也有自己的replica名词，不过kafka的replica是指top 分区后的副本，redis这里replica是从服务器（开源界不建议使用slave这个带有歧视的单词）。通过以下设置，只有当前master主机至少还有一个alive的replica才准许外部客户端写入数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br></pre></td></tr></table></figure>
<h4 id="3、一主两从的redis架构配置"><a href="#3、一主两从的redis架构配置" class="headerlink" title="3、一主两从的redis架构配置"></a>3、一主两从的redis架构配置</h4><p>&#8195;&#8195;第2部分的sentinel 2 高可用的前提是基于1主2两从的架构基础上实现的，如下架构，故首先得让一主两从的redis小集群跑起来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">       +----+</span><br><span class="line">       | M1 |</span><br><span class="line">       | S1 |</span><br><span class="line">       +----+</span><br><span class="line">          |</span><br><span class="line">+----+    |    +----+</span><br><span class="line">| R2 |----+----| R3 |</span><br><span class="line">| S2 |         | S3 |</span><br><span class="line">+----+         +----+</span><br><span class="line"></span><br><span class="line">Configuration: quorum &#x3D; 2</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;M1为1主，两从：R2、R3，在此基础上，每个节点运行sentinel进程，即可实现redis高可用架构。</p>
<h5 id="3-1-配置主从的redis-conf文件"><a href="#3-1-配置主从的redis-conf文件" class="headerlink" title="3.1 配置主从的redis.conf文件"></a>3.1 配置主从的<strong>redis.conf</strong>文件</h5><p>redis的配置文件的注释有分段说明，这里列出仅需修改的地方：<br>”一主redis“的配置说明：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# NETWORK</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定本机IP</span></span><br><span class="line">bind 182.0.0.10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ GENERAL</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台守护进程运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### SNAPSHOTTING</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放快照（数据日志文件的目录）dump.rdb</span></span><br><span class="line">dir /opt/redis-5.0.7/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ REPLICATION</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1主两从架构里，至少一个有个从服务器在线且在10秒以内延迟，主redis才能对外提供写服务器，否则客户端无法写</span></span><br><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br><span class="line"><span class="meta">#</span><span class="bash">这里也需设置，因为当该master挂了再重启，变成replica后，需要密码去认证新的master</span></span><br><span class="line">masterauth foo123</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SECURITY</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 为master设置认证密码</span></span><br><span class="line">requirepass foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## CLIENTS</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# MEMORY MANAGEMENT</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br></pre></td></tr></table></figure></p>
<p>”2个replica“节点的redis.conf配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# NETWORK</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定本机IP</span></span><br><span class="line">bind 182.0.0.11</span><br><span class="line"><span class="meta">#</span><span class="bash"> 另外一台从的IP为182.0.0.12</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ GENERAL</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台守护进程运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### SNAPSHOTTING</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放快照（数据日志文件的目录）dump.rdb</span></span><br><span class="line">dir /opt/redis-5.0.7/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ REPLICATION</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告诉从服务器主服务器的认证密码以及IP端口号，新版redis不再使用slave争议词，原版是slaveof</span></span><br><span class="line">replicaof 182.0.0.10 6379</span><br><span class="line">masterauth foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1主两从架构里，至少一个有个从服务器在线且在10秒以内延迟，主redis才能对外提供写服务器，否则客户端无法写</span></span><br><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SECURITY</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 从redis需要密码认证</span></span><br><span class="line">requirepass foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## CLIENTS</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# MEMORY MANAGEMENT</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br></pre></td></tr></table></figure>
<h5 id="3-2-启动和测试主从"><a href="#3-2-启动和测试主从" class="headerlink" title="3.2 启动和测试主从"></a>3.2 启动和测试主从</h5><p>启动所有主从redis-server，后台守护进程运行<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@p1 opt]# redis-server /opt/redis-5.0.7/redis.conf </span><br></pre></td></tr></table></figure><br>注意：<br>如果只启动master，从服务器还未启动，提示没有足够的从服务器在线，无法对外提供写服务。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set test 1</span><br><span class="line">(error) NOREPLICAS Not enough good replicas to write.</span><br></pre></td></tr></table></figure><br>这是因为<code>min-replicas-to-write 1</code> 要求最少1个从redis在线后master才能接收客户端写数据。<br>在master set一个key<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@p1 opt]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; set foo 1</span><br><span class="line">​```shell</span><br><span class="line">在两个从服务器get key</span><br><span class="line">​```shell</span><br><span class="line">[root@p2 redis-5.0.7]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@p3 redis-5.0.7]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure></p>
<p>通过在master 查看主从信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; INFO Replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">min_slaves_good_slaves:2</span><br><span class="line">slave0:ip=182.0.0.11,port=6379,state=online,offset=1958,lag=0</span><br><span class="line">slave1:ip=182.0.0.12,port=6379,state=online,offset=1958,lag=1</span><br><span class="line">master_replid:1f69dd42ecea58d245859fd716c4eaee83a6e753</span><br><span class="line">master_replid2:0000000000000000000000000000000000000000</span><br><span class="line">master_repl_offset:1958</span><br><span class="line">second_repl_offset:-1</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:1</span><br><span class="line">repl_backlog_histlen:1958</span><br></pre></td></tr></table></figure>
<p>注：INFO [section]命令可以查看多个部分信息，也可指定查看某个section的信息<br>以上说明1主两从redis架构已经构建，该模式下，只有master才能写数据，replica只能get数据。如果尝试在replica上写数据，将提示readonly：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; <span class="built_in">set</span> bar <span class="number">2</span></span><br><span class="line">(error) READONLY You can<span class="string">&#x27;t write against a read only replica.</span></span><br></pre></td></tr></table></figure></p>
<h4 id="4、sentinel-高可用配置"><a href="#4、sentinel-高可用配置" class="headerlink" title="4、sentinel 高可用配置"></a>4、sentinel 高可用配置</h4><p>&#8195;&#8195;sentinel 高可用是基于主-从-从正常运行情况下配置，经过前面2.3点，相信很容易理解该sentinel的逻辑，</p>
<h5 id="4-1-配置sentinel-conf"><a href="#4-1-配置sentinel-conf" class="headerlink" title="4.1 配置sentinel.conf"></a>4.1 配置sentinel.conf</h5><p>&#8195;&#8195;主、从的sentinel.conf都一样，而且也很简单，更改两项属性即可，其他可以按默认值，如果需要调优，可自行参考conf的说明设置相应值。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bind 0.0.0.0</span><br><span class="line">port 26379</span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;</span></span><br><span class="line">sentinel monitor  mymaster  182.0.0.10 6379 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果master设置了密码，那么也告诉sentinel密码</span></span><br><span class="line">sentinel auth-pass mymaster foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  &lt;master-name&gt; 自行定义名称，这里使用mymaster默认值，后面的配置项都用了mymaster这个名，在django项目的settings，redis缓存设置也需要用到该master-name，无特殊需求不用改。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  &lt;quorum&gt; 裁定master挂了的最低通过票数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Tells Sentinel to monitor this master, and to consider it <span class="keyword">in</span> O_DOWN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (Objectively Down) state only <span class="keyword">if</span> at least &lt;quorum&gt; sentinels agree.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告诉Sentinel通监控master，如果有两个sentinel认为master挂了，说明master真的挂了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h5 id="4-2-测试redis高可用"><a href="#4-2-测试redis高可用" class="headerlink" title="4.2 测试redis高可用"></a>4.2 测试redis高可用</h5><p>启动所有主从的sentinel 服务，注意如果用 redis-server启动命令，需要带上选项 ——sentinel<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# redis-server /opt/redis-5.0.7/sentinel.conf --sentinel</span><br><span class="line">或者使用</span><br><span class="line">[root@nn opt]# redis-sentinel /opt/redis-5.0.7/sentinel.conf</span><br></pre></td></tr></table></figure><br>在master上查看sentinel状态，只需连接sentinel的工作端口即可，可以看到该master下带了两个replica，状态正常：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# redis-cli -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;182.0.0.10&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> 7) &quot;runid&quot;</span><br><span class="line"> 8) &quot;7cdc7e518b592168c94268f7d55fc2d237449118&quot;</span><br><span class="line"> 9) &quot;flags&quot;</span><br><span class="line">10) &quot;master&quot;</span><br><span class="line">11) &quot;link-pending-commands&quot;</span><br><span class="line">12) &quot;0&quot;</span><br><span class="line">13) &quot;link-refcount&quot;</span><br><span class="line">14) &quot;1&quot;</span><br><span class="line">15) &quot;last-ping-sent&quot;</span><br><span class="line">16) &quot;0&quot;</span><br><span class="line">17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">18) &quot;516&quot;</span><br><span class="line">19) &quot;last-ping-reply&quot;</span><br><span class="line">20) &quot;516&quot;</span><br><span class="line">21) &quot;down-after-milliseconds&quot;</span><br><span class="line">22) &quot;30000&quot;</span><br><span class="line">23) &quot;info-refresh&quot;</span><br><span class="line">24) &quot;3335&quot;</span><br><span class="line">25) &quot;role-reported&quot;</span><br><span class="line">26) &quot;master&quot;</span><br><span class="line">27) &quot;role-reported-time&quot;</span><br><span class="line">28) &quot;113804&quot;</span><br><span class="line">29) &quot;config-epoch&quot;</span><br><span class="line">30) &quot;0&quot;</span><br><span class="line">31) &quot;num-slaves&quot;</span><br><span class="line">32) &quot;2&quot;</span><br><span class="line">33) &quot;num-other-sentinels&quot;</span><br><span class="line">34) &quot;1&quot;</span><br><span class="line">35) &quot;quorum&quot;</span><br><span class="line">36) &quot;2&quot;</span><br><span class="line">37) &quot;failover-timeout&quot;</span><br><span class="line">38) &quot;180000&quot;</span><br><span class="line">39) &quot;parallel-syncs&quot;</span><br><span class="line">40) &quot;1&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>查看两个replica上的sentinel服务也正常运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:26379&gt; SENTINEL slaves mymaster</span><br><span class="line">1)  1) &quot;name&quot;</span><br><span class="line">    2) &quot;182.0.0.11:6379&quot;</span><br><span class="line">    3) &quot;ip&quot;</span><br><span class="line">    4) &quot;182.0.0.11&quot;</span><br><span class="line">    5) &quot;port&quot;</span><br><span class="line">    6) &quot;6379&quot;</span><br><span class="line">    7) &quot;runid&quot;</span><br><span class="line">    8) &quot;b7a9000c355584be472fe2409406b772b755b0ed&quot;</span><br><span class="line">    9) &quot;flags&quot;</span><br><span class="line">   10) &quot;slave&quot;</span><br><span class="line">   11) &quot;link-pending-commands&quot;</span><br><span class="line">   12) &quot;0&quot;</span><br><span class="line">   13) &quot;link-refcount&quot;</span><br><span class="line">   14) &quot;1&quot;</span><br><span class="line">   15) &quot;last-ping-sent&quot;</span><br><span class="line">   16) &quot;0&quot;</span><br><span class="line">   17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">   18) &quot;430&quot;</span><br><span class="line">   19) &quot;last-ping-reply&quot;</span><br><span class="line">   20) &quot;430&quot;</span><br><span class="line">   21) &quot;down-after-milliseconds&quot;</span><br><span class="line">   22) &quot;30000&quot;</span><br><span class="line">   23) &quot;info-refresh&quot;</span><br><span class="line">   24) &quot;9197&quot;</span><br><span class="line">   25) &quot;role-reported&quot;</span><br><span class="line">   26) &quot;slave&quot;</span><br><span class="line">   27) &quot;role-reported-time&quot;</span><br><span class="line">   28) &quot;169854&quot;</span><br><span class="line">   29) &quot;master-link-down-time&quot;</span><br><span class="line">   30) &quot;0&quot;</span><br><span class="line">   31) &quot;master-link-status&quot;</span><br><span class="line">   32) &quot;ok&quot;</span><br><span class="line">   33) &quot;master-host&quot;</span><br><span class="line">   34) &quot;182.0.0.10&quot;</span><br><span class="line">   35) &quot;master-port&quot;</span><br><span class="line">   36) &quot;6379&quot;</span><br><span class="line">   37) &quot;slave-priority&quot;</span><br><span class="line">   38) &quot;100&quot;</span><br><span class="line">   39) &quot;slave-repl-offset&quot;</span><br><span class="line">   40) &quot;24585&quot;</span><br><span class="line">2)  1) &quot;name&quot;</span><br><span class="line">    2) &quot;182.0.0.12:6379&quot;</span><br><span class="line">    3) &quot;ip&quot;</span><br><span class="line">    4) &quot;182.0.0.12.5&quot;</span><br><span class="line">    5) &quot;port&quot;</span><br><span class="line">    6) &quot;6379&quot;</span><br><span class="line">    7) &quot;runid&quot;</span><br><span class="line">    8) &quot;39edb70865b99916b1fd0013740e457135fe42e4&quot;</span><br><span class="line">    9) &quot;flags&quot;</span><br><span class="line">   10) &quot;slave&quot;</span><br><span class="line">   11) &quot;link-pending-commands&quot;</span><br><span class="line">   12) &quot;0&quot;</span><br><span class="line">   13) &quot;link-refcount&quot;</span><br><span class="line">   14) &quot;1&quot;</span><br><span class="line">   15) &quot;last-ping-sent&quot;</span><br><span class="line">   16) &quot;0&quot;</span><br><span class="line">   17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">   18) &quot;430&quot;</span><br><span class="line">   19) &quot;last-ping-reply&quot;</span><br><span class="line">   20) &quot;430&quot;</span><br><span class="line">   21) &quot;down-after-milliseconds&quot;</span><br><span class="line">   22) &quot;30000&quot;</span><br><span class="line">   23) &quot;info-refresh&quot;</span><br><span class="line">   24) &quot;9197&quot;</span><br><span class="line">   25) &quot;role-reported&quot;</span><br><span class="line">   26) &quot;slave&quot;</span><br><span class="line">   27) &quot;role-reported-time&quot;</span><br><span class="line">   28) &quot;169855&quot;</span><br><span class="line">   29) &quot;master-link-down-time&quot;</span><br><span class="line">   30) &quot;0&quot;</span><br><span class="line">   31) &quot;master-link-status&quot;</span><br><span class="line">   32) &quot;ok&quot;</span><br><span class="line">   33) &quot;master-host&quot;</span><br><span class="line">   34) &quot;182.0.0.10&quot;</span><br><span class="line">   35) &quot;master-port&quot;</span><br><span class="line">   36) &quot;6379&quot;</span><br><span class="line">   37) &quot;slave-priority&quot;</span><br><span class="line">   38) &quot;100&quot;</span><br><span class="line">   39) &quot;slave-repl-offset&quot;</span><br><span class="line">   40) &quot;24585&quot;</span><br></pre></td></tr></table></figure>
<p>高可用测试：</p>
<p>kill 掉master的redis-server进程和redis-sentinel进程，模拟master服务器宕机情况：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# ps -ef |grep redis</span><br><span class="line">root       7385      1  0 *        00:02:35 redis-server 0.0.0.0:6379</span><br><span class="line">root      20367  20127  0 *    00:00:00 redis-cli -a foo123</span><br><span class="line">root      23760      1  0 *       00:00:03 redis-sentinel 0.0.0.0:26379 [sentinel]</span><br><span class="line">[root@nn redis-5.0.7]# kill -9 7385</span><br><span class="line">[root@nn redis-5.0.7]# kill -9 23760</span><br></pre></td></tr></table></figure>
<p>在replica 1查看目前是否已转移到：可以看到两个replica已经选举出新的master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;182.0.0.11&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>也可通过redis-cli上查看：目前182.0.0.11已成为新的master，有个正常连接的replica</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info Replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:1</span><br><span class="line">min_slaves_good_slaves:1</span><br><span class="line">slave0:ip=182.0.0.12,port=6379,state=online,offset=146353,lag=1</span><br><span class="line">master_replid:4c1679086e6e4bb0bdd4956bcf979a6b964a8503</span><br><span class="line">master_replid2:ff0d944d22232a7b3489a8544c3109350aac6cd5</span><br><span class="line">master_repl_offset:146494</span><br><span class="line">second_repl_offset:145062</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:130700</span><br><span class="line">repl_backlog_histlen:15795</span><br></pre></td></tr></table></figure>
<p>在新maser set 值，可在剩余的一个replica get到相应的key</p>
<p>将原宕机的master恢复redis进程和sentinel进程，在新的master：182.0.0.11上，查看10节点已加入到replicas列表:<br>slave0:ip=182.0.0.12,port=6379,state=online,offset=211840,lag=1<br>slave1:ip=182.0.0.10,port=6379,state=online,offset=211840,lag=1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">min_slaves_good_slaves:2</span><br><span class="line">slave0:ip=182.0.0.12,port=6379,state=online,offset=211840,lag=1</span><br><span class="line">slave1:ip=182.0.0.10,port=6379,state=online,offset=211840,lag=1</span><br><span class="line">master_replid:4c1679086e6e4bb0bdd4956bcf979a6b964a8503</span><br><span class="line">master_replid2:ff0d944d22232a7b3489a8544c3109350aac6cd5</span><br><span class="line">master_repl_offset:211840</span><br><span class="line">second_repl_offset:145062</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:130700</span><br><span class="line">repl_backlog_histlen:81141</span><br></pre></td></tr></table></figure>
<p>以上完成redis 1主2从的高可用配置和测试，下面将在实际项目中引入。</p>
<h4 id="5、在python项目或者django的项目引入sentinel集群"><a href="#5、在python项目或者django的项目引入sentinel集群" class="headerlink" title="5、在python项目或者django的项目引入sentinel集群"></a>5、在python项目或者django的项目引入sentinel集群</h4><h5 id="5-1-python项目连接sentinel集群"><a href="#5-1-python项目连接sentinel集群" class="headerlink" title="5.1 python项目连接sentinel集群"></a>5.1 python项目连接sentinel集群</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> redis.sentinel <span class="keyword">import</span> Sentinel  </span><br><span class="line">In [<span class="number">4</span>]: st=Sentinel([(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="number">26379</span>),(<span class="string">&#x27;182.0.0.11&#x27;</span>,<span class="number">26379</span>),(<span class="string">&#x27;182.0.0.12&#x27;</span>,<span class="number">26379</span>)]) </span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: st.discover_master(<span class="string">&#x27;mymaster&#x27;</span>)                                     </span><br><span class="line">Out[<span class="number">7</span>]: (<span class="string">&#x27;182.0.0.10&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line">In [<span class="number">8</span>]: st.discover_slaves(<span class="string">&#x27;mymaster&#x27;</span>)                                     </span><br><span class="line">Out[<span class="number">8</span>]: [(<span class="string">&#x27;182.0.0.11&#x27;</span>, <span class="number">6379</span>), (<span class="string">&#x27;182.0.0.12&#x27;</span>, <span class="number">6379</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>查看用法<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [5]: ?st.master_for                                                                               </span><br><span class="line">Signature:</span><br><span class="line">st.master_for(</span><br><span class="line">    service_name,</span><br><span class="line">    redis_class=&lt;class &#x27;redis.client.Redis&#x27;&gt;,</span><br><span class="line">    connection_pool_class=&lt;class &#x27;redis.sentinel.SentinelConnectionPool&#x27;&gt;,</span><br><span class="line">    **kwargs,</span><br><span class="line">)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a redis client instance <span class="keyword">for</span> the ``service_name`` master.</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>创建连接实例，kwargs参数跟redis.Redis入参一致</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意别漏了sentinel集群设了密码</span></span><br><span class="line">In [<span class="number">9</span>]: master_rd=st.master_for(service_name=<span class="string">&#x27;mymaster&#x27;</span>,password=<span class="string">&#x27;foo123&#x27;</span>,db=<span class="number">0</span>)  </span><br><span class="line">In [<span class="number">10</span>]: replica_rd=st.slave_for(service_name=<span class="string">&#x27;mymaster&#x27;</span>,password=<span class="string">&#x27;foo123&#x27;</span>,db=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: master_rd              </span><br><span class="line">Out[<span class="number">19</span>]: Redis&lt;SentinelConnectionPool&lt;service=mymaster(master)&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向master写数据，不仅实现高可用，而且还实现读写分离</span></span><br><span class="line">In [<span class="number">23</span>]: master_rd.<span class="built_in">set</span>(<span class="string">&#x27;redis-HA&#x27;</span>,<span class="string">&#x27;good&#x27;</span>)                 </span><br><span class="line">Out[<span class="number">23</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: master_rd.get(<span class="string">&#x27;redis-HA&#x27;</span>)                </span><br><span class="line">Out[<span class="number">24</span>]: <span class="string">b&#x27;good&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果尝试向replica写数据则出错提示：replica只能读数据</span></span><br><span class="line">In [<span class="number">25</span>]: replica_rd.<span class="built_in">set</span>(<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;HA&#x27;</span>)    </span><br><span class="line">ReadOnlyError: You can<span class="string">&#x27;t write against a read only replica.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 从replica读数据，不仅实现高可用，而且还实现读写分离</span></span><br><span class="line"><span class="string">In [28]: replica_rd.get(&#x27;</span>redis-HA<span class="string">&#x27;)            </span></span><br><span class="line"><span class="string">Out[28]: b&#x27;</span>good<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在服务器上把当前10节点master kill掉，再看看python取值是否被影响</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 经过3秒左右，再次获取最新的master可以看到已转移到11节点上，所以这3秒时间实际也是丢失数据的时间窗口</span></span><br><span class="line">In [<span class="number">43</span>]: st.discover_master(<span class="string">&#x27;mymaster&#x27;</span>)                                     Out[<span class="number">43</span>]: (<span class="string">&#x27;182.0.0.11&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前集群仅有一个replica</span></span><br><span class="line">In [<span class="number">44</span>]: st.discover_slaves(<span class="string">&#x27;mymaster&#x27;</span>)</span><br><span class="line">Out[<span class="number">44</span>]: [(<span class="string">&#x27;182.0.0.12&#x27;</span>, <span class="number">6379</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主从切换后，不影响程序获取原有数据</span></span><br><span class="line">In [<span class="number">45</span>]: master_rd.get(<span class="string">&#x27;redis-HA&#x27;</span>)             </span><br><span class="line">Out[<span class="number">45</span>]: <span class="string">b&#x27;good&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: master_rd.<span class="built_in">set</span>(<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;test&#x27;</span>)               </span><br><span class="line">Out[<span class="number">47</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: replica_rd.get(<span class="string">&#x27;bar&#x27;</span>)         </span><br><span class="line">Out[<span class="number">48</span>]: <span class="string">b&#x27;test&#x27;</span></span><br></pre></td></tr></table></figure>
<h5 id="5-2-django项目中使用引入sentinel集群"><a href="#5-2-django项目中使用引入sentinel集群" class="headerlink" title="5.2 django项目中使用引入sentinel集群"></a>5.2 django项目中使用引入sentinel集群</h5><h6 id="5-2-1-单redis实例"><a href="#5-2-1-单redis实例" class="headerlink" title="5.2.1 单redis实例"></a>5.2.1 单redis实例</h6><p>在Django项目开发中，一般可以redis作为django后端cache的中间件，很多需求可以满足：例如验证码、session、缓存查询数据等。</p>
<p>首先需要django-redis这个库支持：pip install django-redis，具体用法参考<a href="http://niwinz.github.io/django-redis/latest/">官方doc</a></p>
<p>如果是单实例redis，在setting的设置相对简单：redis的0号db作为默认缓存，1号db作为hp这个App的数据缓存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将redis设为django缓存</span></span><br><span class="line">CACHES = &#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;BACKEND&#x27;</span>: <span class="string">&#x27;django_redis.cache.RedisCache&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;LOCATION&#x27;</span>: [<span class="string">&#x27;redis://182.0.0.10:6379/0&#x27;</span>], //连接redis url</span><br><span class="line">        <span class="string">&#x27;KEY_PREFIX&#x27;</span>: <span class="string">&#x27;dj&#x27;</span>,   //前缀名</span><br><span class="line">        <span class="string">&#x27;OPTIONS&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;CLIENT_CLASS&#x27;</span>: <span class="string">&#x27;django_redis.client.DefaultClient&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;CONNECTTON_POOL_KWARGS&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;max_connections&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;hp&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;BACKEND&#x27;</span>: <span class="string">&#x27;django_redis.cache.RedisCache&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;LOCATION&#x27;</span>: [<span class="string">&#x27;redis://182.0.0.10:6379/1&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;KEY_PREFIX&#x27;</span>: <span class="string">&#x27;dj:bi&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;OPTIONS&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;CLIENT_CLASS&#x27;</span>: <span class="string">&#x27;django_redis.client.DefaultClient&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;CONNECTTON_POOL_KWARGS&#x27;</span>: &#123;</span><br><span class="line">                    <span class="string">&#x27;max_connections&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">SESSION_ENGINE = <span class="string">&#x27;django.contrib.sessions.backends.cache&#x27;</span></span><br><span class="line"><span class="comment">#django自身运行上下文使用默认数据库redis缓存</span></span><br><span class="line">SESSION_CACHE_ALIAS = <span class="string">&#x27;default&#x27;</span>  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>启动django shell测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@nn hp]<span class="comment"># python manage.py shell</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="keyword">from</span> django.core.cache <span class="keyword">import</span> cache,caches     </span><br><span class="line">In [<span class="number">4</span>]: cache.<span class="built_in">set</span>(<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;fofo&#x27;</span>)                     </span><br><span class="line">Out[<span class="number">4</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用default 库</span></span><br><span class="line">In [<span class="number">5</span>]: cache.get(<span class="string">&#x27;name&#x27;</span>)                                                   </span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">&#x27;fofo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用hp 库</span></span><br><span class="line">In [<span class="number">11</span>]: caches[<span class="string">&#x27;hp&#x27;</span>].<span class="built_in">set</span>(<span class="string">&#x27;code&#x27;</span>,<span class="number">133</span>)                                       </span><br><span class="line">Out[<span class="number">11</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: caches[<span class="string">&#x27;hp&#x27;</span>].get(<span class="string">&#x27;code&#x27;</span>)                                           </span><br><span class="line">Out[<span class="number">12</span>]: <span class="number">133</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：如果使用cache方法，则默认使用default数据库，若需要使用其他名称的数据，需要用caches方法，并通过settings里面redis数据库名称来索引</p>
<p>在redis服务器查看default库相应的key：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) &quot;bar&quot;</span><br><span class="line">2) &quot;redis-HA&quot;</span><br><span class="line">3) &quot;dj:1:name&quot;</span><br></pre></td></tr></table></figure><br>因为在cache里面设置key的前缀为dj，加上cache会给key再打上<code>:1:</code>这个默认前缀，故实际存储的完整key名称:”dj:1:name”<br>同理查看db1号库的key<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SELECT 1</span><br><span class="line">OK</span><br><span class="line"> </span><br><span class="line">127.0.0.1:6379[1]&gt; keys *</span><br><span class="line">1) &quot;dj:bi:1:code&quot;</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379[1]&gt; get dj:bi:1:code</span><br><span class="line">&quot;133&quot;</span><br></pre></td></tr></table></figure></p>
<p>若Django项目中需要使用更多进阶的原生client功能连接redis（支持redis所有方法)），需使用get_redis_connection，建议在实际项目使用该方法获取redis连接对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">13</span>]: <span class="keyword">from</span> django_redis <span class="keyword">import</span> get_redis_connection</span><br><span class="line">In [<span class="number">14</span>]: conn = get_redis_connection()                                     </span><br><span class="line">In [<span class="number">15</span>]: conn.<span class="built_in">set</span>(<span class="string">&#x27;fb&#x27;</span>,<span class="number">12</span>)                                                 </span><br><span class="line">Out[<span class="number">15</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: conn.get(<span class="string">&#x27;fb&#x27;</span>)                                                     </span><br><span class="line">Out[<span class="number">16</span>]: <span class="string">b&#x27;12&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h6 id="5-2-2-django项目的cache引入sentinel模式"><a href="#5-2-2-django项目的cache引入sentinel模式" class="headerlink" title="5.2.2  django项目的cache引入sentinel模式"></a>5.2.2  django项目的cache引入sentinel模式</h6><p>该模式需要安装新的django插件，<a href="https://github.com/KabbageInc/django-redis-sentinel">github地址</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install django-redis-sentinel</span><br><span class="line"><span class="meta">#</span><span class="bash">或者pip install django-redis-sentinel-redux 0.2.0</span> </span><br></pre></td></tr></table></figure>
<p>这个插件其实封装了redis.sentinel将其作为django_redis的插件之一，注意，该插件已不再维护，如果在重要项目上，不建议使用。（重要项目直接用codis，或者redis-cluster，以便可以pip install到相关的redis连接插件，或者自己参考模板写一个）</p>
<p>在setting.py中cache的设置：<br>Location 格式: master_name/sentinel_server:port,sentinel_server:port/db_id</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CACHES = &#123;</span><br><span class="line">        <span class="string">&quot;default&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;BACKEND&quot;</span>: <span class="string">&quot;django_redis.cache.RedisCache&quot;</span>,</span><br><span class="line">            <span class="string">&quot;LOCATION&quot;</span>: <span class="string">&quot;mymaster/188.0.0.10:26379,188.0.0.11:26379,188.0.0.12:26379/0&quot;</span></span><br><span class="line">            <span class="string">&quot;OPTIONS&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;PASSWORD&quot;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">                <span class="string">&quot;CLIENT_CLASS&quot;</span>: <span class="string">&quot;django_redis_sentinel.SentinelClient&quot;</span>,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>该 django-redis-sentinel的插件代码实现不到120行，主要看看connect方法，写数据时用sentinel.discover_master(master_name)获取master去写，读数据时，随机取一个replica 读数据random.choice(sentinel.discover_slaves(master_name))</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect</span>(<span class="params">self, write=<span class="literal">True</span>, SentinelClass=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates a redis connection with connection pool.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> SentinelClass <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        SentinelClass = Sentinel</span><br><span class="line">    self.log.debug(<span class="string">&quot;connect called: write=%s&quot;</span>, write)</span><br><span class="line">    master_name, sentinel_hosts, db = self.parse_connection_string(self._connection_string)</span><br><span class="line"></span><br><span class="line">    sentinel_timeout = self._options.get(<span class="string">&#x27;SENTINEL_TIMEOUT&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">    password = self._options.get(<span class="string">&#x27;PASSWORD&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    sentinel = SentinelClass(sentinel_hosts,</span><br><span class="line">                             socket_timeout=sentinel_timeout,</span><br><span class="line">                             password=password)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> write:</span><br><span class="line">        host, port = sentinel.discover_master(master_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            host, port = random.choice(sentinel.discover_slaves(master_name))</span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            self.log.debug(<span class="string">&quot;no slaves are available. using master for read.&quot;</span>)</span><br><span class="line">            host, port = sentinel.discover_master(master_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> password:</span><br><span class="line">        connection_url = <span class="string">&quot;redis://:%s@%s:%s/%s&quot;</span> % (password, host, port, db)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        connection_url = <span class="string">&quot;redis://%s:%s/%s&quot;</span> % (host, port, db)</span><br><span class="line">    <span class="keyword">return</span> self.connection_factory.connect(connection_url)</span><br></pre></td></tr></table></figure>
<p>注意：如果使用sentinel这个插件，那么需使用django_redis的get_redis_connection，而不是使用django.core.cache的cache</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> django_redis <span class="keyword">import</span> get_redis_connection   </span><br><span class="line">In [<span class="number">2</span>]: conn = get_redis_connection()       </span><br><span class="line">In [<span class="number">3</span>]: conn.<span class="built_in">set</span>(<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;airpods&#x27;</span>)               </span><br><span class="line">Out[<span class="number">3</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: conn.get(<span class="string">&#x27;apple&#x27;</span>)                              </span><br><span class="line">Out[<span class="number">4</span>]: <span class="string">b&#x27;airpods&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：get_redis_connection()有bug，当使用conn实例获取哨兵集群的master或者replica，get_redis_connection()调用了<code>your pythonpath/python3.7/site-packages/redis/client.py</code>里面的<code>self.execute_command(&#39;SENTINEL MASTER&#39;, service_name)</code>，然而该方法是在6379端口连接的client执行<code>SENTINEL MASTER mymaster</code>，由于所有的SENTINEL 命令只能在26379端口下启动的client才能执行，因此直接用get_redis_connection()获取sentinel信息会提示未知命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">8</span>]: conn.sentinel_master(<span class="string">&#x27;mymaster&#x27;</span>) </span><br><span class="line">ResponseError: unknown command `SENTINEL`, <span class="keyword">with</span> args beginning <span class="keyword">with</span>: `MASTER`, `mymaster`, </span><br></pre></td></tr></table></figure>
<p>测试以上出错信息也简单：连接redis-server的6379端口client</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -a foo123 </span><br><span class="line">127.0.0.1:6379&gt; sentinel master mymaster</span><br><span class="line">(error) ERR unknown command `sentinel`, with args beginning with: `master`, `mymaster`, </span><br></pre></td></tr></table></figure>
<p>由于sentinel集群监听的是26379端口来执行有关查询命令（端口号在sentinel.conf文件配置），而get_redis_connection()使用的是6379，用此报错。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -a foo123 -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;188.0.0.10&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> 7) &quot;runid&quot;</span><br><span class="line"> 8) &quot;5537ec765629633406942061f5993e475c42df8e&quot;</span><br><span class="line"> 9) &quot;flags&quot;</span><br><span class="line">10) &quot;master&quot;</span><br></pre></td></tr></table></figure>
<p>解决办法有三种种：<br>第一种：修改redis源码，改get_redis_connection，也不难<br>第二种：sentinel监听端口设为默认6379，redis-server设为其他端口号即可<br>第三种：不需要使用django redis插件，直接用redis原生库自行封装相关sentinel的操作</p>
<p>&#8195;&#8195;综上完成基于sentinel模式的redisHA配置以及详细解释了在python项目和django项目中如何使用sentinel集群模式，个人认为，目前该方案足够支持大部分中小企业内部自行开发项目。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Sentinel模式</tag>
        <tag>redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于YARN HA集群的Spark HA集群</title>
    <url>/2019/12/08/%E5%9F%BA%E4%BA%8EYARN%20HA%E9%9B%86%E7%BE%A4%E7%9A%84Spark%20HA%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>，这篇是基于非HA模式下hadoop集群的spark集群HA配置，而本文将给出基于HA模式下hadoop集群的spark集群HA配置，并将yarn HA集群映入到spark中，做资源管理。为何要做些环境的配置呢？因为到本篇文章为止，已经完成hadoop HA集群、hbaseHA集群，hive集群（非HA）、sparkHA集群、flumeHA集群、kafka HA集群，实现实时数据流动，接下的文章重点探讨spark streaming、spark以及pyspark相关知识，这将涉及多个计算任务以及相关计算资源的分配，因此需要借助yarn HA集群强大的资源管理服务来管理spark的计算任务，从而实现完整的、接近生产环境的、HA模式下的大数据实时分析项目的架构。</p>
<a id="more"></a>
<p>服务器资源分配表(仅列出yarn和spark)：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>节点</th>
<th>yarn 角色</th>
<th>spark 角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>nn</td>
<td>ResourceManager， NodeManager</td>
<td>Master，Worker</td>
</tr>
<tr>
<td>dn1</td>
<td>NodeManager</td>
<td>Worker</td>
</tr>
<tr>
<td>dn2</td>
<td>ResourceManager， NodeManager</td>
<td>Master，Worker</td>
</tr>
</tbody>
</table>
</div>
<p>&#8195;&#8195;这里再提下yarn管理大数据集群计算中对资源有效管理（主要指CPU、物理内存以及虚拟内存）的重要性：</p>
<blockquote>
<p>&#8195;&#8195;整个集群的计算任务由ResourceManager和NodeManager共同完成，其中，ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为计算任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。</p>
</blockquote>
<p>&#8195;&#8195;因为spark就是负责计算，有大量计算任务要运行，每个任务总得分配cpu和内存给它用，否则某些计算任务会被“饿死”（巧妇难为无米之炊），这种比喻比较形象。</p>
<h3 id="YARN-HA模式的配置"><a href="#YARN-HA模式的配置" class="headerlink" title="YARN HA模式的配置"></a>YARN HA模式的配置</h3><p>&#8195;&#8195;yarn HA模式的运行是于hadoop HA模式运行的，关于hadoop HA部署和测试可以参考本博客文章<a href="https://blog.csdn.net/pysense/article/details/102635656">《基于Hadoop HA集群部署HBase HA集群（详细版）》</a>的第6章内容，考虑到后面文章将会给出各种spark计算任务，结合测试服务器本身cpu和内存资源有限，这里主要重点介绍yarn-site.xml和mapred-site.xml配置文件说明。</p>
<h4 id="完整-yarn-site-xml配置"><a href="#完整-yarn-site-xml配置" class="headerlink" title="完整 yarn-site.xml配置"></a>完整 yarn-site.xml配置</h4><p>&#8195;&#8195;yarn-site的配置其实分为两大块：第一部分为yarn HA集群的配置，第二部分为根据现有测试服务器资源来优化yarn配置。<br>==yarn-site.xml在三个节点上都使用相同配置，无需更改==<br>第一部分：yarn HA集群的配置<br>（注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code>&lt;configuration&gt;&lt;/configuration&gt;</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;!-- 启用yarn HA高可用性 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定resourcemanager的名字，自行命名，跟服务器hostname无关 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hayarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定nn节点为rm1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定dn2节点为rm2  --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;dn2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定当前机器nn作为主rm1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定zookeeper集群机器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上将nn和dn2作为yarn集群主备节点，对应的id为rm1、rm2</p>
<p>第二部分：yarn的优化配置<br>A、禁止检查每个任务正使用的物理内存量、虚拟内存量是否可用<br>若任务超出分配值，则将其杀掉。考虑到作为测试环境，希望看到每个job都能正常运行，以便记录其他观测事项，这里将其关闭。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> 	&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line"> 	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> 	&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line"> 	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>B、配置RM针对单个Container能申请的最大资源或者RM本身能配置的最大内存<br>配置解释：单个容器可申请的最小与最大内存，Application在运行申请内存时不能超过最大值，小于最小值则分配最小值，例如在本文测试中，因计算任务较为简单，无需太多资源，故最小值设为512M，最大值设为1024M。注意最大最不小于1G，因为yarn给一个executor分配512M时，还需要另外动态的384M内存（Required executor memory (512), overhead (384 MB)）。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;512&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><br>若将yarn.scheduler.maximum-allocation-mb设为例如512M，spark on yarn就会启动失败。</p>
<p>C、NM的内存资源配置，主要是通过下面两个参数进行的</p>
<p>第一个参数：每个节点可用的最大内存，默认值为-1，代表着yarn的NodeManager占总内存的80%，本文中，物理内存为1G</p>
<p>第二个参数：NM的虚拟内存和物理内存的比率，默认为2.1倍<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;nm向本机申请的最大物理内存，默认8G&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><br>vmem-pmem-ratio的默认值为2.1，由于本机器中，每个节点的物理内存为1G，因此单个RM拿到最大虚拟内存为2.1G，例如在跑spark任务，会出现<code>2.5 GB of 2.1 GB virtual memory used. Killing container</code>的提示，Container申请的资源为2.5G，已经超过默认值2.1G，当改为3倍时，虚拟化够用，故解决可该虚拟不足的情况。</p>
<h4 id="mapred-site-xml的配置文件说明"><a href="#mapred-site-xml的配置文件说明" class="headerlink" title="mapred-site.xml的配置文件说明"></a>mapred-site.xml的配置文件说明</h4><p>mapred-site的配置其实分为两大块：第一部分为mapreduce的基本配置，第二部分为根据现有测试服务器资源来优化mapreduce计算资源分配的优化配置。<br>==mapred-site.xml在三个节点上都需要配置，只需把nn主机名改为当前节点的主机名即可==<br>第一部分：mapreduce的基本配置<br>（注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code>&lt;configuration&gt;&lt;/configuration&gt;</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> &lt;!-- 使用yarn框架来管理MapReduce --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!-- mp所需要hadoop环境 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:10020&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定nn作为jobhistory服务器 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">  		&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line"> 		 &lt;value&gt;nn:19888&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!--存放已完成job的历史日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放正在运行job的历史日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done_intermediate&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放yarn stage的日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/history/staging&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>这里主要配置开启jobhistory服务以及MapReduce多种日志存放</p>
<p>第二部分：mapreduce的优化项<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个mapper任务的物理内存限制&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;200&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个reducer任务的物理内存限制&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;每个mapper任务申请的虚拟cpu核心数，默认1&lt;/description&gt; </span><br><span class="line"> &lt;/property&gt; </span><br><span class="line"> </span><br><span class="line"> &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;每个reducer任务申请的虚拟cpu核心数，默认1&lt;/description&gt; </span><br><span class="line"> &lt;/property&gt; </span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx100m&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;mapper阶段的JVM的堆大小&lt;/description&gt;     </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx200m&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;reduce阶段的JVM的堆大小&lt;/description&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><br>根据当前服务器物理配置资源，在内存和CPU方面给mapper和reducer任务进行调优。</p>
<h4 id="yarn-HA的启动"><a href="#yarn-HA的启动" class="headerlink" title="yarn HA的启动"></a>yarn HA的启动</h4><p>首先确保hadoop HA集群已正常启动<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure><br>启动yarn HA服务，只需在nn节点启动yarn后，其他节点会自动启动相应服务。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# start-yarn.sh </span><br><span class="line">[root@nn sbin]# yarn rmadmin -getServiceState rm1</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# yarn rmadmin -getServiceState rm2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure><br>以上完成yarn HA配置，因为涉及hadoop HA和调优，因此不建议刚入门的同学就按此配置继续测试，建议从最原始、最简单的非HA hadoop开始着手。<br>下面开始配置spark。</p>
<h3 id="spark-HA-集群及其基本测试"><a href="#spark-HA-集群及其基本测试" class="headerlink" title="spark HA 集群及其基本测试"></a>spark HA 集群及其基本测试</h3><h4 id="修改spark配置"><a href="#修改spark配置" class="headerlink" title="修改spark配置"></a>修改spark配置</h4><p>&#8195;&#8195;经历第1章节繁琐的yarn HA配置后， 当资源管理问题得到妥善解决，那么接下的计算任务将实现的非常流畅。<br>spark HA集群详细的部署和测试，请参考<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>的第8章节，本文不再累赘。<br>&#8195;&#8195;把spark 的任务交给yarn管理还需要在HA集群上再加入部分配置，改动也简单 ，只需在spark-defaults.conf和spark-env.sh改动。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf]# vi spark-defaults.conf</span><br><span class="line"></span><br><span class="line">#spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"></span><br><span class="line"># spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;hdapp&#x2F;directory</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.yarn.jars                  hdfs:&#x2F;&#x2F;hdapp&#x2F;spark_jars&#x2F;*</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure><br>重点配置项目说明：</p>
<p>原standalone模式下：spark.master设为 spark://nn:7077</p>
<p>因为spark已经配成HA模式，因此无需指定master是谁，交由zookeeper管理。</p>
<p>spark.eventLog.dir              hdfs://hdapp/directory<br>这里hdfs路径从nn:9000改为hdapp，是因为hadoop已经配置为HA模式，注意集群模式下是不需要加上端口： hdfs://hdapp:9000/directory，这会导致NameNode无法解析host部分。</p>
<p>spark.yarn.jars                  hdfs://hdapp/spark_jars/*<br>这里需要将spark跟目录下的jar包都上传到hdfs指定的spark_jars目录下，若不这么处理，每次提交spark job时，客户端每次得先上传这些jar包到hdfs，然后再分发到每个NodeManager，导致任务启动很慢。而且启动spark也会提示：<br>==WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.==</p>
<p>解决办法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -mkdir  &#x2F;spark_jars</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -put  jars&#x2F;*  &#x2F;spark_jars</span><br></pre></td></tr></table></figure><br>spark-defaults.conf在三个节点上使用相同配置。</p>
<p>spark-env.sh的配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf]# vi spark-env.sh</span><br><span class="line"># 基本集群配置</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;opt&#x2F;scala-2.12.8</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url&#x3D;nn:2181,dn1:2181,dn2:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark&quot;</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line"></span><br><span class="line"># yarn模式下的调优配置</span><br><span class="line"># Options read in YARN client&#x2F;cluster mode</span><br><span class="line">export SPARK_WORKER_MEMORY&#x3D;512M</span><br><span class="line"># - SPARK_CONF_DIR, Alternate conf dir. (Default: $&#123;SPARK_HOME&#125;&#x2F;conf) 无需设置，使用默认值</span><br><span class="line"># - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line"># - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN 上面HADOOP_CONF_DIR以已设置即可</span><br><span class="line"># - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1). 无需设置，默认使用1个vcpu</span><br><span class="line"># - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">export SPARK_EXECUTOR_MEMORY&#x3D;512M</span><br><span class="line"># - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">export SPARK_EXECUTOR_MEMORY&#x3D;512M</span><br><span class="line"></span><br><span class="line"># 存放计算过程的日志</span><br><span class="line">export SPARK_HISTORY_OPTS&#x3D;&quot;</span><br><span class="line">-Dspark.history.ui.port&#x3D;9001</span><br><span class="line">-Dspark.history.retainedApplications&#x3D;5</span><br><span class="line">-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;hdapp&#x2F;directory&quot;</span><br></pre></td></tr></table></figure><br>以上的driver和executor的可用内存设为512M，考虑到测试服务器内存有限的调优。若生产服务器，一般32G或者更大的内存，则可以任性设置。</p>
<h4 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h4><p>在nn节点上，启动wokers： start-slaves.sh，该命令自动启动其他节点的worker<br>在nn节点和dn2节点启动master进程：start-master.sh<br>查看nn:8080和dn2:8080的spark web UI是否有active以及standby模式。<br>跑一个wordcount例子，测试spark集群能否正常计算结果。<br>创建一个本地文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  vi &#x2F;opt&#x2F;foo.txt</span><br><span class="line">spark on yarn</span><br><span class="line">yarn </span><br><span class="line">spark HA</span><br></pre></td></tr></table></figure><br>启动pyspark，连接到spark集群<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  .&#x2F;bin&#x2F;pyspark --name bar --driver-memory 512M   --master  spark:&#x2F;&#x2F;nn:7077</span><br><span class="line"># 读取本地文件&#x2F;opt&#x2F;foo.txt</span><br><span class="line">&gt;&gt;&gt; df&#x3D;sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;opt&#x2F;foo.txt&quot;)</span><br><span class="line"># 切分单词，过滤空值</span><br><span class="line">&gt;&gt;&gt; words &#x3D; df.flatMap(lambda line: line.split(&#39; &#39;)).filter(lambda x: x !&#x3D;&quot;&quot;)</span><br><span class="line">&gt;&gt;&gt; words.collect()</span><br><span class="line">[u&#39;spark&#39;, u&#39;on&#39;, u&#39;yarn&#39;, u&#39;yarn&#39;,u&#39;spark&#39;, u&#39;HA&#39;]</span><br><span class="line"># 将个word映射为（word，1）这样的元组，在reduce汇总。</span><br><span class="line">&gt;&gt;&gt; counts &#x3D; words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)</span><br><span class="line">&gt;&gt;&gt; counts.collect()</span><br><span class="line">[(u&#39;spark&#39;, 2), (u&#39;yarn&#39;, 2), (u&#39;on&#39;, 1), (u&#39;HA&#39;, 1)]   </span><br></pre></td></tr></table></figure><br>以上完成spark HA集群和测试。</p>
<h3 id="spark-on-yarn"><a href="#spark-on-yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h3><p>spark on yarn意思是将spark计算人任务提交到yarn集群上运行。</p>
<h4 id="spark集群跑在yarn上的两种方式"><a href="#spark集群跑在yarn上的两种方式" class="headerlink" title="spark集群跑在yarn上的两种方式"></a>spark集群跑在yarn上的两种方式</h4><p>根据spark官网的<a href="http://spark.apache.org/docs/latest/running-on-yarn.html">文档说明</a>，这里引用其内容：</p>
<blockquote>
<p>There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>
</blockquote>
<p>cluster模式下，spark driver 在 AM里运行，客户端（或者应用程序）在提交完任务（初始化）后可直接退出，作业会继续在 YARN 上运行。显然cluster 模式不适合交互式操作。cluster模式的spark计算结果可以保持到<br>外部数据库，例如hbase。这部分内容将是spark streaming可以完成的环境，spark streaming以yarn cluster模式运行，实时将处理结果存到hbase里，web BI 应用再从hbase取数据。</p>
<p>client模式下，spark driver是在本地环境运行，AM仅负责向yarn请求计算资源（Executor 容器），例如交互式运行基本的操作。</p>
<p>在前面第2节的word count例子里，用下面的启动命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# pyspark --name bar --driver-memory 512M   --master  spark:&#x2F;&#x2F;nn:7077</span><br></pre></td></tr></table></figure><br>该命令启动是一个spark shell进程，没有引入yarn管理其资源，因此在yarn集群的管理页面<code>http://nn:8088/cluster/apps/RUNNING</code>，将不会 bar这个application。</p>
<h4 id="测试spark-on-yarn"><a href="#测试spark-on-yarn" class="headerlink" title="测试spark on yarn"></a>测试spark on yarn</h4><p>只需在启动spark shell时，将<code>--master spark://nn:7077</code> 改为<br><code>--master yarn --deploy-mode cluster</code>或者<code>--master yarn --deploy-mode client</code>，那么spark提交的任务就会交由yarn集群管理<br>还是以word count为例，使用yarn client模式启动spark<br>创建测试文件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;opt&#x2F;yarn-word-count.txt</span><br><span class="line">spark on yarn </span><br><span class="line">spark HA </span><br><span class="line">yarn HA</span><br></pre></td></tr></table></figure><br>启动driver<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  pyspark --name client_app	 --driver-memory 512M  --executor-memory 512M  --master yarn --deploy-mode client</span><br><span class="line">Python 2.7.5 (default, Oct 30 2018, 23:45:53) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     &#x2F; __&#x2F;__  ___ _____&#x2F; &#x2F;__</span><br><span class="line">    _\ \&#x2F; _ \&#x2F; _ &#96;&#x2F; __&#x2F;  &#39;_&#x2F;</span><br><span class="line">   &#x2F;__ &#x2F; .__&#x2F;\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\_\   version 2.4.4</span><br><span class="line">      &#x2F;_&#x2F;</span><br><span class="line"></span><br><span class="line">Using Python version 2.7.5 (default, Oct 30 2018 23:45:53)</span><br><span class="line">SparkSession available as &#39;spark&#39;.</span><br><span class="line">&gt;&gt;&gt; sc</span><br><span class="line">&lt;SparkContext master&#x3D;yarn appName&#x3D;client_app	&gt;</span><br></pre></td></tr></table></figure></p>
<p>这里driver和executor都是以最小可用内存512来启动spark-shell<br>因为该spark 任务是提交到yarn 上运行，所以在spark web ui后台：<code>http://nn:8080</code>，running application 为0<br><img src="https://img-blog.csdnimg.cn/20191208110757819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这是需要去yarn后台入口：<code>http://nn:8088</code>，可以看到刚提交的计算任务：<br><img src="https://img-blog.csdnimg.cn/20191208111208596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到该application（计算任务）分配了3个Container<br><img src="https://img-blog.csdnimg.cn/20191208111547983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">通过查看该applicationMaster管理页面，可以看到client-yarn这个app更为详细的计算过程，例如该wordcount在reduceByKey DAG可视化过程。<br><img src="https://img-blog.csdnimg.cn/20191208111947653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>yarn cluster模式下，因为它不是打开一个spark shell让你交互式输入数据处理逻辑，所以需先把处理逻辑封装成一个py模块。<br>以上面的word count为例：<br>word_count.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">word_count</span>():</span></span><br><span class="line">	    conf = SparkConf().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&#x27;cluster-yarn&#x27;</span>)</span><br><span class="line">	    sc = SparkContext(conf=conf)</span><br><span class="line">	    <span class="comment"># 统计文件中包含mape的行数，并打印第一行</span></span><br><span class="line">	    df = sc.textFile(<span class="string">&quot;/tmp/words.txt&quot;</span>)</span><br><span class="line">	    words = df.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&#x27; &#x27;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x !=<span class="string">&quot;&quot;</span>)</span><br><span class="line">	    <span class="built_in">print</span> words.collect()</span><br><span class="line">	    counts = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">	    <span class="built_in">print</span> counts.collect()</span><br><span class="line">	    sc.stop</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	word_count()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>需要使用spark-submit 提交到yarn<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 spark-2.4.4-bin-hadoop2.7]#  ./bin/spark-submit  --driver-memory 512M  --executor-memory 512M  --master yarn  --deploy-mode cluster  --py-files word_count.py</span><br></pre></td></tr></table></figure><br>在yarn管理也可以看到该app，application的命名好像直接用脚本名字，而不是指定的cluster-yarn<br><img src="https://img-blog.csdnimg.cn/20191208120528893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">关于如何提交py文件，官方也给出指引：</p>
<blockquote>
<p>For Python, you can use the —py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application. If you depend on multiple Python files we recommend packaging them into a .zip or .egg.</p>
</blockquote>
<p>如有多个py文件（例如1.py依赖2.py和3.py），需要通过将其打包为.zip或者.egg包： —py-files tasks.zip</p>
<h4 id="提交spark-application的多种方式"><a href="#提交spark-application的多种方式" class="headerlink" title="提交spark application的多种方式"></a>提交spark application的多种方式</h4><p>spark运行有standalone模式（分local、cluster）、on yarn模式（分client、cluster）还有on k8s，而且可以附带jar包或者py包，多种提交的方式的命令模板怎么写？网上其实很多类似文章，但都是给的某个模式的某种文件的提交方式，其实在spark官网的<a href="http://spark.apache.org/docs/latest/submitting-applications.html">submitting-applications</a>章节给出详细的多种相关命令模板。这里统一汇总：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run application locally on 8 cores 本地模式</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master local[8] \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  standalone 集群下的client模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  standalone 集群下的cluster模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> on yarn 集群，且用的class文件和jar包</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a YARN cluster</span></span><br><span class="line">export HADOOP_CONF_DIR=XXX</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \  # can be client for client mode</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里给出如何传入py文件，可以不写 --py-files 选项</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Mesos cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master mesos://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Kubernetes cluster <span class="keyword">in</span> cluster deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master k8s://xx.yy.zz.ww:443 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&#8195;&#8195;本文内容主要为后面的文章——spark streaming 与kafka集群的实时数据计算做铺垫，考虑到测试环境环境资源有限，在做spark streaming的时候，将不会以spark HA模式运行，也不会将任务提交到yarn集群上，而是用一节点作为spark streaming计算节点，具体规划参考该文。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>YARN集群</tag>
        <tag>Spark集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于hadoop3.1.2分布式平台上部署spark HA集群</title>
    <url>/2019/10/13/%E5%9F%BA%E4%BA%8Ehadoop3.1.2%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0%E4%B8%8A%E9%83%A8%E7%BD%B2spark%20HA%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;在此文章<a href="https://blog.csdn.net/pysense/article/details/102490212">《基于Centos7.5完整部署分布式Hadoop3.1.2》</a>里，已经给出详细的hadoop和yarn的部署过程，既然已经解决了大数据开发中“hdfs”的数据存储部署，那么就要考虑如何基于底层分布式文件基础上运行计算框架，以便进行更高层次的应用开发。在本篇文章中，将给出完整部署spark计算框架集群。</p>
<a id="more"></a>
<h3 id="1、spark版本（仅列出spark相关）"><a href="#1、spark版本（仅列出spark相关）" class="headerlink" title="1、spark版本（仅列出spark相关）"></a>1、spark版本（仅列出spark相关）</h3><p>spark-2.4.4-bin-hadoop2.7，该版本的spark支持hadoop2.7以及之后的版本</p>
<p>scala-2.13.1：使用Scala语言开发数据处理逻辑，当然也可使用python进行spark数据处理逻辑开发，官网有给出pyspark相关指导教程。</p>
<p>三台节点都需要配置，目录放置路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">hadoop-3.1.2    jdk1.8.0_161  scala-2.13.1  spark-2.4.4-bin-hadoop2.7</span><br></pre></td></tr></table></figure>
<p>spark HA集群规划，这里只列出spark HA集群的有关进程，hadoop的进程不再列出</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>IP，hostname</th>
<th>spark集群中负责的角色</th>
<th>Spark 路径</th>
<th>Scala路径</th>
<th>物理内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.188.0.4，nn</td>
<td>master，worker，spark-history-server</td>
<td>/opt/spark-2.4.4-bin-hadoop2.7</td>
<td>/opt/scala-2.13.1</td>
<td>2G</td>
</tr>
<tr>
<td>192.188.0.5，dn1</td>
<td>master，worker</td>
<td>/opt/spark-2.4.4-bin-hadoop2.7</td>
<td>/opt/scala-2.13.1</td>
<td>1G</td>
</tr>
<tr>
<td>192.188.0.6，dn2</td>
<td>master，worker</td>
<td>/opt/spark-2.4.4-bin-hadoop2.7</td>
<td>/opt/scala-2.13.1</td>
<td>1G</td>
</tr>
</tbody>
</table>
</div>
<p>这里spark master节点nn的物理内存给了2G，因为该节点不仅仅启动了spark相关主服务，还得启动hadoop相关主服务，如果物理内存不足，在后面章节中启动spark-shell或者跑application都无法正常启动，提示资源不足。</p>
<h3 id="2、设置path环境"><a href="#2、设置path环境" class="headerlink" title="2、设置path环境"></a>2、设置path环境</h3><p>三个节点都需要设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8.0_161</span><br><span class="line">export HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export SCALA_HOME=/opt/scala-2.13.1</span><br><span class="line">export SPARK_HOME=/opt/spark-2.4.4-bin-hadoop2.7/</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SCALA_HOME/bin:</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure>
<h3 id="3、配置spark集群的相关文件"><a href="#3、配置spark集群的相关文件" class="headerlink" title="3、配置spark集群的相关文件"></a>3、配置spark集群的相关文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 拷贝一份spark-env.sh文件用于配置spark环境</span></span><br><span class="line">[root@dn1 ~]# cp /opt/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh.template /opt/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh</span><br><span class="line">[root@dn1 ~]# cd /opt/spark-2.4.4-bin-hadoop2.7/</span><br><span class="line"></span><br><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# ls conf/</span><br><span class="line">docker.properties.template   slaves.template</span><br><span class="line">fairscheduler.xml.template   spark-defaults.conf.template</span><br><span class="line">log4j.properties.template    spark-env.sh</span><br><span class="line">metrics.properties.template  spark-env.sh.template</span><br><span class="line"></span><br><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# vi conf/spark-env.sh</span><br></pre></td></tr></table></figure>
<p>只需在spark-env.sh文件头部加入以下环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export SCALA_HOME=/opt/scala-2.12.8</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8.0_161</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设定192.188.0.4，nn节点为spark master</span></span><br><span class="line">export SPARK_MASTER_IP=nn</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop的配置文件**site.xml所在目录</span></span><br><span class="line">export HADOOP_CONF_DIR=/opt/hadoop-3.1.2/etc/hadoop</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>修改conf目录下的slaves文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@dn1 conf]# cp slaves.template slaves</span><br><span class="line">[root@dn1 conf]# vi slaves</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>为减少spark主节点nn的内存资源消耗，这里不再将nn设为Worker角色</p>
<p>将修改过的两个文件拷贝到其他两个节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# scp -r conf&#x2F; dn1:&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;</span><br><span class="line"></span><br><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# scp -r conf&#x2F; dn2:&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;</span><br></pre></td></tr></table></figure>
<h3 id="4、启动spark集群进程"><a href="#4、启动spark集群进程" class="headerlink" title="4、启动spark集群进程"></a>4、启动spark集群进程</h3><h4 id="4-1-启动spark-master进程"><a href="#4-1-启动spark-master进程" class="headerlink" title="4.1 启动spark-master进程"></a>4.1 启动spark-master进程</h4><p>spark的进程启动是有步骤的，需先启动master服务，再启动worker进程，因为worker启动需要通过spark://nn:7077 spark协议的7077端口与master节点通信，否则master节点和worker之间无法形成集群。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# .&#x2F;start-master.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-nn.out</span><br><span class="line"></span><br><span class="line"># nn节点上</span><br><span class="line">[root@nn sbin]# jps</span><br><span class="line">24292 DataNode</span><br><span class="line">24155 NameNode</span><br><span class="line">25339 NodeManager</span><br><span class="line">30638 Master</span><br><span class="line">30750 Jps</span><br><span class="line"></span><br><span class="line"># dn1节点上：</span><br><span class="line">[root@dn1 ~]# jps</span><br><span class="line">18480 Jps</span><br><span class="line">12805 ResourceManager</span><br><span class="line">12365 DataNode</span><br><span class="line">12942 NodeManager</span><br><span class="line"># dn2节点上：</span><br><span class="line">[root@dn2 ~]# jps</span><br><span class="line">13144 DataNode</span><br><span class="line">13244 SecondaryNameNode</span><br><span class="line">19437 Jps</span><br><span class="line">13599 NodeManager</span><br></pre></td></tr></table></figure>
<p>以上表示主节点已经启动Master进程，其他节点dn1和dn2还未启动Worker进程。可以通过log日志文件内容看到其启动过程，这里不再给出，当然更直观的方式是在web端查看：页面<code>http://nn:8080/</code>或者<code>http://192.188.0.4:8080</code>可以直观看到master状态，此时workers还未启动,可以按到显示workers数量为0</p>
<h4 id="4-2-在spark-master启动后，启动Worker节点"><a href="#4-2-在spark-master启动后，启动Worker节点" class="headerlink" title="4.2 在spark master启动后，启动Worker节点"></a>4.2 在spark master启动后，启动Worker节点</h4><p>在spark主节点上nn，启动workers，这些workers的对应的节点就是路径<code>/opt/spark-2.4.4-bin-hadoop2.7/conf</code>下slaves文件配置到2个节点：dn1,dn2。</p>
<ul>
<li>启动spark集群上所有的workers节点命令：start-slaves.sh</li>
<li><p>启动本节点上的work进程：start-slave.sh</p>
</li>
<li><p>可以对比其shell脚本的差别，在start-slaves.sh脚本后面可以看到</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;$&#123;SPARK_HOME&#125;&#x2F;sbin&#x2F;start-slave.sh&quot; &quot;spark:&#x2F;&#x2F;$SPARK_MASTER_HOST:$SPARK_MASTER_PORT&quot;</span><br></pre></td></tr></table></figure>
<p>start-slaves.sh其实是在其他节点运行<code>./start-slave.sh spark://nn:7077</code>实现批量启动其他work节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;sbin</span><br><span class="line">[root@nn sbin]# .&#x2F;start-slaves.sh </span><br><span class="line">dn1: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn1.out</span><br><span class="line">dn2: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn2.out</span><br><span class="line"></span><br><span class="line"># # nn节点上spark Master</span><br><span class="line">[root@nn sbin]# jps</span><br><span class="line">24292 DataNode</span><br><span class="line">24155 NameNode</span><br><span class="line">25339 NodeManager</span><br><span class="line">30638 Master</span><br><span class="line">30750 Jps</span><br><span class="line"></span><br><span class="line"># dn1节点上spark Worker进程</span><br><span class="line">[root@dn1 ~]# jps</span><br><span class="line">12805 ResourceManager</span><br><span class="line">23045 Jps</span><br><span class="line">23000 Worker</span><br><span class="line">12365 DataNode</span><br><span class="line">12942 NodeManager</span><br><span class="line"></span><br><span class="line"># dn2节点上spark Worker进程</span><br><span class="line">[root@dn2 ~]# jps</span><br><span class="line">24789 Worker</span><br><span class="line">24837 Jps</span><br><span class="line">13144 DataNode</span><br><span class="line">13244 SecondaryNameNode</span><br><span class="line">13599 NodeManager</span><br></pre></td></tr></table></figure>
<p>在spark的master web端:<code>http://nn:8080</code>或者<code>http://192.188.0.4:8080</code>可以看到2个worker均active<br><img src="https://img-blog.csdnimg.cn/20191013195910122.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">每个worker的最大可用内存512m，vCPU 1颗</p>
<h3 id="5、设置启动spark-shell的默认环境（非常关键的配置）"><a href="#5、设置启动spark-shell的默认环境（非常关键的配置）" class="headerlink" title="5、设置启动spark-shell的默认环境（非常关键的配置）"></a>5、设置启动spark-shell的默认环境（非常关键的配置）</h3><h4 id="5-1-配置spark-defaults-conf"><a href="#5-1-配置spark-defaults-conf" class="headerlink" title="5.1 配置spark-defaults.conf"></a>5.1 配置spark-defaults.conf</h4><p>注意，在启动spark-shell之前，如果需要对/opt/spark-2.4.4-bin-hadoop2.7/conf目录下的配置文件：<code>spark-defaults.conf.template</code>相关参数进行修改，例如需要结合spark-history-server的配置，那么除了新建一份<code>spark-defaults.conf</code>，还需要对里面参数正确，否则启动spark-shell会提示出错并退出</p>
<p>因为本文测试使用2G内存，所以需要对配置文件里面做修改，修改如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">[root@nn conf] spark-defaults.conf</span><br><span class="line"></span><br><span class="line"># spark集群主节点的入口</span><br><span class="line">spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"># 在hadoop core-site.xml设置的hdfs入口地址，directory需自行在hdfs文件系统上创建</span><br><span class="line"># 通过命令可创建：hdfs dfs -mkdir &#x2F;directory</span><br><span class="line"># 同时日志目录作为spark-history-server的日志目录</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line"></span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark主节点driver内存，默认为5G，这里设为1g</span><br><span class="line">spark.driver.memory              1g</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure>
<p>如果以上入口地址设错，或者未在namenode节点的hdfs文件系统上创建directory目录，都会导致无法启动spark-shell</p>
<p>==若不对spark-defaults.conf.template参数修改，例如不需要启动history服务，则无需创建spark-defaults.conf文件，也无需进行上述设置，可以直接启动spark-shell==</p>
<h4 id="5-2-spark-defaults-conf的详细的设置"><a href="#5-2-spark-defaults-conf的详细的设置" class="headerlink" title="5.2  spark-defaults.conf的详细的设置"></a>5.2  spark-defaults.conf的详细的设置</h4><p>参考<a href="http://spark.apache.org/docs/latest/configuration.html">官网配置指引</a><br>其实该配置就是用来spark集群调优的关键配置</p>
<p>主要分为几大部分的参数配置：</p>
<ul>
<li>Application Properties</li>
<li>Runtime Environment</li>
<li>Spark UI</li>
<li>Compression and Serialization</li>
<li>Memory Management</li>
<li>Execution Behavior</li>
<li>Networking</li>
<li>Scheduling</li>
<li>Dynamic Allocation</li>
</ul>
<h4 id="5-3-启动spark-shell"><a href="#5-3-启动spark-shell" class="headerlink" title="5.3 启动spark-shell"></a>5.3 启动spark-shell</h4><p>首次启动spark-shell时，会出现‘WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform’的提示，参考文章提示：centos预装的glibc库是2.17版本，而hadoop期望是2.14版本，可以忽略该警告，在hadoop日志配置文件设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# vi &#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop&#x2F;log4j.properties </span><br><span class="line"># 新增以下内容</span><br><span class="line">log4j.logger.org.apache.hadoop.util.NativeCodeLoader&#x3D;ERROR</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动成功提示</span><br><span class="line">[root@nn bin]# spark-shell                </span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Spark context Web UI available at http:&#x2F;&#x2F;nn:4040</span><br><span class="line">Spark context available as &#39;sc&#39; (master &#x3D; spark:&#x2F;&#x2F;nn:7077, app id &#x3D; app-2019*****-0004).</span><br><span class="line">Spark session available as &#39;spark&#39;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     &#x2F; __&#x2F;__  ___ _____&#x2F; &#x2F;__</span><br><span class="line">    _\ \&#x2F; _ \&#x2F; _ &#96;&#x2F; __&#x2F;  &#39;_&#x2F;</span><br><span class="line">   &#x2F;___&#x2F; .__&#x2F;\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\_\   version 2.4.4</span><br><span class="line">      &#x2F;_&#x2F;</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_161)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br></pre></td></tr></table></figure>
<p>可以在<code>http://nn:4040</code>查看，若有计算任务提交，可以直观查看spark job 、excutors等进度，参考官方说明：</p>
<blockquote>
<p>Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:</p>
<ul>
<li>A list of scheduler stages and tasks</li>
<li>A summary of RDD sizes and memory usage</li>
<li>Environmental information.</li>
<li>Information about the running executors</li>
</ul>
</blockquote>
<p>但以上启动是有问题的，表面上看，spark-shell已正常启动，但测试机器最大内存为2G，启动spark-shell若不限定executor-memory内存使用（默认值1G）那么在执行计算任务时，spark-shell会一直提示 scheduler资源不足：</p>
<blockquote>
<p>WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory</p>
</blockquote>
<p>导致job一直waiting状态<br>解决办法：<br>启动spark-shell限制相关资源的使用:</p>
<p><code>spark-shell --executor-memory 512m  --total-executor-cores 3 --executor-cores 1</code></p>
<h3 id="6、在spark-shell交互式计算words"><a href="#6、在spark-shell交互式计算words" class="headerlink" title="6、在spark-shell交互式计算words"></a>6、在spark-shell交互式计算words</h3><h4 id="6-1-存放words的文件已经上传到hdfs文件系统上的-app目录下"><a href="#6-1-存放words的文件已经上传到hdfs文件系统上的-app目录下" class="headerlink" title="6.1 存放words的文件已经上传到hdfs文件系统上的/app目录下"></a>6.1 存放words的文件已经上传到hdfs文件系统上的/app目录下</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# hdfs dfs -ls &#x2F;app</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup         41 ** &#x2F;app&#x2F;title.txt</span><br><span class="line">-rw-r--r--   3 root supergroup         76 ** &#x2F;app&#x2F;words.txt</span><br><span class="line"></span><br><span class="line"># title.txt内容：</span><br><span class="line">hadoop spark zookeeper</span><br><span class="line"> spark zookeeper</span><br><span class="line"></span><br><span class="line"># words.txt内容：</span><br><span class="line">foo is foo</span><br><span class="line">bar is not bar</span><br><span class="line">hadoop file system is the infrastructure of big data </span><br></pre></td></tr></table></figure>
<h4 id="6-2带参数启动spark-shell"><a href="#6-2带参数启动spark-shell" class="headerlink" title="6.2带参数启动spark-shell"></a>6.2带参数启动spark-shell</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# spark-shell --master spark:&#x2F;&#x2F;nn:7077 --executor-memory 512m  --total-executor-cores 3 --executor-cores 1  --num-executors 2</span><br><span class="line"></span><br><span class="line"># SparkContext,也可以在web端查看http:&#x2F;&#x2F;nn:4040</span><br><span class="line">scala&gt; sc</span><br><span class="line">res2: org.apache.spark.SparkContext &#x3D; org.apache.spark.SparkContext@e71bd92</span><br><span class="line"></span><br><span class="line"># 统计hdfs目录&#x2F;app下所有文件里面words，scala语言的链式调用</span><br><span class="line">scala&gt; sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect</span><br><span class="line"></span><br><span class="line"># 统计结果返回一个scala数组</span><br><span class="line">res0: Array[(String, Int)] &#x3D; Array((is,3), (&quot;&quot;,2), (bar,2), (foo,2), (spark,2), (hadoop,2), (zookeeper,2), (not,1), (system,1), (big,1), (infrastructure,1), (the,1), (data,1), (file,1))</span><br></pre></td></tr></table></figure>
<p>或者在此spark-shell上交互式使用Scala写简单的统计语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val file&#x3D;sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;)</span><br><span class="line">file: org.apache.spark.rdd.RDD[String] &#x3D; hdfs:&#x2F;&#x2F;nn:9000&#x2F;app MapPartitionsRDD[21] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd &#x3D; file.flatMap(line &#x3D;&gt; line.split(&quot; &quot;)).map(word &#x3D;&gt; (word,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(String, Int)] &#x3D; MapPartitionsRDD[29] at sortBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res6: Array[(String, Int)] &#x3D; Array((is,3), (&quot;&quot;,2), (bar,2), (foo,2), (hadoop,2), (zookeeper,2), (spark,2), (not,1), (system,1), (data,1), (file,1), (big,1), (infrastructure,1), (the,1))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）"><a href="#7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）" class="headerlink" title="7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）"></a>7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）</h3><p>下面以一个Application 执行job前和执行job后的页面来说明application，job，task等内容</p>
<p>==<strong>Application 执行job前</strong>==</p>
<p><strong>A、查看application执行的详情页面</strong></p>
<p>在nn节点上，启动一个名字为word-count的application：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# spark-shell --executor-memory 512m  --total-executor-cores 3 --executor-cores 1  --num-executors 2 --name word-count</span><br></pre></td></tr></table></figure>
<p>在spark-shell启动后，会提示：<br>Spark context Web UI available at <code>http://nn:4040</code><br>Spark context available as ‘sc’ (master = spark://nn:7077, app id = app-2019*<em>**</em>-0002).</p>
<p><code>http://nn:4040</code>针对当前运行application的job详情，如果执行统计命令后退出spark-shell，那么web服务退出，<code>http://nn:4040</code>将无法访问，也即无法查看当前application执行过程的情况，所有需要配置application 的spark-history-server，用来查看之前已经完成或者未完成的application情况的历史记录<br><code>http://nn:4040</code>页面：<br>app id = app-2019*<strong><strong>-0002Jobs栏目内容：<br>可以看到目前该application没有job需要执行<br><img src="https://img-blog.csdnimg.cn/2019101320274522.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">app id = app-2019*</strong></strong>-0002的executor内容：<br>该application分配了两个executor，分别为dn1节点和dn2节点，nn节点则作为driver<br><img src="https://img-blog.csdnimg.cn/20191013203133344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>B、查看所有正在完成、已完成的application管理页面：spark master：</strong><code>http://nn:8080</code><br>该页面可以看到spark集群的资源分配情况、worker情况、正在runing的application以及已经完成的application<br><img src="https://img-blog.csdnimg.cn/20191013203738203.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>C、在A部分提到如果要回看已经完成application运行情况，则需要启动spark-history-server，这里给出配置文件说明</strong><br>==配置Spark History Server服务==<br>history只需在spark主节点上配置，无需在其他两个节点上配置。<br>spark-history-server其实就是一个web服务，spark.eventLog.dir存放所有application事件日志，web服务通过把这些application运行日志内容以web UI提供查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# vi spark-env.sh</span><br><span class="line"># 从配置说明可以看出，所有配置hisory服务的属性值可由以下属性设定</span><br><span class="line"># - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. &quot;-Dx&#x3D;y&quot;)</span><br><span class="line"></span><br><span class="line"># 配置history日志存放目录，可以配置多个属性值</span><br><span class="line"></span><br><span class="line">SPARK_HISTORY_OPTS&#x3D;&quot;</span><br><span class="line">-Dspark.history.ui.port&#x3D;9001 </span><br><span class="line">-Dspark.history.retainedApplications&#x3D;5</span><br><span class="line">-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory&quot;</span><br></pre></td></tr></table></figure>
<p>web访问端口为9001，保留最近5个application的日志，application的日志存放在<code>hdfs://nn:9000/directory</code></p>
<p>其他配置项</p>
<p>其他相关参数:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Property Name</th>
<th style="text-align:center">Default</th>
<th style="text-align:center">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">spark.history.fs.update.interval</td>
<td style="text-align:center">10s</td>
<td style="text-align:center">文件系统历史提供程序在日志目录中检查新日志或更新日志的周期。较短的间隔可以更快地检测新应用程序，但代价是需要更多的服务器负载重新读取更新的应用程序。一旦更新完成，已完成和未完成的应用程序的清单将反映更改</td>
</tr>
<tr>
<td style="text-align:center">spark.history.retainedApplications</td>
<td style="text-align:center">50</td>
<td style="text-align:center">在缓存中保留UI数据的应用程序数量。如果超过这个上限，那么最老的应用程序将从缓存中删除。如果应用程序不在缓存中，则必须从磁盘加载它(如果是从UI访问它)</td>
</tr>
<tr>
<td style="text-align:center">spark.history.fs.cleaner.enabled</td>
<td style="text-align:center">false</td>
<td style="text-align:center">是否周期性的删除storage中的event log(生产必定是true)</td>
</tr>
<tr>
<td style="text-align:center">spark.history.fs.cleaner.interval</td>
<td style="text-align:center">1d</td>
<td style="text-align:center">多久删除一次</td>
</tr>
<tr>
<td style="text-align:center">spark.history.fs.cleaner.maxAge</td>
<td style="text-align:center">7d</td>
<td style="text-align:center">每次删除多久的event log，配合上一个参数就是每天删除前七天的数据</td>
</tr>
</tbody>
</table>
</div>
<p>spark-history页面截图：<br><img src="https://img-blog.csdnimg.cn/2019101321104720.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">==<strong>Application 执行job后</strong>==<br>当application开始runing后，可以看到相关job运行情况<br><strong>A、application的jobs图示</strong><br>该word-count app启动了两个job<br><img src="https://img-blog.csdnimg.cn/20191013214149503.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>job-0主要负责作业中Transformation链操作：<br><code>sc.textFile(&quot;hdfs://nn:9000/app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</code><br>job-0分解</p>
<p>job-1负责作业最后阶段Action操作：<br><code>.collect</code></p>
<p>application、job、stage、task构成关系<br>这里job-1的stage2是skip的，因为job-0已经完成了同样的操作，其他job无法重复执行。<br><img src="https://img-blog.csdnimg.cn/2019101322183877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>B、job-0、job-1对于的stage图</strong><br>job-0：stage-0和stage-1<br><img src="https://img-blog.csdnimg.cn/20191013222359342.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">job-1：stage-2、stage-3、stage-4<br><img src="https://img-blog.csdnimg.cn/20191013222504141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>C、以job-0为例：stage-0和stage-1的具体任务执行图DAG调度过程</strong><br>==job-0：stage-0，其实就是map阶段，对应shuffle write==<br><img src="https://img-blog.csdnimg.cn/20191013222734564.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>==job-1：stage-1，其实就是reduce阶段，对应shuffle read==<br><img src="https://img-blog.csdnimg.cn/20191013222849671.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="8、启动spark-HA集群"><a href="#8、启动spark-HA集群" class="headerlink" title="8、启动spark HA集群"></a>8、启动spark HA集群</h3><p>前面的测试都是基于一个master带2个slave节点的集群，若nn节点上的master进程挂了，显然无法达到高可用集群，因此本章节也给出其配置过程，后面多篇文章会有大数据实时项目相关组件的部署，全部组件都基于HA方式运行，近可能贴近生产环境。<br>spark HA集群基于zookeeper集群实现，因此需要环境配置好并启动zookeeper服务，这里不再累赘，可以参考本人blog中有关zk集群的配置过程。</p>
<h4 id="8-1-配置文件"><a href="#8-1-配置文件" class="headerlink" title="8.1 配置文件"></a>8.1 配置文件</h4><p>spark HA配置相对简单，改动三个文件： spark-defaults.conf，slaves，spark-env.sh</p>
<p>将三个节点spark-defaults.conf都做以下配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf] vi  spark-defaults.conf</span><br><span class="line"># 因为spark要配成HA模式，因此不再指定nn节点为active节点</span><br><span class="line">#spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"># 注意这里eventLog.dir，因为本文中hadoop 集群还不是HA模式，NameNode主节点仅有nn节点，因此设为nn:9000。若hadoop集群为HA模式，这里的路径需要设为  ：hdfs:&#x2F;&#x2F;hdapp&#x2F;directory。在后面的spark on yarn 文章也还会提到这一点。</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure></p>
<p>将3个节点都为加入到slaves文件，每个节点都需配置该slaves文件。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@dn1 conf]# cp slaves.template slaves</span><br><span class="line">[root@dn1 conf]# vi slaves</span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure><br>更改spark-env.sh<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export SCALA_HOME&#x3D;&#x2F;opt&#x2F;scala-2.12.8</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line"># spark HA配置里，不再指定某个节点为master</span><br><span class="line">#export SPARK_MASTER_IP&#x3D;182.10.0.4</span><br><span class="line">export SPARK_WORKER_MEMORY&#x3D;512m</span><br><span class="line"># 加入zookeeper集群，由zk统一管理</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url&#x3D;nn:2181,dn1:2181,dn2:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark&quot;</span><br><span class="line">#hadoop的配置文件**site.xml所在目录</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br></pre></td></tr></table></figure><br>在三个节点上都需按以上内容做相同配置。</p>
<h4 id="8-2-启动和测试spark-HA"><a href="#8-2-启动和测试spark-HA" class="headerlink" title="8.2 启动和测试spark HA"></a>8.2 启动和测试spark HA</h4><p>首先启动nn节点上slaves进程,此时三个节点都是worker角色<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# .&#x2F;sbin&#x2F;start-slaves.sh </span><br><span class="line">nn: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-nn.out</span><br><span class="line">dn1: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn1.out</span><br><span class="line">dn2: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn2.out</span><br></pre></td></tr></table></figure><br>接着在nn节点上启动master进程，此时nn节点将被选举为active状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# .&#x2F;sbin&#x2F;start-master.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-nn.out</span><br></pre></td></tr></table></figure><br>最后，分别在dn1和dn2节点上启动master进程，此时因nn节点已经优先成为active角色，故这两个节点虽然启动master，但会处于standby模式<br>通过spark web UI查看以上集群情况：<br>首先访问<code>http://nn:8080</code>，可以看到当前nn节点为active状态且有3个alive workers<br><img src="https://img-blog.csdnimg.cn/20191205225822814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">访问<code>http://dn1:8080</code>，dn1节点为standby模式，而且无自己的workers<br><img src="https://img-blog.csdnimg.cn/20191205230110964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">访问<code>http://dn2:8080</code>，dn2节点为standby模式，而且无自己的workers<br><img src="https://img-blog.csdnimg.cn/20191205230256782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">kill掉nn上master进程，观测spark 集群的master切换情况。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# jps</span><br><span class="line">7094 Master</span><br><span class="line">7322 Jps</span><br><span class="line">7019 Worker</span><br><span class="line">4892 QuorumPeerMain</span><br><span class="line">5853 NameNode</span><br><span class="line">5933 DataNode</span><br><span class="line">6253 DFSZKFailoverController</span><br><span class="line">6062 JournalNode</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# kill -9 7094</span><br></pre></td></tr></table></figure></p>
<p>访问<code>http://dn1:8080</code>，dn1节点由standby变为active模式且有3个alive workers，而dn2仍然standby模式，说明HA部署正常。<br><img src="https://img-blog.csdnimg.cn/20191205230624602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="9、结束"><a href="#9、结束" class="headerlink" title="9、结束"></a>9、结束</h3><p>本文详细讨论了基于hadoop上搭建spark HA集群，并对执行的application做了简单的介绍，注意到，这里spark HA集群并没有引入yarn资源调度服务，后面的文章会给出配置过程。同时本文没有对spark架构及其原理做更多的探讨，相关文章也在之后给出。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于redis实现分布式锁（多实例redis+RedLock算法）</title>
    <url>/2019/09/22/%E5%9F%BA%E4%BA%8Eredis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%88%E5%A4%9A%E5%AE%9E%E4%BE%8Bredis+RedLock%E7%AE%97%E6%B3%95%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章中，已经实现单实例redis分布式锁，但这种实现是基于单个redis服务，若redis服务不可用，显然所有客户端无法加锁，该实现还未到高可用的水平，因此需要进一步提升分布式的锁的逻辑，好在redis官方提供了相应的权威描述并称之为Redlock，具体参考文章：<a href="https://redis.io/topics/distlock">DLM</a>，这个锁的算法实现了多redis实例（各个redis是相互独立的，没有主从、集群模式）的情况，实现了真正高可用分布式锁。</p>
<a id="more"></a>
<h3 id="高可用的分布式锁要求："><a href="#高可用的分布式锁要求：" class="headerlink" title="高可用的分布式锁要求："></a>高可用的分布式锁要求：</h3><p>1）Mutual exclusion，互斥性，任何时刻只能有一个client获取锁</p>
<p>2）Deadlock free，死锁也必须能释放，即使锁定资源的redis服务器崩溃或者分区，仍然能释放锁</p>
<p>3）Fault tolerance；只要多数互相独立的redis节点，这里不是指主从模式两个节点或者集群模式，（一半以上）在使用，client才可以进行加锁，而且加锁成功的redis服务数量超过半数，且锁的有效时长还未过期，才认为加锁成功，否则加锁失败</p>
<h3 id="Redlock算法说明"><a href="#Redlock算法说明" class="headerlink" title="Redlock算法说明"></a>Redlock算法说明</h3><p>&#8195;&#8195;首先需理解时钟漂移clock drift概念：服务器时钟偏离绝对参考时钟的差值，例如在分布式系统中，有5台服务器，所有服务器时钟在初始情况下都设置成相同的时间（服务器上没有设置ntp同步）例如都为2019-08-01 10:00:00，随着时间的流逝，例如经过1年后，再“观察”这5台服务器的时间，服务器之间的时间对比，将有可能出现一定的快慢差异：</p>
<p>Server1显示一年后的时间：2020-08-01 10:00:01          </p>
<p>Server2显示一年后的时间：2020-08-01 10:00:02         </p>
<p>Server3显示一年后的时间：2020-08-01 10:00:02           </p>
<p>Server4显示一年后的时间：2020-08-01 09:59:58          </p>
<p>Server5显示一年后的时间：2020-08-01 10:00:01           </p>
<p>那么由这5台服务器组成的分布式系统，在外侧观察，时钟漂移为=2020-08-01 10:00:02减去2020-08-01 09:59:58=4秒，当然这是累计一年的时钟漂移时长，于是可以计算每秒的时间漂移刻度=4/(3600*24*365)，该刻度时长极小完全可以忽略不计，这是redis官方提供这个概念，让分布式锁的redis实现看起来更高级。</p>
<p>redlock加锁流程，假设客户端A按顺序分别在5个完全独立的redis实例作为加锁，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190922224528783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>1）客户端A在redis01加锁操作前，获取当前时间戳T1</p>
<p>2）客户端A使用相同的key和uuid按顺序在5个redis上set key加锁和设定键的过期时长（有效时长），因为set key操作需要一定时间，因此在set过期时长时，需要set大于加锁所消耗的时长，否则客户端A还未在超过半数redis实例加锁成功前，前面redis set的key就已经先失效了，</p>
<p>错误设置：TTL为1s，例如客户端A在redis01加锁耗时为0.1秒、在redis02加锁耗时为0.5秒，但在redis03加锁耗时为1秒，此时redis01、redis02的key已失效，导致客户端A没能在超过半数（3个）的redis实例上加锁成功</p>
<p>正确设置：TTL为5s，例如客户端A在redis01加锁耗时为0.1秒、在redis02加锁耗时为0.5秒，redis03加锁耗时为1秒，此时redis01、redis02、redis03 key还未失效，客户端A成功在超过半数（3个）的redis实例上加锁，但此时客户端A还不能严格意义上成功获得了分布式锁，还需要进行第3步骤的判断</p>
<p>3）客户端A完成在多个redis实例上加锁后，此刻，锁真正有效时间不是一开始设置TTL的10秒，而是由以下得出：</p>
<p>在5个redis上加锁完后所消耗的时长：set_lock_cost=T5-T1=4s</p>
<p>实际锁的最小有效时长：min_validity=TTL-set_lock_cost-时钟漂移耗时</p>
<p>实际锁的最小有效时长=10s-4s-1s=5s，也就是说客户端A虽然在redis服务器设置有效时长为10s，但扣除一系列的加锁操作耗时后，“redis服务端”留给客户端A的实际有效时长为5秒。如果客户端A能在这5秒内完成任务，且按顺序释放锁，那么客户端A完成了一个完整流程的分布式锁条件的任务。</p>
<p>4）如果客户端A超时等原因无法获得超过半数（3）个以上，则必须解锁所有redis实例，否则影响其他进程加锁</p>
<h3 id="RedLock代码实现："><a href="#RedLock代码实现：" class="headerlink" title="RedLock代码实现："></a>RedLock代码实现：</h3><p>&#8195;&#8195;前一篇文章中已经实现的单服务的redis分布式锁，基于该基础上，实现redlock并不复杂，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,datetime</span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLockException</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, locker_key, connection_conf_list=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 retry_times=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 retry_interval=<span class="number">200</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ttl=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 clock_drift=<span class="number">500</span></span>):</span></span><br><span class="line">        self.locker_key = locker_key</span><br><span class="line">        self.retry_times = retry_times</span><br><span class="line">        self.retry_interval = retry_interval</span><br><span class="line">        self.global_ttl = ttl</span><br><span class="line">        self.clock_drift = clock_drift</span><br><span class="line">        self.locker_id = <span class="literal">None</span></span><br><span class="line">        self.is_get_lock = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> connection_conf_list:</span><br><span class="line">            connection_conf_list = [&#123;</span><br><span class="line">                <span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;port&#x27;</span>: <span class="number">6379</span>,</span><br><span class="line">                <span class="string">&#x27;db&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;socket_connect_timeout&#x27;</span>:<span class="number">1</span></span><br><span class="line">            &#125;]</span><br><span class="line"></span><br><span class="line">        self.all_redis_nodes = [redis.StrictRedis(**each_conf) <span class="keyword">for</span> each_conf <span class="keyword">in</span> connection_conf_list]</span><br><span class="line">        self.majority_nodes = <span class="built_in">len</span>(self.all_redis_nodes) // <span class="number">2</span> + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_release_single_lock</span>(<span class="params">self, node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在redis服务端执行原生lua脚本，只能删除加锁者自己的id，而且是原子删除</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        lua_script = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then</span></span><br><span class="line"><span class="string">            return redis.call(&quot;del&quot;,KEYS[1])</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            return 0</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            lua_func = node.register_script(lua_script)</span><br><span class="line">            lua_func(keys=[self.locker_key], args=[self.locker_id])</span><br><span class="line">        <span class="keyword">except</span>(redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_acquire_single_lock</span>(<span class="params">self, node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在单个redis加锁</span></span><br><span class="line"><span class="string">        :param node:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result = node.<span class="built_in">set</span>(self.locker_key, self.locker_id, nx=<span class="literal">True</span>, px=self.global_ttl)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">except</span>(redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_acquire</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.locker_id = <span class="built_in">str</span>(uuid.uuid1())</span><br><span class="line">        loop = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> loop &lt;= self.retry_times:</span><br><span class="line">           <span class="comment"># 这里需要注意：多线程并发模拟过程中，需要在任务执行前加锁，否则线程不安全</span></span><br><span class="line">            ok_lock_count = <span class="number">0</span></span><br><span class="line">            start = time.monotonic()</span><br><span class="line">            <span class="comment"># 按顺序在每个redis上尝试set key 加锁</span></span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> self.all_redis_nodes:</span><br><span class="line">                <span class="keyword">if</span> self._acquire_single_lock(node):</span><br><span class="line">                    print(<span class="string">&#x27;&#123;&#125;：成功加锁&#x27;</span>.<span class="built_in">format</span>(node))</span><br><span class="line">                    ok_lock_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            end = time.monotonic()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在多个redis实例上加锁所消耗的时长</span></span><br><span class="line">            set_lock_cost = (end - start)</span><br><span class="line">            <span class="comment"># 扣除相关操作耗时，得出实际锁的有效时长</span></span><br><span class="line">            real_ttl = self.global_ttl - set_lock_cost - self.clock_drift</span><br><span class="line">            print(<span class="string">&#x27;本次加锁耗时：&#123;0:,.4f&#125; ms 锁实际有效时长：&#123;1:,.4f&#125; ms&#x27;</span>.<span class="built_in">format</span>(set_lock_cost,real_ttl))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果加锁数量超过半数，且实际锁的有效时长大于0，则说明客户端本次成功获得分布式锁</span></span><br><span class="line">            <span class="keyword">if</span> ok_lock_count &gt;= self.majority_nodes <span class="keyword">and</span> real_ttl &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span>, real_ttl</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 客户端本次未能获得分布式锁，需释放本次申请的所有锁</span></span><br><span class="line">                <span class="keyword">if</span> real_ttl &lt;= <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">&#x27;客户端加锁失败，因为锁的实际有效时间太短&#x27;</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">&#x27;客户端加锁失败，因为成功加锁的redis实例少于总数的一半&#x27;</span>)</span><br><span class="line">                <span class="keyword">for</span> node <span class="keyword">in</span> self.all_redis_nodes:</span><br><span class="line">                    self._release_single_lock(node)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 随机休眠后，客户端继续下一轮加锁</span></span><br><span class="line">            loop += <span class="number">1</span></span><br><span class="line">            time.sleep(random.randint(<span class="number">0</span>, self.retry_interval) / <span class="number">1000</span>)</span><br><span class="line">        print(<span class="string">&#x27;超过&#123;&#125;次加锁失败&#x27;</span>.<span class="built_in">format</span>(self.retry_times))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>,<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire</span>(<span class="params">self</span>):</span></span><br><span class="line">        is_lock, validity = self._acquire()</span><br><span class="line">        <span class="keyword">return</span> is_lock</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire_with_validity</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :return: 返回加锁是否成功和锁的有效期</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        is_lock, validity = self._acquire()</span><br><span class="line">        <span class="keyword">return</span> is_lock, validity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">release</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.all_redis_nodes:</span><br><span class="line">            self._release_single_lock(node)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        is_lock, validity = self._acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_lock:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="comment"># raise RedLockException(&#x27;unable to acquire distributed lock&#x27;)</span></span><br><span class="line">        <span class="keyword">return</span> is_lock, validity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r</span>):</span></span><br><span class="line">    <span class="keyword">with</span> RedLock(<span class="string">&#x27;locker_test&#x27;</span>):</span><br><span class="line">        thread_name = threading.currentThread().name</span><br><span class="line">        bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">        total = r.get(bonus)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">            print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">            <span class="keyword">return</span>          </span><br><span class="line">        result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">        print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;,时间:&#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    start_time=time.monotonic()</span><br><span class="line">    thread_nums=<span class="number">100</span></span><br><span class="line">    pool_obj = redis.ConnectionPool(host=<span class="string">&#x27;192.168.100.5&#x27;</span>, port=<span class="number">8002</span>, socket_connect_timeout=<span class="number">5</span>)</span><br><span class="line">    r_conn = redis.Redis(connection_pool=pool_obj)</span><br><span class="line"></span><br><span class="line">    threads = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(thread_nums):</span><br><span class="line">        t = threading.Thread(target=doing_jobs, args=(r_conn,))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    cost=time.monotonic()-start_time</span><br><span class="line">    print(<span class="string">&#x27;任务耗时:&#123;:,.2f&#125; ms&#x27;</span>.<span class="built_in">format</span>(cost))</span><br></pre></td></tr></table></figure>
<h4 id="在单个redis下测试redlock"><a href="#在单个redis下测试redlock" class="headerlink" title="在单个redis下测试redlock"></a>在单个redis下测试redlock</h4><h4 id="1）单redis，100个并发请求"><a href="#1）单redis，100个并发请求" class="headerlink" title="1）单redis，100个并发请求"></a>1）单redis，100个并发请求</h4><p>手动在redis单服务set值，测试客户端发来的100个并发抢资源时，基于redlock的分布式锁是否逻辑正确，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set money 300</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line"># 运行结果</span><br><span class="line">客户端:Thread-100抢到奖金，还剩299,时间:*** 22:14:40.358679</span><br><span class="line">客户端:Thread-9抢到奖金，还剩298,时间:*** 22:14:40.363933</span><br><span class="line">客户端:Thread-36抢到奖金，还剩297,时间:*** 22:14:40.371695</span><br><span class="line"></span><br><span class="line">客户端:Thread-16抢到奖金，还剩202,时间:*** 22:14:40.808868</span><br><span class="line">客户端:Thread-34抢到奖金，还剩201,时间:*** 22:14:40.811364</span><br><span class="line">客户端:Thread-52抢到奖金，还剩200,时间:*** 22:14:40.817055</span><br></pre></td></tr></table></figure>
<p>可以看到，在同一秒内，100个线程都有序的抢到锁和资源</p>
<h4 id="2）5个redis实例，1个并发请求"><a href="#2）5个redis实例，1个并发请求" class="headerlink" title="2）5个redis实例，1个并发请求"></a>2）5个redis实例，1个并发请求</h4><p>在5个独立redis实例下验证redlock分布式锁有效性(这里的5个实例是在同一服务器下开启，模拟5台redis服务)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 redis-5.0.5]# pwd</span><br><span class="line">/opt/redis/redis-5.0.5</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在redis目录下直接拷贝redis.conf，重命名，且只需修改里面的端口项即可，这里端口为8000~8004</span></span><br><span class="line">redis8001.conf  redis8002.conf  redis8003.conf  redis8004.conf  </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 逐个启动redis实例</span></span><br><span class="line">[root@dn2 redis-5.0.5]# redis-server redis8001.conf </span><br><span class="line"></span><br><span class="line">[root@dn2 redis-5.0.5]# ps -ef|grep redis           </span><br><span class="line">root     30321     1  0 20:58 ?        00:00:00 redis-server *:8000</span><br><span class="line">root     30326     1  0 20:58 ?        00:00:00 redis-server *:8001</span><br><span class="line">root     30372     1  0 21:02 ?        00:00:00 redis-server *:8002</span><br><span class="line">root     30381     1  0 21:03 ?        00:00:00 redis-server *:8003</span><br><span class="line">root     30386     1  0 21:03 ?        00:00:00 redis-server *:8004</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 登录其中一个实例<span class="built_in">set</span> key</span></span><br><span class="line">[root@dn2 redis-5.0.5]# redis-cli -p 8002</span><br><span class="line">127.0.0.1:8002&gt; set foo 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:8002&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure>
<p>只有一个并发的条件下，客户端在5个实例加锁情况，在8002实例上加入资源：</p>
<p>以上代码小改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加入5个redis实例连接配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r</span>):</span></span><br><span class="line">    redis_nodes_conf=[</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,<span class="string">&#x27;port&#x27;</span>:<span class="number">8000</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8001</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8002</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8003</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8004</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">with</span> RedLock(locker_key=<span class="string">&#x27;Redlock&#x27;</span>,connection_conf_list=redis_nodes_conf):</span><br><span class="line">        thread_name = threading.currentThread().name</span><br><span class="line">        bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">        total = r.get(bonus)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">            print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">        print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;,时间:&#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result,datetime.datetime.now()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 在其中一个redis实例加入资源</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 redis-5.0.5]# redis-cli -p 8002</span><br><span class="line">127.0.0.1:8002&gt; set money 10</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
<p>可以看到，客户端首先在五个实例上按顺序加锁，执行任务，获得1个资源完成任务后，接着再顺序释放锁，其中加锁耗时0.01ms，锁实际有效时长：4,499.99 ms，任务耗时0.02ms，说明锁的实际有效时长足够大，以至于可以保证任务执行过程中，保持锁不失效。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8000,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8001,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8002,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.01 ms 锁实际有效时长：4,499.99 ms</span><br><span class="line">客户端:Thread-1抢到奖金，还剩9,时间:*** 22:10:46.490385</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8000,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8001,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8002,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">任务耗时:0.02 ms</span><br></pre></td></tr></table></figure>
<h4 id="3）redis实例工作数量小于半数，1个并发请求"><a href="#3）redis实例工作数量小于半数，1个并发请求" class="headerlink" title="3）redis实例工作数量小于半数，1个并发请求"></a>3）redis实例工作数量小于半数，1个并发请求</h4><p>只有一个并发的条件下，客户端在小于3个实例加锁情况，只需把8000、8001、8002端口改掉，模拟只有两个redis实例正常服务。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.0670 ms 锁实际有效时长：4,499.9330 ms</span><br><span class="line">客户端加锁失败，因为成功加锁的redis实例少于总数的一半</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.0728 ms 锁实际有效时长：4,499.9272 ms</span><br><span class="line">客户端加锁失败，因为成功加锁的redis实例少于总数的一半</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.0548 ms 锁实际有效时长：4,499.9452 ms</span><br><span class="line">客户端加锁失败，因为成功加锁的redis实例少于总数的一半</span><br><span class="line">超过3次加锁失败</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=80000,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=80001,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=80002,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">任务耗时:0.53 ms</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;可以看到，客户端尝试3次加锁，在给定的5个redis实例里仅能成功加锁2个，少于半数，故本次分布式加锁失败。当然也可以模拟把锁的ttl设置小值，例如500ms，那么将出现即使加完锁，因为锁的有实效时长太短，导致无法最终得到分布式锁，这里不在模拟。</p>
<h3 id="支持多线程的redlock算法"><a href="#支持多线程的redlock算法" class="headerlink" title="支持多线程的redlock算法"></a>支持多线程的redlock算法</h3><p>&#8195;&#8195;以上未模拟1个线程并发，但其实现不支持多线程，如果要模拟多个并发例如：100个并发，因为在同一进程里，涉及到对多个线程同一时刻更改ok_lock_count的值，因此，在执行任务前，就需要出传入线程锁，保证同一时刻，仅有一个线程更新这个ok_lock_count（本线程在多个redis实例上成功set key 的计数）</p>
<p>任务执行代码小改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r,thread_lock</span>):</span></span><br><span class="line">    redis_nodes_conf=[</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,<span class="string">&#x27;port&#x27;</span>:<span class="number">8000</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8001</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8002</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8003</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8004</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 这里的多线程锁是为了处理&quot;模拟并发情况下&quot;，对ok_lock_count变量进行更新时，保证同一时间只能有一个线程来操作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> thread_lock:</span><br><span class="line">        <span class="keyword">with</span> RedLock(locker_key=<span class="string">&#x27;Redlock&#x27;</span>,connection_conf_list=redis_nodes_conf) <span class="keyword">as</span> (is_lock,validity):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_lock:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            thread_name = threading.currentThread().name</span><br><span class="line">            bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">            total = r.get(bonus)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">                print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">            result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">            print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;,时间:&#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_time=time.monotonic()</span><br><span class="line">    thread_nums=<span class="number">100</span></span><br><span class="line">    pool_obj = redis.ConnectionPool(host=<span class="string">&#x27;192.168.100.5&#x27;</span>, port=<span class="number">8002</span>, socket_connect_timeout=<span class="number">5</span>)</span><br><span class="line">    r_conn = redis.Redis(connection_pool=pool_obj)</span><br><span class="line">    thread_lock=threading.RLock()</span><br><span class="line">    threads = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(thread_nums):</span><br><span class="line">        t = threading.Thread(target=doing_jobs, args=(r_conn,thread_lock))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    cost=time.monotonic()-start_time</span><br><span class="line">    print(<span class="string">&#x27;任务耗时:&#123;:,.2f&#125; ms&#x27;</span>.<span class="built_in">format</span>(cost))</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;以上完成redlock完整的分析、实现和测试，现在回看redlock的实现，它提出的所谓加锁耗时、时钟漂移等，都可以用最简单的方式代替：只需要把key的ttl设置足够长的时间，那么就无需担心在加锁过程中key突然失效。</p>
<p>&#8195;&#8195;综上，个人认为redis实现分布式锁的过程过于繁琐（注意不是复杂），而且要求redis实例之间是独立运行，反正我个人不会在项目中使用这种逻辑，因此Zookeeper在分布式锁方面的可用性，无疑是最优的。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title>深入functools.wraps、partial</title>
    <url>/2019/11/17/%E6%B7%B1%E5%85%A5functools.wraps%E3%80%81partial/</url>
    <content><![CDATA[<p>&#8195;&#8195;在装饰器的定义中，经常引用functools.wraps，本篇文章将深入其内部源码，由于该方法的定义中，还引入其他重要的函数或者类，因此根据其调用链，对每个函数或者方法或者类进行单独分析，所以文章的结构大致如下：</p>
<h4 id="第一部分内容："><a href="#第一部分内容：" class="headerlink" title="第一部分内容："></a>第一部分内容：</h4><p>根据其调用链：functools.wraps——&gt;partial——&gt;update_wrapper<br>functools.wraps需要调用partial，因此需要解析partial的源码<br>partial调用了update_wrapper函数，因此需要解析update_wrapper的源码</p>
<a id="more"></a>
<h4 id="第二部分内容："><a href="#第二部分内容：" class="headerlink" title="第二部分内容："></a>第二部分内容：</h4><p>patial作为关键函数，在第三方库造轮子里，使用频率较高。这里以borax第三方库里面的fetch方法说明partial的使用场合。考虑到fetch还使用的python内建的attrgetter和itemgetter，故还对这两个类进行解析。<br>其调用链为：fetch—-&gt;partial/attrgetter/itemgetter</p>
<h4 id="第三部分内容："><a href="#第三部分内容：" class="headerlink" title="第三部分内容："></a>第三部分内容：</h4><p>在python的内建方法中，attrgetter/itemgetter类，里面用了<code>__slots__</code>方法，本文也给出关于该方法作用的内容</p>
<p>考虑到以上有多个知识点混合，本文采用倒叙方式，文章的内容组织如下</p>
<ul>
<li>1、python的魔法方法<code>__slots__</code>的作用</li>
<li>2、attrgetter/itemgetter类的解析</li>
<li>3、borax.fetch的用法</li>
<li>4、partial的解析和用法</li>
<li>5、update_wrapper的解析和用法</li>
<li>6、functools.wraps的解析和用法</li>
</ul>
<h3 id="1、python的魔法方法-slots-的作用"><a href="#1、python的魔法方法-slots-的作用" class="headerlink" title="1、python的魔法方法__slots__的作用"></a>1、python的魔法方法<code>__slots__</code>的作用</h3><p>首先看看attrgetter/itemgetter的源代码定义(这里仅给出方法名)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class attrgetter:</span><br><span class="line">    __slots__ &#x3D; (&#39;_attrs&#39;, &#39;_call&#39;)</span><br><span class="line">    def __init__(self, attr, *attrs):</span><br><span class="line">    def __call__(self, obj):</span><br><span class="line">    def __repr__(self):</span><br><span class="line">    def __reduce__(self):</span><br></pre></td></tr></table></figure><br>该类里面引用了<code>__slots__</code>方法，仅有两个私有属性：<code>(&#39;_attrs&#39;, &#39;_call&#39;)</code></p>
<p>对于attrgetter，其作用：</p>
<ul>
<li>给类指定一个固定大小的空间存放属性，用于极致减少对象的内存占用，例如当十几万个小类（数据类），对象占用内存利用率将更有效。</li>
<li>更快的属性访问速度</li>
<li>实例后限制绑定新属性</li>
</ul>
<p>为何<code>_slots_</code>方法有以上作用?</p>
<p>&#8195;&#8195;这是因为，在定义个对象时（定义类），Python默认用一个字典来保存一个该对象实例属性。然而，对于有着已知属性的小对象类来说（例如一个坐标点类，仅有几个属性即可），当创建几十万个这些实例时，将有几十万个这样的字典占用大量内存，因此可通过slots方法告诉Python不使用字典，使用一个元组作为这几个属性的存放位置，以节省每个小对象的存储空间。<br>==slots这里不建议使用列表，因为列表占用空间比元组大。==</p>
<h5 id="1-1、-slots方法保证实例不会创建-dict-方法"><a href="#1-1、-slots方法保证实例不会创建-dict-方法" class="headerlink" title="1.1、 slots方法保证实例不会创建__dict__方法"></a>1.1、 slots方法保证实例不会创建<code>__dict__</code>方法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,x,y</span>):</span></span><br><span class="line">        self._x=x</span><br><span class="line">        self._y=y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Point&lt;&#123;0&#125;,&#123;1&#125;&gt;&#x27;</span>.<span class="built_in">format</span>(self._x,self._y)</span><br><span class="line"></span><br><span class="line">a=Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.__dict__)</span><br><span class="line">a.test=<span class="number">3</span></span><br><span class="line">print(a.test)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Point&lt;<span class="number">1</span>,<span class="number">2</span>&gt;</span><br><span class="line">&#123;<span class="string">&#x27;_x&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;_y&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入slot之后</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    __slots__ = (<span class="string">&#x27;_x&#x27;</span>,<span class="string">&#x27;_y&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,x,y</span>):</span></span><br><span class="line">        self._x=x</span><br><span class="line">        self._y=y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Point&lt;&#123;0&#125;,&#123;1&#125;&gt;&#x27;</span>.<span class="built_in">format</span>(self._x,self._y)</span><br><span class="line">a=Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.__dict__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Point&lt;<span class="number">1</span>,<span class="number">2</span>&gt;</span><br><span class="line"><span class="comment">#AttributeError: &#x27;Point&#x27; object has no attribute &#x27;__dict__&#x27;</span></span><br><span class="line"><span class="comment">#可见实例没有使用dict字典存放属性</span></span><br></pre></td></tr></table></figure>
<p>不过需要注意的是：slots魔法方法定义的属性仅对当前类实例起作用，对继承的子类是无效的</p>
<h5 id="1-2、为何列表占用空间比元组大？"><a href="#1-2、为何列表占用空间比元组大？" class="headerlink" title="1.2、为何列表占用空间比元组大？"></a>1.2、为何列表占用空间比元组大？</h5><ul>
<li>==内存占用有区别==</li>
</ul>
<p>首先列表和元组最重要的区别就是，列表是动态的、可变的对象、可读可写，而元组是静态的、不可变的对象，可读不可写。</p>
<p>下面通过实例看看它们存储以及占用空间的区别</p>
<p>查看列表的内存占用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l=[]</span><br><span class="line">l.__sizeof__()</span><br><span class="line"><span class="comment"># 40</span></span><br></pre></td></tr></table></figure>
<p>加入4个字符，每个字符为8字节空间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_obj=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>]</span><br><span class="line">list_obj.__sizeof__()</span><br><span class="line"><span class="comment"># 结果为72字节=列表自身40+4个字符*8</span></span><br></pre></td></tr></table></figure>
<p>此外列表的空间是动态增加的，在数据结构与算法里，大家在设计列表这种数据结构应该知道，当调用append方法时，内部会判断当前列表预留空间是否满足用于存放新元素，若空间不足，会再动态申请新内存，申请的逻辑为：</p>
<p>（1）当原底层数组存满时，list类会自动请求一个空间为原列表两倍的新列表</p>
<p>（2）原列表的所有元素将被一次存入新列表里</p>
<p>（3）删除原列表，并初始化新列表</p>
<p>例如下面测试，原list_obj有4个字符元素，总计为72个字节，再加一个字符，是等于80个字节吗？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_obj.append(<span class="string">&#x27;e&#x27;</span>)</span><br><span class="line">list_obj.__sizeof__()</span><br><span class="line"><span class="comment"># 结果为104字节=列表自身42+原4个字符*8+新1个字符*8+原4个字符*8的新申请预留空间</span></span><br></pre></td></tr></table></figure>
<p>这里列表自身从40变为42字节，是因为增加了一个1索引。</p>
<p>查看元组的内存占用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=()</span><br><span class="line">t.__sizeof__()</span><br><span class="line"><span class="comment"># 24</span></span><br></pre></td></tr></table></figure>
<p>加入4个字符，每个字符为8字节空间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple_obj=(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">tuple_obj.__sizeof__()</span><br><span class="line"><span class="comment"># 结果为56字节=元组自身24+4个字符*8</span></span><br></pre></td></tr></table></figure>
<p>可以看到，存储5个字符，列表用了72个字节，元组只用了56个字节。</p>
<ul>
<li>==对象创建时间有区别==</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line">t1 = timeit.Timer(<span class="string">&#x27;list_obj=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]&#x27;</span>)</span><br><span class="line">t1.timeit()</span><br><span class="line"><span class="comment"># 0.0844487198985604</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t2 = timeit.Timer(<span class="string">&#x27;tuple_obj=(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)&#x27;</span>)</span><br><span class="line">t2.timeit()</span><br><span class="line"><span class="comment"># 0.01631815598959463</span></span><br></pre></td></tr></table></figure>
<p>因为列表数据结构初始化需要方法逻辑比元组负责，而且需要预占空间，可以看到它们之间创建时间差别大，列表创建时间是元组的5倍左右。</p>
<h3 id="2、attrgetter-itemgetter类的解析"><a href="#2、attrgetter-itemgetter类的解析" class="headerlink" title="2、attrgetter/itemgetter类的解析"></a>2、attrgetter/itemgetter类的解析</h3><h4 id="2-1-attrgetter的使用场景"><a href="#2-1-attrgetter的使用场景" class="headerlink" title="2.1 attrgetter的使用场景"></a>2.1 attrgetter的使用场景</h4><p>attrgetter主要用于快速获取对象的keys或者属性。<br>以下以数据类型为BlogItem对象为例，该数据对象有三个attribute，分别博客网址、作者、博客文章数量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogItem</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, website, author, blog_nums</span>):</span></span><br><span class="line">        self.website = website</span><br><span class="line">        self.author = author</span><br><span class="line">        self.blog_nums = blog_nums</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&#123;0&#125;:&#123;1&#125;&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__,self.website)</span><br><span class="line"></span><br><span class="line">    __repr__ = __str__</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">blog_object_list = \</span><br><span class="line">    [BlogItem(<span class="string">&quot;www.aoo.cn&quot;</span>, <span class="string">&#x27;aoo&#x27;</span>, <span class="number">10</span>),</span><br><span class="line">     BlogItem(<span class="string">&quot;www.boo.cn&quot;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="number">5</span>),</span><br><span class="line">     BlogItem(<span class="string">&quot;www.coo.cn&quot;</span>, <span class="string">&#x27;coo&#x27;</span>, <span class="number">20</span>)</span><br><span class="line">     ]</span><br><span class="line">print(blog_object_list)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[BlogItem:www.aoo.cn, BlogItem:www.boo.cn, BlogItem:www.coo.cn]</span><br></pre></td></tr></table></figure>
<p>现要获取每行数据对象的blog属性，并对其实施排序，通常会使用lambda表达式实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">sorted</span>(blog_object_list, key=<span class="keyword">lambda</span> item: item.blog_nums))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[BlogItem:www.boo.cn, BlogItem:www.aoo.cn, BlogItem:www.coo.cn]</span><br></pre></td></tr></table></figure>
<p>有了attrgetter方法后，更方便调用获取对象的属性，例如下面的用法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">sorted</span>(blog_object_list,key=attrgetter(<span class="string">&#x27;blog_nums&#x27;</span>)))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[BlogItem:www.boo.cn, BlogItem:www.aoo.cn, BlogItem:www.coo.cn]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>也可以传入多个属性，按多个属性进行排序，例如这里先根据blog排序、再根据author排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">sorted</span>(blog_object_list,key=attrgetter(<span class="string">&#x27;blog_nums&#x27;</span>,<span class="string">&#x27;author&#x27;</span>)))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[BlogItem:www.boo.cn, BlogItem:www.aoo.cn, BlogItem:www.coo.cn]</span><br></pre></td></tr></table></figure>
<h4 id="2-2-attrgetter的内部实现："><a href="#2-2-attrgetter的内部实现：" class="headerlink" title="2.2 attrgetter的内部实现："></a>2.2 attrgetter的内部实现：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">attrgetter</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return a callable object that fetches the given attribute(s) from its `.</span></span><br><span class="line"><span class="string">    After f = attrgetter(&#x27;name&#x27;), the call f(r) returns r.name.</span></span><br><span class="line"><span class="string">    After g = attrgetter(&#x27;name&#x27;, &#x27;date&#x27;), the call g(r) returns (r.name, r.date).</span></span><br><span class="line"><span class="string">    After h = attrgetter(&#x27;name.first&#x27;, &#x27;name.last&#x27;), the call h(r) returns (r.name.first, r.name.last).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __slots__ = (<span class="string">&#x27;_attrs&#x27;</span>, <span class="string">&#x27;_call&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, attr, *attrs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> attrs:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(attr, <span class="built_in">str</span>):</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">&#x27;attribute name must be a string&#x27;</span>)</span><br><span class="line">            self._attrs = (attr,)</span><br><span class="line">            names = attr.split(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">getattr</span>(obj, name)</span><br><span class="line">            self._call = func</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#  如果获取多个属性，使用map对每个属性实施attrgetter</span></span><br><span class="line">            self._attrs = (attr,) + attrs</span><br><span class="line">            getters = <span class="built_in">tuple</span>(<span class="built_in">map</span>(attrgetter, self._attrs))</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">tuple</span>(getter(obj) <span class="keyword">for</span> getter <span class="keyword">in</span> getters)</span><br><span class="line">            self._call = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, obj</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._call(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s.%s(%s)&#x27;</span> % (self.__class__.__module__,</span><br><span class="line">                              self.__class__.__qualname__,</span><br><span class="line">                              <span class="string">&#x27;, &#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">repr</span>, self._attrs)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__, self._attrs</span><br></pre></td></tr></table></figure>
<p>attrgetter在内部定义了一个闭包函数func，该函数其实就是getattr(obj, name)的功能。<br>attrgetter内有个特殊的魔法方法<code>__reduce__</code>，该方法用于pickle反序列化后可以找到该对象绑定的类及其入参，若要想对某对象进行pickle，那么该对象要定义<code>__reduce__</code>方法，否则在序列化时（使用pickle.load）无法持久化存储，查看其源码pickle.dumps可以看到相关逻辑<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reduce = <span class="built_in">getattr</span>(obj, <span class="string">&quot;__reduce_ex__&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">if</span> reduce <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    rv = reduce(self.proto)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 获取被处理的对象的__reduce__方法，若不存在提示无法pickle持久化（或者称为无法被序列化），所以attrgetter对象或者其实例是可以被序列化</span></span><br><span class="line">    reduce = <span class="built_in">getattr</span>(obj, <span class="string">&quot;__reduce__&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> reduce <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        rv = reduce()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> PicklingError(<span class="string">&quot;Can&#x27;t pickle %r object: %r&quot;</span> %</span><br><span class="line">                            (t.__name__, obj))</span><br></pre></td></tr></table></figure></p>
<p><code>attrgetter的doc文档说：f = attrgetter(&#39;name&#39;), the call f(r) returns r.name。
__call__方法是为了实现f(r）用法，等价于getattr(r, name)，用于获取给定对象的属性值</code>，</p>
<p>与attrgetter不同的是：getattr只能获取单个属性值而且getattr也是python工厂函数，在builtins.py内部定义；而attrgetter可以获取多个属性值，是对getattr的再次封装和加强，只不过它的封装逻辑写得还不错，即清晰易懂。<br>==对比getattr与attrgetter的区别==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">R</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line"></span><br><span class="line"><span class="comment"># getattr用法</span></span><br><span class="line">r=R(<span class="string">&#x27;foo&#x27;</span>,<span class="number">10</span>)</span><br><span class="line">print(<span class="built_in">getattr</span>(r,<span class="string">&#x27;name&#x27;</span>))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">foo</span><br><span class="line"><span class="comment"># attrgetter用法</span></span><br><span class="line">f=attrgetter(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">print(f(r))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">foo</span><br></pre></td></tr></table></figure></p>
<p>这里给出<code>f=attrgetter(&#39;name&#39;)==&gt;f(r)==&gt;&#39;foo&#39;</code>的调用过程，有两种情况</p>
<p>A、只获取对象的一个属性<br><code>f=attrgetter(&#39;name&#39;)==&gt;f(r)==&gt;触发__call__==&gt;self._call(obj)，因为self._call = func==&gt;func(obj)，根据func的定义，就是返回getattr(obj, name)==&gt;getattr(r,&quot;name&quot;)==&gt;“foo”</code></p>
<p>B、获取对象两个属性以上的情况</p>
<p>例如attrgetter(‘name’，‘age’)，那么其传递过程为<br><code>f=attrgetter(&#39;name&#39;，‘age&#39;)==&gt;f(r)==&gt;触发__call__==&gt;self._call(obj)，因为self._call = func==&gt;func(obj)==&gt;因为要获取两个属性，故递归调用attrgetter： getters = tuple(map(attrgetter, self._attrs))==&gt;(attrgetter(&#39;name&#39;)，attrgetter(&#39;age&#39;))==&gt;对元组里面的元素按照步A的传递路线==&gt;(getattr(r,&quot;name&quot;),getattr(r,&quot;age&quot;))==&gt;(&#39;foo&#39;,10)</code></p>
<p>从上面的分析可知，attrgetter的实现有点绕了，所以python的闭包机制虽然可以基于原始函数上封装出具备更强功能的函数，但其代价就像符合函数，层层封装。<br>例如复合函数h，h=f(g(e(x)))，复合函数h作为加强版方法，通过封装f方法，f方法封装g方法，g方法封装e方法，从而使得h方法的功能比最初始e方法具备更强大的功能，但你需要一路往内部追踪，才知道h函数最里面的函数为e函数。</p>
<h4 id="2-3-itemgetter的使用场景"><a href="#2-3-itemgetter的使用场景" class="headerlink" title="2.3 itemgetter的使用场景"></a>2.3 itemgetter的使用场景</h4><p>attrgetter可以理解给定单个key或者多个key，返回这些key的值，而对应另外一个类itemgetter，它用来实现对数据对象在给定索引号后，返回索引号对应的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r=[</span><br><span class="line">    (<span class="string">&quot;www.boo.cn&quot;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="number">20</span>),</span><br><span class="line">    (<span class="string">&quot;www.aoo.cn&quot;</span>, <span class="string">&#x27;aoo&#x27;</span>, <span class="number">10</span>),</span><br><span class="line">    (<span class="string">&quot;www.coo.cn&quot;</span>, <span class="string">&#x27;coo&#x27;</span>, <span class="number">15</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">f=itemgetter(<span class="number">1</span>)</span><br><span class="line">print(f(r))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">(<span class="string">&#x27;www.aoo.cn&#x27;</span>, <span class="string">&#x27;aoo&#x27;</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">f=itemgetter(<span class="number">0</span>，<span class="number">2</span>) <span class="comment"># 注意这里不是范围，而是索引0和索引2</span></span><br><span class="line">print(f(r))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">((<span class="string">&#x27;www.boo.cn&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="number">20</span>), (<span class="string">&#x27;www.coo.cn&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>, <span class="number">15</span>))</span><br></pre></td></tr></table></figure>
<p>其源代码也简单，实现跟attrgetter逻辑一直，attrgetter使用getter(obj，key)获取值，itemgetter使用obj[index]的方式获得值，但要求obj内部必须实现<code>__getitem__(self, item)</code>方法。</p>
<h4 id="2-4-itemgetter的源码分析"><a href="#2-4-itemgetter的源码分析" class="headerlink" title="2.4 itemgetter的源码分析"></a>2.4 itemgetter的源码分析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">itemgetter</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return a callable object that fetches the given item(s) from its operand.</span></span><br><span class="line"><span class="string">    After f = itemgetter(2), the call f(r) returns r[2].</span></span><br><span class="line"><span class="string">    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __slots__ = (<span class="string">&#x27;_items&#x27;</span>, <span class="string">&#x27;_call&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, item, *items</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> items:</span><br><span class="line">            self._items = (item,)</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="comment"># 若只提供一个索引号，则直接按getitem的写法获取该对象的值</span></span><br><span class="line">                <span class="keyword">return</span> obj[item]</span><br><span class="line">            self._call = func</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._items = items = (item,) + items</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="comment"># 若获取多个索引号，则遍历这些索引号，获取每个对象的值</span></span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">tuple</span>(obj[i] <span class="keyword">for</span> i <span class="keyword">in</span> items)</span><br><span class="line">            self._call = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, obj</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._call(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s.%s(%s)&#x27;</span> % (self.__class__.__module__,</span><br><span class="line">                              self.__class__.__name__,</span><br><span class="line">                              <span class="string">&#x27;, &#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">repr</span>, self._items)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__, self._items</span><br></pre></td></tr></table></figure>
<p>取值传递过程有两种情况</p>
<p>A、当给定1个索引号</p>
<p><code>f=itemgetter(1)==&gt;self._call(obj)==&gt;因为self._call = func，等价于func(obj)==&gt;根据func的定义，obj[item]==&gt;r[1]==&gt;也即是列表索引取值的方式，(&#39;www.aoo.cn&#39;, &#39;aoo&#39;, 10)</code></p>
<p>B、给定多个索引号</p>
<p><code>f=itemgetter(0,2)==&gt;self._call(obj)==&gt;因为self._call = func，等价于func(obj)==&gt;根据func的定义以及入参大于1 ==&gt;tuple(obj[i] for i in items)==&gt;(obj[0],obj[1])==&gt;(r[0],r[1])==&gt;((&#39;www.boo.cn&#39;, &#39;boo&#39;, 20), (&#39;www.coo.cn&#39;, &#39;coo&#39;, 15))</code></p>
<h3 id="3、broax-fetch的用法"><a href="#3、broax-fetch的用法" class="headerlink" title="3、broax.fetch的用法"></a>3、broax.fetch的用法</h3><p>borax是一个python第三库轻量库，里面有一些基本中国农历函数、choice、数据结构、设计模式以及fetch函数。它的doc</p>
<blockquote>
<p>Borax is a utils collections for python3 development, which contains<br>some common data structures and the implementation of design patterns</p>
<p>主要的module:</p>
<ul>
<li>borax.calendars : A Chinese lunar calendar package, which contains lunar,festivals, birthday.</li>
<li>borax.choices : choices a enhance module using class-style define for const choices.</li>
<li>borax.fetch : A function sets for fetch the values of some axises.</li>
<li>borax.structures : A useful data structure for dictionary/list/set .</li>
<li>borax.patterns : A implementation for the design patterns.</li>
</ul>
</blockquote>
<p>fetch函数功能：从数据序列中选择一个或多个字段的数据，它很好展示了partial函数的实际项目的用法。</p>
<blockquote>
<p>在这里插播之后会写一篇blog的通告：<br>fetch是从已有的数据序列中，根据指定key或者属性对应的记录行，而records库则是从各类关系型数据库取出数据记录行（当然可完成增删查改），发现records源代码清晰简单，但实现功能确如此强大，所以接下来会单独给出一篇blog用于解析records源码，records不到550行，封装逻辑通俗易懂。</p>
<p>records是kennethreitz的for Humans™系列的库，用于近乎易懂的方式操作数据库，kennethreitz是requests库的作者—<a href="https://github.com/kennethreitz">github地址</a>），kennethreitz总能把底层较为繁琐的逻辑封装成易用的逻辑，其开源的项目的源代码具有不错的学习价值。</p>
</blockquote>
<h4 id="3-1-fetch模块的源码简析"><a href="#3-1-fetch模块的源码简析" class="headerlink" title="3.1  fetch模块的源码简析"></a>3.1  fetch模块的源码简析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> tee</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="comment"># 当使用from fetch import * 时，通过__all__属性来限制import *的导出范围</span></span><br><span class="line">__all__ = [<span class="string">&#x27;fetch&#x27;</span>, <span class="string">&#x27;ifetch&#x27;</span>, <span class="string">&#x27;fetch_single&#x27;</span>, <span class="string">&#x27;ifetch_multiple&#x27;</span>, <span class="string">&#x27;ifetch_single&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Empty</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">EMPTY = Empty()</span><br><span class="line"><span class="comment"># 以下iterable就是要处理的数据序列例如以下数据对象Person序列</span></span><br><span class="line"><span class="comment"># [Person(&#x27;aoo&#x27;,10&#x27;),Person(&#x27;boo&#x27;,21),....,Person(&#x27;coo&#x27;,13)]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ifetch_single</span>(<span class="params">iterable, key, default=EMPTY, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    getter() g(item, key):pass</span></span><br><span class="line"><span class="string">    # 给定单个key或者属性或者索引号，用于获取数据序列对象的值，例如获取每个Person数据对象name的值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_getter</span>(<span class="params">item</span>):</span></span><br><span class="line">        <span class="keyword">if</span> getter:</span><br><span class="line">            <span class="comment"># 这里就是第三库如何把partial引用到自己的代码实现里面的一个示例，这里留到后面章节给出其解释</span></span><br><span class="line">            custom_getter = partial(getter, key=key)</span><br><span class="line">            <span class="keyword">return</span> custom_getter(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 用了第2章节内容提到的attrgetter获取属性值</span></span><br><span class="line">                <span class="comment"># 其实就是return getattr(item,key)，即获取单个数据项key对应的值</span></span><br><span class="line">                attrgetter = operator.attrgetter(key)</span><br><span class="line">                <span class="keyword">return</span> attrgetter(item)</span><br><span class="line">            <span class="keyword">except</span> AttributeError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 用了第2章节内容提到的itemgetter，给定索引号，获取值</span></span><br><span class="line">                <span class="comment"># 其实就是return item[key],即获取给定索引号的单个数据项对应的值           </span></span><br><span class="line">                itemgetter = operator.itemgetter(key)</span><br><span class="line">                <span class="keyword">return</span> itemgetter(item)</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">if</span> default <span class="keyword">is</span> <span class="keyword">not</span> EMPTY:</span><br><span class="line">                <span class="keyword">return</span> default</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Item %r has no attr or key for %r&#x27;</span> % (item, key))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">map</span>(_getter, iterable)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_single</span>(<span class="params">iterable, key, default=EMPTY, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 因为ifetch_single返回的map对象，因此需要list化后，才能得到整个列表数据值</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(ifetch_single(iterable, key, default=default, getter=getter))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ifetch_multiple</span>(<span class="params">iterable, *keys, defaults=<span class="literal">None</span>, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 用于处理给定的key或者属性或者索引号的入参大于1个的情况，例如要获取每个Person数据对象的name属性的值、age属性的值、phone属性的值，所以有三个key：name、age、phone</span></span><br><span class="line">    defaults = defaults <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(keys) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 根据给定key的个数n，生成对应n个数据序列的迭代器，其实就是把要处理的数据序列变成迭代器后，并复制了多份,显然存在设计不合理的地方，拷贝多份数据，占用空间。</span></span><br><span class="line">        <span class="comment">#例如3个key对应生成3个迭代器 iters = (iterable,iterable,iterable)</span></span><br><span class="line">        iters = tee(iterable, <span class="built_in">len</span>(keys))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        iters = (iterable,)</span><br><span class="line">  <span class="comment">#结果为：[ifetch_single(data_list,&#x27;name&#x27;),ifetch_single(data_list,&#x27;age&#x27;),ifetch_single(data_list,&#x27;phone&#x27;)]    </span></span><br><span class="line">    iters = [ifetch_single(it, key, default=defaults.get(key, EMPTY), getter=getter) <span class="keyword">for</span> it, key <span class="keyword">in</span> <span class="built_in">zip</span>(iters, keys)]</span><br><span class="line">    <span class="keyword">return</span> iters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ifetch</span>(<span class="params">iterable, key, *keys, default=EMPTY, defaults=<span class="literal">None</span>, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 该函数就是通过判断需要获取1个属性还是多个属性来决定调用ifetch_single还是ifetch_multiple</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(keys) &gt; <span class="number">0</span>:</span><br><span class="line">        keys = (key,) + keys</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">map</span>(<span class="built_in">list</span>, ifetch_multiple(iterable, *keys, defaults=defaults, getter=getter))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ifetch_single(iterable, key, default=default, getter=getter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">iterable, key, *keys, default=EMPTY, defaults=<span class="literal">None</span>, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 这个fetch其实多此一举，可以直接在ifetch返回处加入list方法即可。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(ifetch(iterable, key, *keys, default=default, defaults=defaults, getter=getter))</span><br></pre></td></tr></table></figure>
<h4 id="3-2-fetch的使用示例"><a href="#3-2-fetch的使用示例" class="headerlink" title="3.2 fetch的使用示例"></a>3.2 fetch的使用示例</h4><p>示例1<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_list = [</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;aro&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">10</span>,<span class="string">&#x27;phone&#x27;</span>:<span class="number">131</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;bro&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">13</span>,<span class="string">&#x27;phone&#x27;</span>:<span class="number">132</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;cro&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">15</span>,<span class="string">&#x27;phone&#x27;</span>:<span class="number">143</span>&#125;,</span><br><span class="line">]</span><br><span class="line">result = fetch(data_list,<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment">#[&#x27;aro&#x27;,&#x27;bro&#x27;,&#x27;cro&#x27;]</span></span><br><span class="line"></span><br><span class="line">result = fetch(data_list,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment">#[[&#x27;aro&#x27;,&#x27;bro&#x27;,&#x27;cro&#x27;],[10,13,15]]</span></span><br></pre></td></tr></table></figure><br>当了解fetch里面的调用了itemgetter的内部逻辑后，其实就是字典的取值：data[‘name’]，data[‘age’]</p>
<p>示例2<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">R</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">data_list=[</span><br><span class="line">	R(<span class="string">&#x27;aro&#x27;</span>,<span class="number">10</span>),</span><br><span class="line">	R(<span class="string">&#x27;bro&#x27;</span>,<span class="number">12</span>),</span><br><span class="line">	R(<span class="string">&#x27;cro&#x27;</span>,<span class="number">19</span>)</span><br><span class="line">]</span><br><span class="line">print(fetch(data_list,<span class="string">&#x27;name&#x27;</span>))      </span><br><span class="line"><span class="comment">#[&#x27;aro&#x27;, &#x27;bro&#x27;, &#x27;cro&#x27;]    </span></span><br></pre></td></tr></table></figure><br>当了解fetch里面的调用了attrgetter的内部逻辑后，其实就是使用内建方法getattr获取对象属性的值：[getattr(data1,’name’),getattr(data2,’name’),getattr(data3,’name’)]</p>
<p>示例3<br>自定义getter，这里用到了partial<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, <span class="built_in">id</span>, name, age, phone</span>):</span></span><br><span class="line">        self.<span class="built_in">id</span> = <span class="built_in">id</span></span><br><span class="line">        self._data = &#123;<span class="string">&#x27;name&#x27;</span>: name, <span class="string">&#x27;age&#x27;</span>: age, <span class="string">&#x27;phone&#x27;</span>: phone&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, key</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._data.get(key)</span><br><span class="line">data_item = [</span><br><span class="line">    Person(<span class="number">1001</span>,<span class="string">&#x27;Aerk&#x27;</span>, <span class="number">22</span>, <span class="number">141</span>),</span><br><span class="line">    Person(<span class="number">1002</span>, <span class="string">&#x27;Berk&#x27;</span>, <span class="number">25</span>, <span class="number">151</span>),</span><br><span class="line">    Person(<span class="number">1003</span>, <span class="string">&#x27;Derk&#x27;</span>, <span class="number">21</span>, <span class="number">181</span>)</span><br><span class="line">]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_getter</span>(<span class="params">item,key</span>):</span></span><br><span class="line">    <span class="keyword">return</span> item.get(key)</span><br><span class="line">values = fetch(data_item, <span class="string">&#x27;name&#x27;</span>, getter=my_getter)</span><br><span class="line">print(values)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment"># [&#x27;Aerk&#x27;, &#x27;Berk&#x27;, &#x27;Derk&#x27;]</span></span><br></pre></td></tr></table></figure></p>
<h3 id="4、本文核心内容"><a href="#4、本文核心内容" class="headerlink" title="4、本文核心内容"></a>4、本文核心内容</h3><p>经过前面3个章节多个知识点的铺垫后，再来看本章节内容，则会更容易理解。本节内容对应前言第一部分：<br>wrap装饰器的调用过程：functools.wraps——&gt;partial——&gt;update_wrapper<br>先看看functools.wraps的示例。</p>
<h4 id="4-1-login-require的装饰器例子"><a href="#4-1-login-require的装饰器例子" class="headerlink" title="4.1 login_require的装饰器例子"></a>4.1 login_require的装饰器例子</h4><p>在Django的app开发中，一般会使用装饰器鉴权，大致如下结构<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># 未使用functools.wraps</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line"></span><br><span class="line"><span class="meta">@login_require</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_blog_list</span>(<span class="params">request</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取blog列表的function&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">200</span></span><br><span class="line"></span><br><span class="line">print(get_blog_list.__doc__)</span><br><span class="line">print(get_blog_list.__name__)</span><br><span class="line">print(get_blog_list.__qualname__)</span><br></pre></td></tr></table></figure><br>输出<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  用于对外部的request做是否已经登录请求鉴权</span></span><br><span class="line"><span class="comment"># inner_wrap</span></span><br><span class="line"><span class="comment"># login_require.&lt;locals&gt;.inner_wrap</span></span><br></pre></td></tr></table></figure><br>这里显然不符合需求，get<em>blog<em>list的<code>__doc__</code>、<code>__name__</code>、<code>__qualname__</code>被改成login<em>require里面闭包函数inner<em>wrap对应属性的值<br>这里如何保证被装饰函数get<em>blog<em>list的属性值不被改动呢？加入functools.wraps(func)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line">print(get_blog_list.__doc__)</span><br><span class="line">print(get_blog_list.__name__)</span><br><span class="line">print(get_blog_list.__qualname__)</span><br></pre></td></tr></table></figure><br>输出<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">获取blog列表的function</span><br><span class="line">get_blog_list</span><br><span class="line">get_blog_list</span><br></pre></td></tr></table></figure><br>这次，get_blog_list的`__doc</em></em><code>、</code>__name</em></em><code>、</code>__qualname</em></em>`属性保持不变。</p>
<p>不使用functools.wraps(func)，也可以实现get_blog_list被装饰后，其属性值保持不变，通过setattr处理即可<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    inner_wrap.__doc__=func.__doc__</span><br><span class="line">    inner_wrap.__name__=func.__name__</span><br><span class="line">    inner_wrap.__qualname__=func.__qualname__</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line">    </span><br><span class="line"><span class="comment">#或者使用setattr设置属性值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__doc__&#x27;</span>,func.__doc__)</span><br><span class="line">    <span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__name__&#x27;</span>,func.__name__)</span><br><span class="line">    <span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__qualname__&#x27;</span>,func.__qualname__)</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line">print(get_blog_list.__doc__)</span><br><span class="line">print(get_blog_list.__name__)</span><br><span class="line">print(get_blog_list.__qualname__)    </span><br></pre></td></tr></table></figure><br>输出<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">获取blog列表的function</span><br><span class="line">get_blog_list</span><br><span class="line">get_blog_list</span><br></pre></td></tr></table></figure><br>update_wrapper正是使用上述setter方式实现对原函数被装饰后，新函数属性和原函数属性和保持一致。<br>但在软件工程中，这种写法是过程式设计，初级的写法，无法被重用，因此需要使用更优雅的方式将这些逻辑封装打包，对外可以重用。</p>
<h4 id="4-2-functools-wraps的定义"><a href="#4-2-functools-wraps的定义" class="headerlink" title="4.2 functools.wraps的定义"></a>4.2 functools.wraps的定义</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wraps</span>(<span class="params">wrapped,</span></span></span><br><span class="line"><span class="function"><span class="params">          assigned = WRAPPER_ASSIGNMENTS,</span></span></span><br><span class="line"><span class="function"><span class="params">          updated = WRAPPER_UPDATES</span>):</span></span><br><span class="line">    <span class="keyword">return</span> partial(update_wrapper, wrapped=wrapped,</span><br><span class="line">                   assigned=assigned, updated=updated)</span><br></pre></td></tr></table></figure>
<p>wraps里面调用partial，partial里面调用update_wrapper，应用在get_blog_list也就是：partial(update_wrapper, wrapped=get_blog_list)<br>接下来，先看看update_wrapper到底实现的什么功能</p>
<h4 id="4-3-update-wrapper的源码分析"><a href="#4-3-update-wrapper的源码分析" class="headerlink" title="4.3 update_wrapper的源码分析"></a>4.3 update_wrapper的源码分析</h4><p>以login_require的装饰器例子，wrapper就是inner_wrap函数，而wrapped则是get_blog_list<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">WRAPPER_ASSIGNMENTS = (<span class="string">&#x27;__module__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;__qualname__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>,<span class="string">&#x27;__annotations__&#x27;</span>)</span><br><span class="line">WRAPPER_UPDATES = (<span class="string">&#x27;__dict__&#x27;</span>,)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_wrapper</span>(<span class="params">wrapper,</span></span></span><br><span class="line"><span class="function"><span class="params">                   wrapped,</span></span></span><br><span class="line"><span class="function"><span class="params">                   assigned = WRAPPER_ASSIGNMENTS,</span></span></span><br><span class="line"><span class="function"><span class="params">                   updated = WRAPPER_UPDATES</span>):</span></span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> assigned:</span><br><span class="line">        <span class="comment"># 将原函数指定的5个属性（__name__、__doc__等）更新到（注册到/覆盖到）新函数，使得原函数被装饰后，指定的5个属性也保不变。</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 获取原函数的属性值</span></span><br><span class="line">            value = <span class="built_in">getattr</span>(wrapped, attr)</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 将原函数的属性值更新到相应的新函数属性中</span></span><br><span class="line">            <span class="built_in">setattr</span>(wrapper, attr, value)</span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> updated:</span><br><span class="line">        <span class="comment"># 原函数的__dict__属性更新到（注册到/覆盖到）新函数的__dict__属性，从而实现原函数被装饰后其__dict__属性保持不变。</span></span><br><span class="line">        <span class="built_in">getattr</span>(wrapper, attr).update(<span class="built_in">getattr</span>(wrapped, attr, &#123;&#125;))</span><br><span class="line">    wrapper.__wrapped__ = wrapped</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><br>所以update_wrapper设计简约，实现了4.1章节内容所提如下内容的功能<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__doc__&#x27;</span>,func.__doc__)</span><br><span class="line"><span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__name__&#x27;</span>,func.__name__)</span><br><span class="line"><span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__qualname__&#x27;</span>,func.__qualname__)</span><br></pre></td></tr></table></figure></p>
<h4 id="4-3-partial的源码分析"><a href="#4-3-partial的源码分析" class="headerlink" title="4.3 partial的源码分析"></a>4.3 partial的源码分析</h4><p>文章到了这里，对partial的了解将会更加深入。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial</span>(<span class="params">func, *args, **keywords</span>):</span></span><br><span class="line">    <span class="comment">#对于login_require的装饰器例子，这里的func参数就是update_wrapper，args参数为空为:keywords就是wrapped=get_blog_list被装饰函数,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES</span></span><br><span class="line">    <span class="comment"># 因为update_wrapper没有`func`属性，所以跳过这部分处理</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(func, <span class="string">&#x27;func&#x27;</span>):</span><br><span class="line">        args = func.args + args</span><br><span class="line">        tmpkw = func.keywords.copy()</span><br><span class="line">        tmpkw.update(keywords)</span><br><span class="line">        keywords = tmpkw</span><br><span class="line">        <span class="keyword">del</span> tmpkw</span><br><span class="line">        func = func.func</span><br><span class="line"></span><br><span class="line"><span class="comment">#   这里newfunc是整个partial设计为最巧妙的地方！！！乃至functools.wraps里面设计最为巧妙的环节。将新加入的位置参数和关键字参数追加到func里</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">newfunc</span>(<span class="params">*fargs, **fkeywords</span>):</span></span><br><span class="line">        <span class="comment">#wrapped=get_blog_list被装饰函数,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES拷贝到newkeywords</span></span><br><span class="line">        newkeywords = keywords.copy()</span><br><span class="line">        newkeywords.update(fkeywords)</span><br><span class="line">        <span class="comment"># 对于login_require例子，这里fkeywords为空</span></span><br><span class="line">        <span class="keyword">return</span> func(*(args + fargs), **newkeywords)</span><br><span class="line"></span><br><span class="line">    newfunc.func = func</span><br><span class="line">    newfunc.args = args</span><br><span class="line">    newfunc.keywords = keywords</span><br><span class="line">    <span class="keyword">return</span> newfunc</span><br></pre></td></tr></table></figure><br>当使用以下写法时<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@functools.wraps(<span class="params">func</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_wrapper</span></span></span><br></pre></td></tr></table></figure><br>就会触发partial调用newfunc，而newfunc被定义Wie函数，是可以被<code>__call__</code>的，它返回func(<em>(args + fargs), *</em>newkeywords)，func就是update_wrapper函数，args为空参数，fargs就是inner_wrapper函数，newkeywords就是wrapped=get_blog_list被装饰函数,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES<br>所以以下这一小段代码<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@functools.wraps(<span class="params">func</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_wrapper</span></span></span><br></pre></td></tr></table></figure></p>
<p>就是转换为以下语句的调用。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">update_wrapper(wrapper=inner_wrapper,wrapped=get_blog_list,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES)</span><br></pre></td></tr></table></figure></p>
<p>4.3章节已经详细指出update<em>wrapper的作用：<br> 将原函数get<em>blog<em>list指定的5个属性（`<em>_name</em></em><code>、</code>__doc</em></em>`等）更新到（注册到/覆盖到）新函数inner_wrapper，使得原函数被装饰后，指定的5个属性也保不变。<br>这就是functools.wraps(func)的内部基于partial的实现逻辑</p>
<h4 id="4-4-再论partial"><a href="#4-4-再论partial" class="headerlink" title="4.4 再论partial"></a>4.4 再论partial</h4><p>partical的官方说明：</p>
<blockquote>
<p>functools.partial(func, <em>args, *</em>keywords)<br>Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords. If more arguments are supplied to the call, they are appended to args. If additional keyword arguments are supplied, they extend and override keywords. </p>
</blockquote>
<p>从英文的解释来看（不建议翻译为中文，直接理解英文更加准确），partial不应该翻译成“偏函数”，partial词意中：有不完整的意思，翻译成“待补全外部参数”的类函数对象貌似更贴切，<br>以下面例子说明<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">x,y,z=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> x+y+z</span><br><span class="line"><span class="comment"># 先给定1个位置参数和1个关键字参数，对于add函数，参数是不完整的，需要待后面补全多一个外部位置参数    </span></span><br><span class="line">add_15=partial(add,<span class="number">5</span>,z=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#这里add_15如果直接调用，会提示缺少一个位置参数，因此对于add_15，3就是待补全外部参数，</span></span><br><span class="line">print(add_15(<span class="number">3</span>)) <span class="comment"># 输出18</span></span><br></pre></td></tr></table></figure><br>以上的实际执行过程如下：<br>partial返回func(<em>(args + fargs), **newkeywords)，这里的func为add函数，args为(5,10)两个位置参数，3为后面补全的参数，就是fargs的值，newkeywords定义为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">newkeywords = keywords.copy()</span><br><span class="line">newkeywords.update(fkeywords)</span><br></pre></td></tr></table></figure><br>add的z=10，就是keywords，fkeywords为空，因此newkeywords为{‘z’:10}<br>所以有以下等价链<br>`partial(add,5,z=10)&lt;===&gt;func(</em>(args + fargs), <em>*newkeywords)&lt;===&gt;add(</em>((5,)+(3,)),z=10)&lt;===&gt;add(5,3,z=10)`</p>
<p>从上面分析可以看出，paritial最大的用处就是基于某个原函数和原函数参数基础上生成一个”待补全外部参数新函数“，提供给该新函数的入参个数比原函数少了，得到高效简洁地调用指定函数，例如新函数add_15只需要提供1个参数即可，而原函数add需要提供3个参数。</p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>functool.wraps</tag>
        <tag>partial</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Kafka</title>
    <url>/2019/12/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章<a href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，介绍大数据实时分析平台生态圈组件——kafka，前向用于连接flume，后向连接spark streaming。在研究Kafka过程中，发现该中间件的设计很巧妙，因此专设一篇文章用于深入理解Kafka核心知识。Kafka已经纳入个人目前最欣赏的中间件list：redis，zookeeper，kafka</p>
<h4 id="1、kafka集群架构图"><a href="#1、kafka集群架构图" class="headerlink" title="1、kafka集群架构图"></a>1、kafka集群架构图</h4><p>以下为kafka集群一种经典的架构图，该图以《在hadoopHA节点上部署kafka集群组件》文章的kafka集群以及sparkapp topic作为示例绘成，本文的内容将以该图为标准作为说明。<br><img src="https://img-blog.csdnimg.cn/20191127231934698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="kafka集群架构图">图1 kafka集群架构图</p>
<a id="more"></a>
<h4 id="2、kafka-高性能读写的设计"><a href="#2、kafka-高性能读写的设计" class="headerlink" title="2、kafka 高性能读写的设计"></a>2、kafka 高性能读写的设计</h4><h5 id="2-1、利用read-ahead-和-write-behind提升写性能"><a href="#2-1、利用read-ahead-和-write-behind提升写性能" class="headerlink" title="2.1、利用read-ahead 和 write-behind提升写性能"></a>2.1、利用read-ahead 和 write-behind提升写性能</h5><p>&#8195;&#8195;kafka底层设计高度依赖现代磁盘优化技术和文件系统的优化技术。在kafka官方文档的：<a href="http://kafka.apache.org/documentation/#design">don’t fear the filesystem</a>章节说明了kafka是如何利用磁盘已有的高性能读写技术：read-ahead 和 write-behind 实现日志在磁盘山高性能顺序写。<br>&#8195;&#8195;read-ahead 是以大的 data block 为单位预先读取数据。write-behind（后写） 是将多个小型的逻辑写合并成一次大型的物理磁盘写入，producer向kafka写入消息日志时，因为消息是一条一条的过来，而且消息本身payload很小，如果每条消息进来立刻执行写入磁盘，显然IO非常高，因此需要将进来的消息先缓存，然后到一定数量或者到一定容量时再触发写入磁盘，kafka用了pagecache实现write-behind而不是通过内存。<br>&#8195;&#8195;官方举例说明用廉价的RAID-5模式sata硬盘可以去到600MB/秒，但随机写入的性能仅约为100k/秒，相差6000倍以上。</p>
<h5 id="2-2、使用pagecache缓存程序数据提升读写性能"><a href="#2-2、使用pagecache缓存程序数据提升读写性能" class="headerlink" title="2.2、使用pagecache缓存程序数据提升读写性能"></a>2.2、使用pagecache缓存程序数据提升读写性能</h5><p>&#8195;&#8195;同样，在kafka官方文档的：<a href="http://kafka.apache.org/documentation/#design">don’t fear the filesystem</a>章节还提到另外一个技术：pagecache。kafka利用了现代操作系统主动将所有空闲内存用作磁盘caching这一机制（代价是在内存回收时性能会有所降低），再次提升基于filesystem的读写性能的效果。<br>&#8195;&#8195;kafka 跑在 jvm之上，那么jvm一定会有复杂的GC情况：</p>
<ul>
<li>对象的内存开销非常高，通常是所存储的数据的两倍(甚至更多)。</li>
<li>随着堆中数据的增加，Java 的垃圾回收变得越来越复杂和缓慢。</li>
</ul>
<p>&#8195;&#8195;受这些因素影响， 维护in-memory cache就会显得很复杂，而kafka通过文件系统方式和 pagecache 读写消息反而显得更有优势（避免复杂低效率的GC），通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番，例如32GB内存的服务器，它的 pagecache缓存容量可以达到28-30GB，并且不会产生额外的 GC 负担。kafka自己也说还有重要一点：简化核心代码。<br>为何这么设计？<br>&#8195;&#8195;kafka自己这么解释：因为相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将消息数据 flush 到文件系统的，kafka写过程把这个过程倒过来：所有消息数据一开始就被写入（write-behind）到文件系统的持久化日志中，而不用在in-memory cache 空间不足的时候 flush 到磁盘。实际上，是先把数据被转移到了内核的 pagecache 中。<br>&#8195;&#8195;这里可以联想到Hbase的MemStore设计：MemStore基于in-memory cache，MemStore 在内存中存在，保存修改key-value数据，当MemStore的大小达到一个阀值（默认64MB）时，MemStore里面的数据会被flush到Hfile文件上，也就是flush到磁盘上。</p>
<p>==为何page cache 会加速读过程？==<br>linux的文件cache分为两层，一个是page cache，另一个是buffer cache；每一个page cache包含若干个buffer cache，结构图如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191130122756221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>page cache：文件系统层级的缓存，从磁盘里读取数据缓存到page cache（属于内核空间，而不是应用用户的空间），这样应用读磁盘数据会被加速，例如使用find等命令查找文件时，第一次会慢很多，第二次查找相同文件时会瞬间读取到。如果page cache的数据被修改过后，也即脏数据，等到写入磁盘时机到来时，会把数据转移到buffer cache 而不是直接写入到磁盘。<br>buffer cache：磁盘等块设备的缓冲。<br>大致流程：<br><img src="https://img-blog.csdnimg.cn/20191130120633422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">page cache其优化读的工作过程如下：<br>A、文件的第一次读请求<br>系统读入所请求的page页并读入紧随其后的的少数几个页面，这种读取方式称为同步预读。<br>B、文件的第二次读请求:<br>如果page页不在第一次的cache中，说明不是顺序读，所以又会重新继续第一次那种同步预读过程。</p>
<p>==如果page页面在cache中，说明是顺序读，Linux会将预读group扩大一倍==，继续把不在首次cache中的文件数据读进来，此为异步预读。kafka之所以设计按顺序读写，完全就是按照底层page cahe的这种预读机制来设计，所以在文件系统底层就已经有不错的性能了。</p>
<h5 id="2-3-通过sendfile（零拷贝机制）提高消费者端的读吞吐量"><a href="#2-3-通过sendfile（零拷贝机制）提高消费者端的读吞吐量" class="headerlink" title="2.3 通过sendfile（零拷贝机制）提高消费者端的读吞吐量"></a>2.3 通过sendfile（零拷贝机制）提高消费者端的读吞吐量</h5><p>&#8195;&#8195;在kafka官方文档的Efficiency章节解释了kafka通过使用sendfile （零拷贝技术）继续提高消费者端的读性能。<br>&#8195;&#8195;前面2.1和2.2解释了kafka里利用相关底层机制，解决了磁盘访问模式不佳的情况。接下来，还需要解决以下两个影响kafka性能的情况：<br>too many small I/O operations, and excessive byte copying<br>（大量的小型 I/O 操作以及过多的字节拷贝 ）</p>
<ul>
<li><p>A、 The small I/O problem happens both between the client and the server and in the server’s own persistent operations.<br>（大量小型的 I/O 操作表现在client和broker之间以及broker服务端自身持久化操作中）<br>解决方式：kafka用一个称为 “消息块” 的抽象基础上，合理将消息分组。 这使得网络请求将多个消息打包成一组，而不是每次发送一条消息，从而使整组消息分担网络中往返的开销。consumer 每次获取多个大型有序的消息块，并由服务端依次将消息块一次加载到它的日志中。<br>这个简单的优化对速度有着数量级的提升。批处理允许更大的网络数据包，更大的顺序读写磁盘操作，连续的内存块等等</p>
</li>
<li><p>B、excessive byte copying<br>另一个低效率的操作是字节拷贝，在消息量少时，这不是什么问题，但是在高负载的情况下，影响就不容忽视。为了避免这种情况，kafka在producer、broker 和 consumer 都是用相同标准化的二进制消息格式，这样数据块不用修改就能在他们之间传递。<br>broker 维护的消息日志本身就是一个文件目录，每个segment文件都由一系列以相同格式消息组成，保持这种通用格式将非常有利于消息日志文件的网络传输的效率。 现代的unix 操作系统提供了一个高度优化的编码方式，用于将数据从 pagecache 转移到 socket 网络连接中，减少内核拷贝次数；在 Linux 中系统调用<a href="http://man7.org/linux/man-pages/man2/sendfile.2.html"> sendfile </a>方式做到这一点。 </p>
</li>
</ul>
<p>先看看数据从磁盘文件到套接字的拷贝过程：<br><code>File.read(fileDesc, buf, len);</code><br><code>Socket.send(socket, buf, len);</code><br>以上两个操作是java语义的读取文件和socket发送数据包，一共有两次拷贝？当然不是的：<br>1） 操作系统从磁盘读取数据到内核空间的 page cache<br>2）应用程序从内核空间 page cache读取数据到用户空间的缓冲区（应用程序的地址空间）<br>3）应用程序将数据(用户空间的缓冲区)写回内核空间到套接字缓冲区(内核空间)<br>4）操作系统将数据从套接字缓冲区(内核空间)复制到通过网络发送的 NIC 缓冲区<br>以上过程有四次 copy 操作和两次系统调用，如果数据传输吞吐量大时，对Kafka来说低效率，如何减少拷贝次数？<br>使用 内核提供的sendfile 方法，使用零拷贝的应用程序要求内核直接将数据从磁盘文件拷贝到套接字，而无需通过应用程序。零拷贝不仅大大地提高了应用程序的性能，而且还减少了内核与用户模式间的上下文切换。例如一个 topic 被多消费者消费时，使用上面zero-copy（零拷贝）优化，消息在使用时只会被复制到pagecache 中一次，节省了每次拷贝到用户空间内存中，再从用户空间进行读取的消耗。这使得消息能够以接近服务器网卡Gb级别的网速来进行消费。</p>
<blockquote>
<p>在应用程序和网络之间提供更快的数据传输方法，从而可以有效地降低通信延迟，提高网络吞吐率。零拷贝技术是实现主机或者路由器等设备高速网络接口的主要技术之一。举例来说，一个 1 GHz 的处理器可以对 1Gbit/s 的网络链接进行传统的数据拷贝操作，但是如果是 10 Gbit/s 的网络，那么对于相同的处理器来说，零拷贝技术就变得非常重要了。</p>
</blockquote>
<p>page cache 和 sendfile 的组合使用意味着，在一个kafka集群中，大多数 consumer 消费时，==将看不到磁盘上的读取活动，因为数据将完全由缓存提供==。<br>有关zero-copy以及Linux IO详细内容，推荐IBM Developer中国区四篇高质量文章：<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-directio/index.html?mhsrc=ibmsearch_a&amp;mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中直接 I/O 机制的介绍》</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html?mhsrc=ibmsearch_a&amp;mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中的零拷贝技术，第 1 部分》</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/index.html?mhsrc=ibmsearch_a&amp;mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中的零拷贝技术，第 2 部分》</a><br><a href="https://www.ibm.com/developerworks/cn/java/j-zerocopy/index.html?mhsrc=ibmsearch_a&amp;mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《通过零拷贝实现有效数据传输》</a>，这篇文章翻译了IBM Developer官方英文原文：<a href="https://developer.ibm.com/articles/j-zerocopy/">《Efficient data transfer through zero copy》</a><br>阅读这几篇文章可以说收益匪浅，不仅深度理解了kafka使用filesystem作为消息队列的底层文件IO，而且也有利于理解任何基于文件系统上的中间件的部分实现机制。</p>
<h4 id="3、kafka的repilcas副本机制"><a href="#3、kafka的repilcas副本机制" class="headerlink" title="3、kafka的repilcas副本机制"></a>3、kafka的repilcas副本机制</h4><h5 id="3-1-主分区的副本"><a href="#3-1-主分区的副本" class="headerlink" title="3.1 主分区的副本"></a>3.1 主分区的副本</h5><p>&#8195;&#8195;Kafka 允许 topic 的 partition 拥有若干副本，也就是说每个partition都有一个 leader 和零或多个 followers，例如图1 kafka集群架构图中，sparkapp这个topic，在broker1有主分区（leader）partition-0，在broker-1和broker-2有followers副本分区（replica）partition-0。  总的副本数是包含 leader 分区的总和。 所有的读写操作都由 leader 处理，各分区的 leader 均 匀的分布在brokers 中，一个topic在当前broker只能有一个leader主分区。followers节点就像普通的 consumer 那样从 leader 节点那里拉取消息并保存在自己的日志文件中。</p>
<h5 id="3-2-leade如何管理follower节点"><a href="#3-2-leade如何管理follower节点" class="headerlink" title="3.2 leade如何管理follower节点"></a>3.2 leade如何管理follower节点</h5><p>&#8195;&#8195;Kafka 判断节点是否存活有两种方式。</p>
<ul>
<li>首先follower所在的broker服务器在线，Zookeeper 通过心跳机制检查每个broker的连接，对应的znode路径/brokers/ids。</li>
<li>要求follower角色的同步进程 ，它必须能及时的同步 leader 的写操作，并且延时不能太多。 </li>
</ul>
<p>&#8195;&#8195;kafka认为满足这两个条件的节点处于 “in sync” 状态， Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本集合ISR中移除。这个ISR列表在zookeeper可以看到，a set of In-Sync Replicas，简称：ISR：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 36] get &#x2F;brokers&#x2F;topics&#x2F;sparkapp&#x2F;partitions&#x2F;1&#x2F;state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:6,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:2,&quot;isr&quot;:[11,12,10]&#125;</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;以上说明：sparkapp的partition-1这个主分区在brokerid为10的服务器上，其他follower的brokerid分别为11和12。<br>可配置在ISR移除follower的触发条件：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果leader发现follower超过10秒没有向它发起同步请求，那么leader会认为follower无法正常同步主分区日志，就把它从ISR集合中中移除。</span><br><span class="line"> rerplica.lag.time.max.ms&#x3D;10000 # 默认值</span><br><span class="line"> # 相差1000条就从ISR集合移除该follower</span><br><span class="line"> rerplica.lag.max.messages&#x3D;1000# 默认值</span><br></pre></td></tr></table></figure></p>
<h5 id="3-3-Replica如何均匀分布到整个kafka集群"><a href="#3-3-Replica如何均匀分布到整个kafka集群" class="headerlink" title="3.3 Replica如何均匀分布到整个kafka集群"></a>3.3 Replica如何均匀分布到整个kafka集群</h5><p>&#8195;&#8195;为了更好的做负载均衡以及HA，Kafka尽量降所有的replicas均匀分配到整个集群上。为了更直观partition的副本是如何被分布到不同节点上，这里以一个小例子为例：创建一个fooTopic（可以先把它理解为消息队列queue，类似RabbitMQ的队列）且有五个分区，每个分区有三个副本replica，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181 --replication-factor 3 --partitions 5 --topic fooTopic</span><br><span class="line">[root@dn1 kafka-2.12]#  bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181 --topic fooTopic</span><br><span class="line">Topic:fooTopic  PartitionCount:5        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: fooTopic Partition: 0    Leader: 11      Replicas: 11,10,12      Isr: 11,10,12</span><br><span class="line">        Topic: fooTopic Partition: 1    Leader: 12      Replicas: 12,11,10      Isr: 12,11,10</span><br><span class="line">        Topic: fooTopic Partition: 2    Leader: 10      Replicas: 10,12,11      Isr: 10,12,11</span><br><span class="line">        Topic: fooTopic Partition: 3    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: fooTopic Partition: 4    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>Kafka分配资源跟很多中间件一样：通过取余实现，具体的规则如下：<br> 1）序号为i的Partition分配到第（i mod n）个Broker上，n为集群的broker总数<br> 2）序列号为i的Partition的第j个Replica分配到第（(i + j) mod n）个Broker上</p>
<p>以上述fooTopic为例，给出其分布过程：首先查看broker ids的列表为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[11, 12, 10]</span><br></pre></td></tr></table></figure><br>[11, 12, 10]列表的项的索引从0开始，因为kafka是用Scala语言开发，Scala获取zk这个ids值后，肯定是转为Scala数组类型，它的索引从0开始。当然也适用replicas数组（列表）。（这里为何不是[10, 11, 12]？因为本次获取结果是最新的集群选举结果数组）<br>假设a=[11, 12, 10]那么a[0]=11,a=[1]=12,a[2]=10</p>
<ul>
<li><p>按规则1）对于partition的序号i，它会分配到第（i mod n）个broker上：<br>那么对于partition0，0 mod 3=0，所以该在a[0]=11这个broker上，<br>同理有：<br>partition1，1 mod 3=1，所以该在a[1]=12这个broker上<br>partition2，2 mod 3=2，所以该在a[2]=10这个broker上<br>partition3，3 mod 3=0，所以该在a[0]=11这个broker上<br>partition4，4 mod 3=1，所以该在a[1]=12这个broker上</p>
</li>
<li><p>按规则 2）序列号为i的Partition的第j个Replica分配到第（(i + j) mod n）个Broker上<br>那么对于序列号为4的partition和序列号为0的replica，（4+0）mod 3=1，所以该在a[1]=12这个broker上，那么这个就是主leader分区，符合规则1partition4在12这个broker的计算结果。<br>那么对于序列号为4的partition和序列号为1的replica，（4+1）mod 3=2，所以该在a[2]=10这个broker上，<br>那么对于序列号为4的partition和序列号为2的replica，（4+2）mod 3=0，所以该在a[0]=11这个broker上<br>也就说partition4的replicas为[12,10,11]</p>
</li>
</ul>
<h4 id="4、Kafka消息的ack机制"><a href="#4、Kafka消息的ack机制" class="headerlink" title="4、Kafka消息的ack机制"></a>4、Kafka消息的ack机制</h4><p>&#8195;&#8195;这里是指producer向broker写消息的确认机制，这直接影响到Kafka集群的吞吐量和消息可靠性。而吞吐量和可靠性是矛盾的，两者不可兼得，只能平衡。<br>&#8195;&#8195;在第3章节提到leader和follower节点日志同步的内容，kafka动态维护了一个同步状态的副本的集合，在这个集合中的节点都是和leader保持高度一致的，任何一条消息只有被这个集合中的每个节点读取并追加到日志中，才会向外部通知说“这个消息已经committed。<br>&#8195;&#8195;也就是说只有当消息被ISR上所有的followers加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。这一环节就是决定了消息队列吞吐量和可靠性的环节。消息从leader复制到follower，可通过producer是否等待消息被提交的通知(ack)来区分同步复制和异步复制。<br>ack有3个可选值，分别是1，0，-1，可通过server.properties进行配置：<br>request.required.asks=0<br>==ack=0==:相当于异步的，producer给broker发送一次就不再发送了，不管本条消息是否在leader和follower都写入成功。可靠性低，吞吐量当然高。<br>==ack=1==：producer等待leader这个主分区成功写入了消息，producer才会认为消息发送成功，这是默认值，显然是吞吐量与可靠性的一个折中方案<br>==ack=-1==：当所有的follower都同步消息成功后，leader再向producer发送ack，producer才认为此消息发送成功，显然牺牲了吞吐量，因为如果leader有多个followers，同步需要一定时间，producer当然要等待一段时间后，再能继续向leader发送新消息。</p>
<p>当然ack=1的情况下，消息也可能会丢失，这是因为：<br>producer只要收到分区leader成功写入的通知就会认为消息发送成功了，但是也有这样的情况：leader成功写入后，还没来得及把数据同步到follower节点超时等，这时候消息会丢失。</p>
<p>清楚了kafka的ack机制后，来看看ack=-1的同步复制过程：<br>1）producer首先取zookeeper找到指定topic的主分区leader，向leader发送消息<br>2）leader收到消息写入到本地segment文件<br>3）所有follower从leader pull消息并写入自己segment文件<br>4）所有follower向leader发送ack消息<br>5）leader收到所有follower的ack消息<br>6）leader向producer发送ack<br>7）producer收到成功写入的响应</p>
<h4 id="5、kafka-消息索引机制"><a href="#5、kafka-消息索引机制" class="headerlink" title="5、kafka 消息索引机制"></a>5、kafka 消息索引机制</h4><p>&#8195;&#8195;前面的四节内容更多是kafka集群本身的一些机制，其实对于consumer侧，当consumer去broker pull一条的消息时，broker是如何快速找出相应的消息呢？<br>&#8195;&#8195;在前面部署Kafka集群已经知道每个partition都是一个文件目录，每个目录下有成index文件和log文件，它们是成对出现，后缀 “.index” 和 “.log” 分表表示 segment 索引文件和数据文件（存放消息的地方），segment的大小以及相关配置可在server.properties进行设置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#segment文件的大小，默认为 1G</span><br><span class="line">log.segment.bytes&#x3D;1024*1024*1024</span><br><span class="line">#滚动生成新的segment文件的最大时长</span><br><span class="line">log.roll.hours&#x3D;24*7</span><br><span class="line">#segment文件保留的最大时长，超过7天将被删除</span><br><span class="line">log.retention.hours&#x3D;24*7</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;Segment 是 Kafka 存储消息的最小单位，Segment 文件命名规则：对于某个partition，例如partition0，全局的第一个 Segment 从 0 开始，后续每个 Segment 文件名为上一个 Segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用 0 填充。如 00000000000000368769.index 和 00000000000000368769.log。<br>对于parition1，它全局的第一个 Segment 也是从 0 开始，切勿认为parition0的最后一个segment的offset会顺延到partition1！<br>以下图为例，分析costumer如何根据offset拿到消息数据：<br>（因为集群测试环境还没有太多segment数据，所以这里参考这篇<a href="https://mp.weixin.qq.com/s/fX26tCdYSMgwM54_2CpVrw">文章的内容</a>：）<br><img src="https://img-blog.csdnimg.cn/20191130215819665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;假设现在研究partition0，在它.index文件中，存储的是key-value格式的值：<n,m>，n代表在.log中按顺序开始第n条消息，m代表该消息的位置偏移m，这里以索引文件中元数据 <code>&lt;3, 497&gt;</code> 为例，表示该368769.log文件中第 3 条Message，该消息所在物理位置为 497。<br>现在consumer要取offset为368773的消息，以下为查找过程：<br>1）在partition0下，有多个segment的index文件，根据二分法，可以快速地位到368772条消息在368769.index上<br>2）在368769.index索引文件，找出<n,m>的具体值，n=368774-368769=4（一般称为base offset），因为索引文件是稀疏结构，4这个值不在索引文件上<br>3）再根据二分法，很快找到3这个base offset<3,497>，因为索引值4没有，只能用不大于4的索引值3。<br>4）再回到368769.log上，从物理位置497开始按顺序查找，当物理位置到达830时，offset为368773的消息被找到。</p>
<p>&#8195;&#8195;从上图可以知道 Index 文件也不是每次递增 1 的，这是因为 Kafka 采取稀疏索引存储的方式，每隔一定字节的数据建立一条索引。它减少了索引文件大小，使得能够把 Index 映射到内存，降低了查询时的磁盘 IO 开销，同时也并没有给查询带来太多的时间消耗。<br>&#8195;&#8195;要满足以上的搜索策略：Kafka为在 Partition 中的每一条 Message 都定义了以下三个属性：</p>
<ul>
<li>Offset：表示 Message 在当前 Partition 中的偏移量，是一个逻辑上的值，唯一确定了在当前Partition 中的一条 Message</li>
<li>MessageSize：表示 Message 内容 Data 的大小。</li>
<li>Data：Message 的具体内容。<br>因此只要消费者只需要订阅的topic后，一旦拿到消息的offset，broker就会按以上检索策略将消息取出。</li>
</ul>
<h4 id="6、consumer-group的工作机制"><a href="#6、consumer-group的工作机制" class="headerlink" title="6、consumer group的工作机制"></a>6、consumer group的工作机制</h4><p>&#8195;&#8195;在图1可看到有多个consumer group，本章节主要探讨为何kafka使用consumer group的设计。<br>&#8195;&#8195;consumer group是kafka实现单播和广播两种消息模型的方式：<br>同一个topic的消息，可以广播给不同的group；<br>同一个topic的消息，每个group里面只能被其中的一个consumer消费。group内的consumer可以使用多线程或多进程来实现，consumer数量建议与partition成整数倍关系，因为kafka设计一个partition只能被group内一个consumer消费，也即是单播模式。</p>
<h5 id="6-1-一个topic为何需要被多个consumer消费？"><a href="#6-1-一个topic为何需要被多个consumer消费？" class="headerlink" title="6.1  一个topic为何需要被多个consumer消费？"></a>6.1  一个topic为何需要被多个consumer消费？</h5><p>&#8195;&#8195;以图1的sparkapp这个topic为例，假设没有consumer group，假设只有一个consumer去消费kafka集群的sparkapp，考虑到consumer只能从leader分区消费，相当于以下架构图：<br><img src="https://img-blog.csdnimg.cn/20191201103918787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">从单个consumer视角观察sparkapp，集群为consumer提供三个分区消费：（leader）partition0、（leader）partition1、（leader）partition2，所以上图简化成下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20191201104854685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">但如果当producer写入消息的速度比consumer读取的速度快呢？结果是：消息堆积越来越严重，对于这种情况，需要增加多个消费者来进行水平扩展消息的读取。<br>可以用实际案例说明：<br>例如这个sparkapp的消息是待发送邮箱的内容和用户邮箱地址，如果仅有一个consumer去读取消息再发邮箱通知用户，那么随消息堆积越来越严重，将会有大量用户不能及时收到邮件通知。<br>解决办法：增加多个consumer，一般是跟leader分区的数目一致，例如本例3个leader分区，对应3个consumer进行消费，每个consumer消费分别对应一个分区进行消费，如下图所示：<br><img src="https://img-blog.csdnimg.cn/2019120111063329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这就保证了生产的消息能够及时被consumer消费处理掉，表现在实际应用场景的效果：大量用户能够及时收到邮箱通知。<br>==注意：consumer数量建议与partition成整数倍关系，例如上面的1倍关系，因为kafka设计一个partition只能被group内一个consumer消费，也即是单播模式。<br>consumer数量由客户端自己通过多线程方式或者多进程方式实现。==</p>
<h5 id="6-2-同一个partition能否被多个consumer同时消费？"><a href="#6-2-同一个partition能否被多个consumer同时消费？" class="headerlink" title="6.2 同一个partition能否被多个consumer同时消费？"></a>6.2 同一个partition能否被多个consumer同时消费？</h5><p>例如（leader）partition0同时被两个consumer消费，如下所示：<br><img src="https://img-blog.csdnimg.cn/20191201111807824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">以邮箱通知这个为例子：<br>两个consumer将拿到重复的消息，在用户侧的效果就是：所有用户都会重复收到同一内容邮箱通知，显然不能接受。<br>kafka设计早已考虑到这些情况，所有kafka不允许同一个consumer group中的两个consumer读取同一个partition。</p>
<h5 id="6-3-kafka为何设计多个consumer-group这样的模型？"><a href="#6-3-kafka为何设计多个consumer-group这样的模型？" class="headerlink" title="6.3 kafka为何设计多个consumer group这样的模型？"></a>6.3 kafka为何设计多个consumer group这样的模型？</h5><p>以邮箱通知这个为例子：<br>这个sparkapp的消息是待发送邮箱的内容和用户邮箱地址，现在有两个应用需要拉取sparkapp这个topic的消息，一个是邮箱通知应用A，另外一个是存储邮箱内容和用户邮箱地址的应用B。</p>
<h6 id="6-3-1-无consumer-group，应用A和应用B会出现什么情况？"><a href="#6-3-1-无consumer-group，应用A和应用B会出现什么情况？" class="headerlink" title="6.3.1 无consumer group，应用A和应用B会出现什么情况？"></a>6.3.1 无consumer group，应用A和应用B会出现什么情况？</h6><p><img src="https://img-blog.csdnimg.cn/20191201114330792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">因为kafka设计每个consumer只能消费一个partition，如上图示，<br>==对于应用A，它开了2个线程==，消费(leader)partition0和(leader)partition1，在用户侧的出现情况：有一部分用户根本没收到邮件通知，漏了(leader)partition2这部分的数据。<br>==对于应用B，它只能开一个线程==，也即是一个consumer，而且只能拿到(leader)partition2的消息，最终出现的情况：数据库里面，根本没有存储到一部分用户的邮件记录。显然是漏了(leader)partition0和(leader)partition1的数据。<br>如何解决上述问题？</p>
<h6 id="6-3-2-为应用建立consumer-group，观测应用A和应用B的情况。"><a href="#6-3-2-为应用建立consumer-group，观测应用A和应用B的情况。" class="headerlink" title="6.3.2 为应用建立consumer group，观测应用A和应用B的情况。"></a>6.3.2 为应用建立consumer group，观测应用A和应用B的情况。</h6><p>考虑到两个应用，这里对应两个group，如下图示：<br><img src="https://img-blog.csdnimg.cn/20191201120617771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">group A 对应应用A，group B 对应应用B<br>kafka限定：<br>group内的consumer只能消费一个分区<br>同一分区（的一条信息）可以被不同group消费，广播模式。<br>从上图的结构可以看出：<br>应用A可以把三个分区的邮箱通知内容都发送到所有用户，不会出现像6.3.1 的情况：遗漏部分数据。<br>应用B可以把三个分区的邮箱通知内容都存储到数据库，不会出现像6.3.1 的情况：遗漏部分数据。<br>这就是kafka的consumer group的设计逻辑。<br>小结：<br>1）如果一个应用需要读取全量消息，那么可为该应用设置一个消费组；<br>如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。<br>2） kafka支持写入的一条消息能够被若干个应用读取这条消息。也就是说：<br>每个应用都可以读到全量的消息，通过为每个应用设置自己的消费组。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka原理</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Spark</title>
    <url>/2019/12/22/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Spark/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面博客文章里，已经把大数据实时分析项目在spark组件之前的各个组件原理、部署和测试都给出相关讨论，接下来是项目最核心的内容：实时计算部分，因为项目将使用spark streaming做微批计算（准实时计算），因此接下的文章内容将深入spark以及spark streaming架构原理，为后面实际计算编程做铺垫。</p>
<h4 id="1、Spark-是什么？"><a href="#1、Spark-是什么？" class="headerlink" title="1、Spark 是什么？"></a>1、Spark 是什么？</h4><p>&#8195;&#8195;Spark是一种分布式的并行计算框架，什么是计算框架？所谓的计算（在数据层面理解）其实是用于数据处理和分析的一套解决方案，例如Python的Pandas，相信用过Pandas都很容易理解Pandas擅长做什么，加载数据、对数据进行各类加工、分析数据等，只不过Pandas只适合在单机上的、数据量百万到千万级的计算组件，而Spark则是分布式的、超大型多节点可并行处理数据的计算组件。</p>
<p>&#8195;&#8195;Spark通常会跟MapReduce做对比，它与MapReduce 的最大不同之处在于Spark是基于内存的迭代式计算——Spark的Job处理的中间（排序和shuffling）输出结果可以保存在内存中，而不是在HDFS磁盘上反复IO浪费时间。除此之外，一个MapReduce 在计算过程中只有Map 和Reduce 两个阶段。而在Spark的计算模型中，它会根据rdd依赖关系预选设计出DAG计算图，把job分为n个计算阶段（Stage），因为它内存迭代式的，在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。<br>&#8195;&#8195;Spark提供了超过80种不同的Transformation和Action算子，如map，reduce，filter，reduceByKey，groupByKey，sortByKey，foreach等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小（尤其用Scala和Python写计算业务代码方面）。正是基于使用易用性，因此Spark能更好地用于基于分布式大数据的数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark生态如下：<br><img src="https://img-blog.csdnimg.cn/20191222113158864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>
<h4 id="2、Spark-运行模式"><a href="#2、Spark-运行模式" class="headerlink" title="2、Spark 运行模式"></a>2、Spark 运行模式</h4><p>目前最为常用的Spark运行模式有：</p>
<ul>
<li>Local：本地进程运行，例如启动一个pyspark交互式shell，一般用于开发调试Spark应用程序</li>
<li>Standalone：利用Spark自带的资源管理与调度器运行Spark集群，采用Master/Slave结构，可引入ZooKeeper实现spark集群HA</li>
<li>Hadoop YARN : 集群运行在YARN资源管理器上，资源管理交给YARN，Spark只负责进行任务调度和计算，参考本博客<a href="https://blog.csdn.net/pysense/article/details/103434832">《基于YARN HA集群的Spark HA集群》</a></li>
<li>Apache Mesos ：运行在著名的Mesos资源管理框架基础之上，该集群运行模式将资源管理交给Mesos，Spark只负责进行任务调度和计算<br>==<strong>Mesos和YARN两种资源有什么区别：</strong>==<br>之前看一个视频，对其给出的解释印象深刻：<br>Mesos：细腻度资源管控<br>YARN：粗粒度资源管控<br>例如有个老师要给45个学生上课，向教务处申请课室资源，若教务处以Mesos模式发放资源，那么它会发放只能容纳45个学生的课室，典型的按需分配；若教务处以YARN模式发放资源，那么它会发放能容200个学生的大教室，但实际上还有155个人位置资源空闲。这就是资源的细腻度和粗粒度的区别。</li>
</ul>
<h4 id="3、适合Spark的场景"><a href="#3、适合Spark的场景" class="headerlink" title="3、适合Spark的场景"></a>3、适合Spark的场景</h4><ul>
<li><p>Spark适用场景：<br>Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合，基于大数据的机器学习再适合不过，例如梯度下降法，需要不断迭代找到全局或局部最优解。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。<br>准实时计算场合：实时接收用户行为原始数据，并通过Spark Streaming计算（转换+加工），例如在广告、报表、推荐系统等业务上，在广告业务方面需要大数据做应用分析、效果分析、定向优化等，在推荐系统方面则需要大数据优化相关排名、个性化推荐以及热点点击分析等，这些业务天生适合大型的互联网巨头。</p>
</li>
<li><p>Spark不适用场景：</p>
<p>  内存消耗极大，在内存不足的情况下，Spark会下放到磁盘，会降低应有的性能<br>  有高实时性要求的流式计算业务，例如实时性要求毫秒级，对每一条数据都触发实时计算的，这种场合已经被Flink称霸。<br>  流线长或文件流量非常大的数据集不适合，这是因为这种场合rdd消耗极大的内存集群压力大时，一旦一个task失败会导致它前面一条线所有的前置任务全部重跑（尤其对于RDD 血缘关系链长且有多个宽依赖的情况），JVM的GC不够及时，内存不能及时释放，将会出现恶性循环导致更多的task失败，导致整个Application效率极低。所以为什么说Spark是适合“微批”处理，意味着每隔一段时间（1秒或者几秒不等）来一批次数据，Spark适合“一小口一小口准实时地吃数据”。</p>
</li>
</ul>
<h4 id="4、Spark相关术语"><a href="#4、Spark相关术语" class="headerlink" title="4、Spark相关术语"></a>4、Spark相关术语</h4><p><img src="https://img-blog.csdnimg.cn/20191221131426758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">一个完整的Spark应用程序，例如wordcount，在提交集群运行时:<br><code>./bin/spark-submit  --name word_count_app --master yarn  --deploy-mode cluster  --py-files word_count.py</code><br>它涉及到上图流程的相关术语： </p>
<ul>
<li>SparkContext:整个应用的上下文，控制应用的生命周期。</li>
<li><p>RDD：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型（鉴于RDD是Spark的核心概念，后面的有一篇博客给出相关讨论）。无法快速理解RDD？这里有两种方式可助于理解：<br>A、把它设想为分布式的Pandas dataframe，df也是一个数据集，也是被加载到内存上，然后利用pandas各个算子对df反复迭代最后得出计算结果。<br>B、<code>rdd = sc.parallelize(list(range(1000)), 10)</code>，这就创建了一个“轻量”的rdd数据集，设想下：这个数组有10亿项，分10个分区，有10个计算节点，每个节点负责1个分区的计算，也就是说从计算节点来看，每个节点负责1亿行的“小块rdd”；而从用户逻辑层面来看：就一个大rdd，包含10亿项数据</p>
</li>
<li><p>RDD的窄依赖和宽依赖</p>
<p>   A、窄依赖NarrowDependency（一对一）：不会产生分区之间的shuffle，所有的父RDD的partition会一一映射到子RDD的partition中，例如Map、FlatMap、Filter算子等</p>
<p>  B、宽依赖ShuffleDependency（一对多）：会引起多个分区之间的shuffle，父RDD中的partition会根据key的不同进行切分，划分到子RDD中对应的partition中，例如reduceByKey的任务。</p>
</li>
</ul>
<ul>
<li><p>DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系（关系链）。</p>
</li>
<li><p>Driver Program：Application中运行main函数并创建的SparkContext，创建SparkContext的目的是和集群的ClusterManager通信，进行计算资源的申请、任务的分配和监控等。因此也可认为SparkContext代表Driver控制程序。Driver负责对Application构建DAG图。</p>
</li>
<li><p>Cluster Manager：集群资源管理中心，例如YARN里面的ResourceManage，负责分配计算资源分配和回收。</p>
</li>
<li><p>Worker Node：启动Executor或Driver负责完成具体计算，在YARN模式中 Worker Node就是NodeManager节点。</p>
</li>
<li><p>Executor：是Application在Worker上面的一个进程，该进程会启动线程池方式去跑task，并负责把数据存在内存或者磁盘上。每个Application都有属于自己的一组Executors。在Spark on YARN模式下，Executor进程名为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor实例，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task。</p>
</li>
</ul>
<ul>
<li><p>Application：用户编写的Spark应用程序，例如下面启动一个名字为word_count_app的Application（简称app），该app的业务逻辑在word_count.py实现。一个Application包含多个Job。<br><code>./bin/spark-submit  --name word_count_app --master yarn  --deploy-mode cluster  --py-files word_count.py</code></p>
</li>
<li><p>Job：包含多个Task组成的并行计算，由Spark Action算子（collect、groupByKey、ReduceByKey、count、takeOrdered等）触发产生。一个action产生一个job，如果一个Application里面的业务代码有多少个action算子，就产生多少个Job。一个Job包含多个RDD及作用于相应RDD上的各种操作。</p>
</li>
</ul>
<ul>
<li><p>Task：任务，运行在Executor上的工作单元，是Executor中的一个线程（Executor是JVM进程，在YARN模式下，就是一个跑在container上JVM进程），与Hadoop MapReduce中的MapTask和ReduceTask一样，是运行Application的基本单位。多个Task组成一个Stage，而Task的调度和管理由TaskScheduler负责。</p>
</li>
<li><p>Stage：DAGScheduler将一个Job划分为若干Stages，每个Stage打包成一组Tasks，又称TaskSet。Stage的调度和划分由DAGScheduler负责。Stage又分为Shuffle Map Stage和Result Stage两种。Stage的边界就在发生Shuffle（rdd宽依赖）的地方。 </p>
</li>
<li><p>DAGScheduler：根据Job构建基于Stage的DAG（有向无环任务图），DAGScheduler会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，并提交Stage给TaskScheduler。</p>
</li>
<li><p>TaskScheduler：将任务(Task)分发给Executor，每个Executor负责运行什么Task由TaskScheduler分配。 </p>
</li>
<li><p>Shared Variables共享变量：Application在整个运行过程中，可能需要一些变量在每个Task中都使用，用于节省计算时间和IO。Spark有两种共享变量：一种缓存到各个节点的广播变量：broadcast；一种只支持加法操作：accumulator，一般用于对rdd求和sum以及累加计数counter。 这个概念需结合实际用例说明，否则难以理解。</p>
</li>
</ul>
<p>&#8195;&#8195;一句话：一个Application(其实就是你设计的Spark业务程序)由多个Job组成，一个Job由多个Stage组成，一个Stage由多个Task组成一个TaskSet。Stage是作业调度的基本单位，Task是执行计算和操作rdd的最小工作单元，以下面的wordcount语句对于的关系图说明：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/20191221150736129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;这里为何画了2个Job，因为该计算任务包含两个Action：reduceByKey和sortBy。Application里面有多少个Action算子，Driver就给Application分配多少个Job。</p>
<p>&#8195;&#8195;Spark计算涉及的相关部件比较多而且相对抽象，需要在实际spark集群上跑几个测试application结合理解，本博客在前面的文章已经在测试项目中结合spark UI的截图就Job、Stage、Task等给出较为详细的说明，参考<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>第7章内容。</p>
<h4 id="5、Spark程序执行流程"><a href="#5、Spark程序执行流程" class="headerlink" title="5、Spark程序执行流程"></a>5、Spark程序执行流程</h4><p><img src="https://img-blog.csdnimg.cn/20191221131426758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;Spark 程序执行流程基于不同资源管理器其有不同的执行流程，以下以Standalone模式说明执行流程。其他资源管理模式的执行流程可以参考这篇文章：<a href="https://www.cnblogs.com/frankdeng/p/9301485.html">《Spark任务提交方式和执行流程》</a></p>
<ul>
<li>1)构建Spark Application的运行环境，启动SparkContext也即启动Driver，Driver向资源管理器注册并申请运行Executor资源；</li>
<li>2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；</li>
<li><p>3)Driver根据算子链预先构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task。</p>
</li>
<li><p>4)TaskScheduler将Task发送给Executor运行，同时Driver将应用程序代码传给Executor。</p>
</li>
<li>5)Task在Executor上运行，运行完毕释放所有资源。</li>
</ul>
<p>其实不管在哪种种资源模式下，Spark的程序执行流程一定离不开三条主线：</p>
<ul>
<li>Driver 申请资源用于启动Executor</li>
<li>Driver 构建DAG图，切分Stage，最终生产出多组TaskSet</li>
<li>Executor 领取Task和业务代码并执行</li>
</ul>
<h4 id="6、理解Spark-Stage的划分"><a href="#6、理解Spark-Stage的划分" class="headerlink" title="6、理解Spark Stage的划分"></a>6、理解Spark Stage的划分</h4><p>&#8195;&#8195;本部分内容比较重要，也是理解spark任务调度原理的关键，本章节内容参考<a href="http://sharkdtu.com/posts/spark-scheduler.html">《Spark Scheduler内部原理剖析》</a>其中任务调度内容</p>
<h5 id="6-1-Spark-Stage的划分"><a href="#6-1-Spark-Stage的划分" class="headerlink" title="6.1 Spark Stage的划分"></a>6.1 Spark Stage的划分</h5><p>&#8195;&#8195;这里以wordcount的Scala语句分析Spark对于一个application是如何划分stage的<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;result&quot;)</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;以上就是一个Job，由rdd和Action方法封装而成，SparkContext将Job交给DAGScheduler提交，它会根据rdd的血缘关系的DAG图（你可以理解为预先规划的计算流程）进行切分，将一个Job划分为若干Stages，具体划分策略是，由处于末端的RDD不断通过依赖回溯判断父依赖是否为宽依赖，即以Shuffle为界，划分Stage。<br>划分的Stages分两类，一类叫做ResultStage，由Action方法决定，是DAG计算流程图最下游的Stage，这个Stage会最先被划分出来（因为是从末端回溯，因此首先遇到宽依赖reduceByKey，因此ResultStage最先被划出）。另一类叫做ShuffleMapStage，为下游ResultStage准备数据。<br>以下面的DAG流程作为说明：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;result&quot;)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/20191221172609660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;Job由Action算子saveAsFile触发，该Job由rdd3和saveAsTextFile方法组成，根据rdd之间的依赖关系：</p>
<ul>
<li>首先从rdd3开始回溯搜索，在回溯搜索过程中，rdd3依赖rdd2，遇到reduceByKey需要宽依赖，所以在rdd3和rdd2之间划分Stage，该Stage为ResultStage</li>
<li>继续回溯，rdd2依赖rdd1，遇到Map窄依赖，不划分stage</li>
<li>rdd1依赖rdd0，遇到flapMap窄依赖，不划分Stage</li>
<li>最终回溯到源头rdd0，rdd0无父依赖，因此rdd2、rdd1和rdd0都划分到同一个Stage，即ShuffleMapStage。</li>
</ul>
<p>小结：一个Spark应用程序包括Job、Stage以及Task三个概念：</p>
<ul>
<li>Job是以Action方法(算子)为界，遇到一个Action方法则触发一个Job</li>
<li>Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次Stage划分<ul>
<li>Task是Stage的子集，以并行度(分区数)来衡量，一个Stage有多少个partition就有多个task</li>
</ul>
</li>
</ul>
<h5 id="6-2-Spark-DAG的可视化"><a href="#6-2-Spark-DAG的可视化" class="headerlink" title="6.2 Spark DAG的可视化"></a>6.2 Spark DAG的可视化</h5><p>&#8195;&#8195;为了更直观理解6.1 stage的划分，以及DAG可视化，这里用另外一个名为word-count的application语句跑一个测试，并在spark web UI查看相关划分情况以及执行计划：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br></pre></td></tr></table></figure><br>从提交语句的逻辑即可直接看出分为两个Stage。在web端可直观看到Application有2个executor分别位于两个不同spark节点上。<br>==对于Stage 0：==<br>Stage 0 的Taskset有3个task，其中1个task在节点5上的executor0进程跑，另外2个task在节点6的executor1进程上跑。executor进程会使用多线程方式运行自己管辖的tasks<br><img src="https://img-blog.csdnimg.cn/20191222095742807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">==对于Stage 1：==<br>Stage 1 的Taskset有3个task，其中1个task在节点5上的executor0进程跑，另外2个task在节点6的executor1进程上跑。executor进程会使用多线程方式运行自己管辖的tasks<br><img src="https://img-blog.csdnimg.cn/20191222101000161.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这从两张图也可以看到Application的运行机制：引用官网文档<a href="http://spark.apache.org/docs/latest/job-scheduling.html">Scheduling Across Applications</a>的一句话：</p>
<blockquote>
<p>When running on a cluster, each Spark application gets an independent set of executor JVMs that only run tasks and store data for that application<br>当提交的application是在集群上跑时，每个application包含多个executor JVMs运行进程，这些executors（JVM进程）只负责为当前的application运行它的tasks任务和存储数据</p>
</blockquote>
<h4 id="7、Spark调度过程"><a href="#7、Spark调度过程" class="headerlink" title="7、Spark调度过程"></a>7、Spark调度过程</h4><p>&#8195;&#8195;有了前面内容铺垫后，本章节内容才能比较好理解，本章节内容参考<a href="http://sharkdtu.com/posts/spark-scheduler.html">《Spark Scheduler内部原理剖析》</a>其中的”Spark任务调度总览“，这篇文章质量不错。</p>
<h5 id="7-1-Spark的两级调度模型"><a href="#7-1-Spark的两级调度模型" class="headerlink" title="7.1 Spark的两级调度模型"></a>7.1 Spark的两级调度模型</h5><p>&#8195;&#8195;Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度。Stage级别的调度前面第6章节已经给出详细说明，至于Task级的调度相对复杂，原文作者给出了非常专业的、从源代码执行流程的说明，但这里不再重复累赘，毕竟重心还是以大数据项目在应用层的开发为主。<br>总体调度流程如下图所示。<br><img src="https://img-blog.csdnimg.cn/20191222111758872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>1)Spark RDD通过其Transactions和Action，形成了RDD的计划执行流程图，即DAG，最后通过Action的调用，触发Job并调度执行。</li>
<li>2)DAGScheduler负责Stage级的调度，主要是将DAG按照RDD的宽依赖切分成若干Stages，并将每个Stage里面的多个task打包成一个TaskSet。多个Stage就有多个TaskSets，这些TaskSets由DAGScheduler交给TaskScheduler调度。</li>
<li>3)TaskScheduler负责Task级的调度，TaskSets按照指定的调度策略分发到不同的Executor上执行。</li>
<li>4)调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多个实现API，分别对接不同的资源管理器YARN/Mesos/Standalone。你可以看成SchedulerBackend就是资源的代理，这个代理不断询问TaskScheduler是否需要资源去运行task。</li>
</ul>
<p>==这里会有一个疑问：每个Stage里面的task的数量怎么确定？==<br>每个Stage里面的task的数量是由该Stage中最后一个RDD的Partition数量所决定！</p>
<h5 id="7-2-以Spark-On-Yarn说明调度过程"><a href="#7-2-以Spark-On-Yarn说明调度过程" class="headerlink" title="7.2 以Spark On Yarn说明调度过程"></a>7.2 以Spark On Yarn说明调度过程</h5><p>&#8195;&#8195;Spark-On-Yarn模式下在任务调度期间，ApplicationMaster、Driver、DAGScheduler、TaskScheduler、Executor等内部模块的交互过程，以进一步巩固理解7.1章节的内容。<br>（从下图中有无发现一个有趣的现象：大数据多个组件理解难，其实不是那种像操作系统底层原理的难，而是在于：大数据的每个组件内部有很多个角色模块，每个角色负责的”工作“不一样，整个hadoop生态圈，每个组件和组件之间联系，这就会涉及到几十个实现模块，你要记忆和理解这几十个不同名字模块及其具体能做什么，以及他们之间的逻辑联系关系。<br>举个栗子：</p>
<ul>
<li>如果你是部门经理，部门只有9个人，3个岗位，如果现在接到一个开发项目，你当然很清楚和也容易安排每个岗位的人负责项目哪部分开发工作，并指定谁跟谁如何协调某部分内容。</li>
<li>如果你是市长，假设现在你接到一个上级大型项目，需要你亲自统筹和设计出如何让20个局完成该大型项目的方案。首先需要设计出10个局单位之间怎么配合，10个局单位共有100个岗位，每个局的每个岗位具体负责哪部分工作，并且你要设计出项目某部分内容是由哪个岗位和哪个岗位直接如何调度完成，够崩溃的。这就是大数据生态圈，概念多、杂，概念之间有一定逻辑关系，这些都需要你去理解和记忆，这就是难点所在。<br>）<br><img src="https://img-blog.csdnimg.cn/2019122211433139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver（这四部分是程序里面类或者模块，不是线程），并启动SchedulerBackend线程以及HeartbeatReceiver线程。</li>
<li>当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。</li>
<li>SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。</li>
<li>DAGScheduler负责的Stage调度</li>
<li>TaskScheduler负责的Task调度。</li>
<li>work node上Executor进程负责运行Task和把数据存在内存或者磁盘上</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本章内容对于博客接下来有关Spark以及Spark Streaming项目的文章理解起着关键的基础作用，下一篇文章，重点细讲RDDs。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>深入解析Python元类作用</title>
    <url>/2019/12/18/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Python%E5%85%83%E7%B1%BB%E4%BD%9C%E7%94%A8/</url>
    <content><![CDATA[<p>&#8195;&#8195;python的元类使用场景一般在大型框架里面，例如Django的ORM框架、基于python实现的高级设计模式，元类的这部分内容相对晦涩，但也是作为python非常核心的知识点，通过解析其机制，有利于阅读和学习优秀中间件源代码的设计逻辑，在面向对象设计的重要性不言而喻。本博客后面的内容将会给出较为复杂的设计模式的文章，里面会出现较多的元类编程，因此有必要单独开一篇文章讨论python元类，相关内容将参考Stack Overflow上一篇很受欢迎的关于python metaclasses的文章：<a href="https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python">what-are-metaclasses-in-python</a></p>
<a id="more"></a>
<h4 id="1、python的class对象"><a href="#1、python的class对象" class="headerlink" title="1、python的class对象"></a>1、python的class对象</h4><h5 id="1-1-python的class也是一种object"><a href="#1-1-python的class也是一种object" class="headerlink" title="1.1 python的class也是一种object"></a>1.1 python的class也是一种object</h5><p>”在python的世界里，一切皆对象（object）“，这句话经常出现在很多python书籍中有关”面向对象或者类“文章里。如果你要深入python，首先面向对象的思维和面向对象的编程经历较为丰富。掌握对类的理解和运用，是理解元类的重要基础。<br>1.1 类即对象<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [1]: class Foo(object):</span><br><span class="line">   ...:     pass</span><br><span class="line">   ...:</span><br><span class="line">In [2]: my_instance&#x3D;Foo()</span><br><span class="line">In [3]: print(my_instance)</span><br><span class="line">&lt;__main__.Foo object at 0x108561b00&gt;</span><br></pre></td></tr></table></figure><br>这里创建了一个名为Foo的类，打印它的实例，可以看到该实例是一个Foo object 存放在内存地址：0x108561b00<br>这里只是说明Foo类的实例是object，怎么确认Foo是一个object呢？<br>两种方式可以回答：<br>方式一：在定义阶段：Foo(object)，Foo这个类继承object，所以Foo是object<br>方式二：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [8]: isinstance(Foo,object)</span><br><span class="line">Out[8]: True</span><br></pre></td></tr></table></figure><br>既然Foo是一个object，那么对于该object则可以扩展其功能：</p>
<ul>
<li>可赋值给变量</li>
<li><p>可被复制</p>
<ul>
<li>可添加属性</li>
<li>可将其当作函数参数传递</li>
<li>当然也可绑定新的类或者对象</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">13</span>]: NewFoo=Foo</span><br><span class="line">In [<span class="number">14</span>]: print(NewFoo)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Foo</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">In</span> [16]:</span> CloneFoo=copy.deepcopy(Foo)</span><br><span class="line">In [<span class="number">17</span>]: print(CloneFoo)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Foo</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">In</span> [20]:</span> Foo.new_attr=<span class="string">&#x27;bar&#x27;</span></span><br><span class="line">In [<span class="number">21</span>]: Foo.new_attr</span><br><span class="line">Out[<span class="number">21</span>]: <span class="string">&#x27;bar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [22]: def myfunc(obj):</span></span><br><span class="line"><span class="string">    ...:     print(obj.__name__)</span></span><br><span class="line"><span class="string">    ...:</span></span><br><span class="line"><span class="string">In [23]: myfunc(Foo)</span></span><br><span class="line"><span class="string">Foo</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [24]: class  NewFoo(object):</span></span><br><span class="line"><span class="string">    ...:     pass</span></span><br><span class="line"><span class="string">    ...:</span></span><br><span class="line"><span class="string">In [25]: Foo.x=NewFoo</span></span><br><span class="line"><span class="string">In [26]: Foo.x</span></span><br><span class="line"><span class="string">Out[26]: __main__.NewFoo</span></span><br></pre></td></tr></table></figure>
<p>总之，只要拿到一个object，你可以对其扩展任意你想得到效果</p>
</li>
</ul>
<h5 id="1-2-动态创建类"><a href="#1-2-动态创建类" class="headerlink" title="1.2 动态创建类"></a>1.2 动态创建类</h5><p>什么是动态创建类？只有运行这个程序后，通过判断给定参数来决定创建的是类A还是类B，而不是给程序写为固定生产类A。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">28</span>]: <span class="function"><span class="keyword">def</span> <span class="title">choose_class</span>(<span class="params">which</span>):</span></span><br><span class="line">    ...:     <span class="keyword">if</span> which ==<span class="string">&#x27;Foo&#x27;</span>:</span><br><span class="line">    ...:         <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:             <span class="keyword">pass</span></span><br><span class="line">    ...:         <span class="keyword">return</span> Foo</span><br><span class="line">    ...:     <span class="keyword">elif</span> which == <span class="string">&#x27;Bar&#x27;</span>:</span><br><span class="line">    ...:         <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:             <span class="keyword">pass</span></span><br><span class="line">    ...:         <span class="keyword">return</span> Bar</span><br><span class="line">    ...:</span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: myclass=choose_class(<span class="string">&#x27;Bar&#x27;</span>)</span><br><span class="line">In [<span class="number">32</span>]: print(myclass.__name__)</span><br><span class="line">Bar</span><br></pre></td></tr></table></figure><br>前面提到，既然Foo创建一个实例就是一个对象，把这个逻辑放在Foo身上：既然（某某某）创建一个对象就是一个Foo类，这个某某某是什么？可以做什么？<br>这个某某某就是type这个内建函数（函数也是一个对象），用type也可以像上面一样动态的创建一个类，用法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type(要创建的类名，该类的所有父类名字组成的元组（若无父类，则为空元组），要创建该类需要用到入参：属性的字典)</span><br><span class="line">一般写成：</span><br><span class="line">type(class_name,class_bases,class_dict)</span><br></pre></td></tr></table></figure><br>经典方式一般如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    car=<span class="string">&#x27;Model 3&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=age</span><br><span class="line">        self.age=age</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;my name is &#123;&#125; and &#123;&#125; years old&#x27;</span>.<span class="built_in">format</span>(self.name,self.age))</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>使用type函数动态创建以上Person类的过程：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 因为Person继承object，所以type的第二个位置参数为(object,)，Person类有三个属性因此class_dict为&#123;&#x27;car&#x27;:car,&#x27;__init__&#x27;:__init__,&#x27;info&#x27;:info&#125;)</span></span><br><span class="line">car = <span class="string">&#x27;Model 3&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">    self.name = name</span><br><span class="line">    self.age = age</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self</span>):</span></span><br><span class="line">    print(self.name,self.age)</span><br><span class="line"></span><br><span class="line">Person = <span class="built_in">type</span>(<span class="string">&#x27;Person&#x27;</span>,(<span class="built_in">object</span>,),&#123;<span class="string">&#x27;car&#x27;</span>:car,<span class="string">&#x27;__init__&#x27;</span>:__init__,<span class="string">&#x27;info&#x27;</span>:info&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: Person.__dict__</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">mappingproxy(&#123;<span class="string">&#x27;car&#x27;</span>: <span class="string">&#x27;Model 3&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__init__&#x27;</span>: &lt;function __main__.__init__(self, name, age)&gt;,</span><br><span class="line">              <span class="string">&#x27;info&#x27;</span>: &lt;function __main__.info(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;__module__&#x27;</span>: <span class="string">&#x27;__main__&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__dict__&#x27;</span>: &lt;attribute <span class="string">&#x27;__dict__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__weakref__&#x27;</span>: &lt;attribute <span class="string">&#x27;__weakref__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__doc__&#x27;</span>: <span class="literal">None</span>&#125;)</span><br><span class="line">In [<span class="number">5</span>]: person = Person(<span class="string">&#x27;Watt&#x27;</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: person</span><br><span class="line">Out[<span class="number">6</span>]: &lt;__main__.Person at <span class="number">0x10896f9e8</span>&gt;</span><br></pre></td></tr></table></figure><br>type创建完整Person类！本章内容主要通过类的创建，因此type这个函数并用其实现动态创建类，为元类这个话题做了铺垫，通过以上type创建的实例推出，python创建类必须要具备以下三个参数：</p>
<ul>
<li>1、类名class_name</li>
<li>2、继承关系class_bases</li>
<li>3、类的名称空间class_dict<br>这三个参数是揭开元类是如何改变类的秘密。</li>
</ul>
<h4 id="2-、Python的metaclass元类"><a href="#2-、Python的metaclass元类" class="headerlink" title="2 、Python的metaclass元类"></a>2 、Python的metaclass元类</h4><h5 id="2-1-认识type"><a href="#2-1-认识type" class="headerlink" title="2.1 认识type"></a>2.1 认识type</h5><p>前面的内容已经说明Python中的类也是对象，那么metaclass元类（元类自己也是对象）就是用来创建这些类的类，例如可以这样理解：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MyClass &#x3D; MetaClass()    #元类创建了类</span><br><span class="line">MyObject &#x3D; MyClass()     #被元类创建的类后，用它创建了实例</span><br></pre></td></tr></table></figure><br>在上一节内容，type可创建MyClass类：<br><code>MyClass = type(&#39;MyClass&#39;, (), &#123;&#125;)</code><br>MyClass是type()这个特殊类的一个实例，只不过这个实例直接就是类。<br>以上的逻辑主要说明一件事：type这个特殊类，就是python的一个元类，type是Python在背后用来创建所有类的元类，这句话如何理解？<br>首先，还是那句熟悉的话：在python的世界里，一切皆对象（object），包括各类数据结构、函数、类以及元类，它们都来源于一个“创物者”，这个强大的创物者这就是type元类。<br>查看每种对象的<code>__class__</code>属性：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">55</span>]: num=<span class="number">10</span></span><br><span class="line">In [<span class="number">56</span>]: num.__class__</span><br><span class="line">Out[<span class="number">56</span>]: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: alist=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">In [<span class="number">59</span>]: alist.__class__</span><br><span class="line">Out[<span class="number">59</span>]: <span class="built_in">list</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">60</span>]: <span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    ...:     <span class="keyword">pass</span></span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: foo.__class__</span><br><span class="line">Out[<span class="number">61</span>]: function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:     <span class="keyword">pass</span></span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: b=Bar()</span><br><span class="line">In [<span class="number">65</span>]: b.__class__</span><br><span class="line">Out[<span class="number">65</span>]: __main__.Bar</span><br></pre></td></tr></table></figure><br>说明每个对象都是某种类，<br>那么，一个<code>__class__</code>.<code>__class__</code>又是属于哪种类呢？<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">69</span>]: num.__class__.__class__</span><br><span class="line">Out[<span class="number">69</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">70</span>]: foo.__class__.__class__</span><br><span class="line">Out[<span class="number">70</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: alist.__class__.__class__</span><br><span class="line">Out[<span class="number">71</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">72</span>]: b.__class__.__class__</span><br><span class="line">Out[<span class="number">72</span>]: <span class="built_in">type</span></span><br></pre></td></tr></table></figure><br>在继续往“创物者”方向靠近，发现最后都是type：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">74</span>]: foo.__class__.__class__.__class__</span><br><span class="line">Out[<span class="number">74</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: foo.__class__.__class__.__class__.__class__</span><br><span class="line">Out[<span class="number">75</span>]: <span class="built_in">type</span></span><br></pre></td></tr></table></figure><br>以上说明：python的各类数据结构、函数、类以及元类，它们都来源于一个“创物者”，这个强大的创物者这就是type元类。</p>
<h5 id="2-2-认识-metaclass-属性"><a href="#2-2-认识-metaclass-属性" class="headerlink" title="2.2 认识__metaclass__属性"></a>2.2 认识<code>__metaclass__属性</code></h5><p>在python的元类的设计中，通常会出现<code>__metaclass__</code>属性，一般用法如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span>   <span class="comment">#python2版本的写法</span></span><br><span class="line">    __metaclass__ = something…</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params">metaclass=something</span>):</span>   <span class="comment">#python3版本的写法</span></span><br><span class="line">    __metaclass__ = something…</span><br></pre></td></tr></table></figure><br>当一个类的内部属性定义了<code>__metaclass__</code>属性，说明这个类将由某个元类来创建，当Foo类一旦被调用，因为设计类时可能有继承关系，因此会出现属性搜索过程：<br>1）Foo的定义里面有<code>__metaclass__</code>这个属性？如果有，解释器在内存中通过<code>something...</code>这个元类创建一个名字为Foo的类（对象）<br>2）如果在Foo的作用域内未找到<code>__metaclass__</code>属性，则继续在父类中寻找，若在父类找到，则用<code>something...</code>这个元类创建一个名字为Foo的类（对象）。<br>3）如果任何父类中都找不到<code>__metaclass__</code>属性，它就会在模块层次中去寻找<code>__metaclass__</code>，若找到，则用<code>something...</code>这个元类创建一个名字为Foo的类（对象）。<br>4）如果还是找不到<code>__metaclass__</code>,解释器最终使用内置的type来创建这个Foo类对象。</p>
<p>从上面过程可知，既然找到<code>something...</code>这个元类后它就可以创建类，说明它与type这个终极元类作用一样：都是用来创建类。<br>所以可推出：<code>__metaclass__</code>指向某个跟type功能相仿的元类———任何封装type的元类、继承type的子类、type本身</p>
<p>下面用元类实现的redis连接单例来感受下以上的逻辑：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisMetaSingleton</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;元类做初始化&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#super(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict) python2写法</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(class_name, class_bases, class_dict) <span class="comment"># python3写法</span></span><br><span class="line">        cls._instance =<span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">cls,host,port,db</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;call调用即完成类的实例化，用类的入参创建redis连接实例&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> cls._instance:</span><br><span class="line">            <span class="comment"># cls._instance = super(RedisMetaSingleton, cls).__call__(host,port,db) python2写法</span></span><br><span class="line">            cls._instance = <span class="built_in">super</span>().__call__(host,port,db)<span class="comment"># python3写法</span></span><br><span class="line">        <span class="keyword">return</span> cls._instance</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisSingleton</span>(<span class="params">metaclass=RedisMetaSingleton</span>):</span></span><br><span class="line">    <span class="string">&quot;redis操作专用类&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span>(<span class="params">self,host,port,db</span>):</span></span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.db=db</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conn</span>(<span class="params">self</span>):</span></span><br><span class="line">    	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>测试其实例是否为单例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">37</span>]: r1=RedisSingleton(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="string">&#x27;6379&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">In [<span class="number">38</span>]: r1</span><br><span class="line">Out[<span class="number">38</span>]: &lt;__main__.RedisSingleton at <span class="number">0x10fdc6080</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: r2=RedisSingleton(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="string">&#x27;6379&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">In [<span class="number">40</span>]: r1 <span class="keyword">is</span> r2</span><br><span class="line">Out[<span class="number">40</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure><br>将单例逻辑放在定义元类这里，其他redis常用方法则放在子类实现。此外，该测试用例需要注意的两点：<br>1）RedisMetaSingleton的<code>__init__</code>和<code>__call__</code>第一个参数为cls，表示元类要创建的”类对象“，因此用cls而不是self。元类至于类对象（mataclass==&gt;class object），就像类至于实例(class==&gt;instance)，反复理解该句。<br>2）<code>__init__(cls,class_name,class_bases,class_dict)</code>，第2个到4个参数，其实就是type元类创建类的所需参数：<br>type（类名，父类元组（若无父类，则为空元组），类属性或内部方法的字典）<br>3）由于RedisMetaSingleton继承type，那么super(RedisMetaSingleton, cls)经过搜索后，父类就是type，因此<br>A: <code>super(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict)</code>的初始化就等价于<br><code>type.__init__(class_name, class_bases, class_dict)</code>的初始化<br>B:<code>super(RedisMetaSingleton, cls).__call__(host,port,db)</code>创建类对象就等价于<code>type.__call__(host,port,db)创建类对象</code><br>这就说明RedisSingleton指定由RedisMetaSingleton来创建，在RedisMetaSingleton内部最后交由<code>type.__init__</code>初始化，证明过程如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">119</span>]: <span class="class"><span class="keyword">class</span> <span class="title">RedisMetaSingleton</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">     ...:     <span class="string">&quot;&quot;&quot;在元类层面实现单例&quot;&quot;&quot;</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">     ...:         <span class="built_in">super</span>(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict)</span><br><span class="line">     ...:         print(<span class="string">&#x27;class_name:&#123;&#125; class_bases:&#123;&#125; class_dict:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(class_name,class_bases,class_dict))</span><br><span class="line">     ...:         cls.cls_object =<span class="literal">None</span></span><br><span class="line">     ...:</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">cls,host,port,db</span>):</span></span><br><span class="line">     ...:         <span class="keyword">if</span> <span class="keyword">not</span> cls.cls_object:</span><br><span class="line">     ...:             cls.cls_object = <span class="built_in">super</span>(RedisMetaSingleton, cls).__call__(host,port,db)</span><br><span class="line">     ...:         <span class="keyword">return</span> cls.cls_object</span><br><span class="line">     ...:</span><br><span class="line">     ...: <span class="class"><span class="keyword">class</span> <span class="title">RedisSingleton</span>(<span class="params">metaclass=RedisMetaSingleton</span>):</span></span><br><span class="line">     ...:     <span class="string">&quot;redis操作专用类&quot;</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span>(<span class="params">self,host,port,db</span>):</span></span><br><span class="line">     ...:         self.host=host</span><br><span class="line">     ...:         self.port=port</span><br><span class="line">     ...:         self.db=db</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">conn</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         <span class="keyword">pass</span></span><br><span class="line">     ...:</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>当以上代码在ipython解释器敲下去后，解释器对RedisMetaSingleton做了<code>__init__</code>初始化工作，故可得到以下打印信息<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class_name:RedisSingleton  # 类名</span><br><span class="line">class_bases:() # 父类元组</span><br><span class="line">class_dict:&#123;&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;RedisSingleton&#39;, &#39;__doc__&#39;: &#39;redis操作专用类&#39;, &#39;__init__&#39;: &lt;function RedisSingleton.__init__ at 0x10ffa0158&gt;, &#39;conn&#39;: &lt;function RedisSingleton.conn at 0x10ffa00d0&gt;&#125; # 要创建类的所有属性字典</span><br></pre></td></tr></table></figure><br>这三个参数就是type创建类的所需的参数:<br><code>type（类名，父类元组（若无父类，则为空元组），类属性或内部方法的字典）</code></p>
<p>以上内容略显复杂：归结起来，只要一个普通类指定需要元类创建，那么最终一定是由type这个终极元类来创建。</p>
<h5 id="2-3-自定义元类"><a href="#2-3-自定义元类" class="headerlink" title="2.3 自定义元类"></a>2.3 自定义元类</h5><p>对元类的构建和原理有一定认识后，那么可通过元类定制普通类，真正站在创物者的上帝视野来创建普通类。<br>现在有这样一个需求，要求创建的普通类的属性满足以下条件：<br>对于开头不是<code>__</code>的属性，都要大写，例如get<em>name(self)，在元类创建该普通类后都会被改为GET<em>NAME(self)<br>开头为<code>__</code>的属性，大小写保持不变。<br>从type创建普通类的“公式“可知：type（class<em>name,class<em>bases,class<em>dict）,class<em>dict就是放置了普通类属性或内部方法的字典），故只需要对其修改后，再重新传入type即可实现，需要基于type的<code>__new__</code>方法实现，type当然有`__new</em></em><code>方法，因为type是元类，也是类。（元类必然有</code>__new</em></em><code>方法，它创建的普通类例如Person、RedisConn才有这个内建的</code>__new</em></em><code>方法。）
具体实现:
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="comment"># 在这里，被创建的对象是类，因此第一个参数为cls，而不是类实例化的self，且需重写__new__方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        uppercase_attr = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, val <span class="keyword">in</span> class_dict.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>):</span><br><span class="line">                uppercase_attr[name.upper()] = val</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                uppercase_attr[name] = val</span><br><span class="line">        <span class="comment"># 用uppercase_attr替换了原class_dict，再传入到type，由type创建类，实现了自定义创建类的目标      </span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(class_name,class_bases, uppercase_attr)</span><br></pre></td></tr></table></figure>
考虑到</code>return type(class_name,class_bases, uppercase_attr)`的写法不是pythone的OOP的写法（不够高级、抽象），因此又转化为以下OOP写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="comment"># 在这里，被创建的对象是类，因此第一个参数为cls，而不是实例self，且需重写__new__方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        attrs = ((name, value) <span class="keyword">for</span> name, value <span class="keyword">in</span> class_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>))</span><br><span class="line">        uppercase_attr  = <span class="built_in">dict</span>((name.upper(), value) <span class="keyword">for</span> name, value <span class="keyword">in</span> attrs)</span><br><span class="line">        <span class="comment"># 用uppercase_attr替换了原class_dict，再传入type，由type创建类，实现了自定义创建类的目标      </span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases, uppercase_attr)</span><br></pre></td></tr></table></figure>
<p>以上OOP风格在知名的python框架中到处可见！此外，我们知道通过super(UpperAttrMetaclass,cls)可以搜索到父类type，因此开发者会习惯写成以下形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        attrs = ((name, value) <span class="keyword">for</span> name, value <span class="keyword">in</span> class_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>))</span><br><span class="line">        uppercase_attr  = <span class="built_in">dict</span>((name.upper(), value) <span class="keyword">for</span> name, value <span class="keyword">in</span> attrs)  </span><br><span class="line">        <span class="comment"># return super(UpperAttrMetaclass,cls).__new__(cls,class_name,class_bases, uppercase_attr)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().__new__(cls,class_name,class_bases, uppercase_attr) <span class="comment"># python3的写法</span></span><br></pre></td></tr></table></figure>
<p>定义一个普通类测试：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">124</span>]: <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">metaclass=UpperAttrMetaclass</span>):</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">     ...:         self.name=name</span><br><span class="line">     ...:         self.age=age</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">get_name</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         print(<span class="string">&#x27;name is:&#x27;</span>,self.name)</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         print(<span class="string">&#x27;age is:&#x27;</span>,self.age)</span><br><span class="line">     ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">125</span>]: Person.__dict__</span><br><span class="line">Out[<span class="number">125</span>]:</span><br><span class="line">mappingproxy(&#123;<span class="string">&#x27;GET_NAME&#x27;</span>: &lt;function __main__.Person.get_name(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;GET_AGE&#x27;</span>: &lt;function __main__.Person.get_age(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;__module__&#x27;</span>: <span class="string">&#x27;__main__&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__dict__&#x27;</span>: &lt;attribute <span class="string">&#x27;__dict__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__weakref__&#x27;</span>: &lt;attribute <span class="string">&#x27;__weakref__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__doc__&#x27;</span>: <span class="literal">None</span>&#125;)</span><br><span class="line">	</span><br></pre></td></tr></table></figure><br>当普通类Person定义后，解释器已经用元类UpperAttrMetaclass创建了Person普通类，其get_name和get_age方法名都被改为大写：GET_NAME和GET_AGE，其他双下划线的的方法名字保持不变。</p>
<p>此外，metaclass不局限于类的调用，也可以在任何对象内部调用，例如函数内部调用，例如以下一个模块upper_attr_by_func.py：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper_attr</span>(<span class="params">class_name,class_bases,class_dict</span>):</span></span><br><span class="line">    uppercase_attr = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name, val <span class="keyword">in</span> class_dict.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>):</span><br><span class="line">            uppercase_attr[name.upper()] = val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            uppercase_attr[name] = val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">type</span>(class_name,class_bases,class_dict)</span><br><span class="line"></span><br><span class="line">__metaclass__ = upper_attr  <span class="comment"># 该元类只能作用在本模块的所有类，对其他模块a.py、b.py无影响。</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">metaclass=UpperAttrMetaclass</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;name is:&#x27;</span>,self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;age is:&#x27;</span>,self.age)</span><br></pre></td></tr></table></figure><br>但需要注意的是：这种方式，元类的作用域将受到限制，仅能影响本模块upper_attr_by_func.py的所有类，对其他模块的类不产生作用。</p>
<p>综上，可总结元类定制普通类的创建一般如下过程：</p>
<ul>
<li>拦截一个普通类，一般在会使用<code>__new__，__init__ 和 __call__</code>，这些方法的内部可以放入对普通类进行不同定制的代码逻辑，其中：<br>A、<code>__new__</code>和<code>__init__</code>方法用于控制类的行为<br>B、 <code>__call__</code>方法用于控制类实例化的行为（</li>
<li>修改普通类，一般是指修改type(class<em>name,class<em>bases,class_dict）里面的三个参数，尤其对class_dict修改频繁，例如要求class_dict里面的必须要有`__doc</em></em>`属性，甚至针对父类元组class_bases来操作其继承关系。</li>
</ul>
<ul>
<li>通过<code>return super().__new__(cls,class_name,custom_bases, custom_class_dict)</code>创建并返回普通类</li>
</ul>
<p>元类一般用于复杂的框架上改变类的行为，对于普通简单的类，还有其他两种手段用来改变类：</p>
<ul>
<li>monkey patching</li>
<li>类装饰器<br>按Stack Overflow上的”建议“：<br>如果需要改变类，99%的情况下使用这两种方法，但其实98%的情况你根本不需要改变类。所以你看到很多较为简单的python轮子，一般是几个普通类就可以完成，根本无需动用元类来构建普通类。</li>
</ul>
<h4 id="3、元类定制普通类的示例——myORM"><a href="#3、元类定制普通类的示例——myORM" class="headerlink" title="3、元类定制普通类的示例——myORM"></a>3、元类定制普通类的示例——myORM</h4><p>本节内容参考了廖雪峰的<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072">文章</a>，但其文章有很多关键的语句并无做更细致的说明，本节内容会在重要的元类实现逻辑上给出更详细的文字说明。<br>这里将实现一个轻量ORM——myORM：<br>在架构层面（不面向用户）</p>
<ul>
<li>架构定义了一个元类ModelMetaClass，用于拦截和修改普通类User定义阶段的class_dict属性字典，并用改造后的class_dict传入type来创建普通类User对象。</li>
<li>架构定义了一个Model类，用于把字段属性名和字段值封装在拼接的SQL语句，主要负责与数据库的增删查改。</li>
<li>架构定义了一个基本字段类Field：包含字段名和字段类型</li>
</ul>
<p>在用户层面（面向用户，用户可自行定义各种模型）</p>
<ul>
<li>用户定义一个整数字段类IntField，用于存放整型类型数据，例如id号，age</li>
<li>用户定义一个字符串字段类CharField，用于存放字符类型数据，例如name，email</li>
<li>创建一个User模型的一条行记录</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelMetaClass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    例如定义User普通类,如下：</span></span><br><span class="line"><span class="string">    class User(Model):</span></span><br><span class="line"><span class="string">        id=IntField(&#x27;user_id&#x27;)</span></span><br><span class="line"><span class="string">        name=CharField(&#x27;user_name&#x27;)</span></span><br><span class="line"><span class="string">        email=CharField(&#x27;email&#x27;)</span></span><br><span class="line"><span class="string">        password=CharField(&#x27;password&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    那么元类ModelMetaClass捕获到的属性字典为： </span></span><br><span class="line"><span class="string">    class_dict=&#123;</span></span><br><span class="line"><span class="string">        &#x27;__module__&#x27;: &#x27;__main__&#x27;, </span></span><br><span class="line"><span class="string">        &#x27;__qualname__&#x27;: &#x27;User&#x27;,</span></span><br><span class="line"><span class="string">        &#x27;id&#x27;:IntField&lt;user_id,bigint&gt;,</span></span><br><span class="line"><span class="string">        &#x27;name&#x27;:CharField&lt;&#x27;user_name&#x27;,varchar(100)&gt;,</span></span><br><span class="line"><span class="string">        &#x27;email&#x27;:CharField&lt;&#x27;email&#x27;,varchar(100)),</span></span><br><span class="line"><span class="string">        &#x27;password&#x27;:CharField&lt;&#x27;password&#x27;,varchar(100)&gt;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        <span class="keyword">if</span> class_name == <span class="string">&quot;Model&quot;</span>:</span><br><span class="line">            <span class="comment"># 如果创建的普通类为Model，不修改该类，直接创建即可</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases,class_dict)</span><br><span class="line">        print(<span class="string">&#x27;在ModelMetaClass元类捕获到普通类的属性字典:\n&#x27;</span>,class_dict)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 用于存放字段类型的属性</span></span><br><span class="line">        fields_dict=<span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> attr_name,attr <span class="keyword">in</span> class_dict.items():</span><br><span class="line">            <span class="comment"># 因为普通类属性字典还有__doc__,__qualname__等属性，因此要过滤出属于Field类型属性</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(attr,Field):</span><br><span class="line">                <span class="comment"># 打印结果：attr_name is &quot;id&quot;,field object is &quot;&lt;class &#x27;__main__.IntField&#x27;&gt;&quot;等字段信息</span></span><br><span class="line">                print(<span class="string">&#x27;attr_name is &quot;&#123;&#125;&quot;,field object is &quot;&#123;&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(attr_name,<span class="built_in">type</span>(attr)))</span><br><span class="line">                fields_dict[attr_name]=attr</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> field_name <span class="keyword">in</span> fields_dict.keys():</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            把Field类型的属性在原属性字典剔除：</span></span><br><span class="line"><span class="string">            u=User(name=&#x27;Wott&#x27;)</span></span><br><span class="line"><span class="string">            print(u.name)# 打印结果Wott</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            若不在原字典删除，那么u.name的值为：CharField&lt;email,varchar(100)&gt;</span></span><br><span class="line"><span class="string">            这个值是元类创建User类的属性值，它会覆盖了值为&#x27;Wott&#x27;的u.name实例属性，显然不符合实际情况。</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            class_dict.pop(field_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在原属性字典里增加一个私有属性（字典类型），这个私有属性保存了要创建字段的信息</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__fields_dict__&#x27;</span>]=fields_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取自定义模型中，指定Meta信息的数据库表名 </span></span><br><span class="line"><span class="string">        class User(Model):</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">            class Meta:</span></span><br><span class="line"><span class="string">                db_table=USER_T</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        attr_meta=class_dict.get(<span class="string">&#x27;Meta&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">        print(<span class="string">&#x27;User模型定义的Meta属性:&#x27;</span>,attr_meta) <span class="comment"># Meta: &lt;class &#x27;__main__.User.Meta&#x27;&gt;</span></span><br><span class="line">        meta_table_name=<span class="built_in">getattr</span>(attr_meta,<span class="string">&#x27;db_table&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">        print(<span class="string">&#x27;User模型在Meta指定的数据库表名为：&#x27;</span>,meta_table_name) <span class="comment"># User模型在Meta指定的数据库表名为：： USER_T</span></span><br><span class="line">        <span class="keyword">if</span> attr_meta <span class="keyword">and</span> meta_table_name:</span><br><span class="line">                table=meta_table_name</span><br><span class="line">                <span class="keyword">del</span> class_dict[<span class="string">&#x27;Meta&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">                table=class_name <span class="comment"># 若User模型没指定Meta中的数据库表名，则默认用User模型类名作为数据库表名</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__table_name__&#x27;</span>]=table </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把原Meta属性变成私有属性，这样创建出来的类更具OOP风格</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__Meta__&#x27;</span>]=attr_meta</span><br><span class="line">        <span class="comment"># 以上完成对普通User模型的属性字典改造后，再重新把它传入到type，从而元类ModelMetaClass完成拦截=&gt;定制=&gt;创建普通类的过程。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases,class_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params"><span class="built_in">dict</span>,metaclass=ModelMetaClass</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    1、Model类继承dict，目的是为了满足ORM中使用字典赋值和取值的方式例如</span></span><br><span class="line"><span class="string">    创建u=User(id=1,name=&#x27;Wott&#x27;,email=&#x27;11@11.com&#x27;,password=&#x27;1213&#x27;)</span></span><br><span class="line"><span class="string">	2、Model内部通过拼接普通类的字段属性信息，封装了原生sql语句，例如save(),filter(),update()等方法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self,attr_name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重写内建getattr方法，可实现类似u.name这种点号获取属性值得方式,用起来更具ORM风格</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self[attr_name]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">raise</span> AttributeError(<span class="string">&quot;Model object has no attribute &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attr_name))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self,col_name,col_value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重写setattr方法，可实现类似u.name=&quot;Foo&quot;这种通过点号设置属性值的方式，用起来更具ORM风格</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self[col_name]=col_value</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 字段名列表</span></span><br><span class="line">        column_name_list=[]</span><br><span class="line">        place_holder_list=[]</span><br><span class="line">        <span class="comment"># 字段值列表</span></span><br><span class="line">        column_value_list=[]</span><br><span class="line">        fields_dict=self.__fields_dict__</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 存放字段的字典，key就是字段名，放在字段名列表，value就是字段值，放在字段值列表，两个列表用于拼接sql语句</span></span><br><span class="line">        <span class="keyword">for</span> attr_name,attr <span class="keyword">in</span> fields_dict.items():</span><br><span class="line">            <span class="comment"># 打印为：attr_name==&gt;id,attr==&gt;&lt;class &#x27;__main__.IntField&#x27;&gt;,attr.col_name==&gt;user_id</span></span><br><span class="line">            column_name_list.append(attr.col_name)</span><br><span class="line">            place_holder_list.append(<span class="string">&#x27;%s&#x27;</span>)</span><br><span class="line">            print(self) <span class="comment"># Model Dict:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125; </span></span><br><span class="line">            column_value_list.append(self[attr_name])</span><br><span class="line">            <span class="comment"># 或者column_value_list.append(getattr(self,attr_name))</span></span><br><span class="line"></span><br><span class="line">            sql = <span class="string">&#x27;insert into %s (%s) values (%s)&#x27;</span> % (self.__table_name__, <span class="string">&#x27;,&#x27;</span>.join(column_name_list), <span class="string">&#x27;,&#x27;</span>.join(place_holder_list))</span><br><span class="line">        print(<span class="string">&#x27;SQL语句:&#x27;</span>,sql)</span><br><span class="line">        print(<span class="string">&#x27;SQL的入参值列表:&#x27;</span>,column_value_list)</span><br><span class="line">            <span class="comment"># 连接mysql数据库后，使用cur.execute(sql,column_value_list)即可存入数据</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Model type:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">super</span>().__str__())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Field</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,col_name,col_type</span>):</span></span><br><span class="line">        self.col_name=col_name <span class="comment"># 字段名</span></span><br><span class="line">        self.col_type=col_type <span class="comment"># 字段类型</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        例如将会打印以下格式：</span></span><br><span class="line"><span class="string">        &#x27;id&#x27;: IntField&lt;user_id,bigint&gt;等</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&#123;&#125;&lt;&#123;&#125;,&#123;&#125;&gt;&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__,self.col_name,self.col_type)</span><br><span class="line">    </span><br><span class="line">    __repr__=__str__</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharField</span>(<span class="params">Field</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义字符型字段,默认可变字符类型长度为100</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, col_name, max_length=<span class="number">100</span></span>):</span></span><br><span class="line">        varchar_type=<span class="string">&quot;varchar(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(max_length)</span><br><span class="line">        <span class="built_in">super</span>().__init__(col_name, varchar_type)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IntField</span>(<span class="params">Field</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义整型字段</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, col_name, col_type=<span class="string">&quot;bigint&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(col_name, col_type)</span><br></pre></td></tr></table></figure>
<p>ModelMetaClass负责顶层设计（改造），用户创建所有的普通类如User、Article、Department等，都会被该元类重设设计（改造它们的class_dict）后再创建出这些普通类。</p>
<p>用户定义了一个User模型，有四个字段，并指定创建为表名为USER_T<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="built_in">id</span>=IntField(<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">    name=CharField(<span class="string">&#x27;user_name&#x27;</span>)</span><br><span class="line">    email=CharField(<span class="string">&#x27;email&#x27;</span>,max_length=<span class="number">200</span>)</span><br><span class="line">    password=CharField(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        自定义数据库表名，这里虽然Meta定义为类，</span></span><br><span class="line"><span class="string">        但在元类ModelMetaClass的视角来看，它是一个属性，放在class_dict里面</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        db_table=<span class="string">&#x27;USER_T&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>当用户定义完以上的普通类User后，Python解释器首先在当前类User的定义中查找metaclass，显然当前上下文环境没有找到，则继续在父类Model中查找metaclass，发现Model定义了metaclass=ModelMetaClass，故直接交由ModelMetaclass来创建该普通的User类。</p>
<p>用户创建了User实例并尝试向db插入该条数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u=User(<span class="built_in">id</span>=<span class="number">1</span>,name=<span class="string">&#x27;Wott&#x27;</span>,email=<span class="string">&#x27;11@11.com&#x27;</span>,password=<span class="string">&#x27;1213&#x27;</span>) <span class="comment"># Model继承dict，因此Model子类User当然可用字典创建方式来创建实例</span></span><br><span class="line">u[<span class="string">&#x27;name&#x27;</span>]=<span class="string">&#x27;Foo&#x27;</span><span class="comment"># Model继承dict，因此Model子类User当然可使用字典方式赋值</span></span><br><span class="line">u.password=<span class="string">&#x27;Pa33Wood&#x27;</span><span class="comment"># Model内部定义__setattr__方法，故可用点号给属性赋值</span></span><br><span class="line">print(u.email) <span class="comment"># Model内部定义__getattr__方法，故可用点号取属性值</span></span><br><span class="line">u.save()</span><br></pre></td></tr></table></figure></p>
<p>以上代码各个位置上的print输出结果如下<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">第一句print输出：</span><br><span class="line">在ModelMetaClass元类捕获到普通类的属性字典:</span><br><span class="line"> &#123;&#x27;__module__&#x27;: &#x27;__main__&#x27;, &#x27;__qualname__&#x27;: &#x27;User&#x27;, &#x27;id&#x27;: IntField&lt;user_id,bigint&gt;, &#x27;name&#x27;: CharField&lt;user_name,varchar(100)&gt;, &#x27;email&#x27;: CharField&lt;email,varchar(200)&gt;, &#x27;password&#x27;: CharField&lt;password,varchar(100)&gt;, &#x27;Meta&#x27;: &lt;class &#x27;__main__.User.Meta&#x27;&gt;&#125;</span><br><span class="line"></span><br><span class="line">第二句print输出：</span><br><span class="line">attr_name is &quot;id&quot;,field object is &quot;&lt;class &#x27;__main__.IntField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;name&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;email&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;password&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line"></span><br><span class="line">第三句print输出：</span><br><span class="line">User模型定义的Meta属性: &lt;class &#x27;__main__.User.Meta&#x27;&gt;</span><br><span class="line"></span><br><span class="line">第四句print输出：</span><br><span class="line">User模型在Meta指定的数据库表名为： USER_T</span><br><span class="line"></span><br><span class="line">第五句print输出：</span><br><span class="line">11@11.com</span><br><span class="line"></span><br><span class="line">第六句print输出：</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line"></span><br><span class="line">第七句print输出：</span><br><span class="line">SQL语句: insert into USER_T (user_id,user_name,email,password) values (%s,%s,%s,%s)</span><br><span class="line"></span><br><span class="line">第八句print输出：</span><br><span class="line">SQL的入参值列表: [1, &#x27;Foo&#x27;, &#x27;11@11.com&#x27;, &#x27;Pa33Wood&#x27;]</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>python元类</tag>
      </tags>
  </entry>
  <entry>
    <title>理解HDFS文件系统架构和原理</title>
    <url>/2019/10/15/%E7%90%86%E8%A7%A3HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%92%8C%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h4 id="1、Hadoop是一种具体的技术吗？"><a href="#1、Hadoop是一种具体的技术吗？" class="headerlink" title="1、Hadoop是一种具体的技术吗？"></a>1、Hadoop是一种具体的技术吗？</h4><p>&#8195;&#8195;准确的说，Hadoop是一套大数据的解决方案或者技术栈，不仅仅特指某种大数据技术，由Apache基金会上多个与大数据有关的明星组件构成，包括HDFS（分布式文件系统），YARN（分布式资源调度系统），MapReduce（分布式计算系统）、Spark、Hive、Hbase、Mahout、Zookeeper、Flume等，如下图所示。<img src="https://img-blog.csdnimg.cn/20191014221453428.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>
<p>&#8195;&#8195;本文将重点讨论HDFS、YARN以及MapReduce，为何？<br>设想这么一个场景：<br>1）、如果T企业每天产生10T的数据，一年为3650T，显然单台服务器是无法存储的，必须分别存放在多台服务器上。HDFS就是负责存放这些超大数据文件的核心组件。<br>2）、对于初始数据价值密度低的数据文件，T企业需要对其进行数据挖掘，提取高价值密度数据，用以协助调整自身的商业策略，那么这环节会涉及到从多台服务器上读数据、并执行相应的计算、统计逻辑任务（业务代码），这里就涉及如何从多台服务器上的数据计算出一份有价值数据出来呢？这是MapReduce要负责的活。<br>3）、在MapReduce这一场景中，例如T企业同时运行多个主题的计算任务，如果没有对于多台服务器的cpu+内存+网络吞吐资源进行统一分配、有效管理、回收，那么有可能计算非核心任务a占据集群资源的80%，而核心任务确只能分配到20%计算资源，显然无法达到计算资源最优化，这就是YARN要做的事情。<br>当然三个组件要负责的具体工作远不止以上的描述，博客将分别发布三篇文章讨论对应以上三个组件。<br>此外，从Hadoop的演进历史来看：<br>Hadoop1.0版本为两个核心（分布式存储+计算）：HDFS+MapReduce<br>Hadoop2.0版本，引入了Yarn：HDFS+Yarn+Mapreduce<br>Yarn是资源调度框架。能够细粒度的管理和调度任务。此外，还能够支持其他的计算框架，比如Spark等。</p>
<h4 id="2、HDFS"><a href="#2、HDFS" class="headerlink" title="2、HDFS"></a>2、HDFS</h4><p>&#8195;&#8195;一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。<br>HDFS 采用Master/Slave的架构来存储数据，这里以下图所示作为说明：<br><img src="https://img-blog.csdnimg.cn/20191014231209135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="2-1-Hadoop-block块概念"><a href="#2-1-Hadoop-block块概念" class="headerlink" title="2.1 Hadoop block块概念"></a>2.1 Hadoop block块概念</h5><p>&#8195;&#8195;block是物理的文件，真正存储的位置在磁盘中例如目录：{hadoop.tmp.dir}/data，将客户端上传的完整数据文件切分多个block后存储的块文件。Hadoop1.0是按64MB切，BlockSize=64MB。Hadoop2.0 BlockSize=128MB<br>&#8195;&#8195;文件分块存储，DateNode中存储以数字编号的方块用于备份，每个块都会复制到其他节点上（默认3个节点）,如果一个块不可用，可从其它节点读取副本，副本默认为3份，如果配置文件中副本设置为 4 ，而仅有2台Datanode，最后block副本还是2</p>
<p>&#8195;&#8195;对存储小文件，1000个1M的小文件会占用1024个块和1024个 inode，每个1M文件只是占用1个物理文件块中的1M，不会占用整个128M的完整block，但因为inode存储在NameNode的内存里，如果NameNode内存不足以存储更多的inode，那么磁盘也无法存储更多数据block文件，白白浪费存储资源，因此HDFS并不适合存储小文件，务必考虑将小文件合并为大文件，再扔到HDFS上。</p>
<p>&#8195;&#8195;补充：inode的内容：文件（这里指代Linux一切皆文件的文件）索引数据结构，inode中主要存储以下这些元数据：</p>
<pre><code>- inode编号
</code></pre><ul>
<li>文件大小</li>
<li>占用的块数目与块大小</li>
<li>文件类型（普通文件、目录、管道，etc.）</li>
<li>存储该文件的设备号</li>
<li>链接数目</li>
<li>读、写、执行权限</li>
<li>拥有者的用户ID和组ID</li>
<li>文件的最近访问、数据最近修改时间<ul>
<li>inode最近修改时间<br>其中，inode编号相当于这个结构中的“主键”，说明Linux使用inode编号唯一标识一个文件，通过stat命令可以查看元数据信息，如下图所示。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# stat /opt/hadoop-3.1.2/ </span><br><span class="line">  File: ‘/opt/hadoop-3.1.2/’</span><br><span class="line">  Size: 204             Blocks: 0          IO Block: 4096   directory</span><br><span class="line">Device: fd00h/64768d    Inode: 4216030   Links: 13</span><br><span class="line">Access: (0755/drwxr-xr-x)  Uid: ( 1001/ UNKNOWN)   Gid: ( 1002/ UNKNOWN)</span><br><span class="line">Context: unconfined_u:object_r:usr_t:s0</span><br><span class="line">Access: 2019-1**51.243402848 +0800</span><br><span class="line">Modify: 2019-**:40.433267611 +0800</span><br><span class="line">Change: 2019-**:40.433267611 +0800</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure>
显示块信息命令：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 显示hdfs目录&#x2F;app下TJKL.txt数据文件，大小为536M，到底被切分为多少文件块，以及这些文件块的详细情况及其所在位置</span><br><span class="line">[root@nn ~]# hdfs fsck &#x2F;app -files -blocks -locations -racks</span><br><span class="line">&#x2F;app&#x2F;TJKL.txt 562227059 bytes, replicated: replication&#x3D;3, 5 block(s):  OK</span><br><span class="line">0. BP-1004123743-192.188.0.4-15**1724:blk_1073741912_1096 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">1. BP-1004123743-192.188.0.4-15**1724:blk_1073741913_1097 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">2. BP-1004123743-192.188.0.4-15**1724:blk_1073741914_1098 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">3. BP-1004123743-192.188.0.4-15**1724:blk_1073741915_1099 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">4. BP-1004123743-192.188.0.4-15**1724:blk_1073741916_1100 len&#x3D;25356147 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br></pre></td></tr></table></figure>
以上信息说明：TJKL.txt被切分为5个文件块（前4个block都是128M，第5个block为24M），每个文件块都有3个副本，每个副本分别放在同一Rack（机架）上三台服务器上</li>
</ul>
</li>
</ul>
<h5 id="2-2-Client"><a href="#2-2-Client" class="headerlink" title="2.2 Client"></a>2.2 Client</h5><p>&#8195;&#8195;连接到hadoop集群的客户端， 或者说使用API或指令操作的一端都可以看做是客户端。<br>文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储<br>与 NameNode 交互，获取文件的位置信息<br>与 DataNode 交互，读取或者写入数据，例如使用管道把文件块从上游节点写到下游节点<br>Client 提供一些命令来管理 HDFS，比如启动或者关闭HDFS<br>Client 可以通过一些命令来访问 HDFS</p>
<h5 id="2-3-NameNode：Master角色，名字节点"><a href="#2-3-NameNode：Master角色，名字节点" class="headerlink" title="2.3 NameNode：Master角色，名字节点"></a>2.3 NameNode：Master角色，名字节点</h5><p>&#8195;&#8195;管理元数据信息（Metadata），只存储元数据信息，存放在内存中，会通过fsimage和edits文件，将元数据信息持久化到磁盘上<br>管理数据块（Block）映射信息<br>配置副本策略<br>处理客户端读写请求<br>注意：Hadoop1.0版本使用SecondaryNamenode做fsimage和edits文件的合并，但是这种机制达不到热备的效果，即存在单点故障问题，Hadoop2.0解决了该缺点。 </p>
<h5 id="2-4-fsimage、edits"><a href="#2-4-fsimage、edits" class="headerlink" title="2.4 fsimage、edits"></a>2.4 fsimage、edits</h5><p>&#8195;&#8195;fsimage 文件，记录元数据信息的文件edits文件，记录元数据信息改动的文件。只要元数据发生变化，这个edits文件就会有对应记录。<br>fsimage和edits文件会定期做合并，这个周期默认是3600s。fsimage根据edits里改动记录进行元数据更新。<br>元数据信息如果丢失，HDFS就不能正常工作了，因此在生产环境中，元数据是需要做备份的。<br>hadoop集群部署中，命令hadoop namenode -format 执行时，创建了初始的fsimage文件和edits文件，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn current]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;namenode&#x2F;current</span><br><span class="line">......</span><br><span class="line">edits_0000000000000000556-0000000000000000557  fsimage_0000000000000001186</span><br><span class="line">edits_0000000000000000558-0000000000000000655  fsimage_0000000000000001186.md5</span><br></pre></td></tr></table></figure>
<h5 id="2-5-Secondary-Namenode"><a href="#2-5-Secondary-Namenode" class="headerlink" title="2.5 Secondary Namenode"></a>2.5 Secondary Namenode</h5><p>辅助 NameNode，分担其工作量。<br>定期合并 fsimage和fsedits，并推送给NameNode。<br>在紧急情况下，可辅助恢复 NameNode。<br>Hadoop集群最开始启动的时候，创建Fsimage和edits文件，这个namenode负责，此外，namenode会做一次文件合并工作，这么做的目的是确保元数据信息是最新的，之后的合并工作，就交给SN去做了。这种SN机制是Hadoop1.0的机制，该机制达不到元数据的实时更新，若NN宕机，元数据信息可能还会丢失。</p>
<h5 id="2-6-DataNode"><a href="#2-6-DataNode" class="headerlink" title="2.6 DataNode"></a>2.6 DataNode</h5><p>Slave角色，具体干活者，NameNode下发指令操作，DataNode 执行实际的操作<br>存储实际的物理数据块block。<br>执行数据块的读/写操作。<br>为了防止datanode挂掉造成的数据丢失，对于文件块要有备份，一个文件块有三个副本。这里体现出hdfs是一种高容错的文件系统。<br>在服务器上的构成：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 subdir0]# pwd</span><br><span class="line">/opt/hadoop-3.1.2/datanode/current/BP-1004123743-192.188.0.4-***current/finalized/subdir0/subdir0</span><br><span class="line">......</span><br><span class="line">blk_1073741876_1052.meta  blk_1073741901_1078.meta  blk_1073741916_1100.meta</span><br><span class="line">blk_1073741877            blk_1073741902</span><br><span class="line">blk_1073741877_1053.meta  blk_1073741902_1079.meta</span><br></pre></td></tr></table></figure>
<h5 id="2-7-HDFS写文件流程图"><a href="#2-7-HDFS写文件流程图" class="headerlink" title="2.7 HDFS写文件流程图"></a>2.7 HDFS写文件流程图</h5><p><img src="https://img-blog.csdnimg.cn/20191015000223140.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>1）、发起一个写数据请求，并指定上传文件的路径，然后去找NN。NN首先会判断路径合法性以及客户端是否有写权限，若符合，则NN会给客户端返回一个输出流。此外，NN会为文件分配块存储信息（理解为记录block文件位置大小等信息的索引文件）。注意NN也是分配块的存储信息，但不做物理切块工作。</p>
<p>2）、客户端拿到输出流以及块存储信息之后，就开始向DN节点写数据。因为一个块数据，有三个副本，所以图里有三个NN节点。<br>3）、数据块的发送（管道发送或者接力棒传递方式），先发给第一台DN节点，数据再从第一台DN发往第二台DN，……，此方式用到了pipeLine 数据流管道的机制，就像redis发送管道命令，然后批量返回结果，从而减少网络来回RTT时间消耗。<br>pipeLine:[bl1,datanode01-datanode02-datanode-03]<br>数据流管道充分利用每台机器的带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时，提高传输效率。<br>packet 默认为64kb大小的数据包</p>
<p>4）、通过ack确认机制，向上游节点发送确认，例如DN3向DN2ack说我已写好数据，DN2继续向DN1说我已写好数据，DN1再跟客户端说:数据块已经在DN1、DN2、DN3都存储好，这么做的目的是确保块数据复制的完整性。</p>
<p> 5）、通过最上游节点DN1，向客户端发送ack，如果块数据没有发送完，客户端会就继续发送下一块，直到所有块数据都已发完，关闭文件关流。</p>
<p>6）、所有块数据都写完后，关闭文件流。</p>
<h5 id="2-8-hadoop读取文件流程图"><a href="#2-8-hadoop读取文件流程图" class="headerlink" title="2.8 hadoop读取文件流程图"></a>2.8 hadoop读取文件流程图</h5><ul>
<li><img src="https://img-blog.csdnimg.cn/20191015001052278.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<p>  1.客户端发出读数据请求，Open File指定读取的文件路径，在NN节点获取元数据信息。<br>    2.NN将目标文件的元数据信息返回给客户端。<br>    3.客户端根据返回的元数据信息，去对应的DN去读块数据。<br>    4.假如一个文件特别大，比如1TB，会分成好多块，此时，NN并是不一次性把所有的元数据信息返回给客户端，而是分节点返回。<br>    5.客户端读完此部分后，再向NN节点要下一部分的元数据信息，再接着读。<br>    6.读完之后，通知namenode关闭流</p>
<h4 id="3、再讨fsimage和editlog"><a href="#3、再讨fsimage和editlog" class="headerlink" title="3、再讨fsimage和editlog"></a>3、再讨fsimage和editlog</h4><ul>
<li><p>NameNode通过组织两个核心数据结构：“FSImage”和“EditLog”文件，实现==维护文件系统树内所有文件和目录，记录每个文件在哪个DateNode的位置和副本信息，来通知客户端应该去哪个节点访问文件blocks。==</p>
</li>
<li><p>fsImage_*：元数据镜像文件，即系统的目录树，包括文件目录和inodes元信息（文件名，文件大小，创建时间，备份级别，访问权限，block size，所有block的构成)，每个inode是hdfs的一个代表文件或者目录的元数据。这个镜像文件相当于hdfs的元数据额数据库文件。</p>
</li>
<li><p>edits<em>*：编辑日志文件，也就是事务日志文件，也就是针对文件系统做的修改操作记录，记录元数据的变化，相当于操作日志文件。一个文件的创建，追加，移动等。 NameNode内存中存储的是=fsimage+edits 检查点：NameNode启动时，从磁盘中读取上面两种文件，然后把edits</em><em>里面记录的事务全部刷新到 fsimage_</em>中，这样就截去了旧的edits_*事务日志，这个过程叫checkpoint。</p>
</li>
<li><p>==为何使用一大一小两种数据结构的文件？ 因为fsimage本身是大文件，试想你要向大文件每时每刻open file—&gt;append new line—&gt; close file，也太累了吧。因此可把新来的操作记录放到小的EditLog里， 再设定每隔一段时间，把一个FsImage和一个Editlog 进行合并会得到一个新的FsImage。==</p>
</li>
<li>==fsimage中存储的信息就相当于整个hdfs在某一时刻的一个快照==，既然这个快照可以管理整个hdfs集群的文件信息，那么对其备份则非常重要，于是<br>引入一个叫SendaryNamenode的节点用来做备份fsimage，它会定期的和namenode就行通信来完成整个的备份操作，具体工作原理见第4点：</li>
</ul>
<h4 id="4、SecondaryNameNode合并FSImage"><a href="#4、SecondaryNameNode合并FSImage" class="headerlink" title="4、SecondaryNameNode合并FSImage"></a>4、SecondaryNameNode合并FSImage</h4><p><img src="https://img-blog.csdnimg.cn/20191015220557202.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;为何NN节点不负责合并工作而是由其他节点服务器来运行此任务？<br>因为这两个文件合并过程需要消耗内存、磁盘io以及cpu，因此将这些“蓝领”工作交给其他节点做，否则NN主节点消耗大量资源。<br>具体流程：<br>1）、SN定时和NN通信chectpoint，在什么时候进行checkpoint？由两个参数dfs.namenode.checkpoint.preiod(默认值是3600，即1小时)和dfs.namenode.checkpoint.txns(默认值是1000000)来决定），通过请求NN其停止使用edits文件，暂时将新的操作写到一个新的edits.new文件<br>2）、SN通过HTTP GET方式从NN上获取到fsimge和edits文件，并下载到本地的相应目录下；<br>3）、SN将下载下来的fsimage载入到内存，然后一条一条地执行edits文件中的各项更新操作，使得内存中的fsimge保持最新；这个过程就是edits和fsimage文件合并，同时SN节点也会在磁盘上存放一份fsimage（不就实现了fsimage的备份吗）<br>4）、SN执行完（3）操作之后，会通过HTTP POST方式将新的fsimage文件发送到NN节点上<br>5）、 NN将从SN接收到的新的fsimage替换掉旧fsimage文件，同时将edits.new替换edits文件，通过这个过程edit内容就变小而且都是最新的操作日志。</p>
<h4 id="5、文件blocks副本存放策略"><a href="#5、文件blocks副本存放策略" class="headerlink" title="5、文件blocks副本存放策略"></a>5、文件blocks副本存放策略</h4><p>&#8195;&#8195;NN节点如何选择在哪个datanode 存储block副本？<br>这里需要对可靠性、写入带宽和读取带宽进行权衡。Hadoop对DN存储副本有自己的副本策略，在其发展过程中一共有两个版本的副本策略，分别如下所示：<br><img src="https://img-blog.csdnimg.cn/20191015223215262.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS原理</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：AQS核心源代码实现Condition的深入解析</title>
    <url>/2021/06/27/AQS%E6%A0%B8%E5%BF%83%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0Condition%E7%9A%84%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>关于AQS的独占模式同步器设计原理（以ReentrantLock为例）以及共享模式的同步器设计原理（以Semaphore为例）在前面文章的已经讨论完毕，这两种模式让我们理解了AQS通过底层的FIFO阻塞队列（又称同步队列/变体的CLH队列/Sync queue）实现了相当巧妙的多线程协调调度的复杂逻辑。当然AQS还有一个更为关键的设计：结合FIFO阻塞队列+条件队列（又称condition queue/wait queue）实现一种基于条件的await和signal的多线程间的协调机制，也即本文内容。</p>
<h4 id="关于条件队列和阻塞队列的说明"><a href="#关于条件队列和阻塞队列的说明" class="headerlink" title="关于条件队列和阻塞队列的说明"></a>关于条件队列和阻塞队列的说明</h4><p>对于阻塞队列，这里只给出独占模式的线程节点说明：阻塞队列其实有AQS内部定义的双向链表节点的属性如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">       <span class="keyword">volatile</span> <span class="keyword">int</span> waitStatus;</span><br><span class="line">       <span class="keyword">volatile</span> Node prev;</span><br><span class="line">       <span class="keyword">volatile</span> Node next;</span><br><span class="line">       <span class="keyword">volatile</span> Thread thread;</span><br><span class="line">   &#125;</span><br><span class="line">Node head; <span class="comment">// Head of the wait queue</span></span><br><span class="line">Node tail; <span class="comment">// Tail of the wait queue</span></span><br></pre></td></tr></table></figure>
<p>而条件队列中，它是有单向链表实现，该单向链表的节点的属性为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Node firstWaiter; <span class="comment">// First node of condition queue.</span></span><br><span class="line">Node lastWaiter; <span class="comment">// Last node of condition queue.</span></span><br><span class="line">Node nextWaiter; <span class="comment">// Link to next node waiting on condition,</span></span><br></pre></td></tr></table></figure>
<p>可以看到三个属性用于构成单向链表，而条件队里的nextWaiter指针是为了区别阻塞队列中的next指针。</p>
<p>阻塞队列的作用在前面的文章已经给出很明确的说明：只要当前线程没有请求到锁资源（state），则需要进入CLH阻塞队列进行排队等候，那么AQS给出的条件队列到底是解决什么场景呢？</p>
<p>这里不妨考虑这种场景：</p>
<p>消费者线程：消费者线程原本在阻塞队列等待，当外界有线程释放了锁资源，那么此消费者线程从阻塞队列被唤醒后出队并拿到锁资源后，发现“存放货物的队列是空的”，这种未加入“条件限制”的等待线程调度策略则显得不够明智。</p>
<p>因此可以这么设计：给当前独占锁lock对象添加添加一个条件队列：如果有一个或者多消费者线程过来取“货物”，当遇到“仓库没有货物可取”这种条件时，那么这些消费者线程先被安排在条件队列等待（阻塞自己）：</p>
<p>firstWaiter(c0)-&gt;c1-&gt;c2-c3-&gt;c4-&gt;lastWaiter(c5)-&gt;null</p>
<p>直到条件“仓库有货物可取时”，那么条件队列的消费者线程再转移到阻塞队列里面排队等候被唤醒去抢占锁资源以实施消费行为</p>
<a id="more"></a>
<h4 id="基于Condition实现的生产者和消费者模型"><a href="#基于Condition实现的生产者和消费者模型" class="headerlink" title="基于Condition实现的生产者和消费者模型"></a>基于Condition实现的生产者和消费者模型</h4><p>这里将以一个经典多线程间协调案例作为分析基于AQS实现的Condition底层工作机制</p>
<p>生产者线程：每次向“仓库”生产1个产品，等待2秒后，并使用signal通知消费者线程</p>
<p>消费者线程：在启动后如果条件“仓库为空”成立则进入await阻塞状态。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.locks.Condition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.locks.ReentrantLock;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConditionDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ReentrantLock lock=<span class="keyword">new</span> ReentrantLock(<span class="keyword">true</span>); <span class="comment">// 使用公平模式，以方便debug观察多个消费者线程的入队顺序和出队顺序等。</span></span><br><span class="line">    <span class="keyword">static</span>   Condition condition=lock.newCondition(); <span class="comment">// 可以看到锁对象关联了一个条件对象</span></span><br><span class="line">    <span class="keyword">static</span> List&lt;ElectricCar&gt;  teslaWarehouse=<span class="keyword">new</span> ArrayList&lt;&gt;(); <span class="comment">// 存放车的“仓库”</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">      	<span class="comment">// 启动5个消费者去“消费”车</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> Consumer(condition,teslaWarehouse,lock).start();</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">//让消费者线程先启动，接着再启动1个生产者线程，每隔2秒生产一台车并放入“仓库”</span></span><br><span class="line">        Thread.sleep(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">new</span> Producer(condition,teslaWarehouse,lock).start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">ElectricCar</span> </span>&#123;&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelY</span> <span class="keyword">implements</span> <span class="title">ElectricCar</span></span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Condition condition;</span><br><span class="line">    <span class="keyword">private</span> List&lt;ElectricCar&gt; list;</span><br><span class="line">    <span class="keyword">private</span> ReentrantLock lock;</span><br><span class="line">    Producer(Condition condition,List&lt;ElectricCar&gt; list,ReentrantLock lock)&#123;</span><br><span class="line">        <span class="keyword">this</span>.condition=condition;</span><br><span class="line">        <span class="keyword">this</span>.list=list;</span><br><span class="line">        <span class="keyword">this</span>.lock=lock;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">            lock.lock(); <span class="comment">// 注意：Condition对象使用await或者signal方法前需要获得于条件对象关联的独占锁，否则抛出非法错误。至于为何这么设计，需要等到文章最后的解释。</span></span><br><span class="line">            <span class="keyword">this</span>.list.add(<span class="keyword">new</span> ModelY()); </span><br><span class="line">            System.out.println(<span class="string">&quot;生产了一台modelY&quot;</span>);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            condition.signal(); <span class="comment">// 通知消费者线程可以“消费”车了，</span></span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Condition condition;</span><br><span class="line">    <span class="keyword">private</span> List&lt;ElectricCar&gt; list;</span><br><span class="line">    <span class="keyword">private</span> ReentrantLock lock;</span><br><span class="line"></span><br><span class="line">    Consumer(Condition condition, List&lt;ElectricCar&gt; list, ReentrantLock lock) &#123;</span><br><span class="line">        <span class="keyword">this</span>.condition = condition;</span><br><span class="line">        <span class="keyword">this</span>.list = list;</span><br><span class="line">        <span class="keyword">this</span>.lock = lock;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();<span class="comment">// 注意：Condition对象使用await或者signal方法前需要获得于条件对象关联的独占锁，否则抛出非法错误。至于为何这么设计，需要等到文章最后的解释。</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (list.isEmpty())</span><br><span class="line">                condition.await(); <span class="comment">// 消费者线程会在此释放锁资源并进入条件队列且阻塞自己，直到收到生产者生产了车信号</span></span><br><span class="line">            list.remove(<span class="number">0</span>);</span><br><span class="line">            System.out.println(Thread.currentThread().getName() + <span class="string">&quot;消费一台modelY&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行结果如下：生产者线程循环一次生产一台车，消费者线程有序的“消费”对应的一台车，由于只启动5个消费者线程，因此当生产者线程生产到第6台车时，此时已经没有其他新来的消费者线程来“消费车”，因此不断打印生产者生产车的信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">生产了一台modelY</span><br><span class="line">Thread-0消费一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">Thread-1消费一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">Thread-2消费一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">Thread-3消费一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">Thread-4消费一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">生产了一台modelY</span><br><span class="line">生产了一台modelY</span><br></pre></td></tr></table></figure>
<p>以上的多个消费者线程await和生产者线程signal并发协调机制及其源码解析将在下面给出。</p>
<h4 id="消费者线程的await底层设计解析"><a href="#消费者线程的await底层设计解析" class="headerlink" title="消费者线程的await底层设计解析"></a>消费者线程的await底层设计解析</h4><p>首先，注释<code>new Producer(condition,teslaWarehouse,lock).start()</code>这一行代码，以考察多个消费者线程<code>lock.lock()-&gt;await()</code>内部逻辑，main启动后会发现主程序阻塞了，你会好奇到底消费者线程在AQS的哪个地方阻塞了？是因为都进入条件队列导致的main阻塞，还是在阻塞队列里面导致main阻塞？根据之前关于<code>ReentrantLock</code>的独占模式原理解析，可知：</p>
<p>（1）首先，考察5个线程并发执行时都使用<code>lock.lock()</code>争抢独占锁资源，由于lock对象使用公平模式实例化，因此Thread-0首先成功占有锁资源，而其他4个线程将有序进入lock对象的阻塞队列，state锁资源和阻塞队列的结构如下：</p>
<p>Thread-0作为优先抢占了锁资源的独占线程，可以继续执行自己的逻辑</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exclusiveOwnerThread&#x3D;Thread-0</span><br></pre></td></tr></table></figure>
<p>Thread-1到Thread-4因为并发执行<code>lock.lock()</code>因此进入lock对象关联的阻塞队列且线程状态处于waiting阻塞状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">head(null,-1)&lt;-&gt;node(Thread1,-1)&lt;-&gt;node(Thread2,-1)&lt;-&gt;node(Thread3,-1)&lt;-&gt;node(Thread4,0)-&gt;null</span><br></pre></td></tr></table></figure>
<p>显然位于阻塞队列的第一个线程节点Thread-1正在等待Thread-0的唤醒。</p>
<p>注意：此时条件队列还未出现！此时条件队列还未出现！</p>
<p>（2） Thread-0可以继续执行自己逻辑，也即下面执行流（注意，此时锁资源还是由Thread-0占有！）:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (list.isEmpty()) <span class="comment">// 由于消费者线程早于生产者线程启动，因此一开始“仓库列表”是空的，因此进入以下await逻辑</span></span><br><span class="line">    condition.await(); <span class="comment">// 这里是理解条件队列的关键入口</span></span><br></pre></td></tr></table></figure>
<p><code>condition.await()</code>调用的是AQS内部类ConditionObject的await()方法，继续以消费者线程Thread-0作为分析对象：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 由于Thread-0已经作为独占锁线程，意味着await方法是线程安全的，因此在这里你不会看到自旋和CAS。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">await</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">      	<span class="comment">// await方法是响应外部中断的：当线程调用condition.await()前，被外部中断过，则这里马上响应中断</span></span><br><span class="line">        <span class="keyword">if</span> (Thread.interrupted())</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">        <span class="comment">// ① 将Thread-0添加到条件队列（单向链表）</span></span><br><span class="line">        Node node = addConditionWaiter();</span><br><span class="line">      	<span class="comment">/* ② Thread-0添加到条件队列后，需要释放占有的锁资源。内部其实调用release方法，也即Thread-0会唤醒阻塞队列里面的第一个线程节点Thread-1，此外：</span></span><br><span class="line"><span class="comment">      	进入条件队列后，为何还要释放自己占有的锁资源？ 务必理解其设计原因，参考后面的解释。</span></span><br><span class="line"><span class="comment">        */</span> </span><br><span class="line">      	<span class="comment">// 注意这里：一旦Thread-0释放成功，那么就会唤醒位于阻塞队列的Thread-1</span></span><br><span class="line">        <span class="keyword">int</span> savedState = fullyRelease(node);</span><br><span class="line">      	<span class="comment">// ③ 记录Thread-0可能被外部中断的中断标记</span></span><br><span class="line">        <span class="keyword">int</span> interruptMode = <span class="number">0</span>;</span><br><span class="line">      	<span class="comment">// ④ 显然Thread-0节点不在阻塞队列（这里SyncQueue就是上面Thread-1~Thread-4所在的阻塞队列），</span></span><br><span class="line">        <span class="keyword">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class="line">           <span class="comment">//⑤ 将Thread-0进行阻塞处理，避免浪费cpu时间片,这里正是解释了为何在main在启动仅有5个消费者线程后，主程序一直被阻塞运行的原因</span></span><br><span class="line">            LockSupport.park(<span class="keyword">this</span>);</span><br><span class="line">        <span class="comment">// 以下的四个if都是为了处理“当线程被signal通知唤醒后响应中断或者取消在条件队列排队的情况，其设计思路会在文章后面给出，这里先分析其阻塞设计。</span></span><br><span class="line">            <span class="keyword">if</span> ((interruptMode = checkInterruptWhileWaiting(node)) != <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)</span><br><span class="line">            interruptMode = REINTERRUPT;</span><br><span class="line">        <span class="keyword">if</span> (node.nextWaiter != <span class="keyword">null</span>) <span class="comment">// clean up if cancelled</span></span><br><span class="line">            unlinkCancelledWaiters();</span><br><span class="line">        <span class="keyword">if</span> (interruptMode != <span class="number">0</span>)</span><br><span class="line">            reportInterruptAfterWait(interruptMode);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上就是消费者线程首次调用condition.await()的内部工作流程，此时阻塞队列结构和条件队列结构如下：</p>
<p>Thread-0进入条件队列—&gt;释放自己占有的锁资源（此操作会唤醒阻塞队列的Thread-1）—&gt;在条件队列阻塞自己</p>
<p>同理被唤醒的Thread-1会继续执行await()方法，跟Thread-0执行流类似，以此类推，最后，5个消费者线程都在条件队列里面阻塞了：</p>
<p>firstWaiter(Thread-0,-2)-&gt;node(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>支持await的内部几个核心方法</p>
<ul>
<li>addConditionWaiter</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ①将Thread-0添加到条件队列（单向链表）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">addConditionWaiter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">           Node t = lastWaiter; </span><br><span class="line">           <span class="comment">// If lastWaiter is cancelled, clean out.</span></span><br><span class="line">         	<span class="comment">// 只要条件队列的尾部线程节点是取消状态，那么将其剔除出队列。为何要从尾部判断呢？参考fullyRelease的解析</span></span><br><span class="line">           <span class="keyword">if</span> (t != <span class="keyword">null</span> &amp;&amp; t.waitStatus != Node.CONDITION) &#123;</span><br><span class="line">               unlinkCancelledWaiters();</span><br><span class="line">               t = lastWaiter; <span class="comment">// 从这里可以看出t的含义：指向非取消状态的尾部节点</span></span><br><span class="line">           &#125;</span><br><span class="line">         	<span class="comment">// 这里即可表明条件队列节点构成：将调用await的线程包装为节点，且状态值node.waitStatus为CONDITION</span></span><br><span class="line">           Node node = <span class="keyword">new</span> Node(Thread.currentThread(), Node.CONDITION);</span><br><span class="line">           <span class="keyword">if</span> (t == <span class="keyword">null</span>)</span><br><span class="line">               firstWaiter = node;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               t.nextWaiter = node; <span class="comment">// 因为上面已经知道t不为null，那么才能将await新来的线程节点放入条件队列</span></span><br><span class="line">           lastWaiter = node;</span><br><span class="line">           <span class="keyword">return</span> node;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>fullyRelease</li>
</ul>
<p>当Thread-0线程使用addConditionWaiter进入条件队列后，需要释放自己占有的独占锁资源，但为何要安排释放独占锁资源的逻辑？</p>
<p>以下可通过反证法考察其设计意图：</p>
<p>如果Thread-0进入条件队列后且不释放独占锁资源就进入阻塞状态，那么就会生产者线程lock.lock()加锁失败从而进入阻塞队列队尾，而阻塞队列里面第一个线程节点Thread-1显然已经没有外面线程来唤醒，结果就是：整个main程序直接stuck（卡住了），而且此时阻塞队列结构和条件队列结构如下：</p>
<p>head(null,-1)&lt;-&gt;node(Thread1,-1)&lt;-&gt;node(Thread2,-1)&lt;-&gt;node(Thread3,-1)&lt;-&gt;node(Thread4,-1)&lt;-&gt;node(生产者线程,0)-&gt;null</p>
<p>条件队列结构：firstWaiter(Thread-0,-2)-&gt;null</p>
<p>以上的流程可以这样通俗理解：你拿了办公室钥匙（唯一一把钥匙），你直接去休息室“休眠”了，且没有把钥匙“释放”给后面的同事，那么你的同事1、同事2….只能等你，如果你一直“休眠”，那么他们一直无法进入办公室。</p>
<p>那么考察加入释放的独占锁资源逻辑后，其工作过程将如何？</p>
<p>（1）Thread-0的fullyRelease会调用AQS的release方法，从而唤醒了阻塞队列里面的Thread-1，此时Thread-1独占锁资源，Thread-1继续执行业务代码await()，同理进入条件队列，并使用fullyRelease独占锁资源同时该操作也会唤醒阻塞队列里面的Thread-2，以此类推，这种进入条件队后释放独占锁资源的流程可以这样通俗理解：</p>
<p>你拿了办公室钥匙（唯一一把钥匙），你直接去休息室“休眠”前，把钥匙“释放”给后面的同事1，接着，同事1也要去休息室“休眠”，去之前把钥匙“释放”给后面的同事2，以此类推</p>
<p>（2）显然此时5个消费者线程都在条件队列里面阻塞着（5个同事都在休息室“休眠”），直到生产者线程使用使用signal后，条件队列的Thread-0将被唤醒，之后的逻辑就是signal支持实现。</p>
<p>这就是fullyRelease的设计意图！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 进入条件队列的线程释放自己占有的独占锁，可能释放一个或者多个锁资源，如何理解多个锁资源？由于是condition对象是关联ReentrantLock对象，支持同一线程的一次（state=1）或者多次重入占有锁（state&gt;1），因此这里fully是指：不管同一线程占用一个锁资源或者同一线程多次重入占用多个锁资源，都可以在这里一次性释放此线程占有的所有锁资源。</span></span><br><span class="line"> <span class="comment">// 如果释放失败，在finally里面将此节点标记为取消状态，从这里可以看出：条件队列的线程节点取消状态可以在这里产生。</span></span><br><span class="line">   <span class="function"><span class="keyword">final</span> <span class="keyword">int</span> <span class="title">fullyRelease</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="keyword">int</span> savedState = getState();</span><br><span class="line">         	<span class="comment">// 如果Thread-0在release能成功释放锁资源，由于release方法（内部调用unparkSuccessor）会唤醒阻塞队列的第一个线程节点，因此Thread-0此时会唤醒阻塞队列里面的Thread-1</span></span><br><span class="line">           <span class="keyword">if</span> (release(savedState)) &#123;</span><br><span class="line">               failed = <span class="keyword">false</span>;</span><br><span class="line">               <span class="keyword">return</span> savedState;</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">   				<span class="comment">// 这里做了一个优化设计：每次新节点条件队列后，如果此节点出现异常，则马上将其标记为取消状态，意味着条件队列的队尾要么是正常节点要么就是被标记为取消状态的节点，这种设计将方便addConditionWaiter加入if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) 的逻辑，也即：每次添加新节点到条件队列前，只需判断队列尾部是否存在取消状态的节点，如果有则先使用unlinkCancelledWaiters删除这类节点，然后再new Node入队。</span></span><br><span class="line">           <span class="keyword">if</span> (failed)</span><br><span class="line">               node.waitStatus = Node.CANCELLED;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">release</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (tryRelease(arg)) &#123;</span><br><span class="line">           Node h = head;</span><br><span class="line">           <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h.waitStatus != <span class="number">0</span>)</span><br><span class="line">               unparkSuccessor(h);</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>isOnSyncQueue</li>
</ul>
<p>判断刚进入条件队列的线程节点是否已经被转移到阻塞队列中，若不在，则使用 LockSupport.park(this)将当前条件队列的线程节点阻塞，避免浪费cpu时间片。对于Thread-0，显然刚进入条件队列，一定不在阻塞队列，因此将其阻塞在条件队列里等待，isOnSyncQueue就控制了条件队列节点的阻塞。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns true if a node, always one that was initially placed on</span></span><br><span class="line"><span class="comment"> * a condition queue, is now waiting to reacquire on sync queue.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if is reacquiring</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">isOnSyncQueue</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">  	 <span class="comment">//① 节点状态为CONDITION，或者节点的prev为null（因为条件队列的节点只用到firstWaiter、lastWaiter、nextWaiter）说明该节点在条件队列，但一定不在阻塞队列</span></span><br><span class="line">    <span class="keyword">if</span> (node.waitStatus == Node.CONDITION || node.prev == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  	<span class="comment">//② 阻塞队列使用next指针，因此当前节点的next不为空，必然是在阻塞队列里面</span></span><br><span class="line">    <span class="keyword">if</span> (node.next != <span class="keyword">null</span>) <span class="comment">// If has successor, it must be on queue</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * node.prev can be non-null, but not yet on queue because</span></span><br><span class="line"><span class="comment">     * the CAS to place it on queue can fail. So we have to</span></span><br><span class="line"><span class="comment">     * traverse from tail to make sure it actually made it.  It</span></span><br><span class="line"><span class="comment">     * will always be near the tail in calls to this method, and</span></span><br><span class="line"><span class="comment">     * unless the CAS failed (which is unlikely), it will be</span></span><br><span class="line"><span class="comment">     * there, so we hardly ever traverse much.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	</span><br><span class="line">  	<span class="comment">//③ 在①和②都不满足时，官方注释做了这么一个特殊情况说明：在第一个if判断线程节点的prev不为空时表明此刻它还未进入阻塞队列（因为casTail尾部入队失败导致），但可能在下一个时刻就会进入阻塞队列尾部，因此需要重新读取阻塞队列也即在阻塞队列的尾部开始向前遍历，来判断当前node是否在阻塞队列。</span></span><br><span class="line">    <span class="keyword">return</span> findNodeFromTail(node);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过以上的<code>lock.lock()到condition.await()</code>的线程节点工作原理分析，现在可以清晰看到线程们在阻塞队列结构过渡到条件队列结构的过程：</p>
<p>（1）5个消费者线程并发执行<code>lock.lock()</code>，形成的阻塞队列结构如下：</p>
<p>Thread-0</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">exclusiveOwnerThread&#x3D;Thread-0</span><br></pre></td></tr></table></figure>
<p>Thread-1到Thread-4因为并发执行<code>lock.lock()</code>因此进入lock对象关联的阻塞队列且线程状态处于waiting阻塞状态：</p>
<p>head(null,-1)&lt;-&gt;node(Thread1,-1)&lt;-&gt;node(Thread2,-1)&lt;-&gt;node(Thread3,-1)&lt;-&gt;node(Thread4,0)-&gt;null</p>
<p>（2）5个消费者线程的执行流来到<code>condition.await()</code>，形成的条件队列结构如下(单向链表)，且每个消费者线程都处于阻塞状态：</p>
<p>firstWaiter(Thread-0,-2)-&gt;node(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>在前面demo的mian方法，我们给出注释<code>new Producer(condition,teslaWarehouse,lock).start()</code>,也即没有生产者线程启动，也即没有调用condition.signal方法，那么消费者线程Thread-0只能在条件队列一直被阻塞，当然后面的Thread-1到Thread-4也同样处于阻塞状态。</p>
<p>下面将加入生产者的signal后的工作过程分析</p>
<h4 id="生产者线程的signal底层设计解析"><a href="#生产者线程的signal底层设计解析" class="headerlink" title="生产者线程的signal底层设计解析"></a>生产者线程的signal底层设计解析</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">new</span> Consumer(condition,teslaWarehouse,lock).start();</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 执行流到此，5个消费线程已经在条件队列阻塞中</span></span><br><span class="line">    Thread.sleep(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 启动生产者线程</span></span><br><span class="line">   <span class="keyword">new</span> Producer(condition,teslaWarehouse,lock).start();</span><br></pre></td></tr></table></figure>
<p>Producer线程的运行逻辑</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">        lock.lock(); <span class="comment">// 此时没有其他线程和生产者线程抢锁资源，因此生产者线程成功持有独占锁</span></span><br><span class="line">        <span class="keyword">this</span>.list.add(<span class="keyword">new</span> ModelY()); </span><br><span class="line">        System.out.println(<span class="string">&quot;生产了一台modelY&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        condition.signal(); <span class="comment">//这里是关键：通知条件队列里面的第一个非取消节点：消费者Thread-0</span></span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>signal()设计逻辑</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    将当前lock对象关联的条件队列中等待时间最长的线程节点，这里为何不是说头节点呢？因为头节点或者其他节点都有可能变成取消状态，那么真正处理的目标是那些非取消状态的且等待时间最长的线程节点转移到阻塞队列中</span></span><br><span class="line"><span class="comment">     * Moves the longest-waiting thread, if one exists, from the</span></span><br><span class="line"><span class="comment">     * wait queue for this condition to the wait queue for the</span></span><br><span class="line"><span class="comment">     * owning lock.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IllegalMonitorStateException if &#123;<span class="doctag">@link</span> #isHeldExclusively&#125;</span></span><br><span class="line"><span class="comment">     *         returns &#123;<span class="doctag">@code</span> false&#125;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">signal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 显然生产者线程确实是独占锁资源的线程，这里不会抛出非法状态异常</span></span><br><span class="line">        <span class="keyword">if</span> (!isHeldExclusively())</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();</span><br><span class="line">        Node first = firstWaiter;</span><br><span class="line">        <span class="keyword">if</span> (first != <span class="keyword">null</span>)</span><br><span class="line">            doSignal(first);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Removes and transfers nodes until hit non-cancelled one or</span></span><br><span class="line"><span class="comment">     * null. Split out from signal in part to encourage compilers</span></span><br><span class="line"><span class="comment">     * to inline the case of no waiters.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> first (non-null) the first node on condition queue</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 在将首个非取消状态的条件队列节点（也即Thread-0）转移到阻塞队列，且顺便在这一过程中移除条件队列中不是condition状态的节点。</span></span><br><span class="line"><span class="comment">// 需要注意的是：对于未曾研究过AQS工作原理的同学，也许会错以为条件队列的节点被signal后会马上去抢独占锁，实际并非如此。所谓的转移就是要求条件队列的节点被signal后要先移动到阻塞队列中去排队等候被唤醒，这种设计也保证了Condition的设计思路和AQS独占模式设计思路的一致性。</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSignal</span><span class="params">(Node first)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">          	<span class="comment">// 因为转移的是first节点，因此将firstWaiter后移指向第二个节点也即Thread-1，当然如果第二个节点已经是null说明当前条件队列的所有节点都转移到阻塞队列中。</span></span><br><span class="line">            <span class="keyword">if</span> ( (firstWaiter = first.nextWaiter) == <span class="keyword">null</span>)</span><br><span class="line">                lastWaiter = <span class="keyword">null</span>;</span><br><span class="line">          	<span class="comment">// first节点出队，也即Thread-0断开和后面的Thread-1链接。（如果first是取消状态，则正好可以在此过程删除这些取消状态的节点）</span></span><br><span class="line">            first.nextWaiter = <span class="keyword">null</span>;</span><br><span class="line">          <span class="comment">/* </span></span><br><span class="line"><span class="comment">          转移节点也不是随便转移的：只有first节点是非取消节点才能转移到阻塞队列。或者说</span></span><br><span class="line"><span class="comment">          只要转移操作失败（因为当前first节点是取消状态才会转移失败）且条件队列还存在下一个节点，就继续找一个非取消状态的节点去转移</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        &#125; <span class="keyword">while</span> (!transferForSignal(first) &amp;&amp; <span class="comment">//真正转移的逻辑是在transferForSignal实施的</span></span><br><span class="line">                 (first = firstWaiter) != <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">			</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transfers a node from a condition queue onto sync queue.</span></span><br><span class="line"><span class="comment"> * Returns true if successful.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if successfully transferred (else the node was</span></span><br><span class="line"><span class="comment"> * cancelled before signal)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">transferForSignal</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * If cannot change waitStatus, the node has been cancelled.</span></span><br><span class="line"><span class="comment">     // 这里正好回答了上面doSignal为何需要重试的原因：因为当前转移的节点已经变为取消状态了，因此在判断!transferForSignal(first)后需要继续找条件队列链表的下一个非取消节点来转移。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">if</span> (!compareAndSetWaitStatus(node, Node.CONDITION, <span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Splice onto queue and try to set waitStatus of predecessor to</span></span><br><span class="line"><span class="comment">     * indicate that thread is (probably) waiting. If cancelled or</span></span><br><span class="line"><span class="comment">     * attempt to set waitStatus fails, wake up to resync (in which</span></span><br><span class="line"><span class="comment">     * case the waitStatus can be transiently and harmlessly wrong).</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 这里就是条件队列节点转移到阻塞队列的秘密：条件队列节点头节点（假设first是非取消节点）维持自己阻塞状态出队，然后使用enq方法将此头节（注意此节点依旧处于阻塞状态）点放入阻塞队列的队尾。</span></span><br><span class="line">    Node p = enq(node);</span><br><span class="line">    <span class="keyword">int</span> ws = p.waitStatus;</span><br><span class="line">		<span class="comment">// 如果前一个节点是取消状态ws&gt;0或者无法将前节点设为SIGNAL值，那么就无法让前节点来唤醒刚入阻塞队列的线程节点。为了处理这种特殊情况，直接唤醒此节点。</span></span><br><span class="line">    <span class="keyword">if</span> (ws &gt; <span class="number">0</span> || !compareAndSetWaitStatus(p, ws, Node.SIGNAL))</span><br><span class="line">        LockSupport.unpark(node.thread);</span><br><span class="line">  	<span class="comment">// 执行到这里，说明：条件队列节点头节点（带着阻塞状态）已经成功进入阻塞队列且还处于阻塞状态</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>有了对signal的底层设计解析，现在分析这几个线程之间的内部协调</p>
<p>（1）生产者线程调用signal前，独占锁资源、阻塞队列和条件队列结构如下：</p>
<p>独占锁资源：生产者持有</p>
<p>阻塞队列：还未形成</p>
<p>条件队列如下：</p>
<p>firstWaiter(Thread-0,-2)-&gt;node(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>（2）生产者线程调用signal后，上面的独占锁资源、阻塞队列和条件队列结构如下：</p>
<p>独占锁资源：生产者持有</p>
<p>阻塞队列：head(null,-1)&lt;-&gt;node(Thread-0,0) ，因为signal内部会将条件队列的头节点Thread-0转移（enq）到阻塞队列，此时Thread-0等待生产者线程使用unlock唤醒</p>
<p>条件队列如下：</p>
<p>firstWaiter(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>（3）生产者线程调用signal后且执行了lock.unlock()，唤醒了阻塞队列里面的Thread-0消费者线程，唤醒流程,lock.unlock—&gt;release—&gt;tryRelease—&gt;unparkSuccessor。上面的独占锁资源、阻塞队列和条件队列结构如下：</p>
<p>独占锁资源：：Thread-0持有</p>
<p>阻塞队列：head(null,0)</p>
<p>条件队列如下：</p>
<p>firstWaiter(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>（4）显然此时Thread-0可以从await唤醒的位置继续运行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (list.isEmpty())</span><br><span class="line">            condition.await(); <span class="comment">// Thread-0从这里唤醒后，继续执行</span></span><br><span class="line">      			</span><br><span class="line">        list.remove(<span class="number">0</span>);</span><br><span class="line">      	<span class="comment">// Thread-0消费了生产者线程刚刚生产的车：也即对应的打印：Thread-0消费一台modelY</span></span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">&quot;消费一台modelY&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      	<span class="comment">// Thread-0释放独占锁 </span></span><br><span class="line">        lock.unlock();</span><br></pre></td></tr></table></figure>
<p>当Thread-0消费了一台车并在finally释放锁资源前、后过程中，生产者线程同时会进行第二次循环，执行流程如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  	<span class="comment">//第二次循环：</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">        lock.lock(); <span class="comment">// Thread-0执行lock.unlock()前，生产者线程在第二次循环执行到lock.lock()时，因为此时锁资源还被消费者Thread-0占用，因此生产者只能进入阻塞队列等待。直到Thread-0执行lock.unlock()后，生产者马上抢占锁成功（因为消费者线程Thread-1~Thread-4还是条件队列中等待）</span></span><br><span class="line">        <span class="keyword">this</span>.list.add(<span class="keyword">new</span> ModelY());</span><br><span class="line">        System.out.println(<span class="string">&quot;生产了一台modelY&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        condition.signal(); <span class="comment">// 通知条件队列里面的消费者Thread-1线程，以后的执行流程就按（2）~（4）不断循环执行，直到条件队列的消费者Thread-4完成lock.unlock()后，之后就没有消费者线程去消费生产者线程新生产的车，对应main程序可以观察到不断打印&quot;生产了一台modelY&quot;</span></span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="await响应中断的逻辑分析"><a href="#await响应中断的逻辑分析" class="headerlink" title="await响应中断的逻辑分析"></a>await响应中断的逻辑分析</h4><p>在上面await设计分析中，重点放在消费者线程入队、释放锁资源、阻塞操作的原理分析。在这一小节，则重点分析await是被设计为可响应外界中断的，await响应中断的逻辑相对复杂，两种特别的中断处理策略：</p>
<p>（1）中断处理策略1：如果条件队列的线程节点（例如上面提到的Thread-0）在waiting过程中被外界中断过，且还未被生产者signal过，那么此线程调用await就需要抛出InterruptedException</p>
<p>（2）中断处理策略2：如果条件队列的线程节点（例如上面提到的Thread-0）在waiting过程中没被外界中断，而是在signal后，再被外界中断过，那么此线程调用await只需补一次<code>selfInterrupt()</code></p>
<p>当然此方法的注释也给出详细的说明（4、6）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Implements interruptible condition wait.</span></span><br><span class="line"><span class="comment">1. If current thread is interrupted, throw InterruptedException.</span></span><br><span class="line"><span class="comment">2. Save lock state returned by getState.</span></span><br><span class="line"><span class="comment">3. Invoke release with saved state as argument, throwing IllegalMonitorStateException if it fails.</span></span><br><span class="line"><span class="comment">4. Block until signalled or interrupted.</span></span><br><span class="line"><span class="comment">5. Reacquire by invoking specialized version of acquire with saved state as argument.</span></span><br><span class="line"><span class="comment">6. If interrupted while blocked in step 4, throw InterruptedException.</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>所以到此你应该猜测“await响应中断的设计中”是通过什么方式判断线程节点到底是被signal过还是没被signal呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">				<span class="comment">// 使用interruptMode表征阻塞队列的线程的中断模式，根据情况，取以下两种值</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        对应上述的中断模式2，这里的exit是指线程从阻塞队列获取锁资源后采取的处理中断策略（别错把exit看成是条件队列的线程出队）</span></span><br><span class="line"><span class="comment">        Mode meaning to reinterrupt on exit from wait </span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> REINTERRUPT =  <span class="number">1</span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        中断模式2：对应上述的中断模式1</span></span><br><span class="line"><span class="comment">        Mode meaning to throw InterruptedException on exit from wait </span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> THROW_IE    = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        如果当前线程节点在signal前就被外界中断，则标记为为THROW_IE。</span></span><br><span class="line"><span class="comment">        如果当前线程节点在signal之后被外界中断，那么则标记为REINTERRUPT，</span></span><br><span class="line"><span class="comment">        如果当前节点在阻塞队列都没有被外界中断，则标记为0</span></span><br><span class="line"><span class="comment">         * Checks for interrupt, returning THROW_IE if interrupted</span></span><br><span class="line"><span class="comment">         * before signalled, REINTERRUPT if after signalled, or</span></span><br><span class="line"><span class="comment">         * 0 if not interrupted.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">checkInterruptWhileWaiting</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Thread.interrupted() ?</span><br><span class="line">                (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) :</span><br><span class="line">                <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 以下就是判断当前线程节点到底是有没有被signal过的核心设计</span></span><br><span class="line"><span class="comment">     * Transfers node, if necessary, to sync queue after a cancelled wait.</span></span><br><span class="line"><span class="comment">     * Returns true if thread was cancelled before being signalled.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> true if cancelled before the node was signalled</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">transferAfterCancelledWait</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">/* 1、在条件队列的线程节点，其waitStatus一定是CONDITION，因此如果这里能CAS成功就说明：在signal之前，该线程已经被取消（被外界中断），但仍然需要将此节点放入阻塞队列。</span></span><br><span class="line"><span class="comment">      			thread was cancelled before being signalled.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">if</span> (compareAndSetWaitStatus(node, Node.CONDITION, <span class="number">0</span>)) &#123;</span><br><span class="line">            enq(node);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        	2、执行流来到这里，说明此时线程节点waitStatus不是CONDITION肯定被signal过，也即出了条件队列而且在转移到阻塞队列的过程中（可能还未进入阻塞队列），waitStatus在转移逻辑transferForSignal被更改过。换句话说：线程节点被signal过，已经出了条件队列但还未成功进入阻塞队列，因为enq()操作需要此线程和其他线程竞争入阻塞队列队尾，因此出现这种“incomplete transfer”不完整的转移是有可能的，但此情况即罕见又短暂，那么线程自己通过spin直到enq完成入阻塞队列就行，这就是源码注释要表达的含义。</span></span><br><span class="line"><span class="comment">        	</span></span><br><span class="line"><span class="comment">         * If we lost out to a signal(), then we can&#x27;t proceed</span></span><br><span class="line"><span class="comment">         * until it finishes its enq().  Cancelling during an</span></span><br><span class="line"><span class="comment">         * incomplete transfer is both rare and transient, so just</span></span><br><span class="line"><span class="comment">         * spin.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">while</span> (!isOnSyncQueue(node))</span><br><span class="line">            Thread.yield();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Unlinks cancelled waiter nodes from condition queue.</span></span><br><span class="line"><span class="comment">         * Called only while holding lock. This is called when</span></span><br><span class="line"><span class="comment">         * cancellation occurred during condition wait, and upon</span></span><br><span class="line"><span class="comment">         * insertion of a new waiter when lastWaiter is seen to have</span></span><br><span class="line"><span class="comment">         * been cancelled. This method is needed to avoid garbage</span></span><br><span class="line"><span class="comment">         * retention in the absence of signals. So even though it may</span></span><br><span class="line"><span class="comment">         * require a full traversal, it comes into play only when</span></span><br><span class="line"><span class="comment">         * timeouts or cancellations occur in the absence of</span></span><br><span class="line"><span class="comment">         * signals. It traverses all nodes rather than stopping at a</span></span><br><span class="line"><span class="comment">         * particular target to unlink all pointers to garbage nodes</span></span><br><span class="line"><span class="comment">         * without requiring many re-traversals during cancellation</span></span><br><span class="line"><span class="comment">         * storms.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">				<span class="comment">/* </span></span><br><span class="line"><span class="comment">				两种情况会调用unlinkCancelledWaiters方法：</span></span><br><span class="line"><span class="comment">				1、addConditionWaiter，添加一个新节点到条件队列队尾时，如果队尾节点恰好是取消状态的节点，则使用unlinkCancelledWaiters剔除它</span></span><br><span class="line"><span class="comment">				2、在await方法中，当线程节点在条件队列中被中断（此时还未被signal，对应的interruptMode为THROW_IE），那么表示此节点取消在条件队列的排队，需要将其剔除。</span></span><br><span class="line"><span class="comment">				*/</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">unlinkCancelledWaiters</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            Node t = firstWaiter; <span class="comment">// 每次循环，t滑向当前要处理的节点，相当于遍历指针</span></span><br><span class="line">            Node trail = <span class="keyword">null</span>; <span class="comment">// trail指向非取消状态的节点</span></span><br><span class="line">          	<span class="comment">// </span></span><br><span class="line">            <span class="keyword">while</span> (t != <span class="keyword">null</span>) &#123;</span><br><span class="line">                Node next = t.nextWaiter;</span><br><span class="line">                <span class="comment">// 1、如果t指向的当前节点是取消状态，断开t和子链next的连接（也即将t剔除出队）</span></span><br><span class="line">                <span class="keyword">if</span> (t.waitStatus != Node.CONDITION) &#123;</span><br><span class="line">                    t.nextWaiter = <span class="keyword">null</span>;</span><br><span class="line">         <span class="comment">//1.1 如果trail还是null，说明条件队列的第一个节点是取消节点，把next作为条件队列的头节点即可</span></span><br><span class="line">                    <span class="keyword">if</span> (trail == <span class="keyword">null</span>)</span><br><span class="line">                        firstWaiter = next;</span><br><span class="line">         <span class="comment">// 1.2 如果trial不为空，将trail--&gt;t(cancelled)--&gt;next变成trail--&gt;next</span></span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        trail.nextWaiter = next;</span><br><span class="line">         <span class="comment">/*1.3 如果来到条件队列队尾，那么原条件队列的所有取消状态的节点都被剔除，之后的条件队列：</span></span><br><span class="line"><span class="comment">         firstWaiter(CONDITION)--&gt;全部都是CONDITION状态的节点--&gt;lastWaiter(CONDITION)    </span></span><br><span class="line"><span class="comment">         */</span> </span><br><span class="line">                    <span class="keyword">if</span> (next == <span class="keyword">null</span>)</span><br><span class="line">                        lastWaiter = trail;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                 <span class="comment">// 2、说明t当前处理节点是正常状态，trail滑向（指向）t</span></span><br><span class="line">                    trail = t;</span><br><span class="line">              	<span class="comment">// 3、t滑向下一个条件队列节点</span></span><br><span class="line">                t = next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        根据线程节点interruptMode被标记的状态：要么抛出异常、要么补一次自我中断、或者无需做其他操作</span></span><br><span class="line"><span class="comment">         * Throws InterruptedException, reinterrupts current thread, or</span></span><br><span class="line"><span class="comment">         * does nothing, depending on mode.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">reportInterruptAfterWait</span><span class="params">(<span class="keyword">int</span> interruptMode)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (interruptMode == THROW_IE)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (interruptMode == REINTERRUPT)</span><br><span class="line">                selfInterrupt();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下以Thread-0调用await后进行中断逻辑分析，从唤醒处开始：</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">await</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">          	<span class="comment">// 如果Thread-0在进入条件队列等待之前就已经被外界中断过，此线程直接抛出异常，不再参与条件队列等待</span></span><br><span class="line">            <span class="keyword">if</span> (Thread.interrupted())</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> InterruptedException();</span><br><span class="line">            Node node = addConditionWaiter();</span><br><span class="line">            <span class="keyword">int</span> savedState = fullyRelease(node);</span><br><span class="line">            <span class="keyword">int</span> interruptMode = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (!isOnSyncQueue(node)) &#123;</span><br><span class="line">                LockSupport.park(<span class="keyword">this</span>); </span><br><span class="line">                <span class="comment">/* Thread-0被唤醒后继续之后执行流，被唤醒的原因有以下两种：</span></span><br><span class="line"><span class="comment">                  1、被生产者线程使用signal唤醒的</span></span><br><span class="line"><span class="comment">                  注意：很多人以为是被生产者线程signal方法唤醒，这是理解不透彻的表现：signal方法只是把条件队列的Thread-0转移到阻塞队列中，真正唤醒Thread-0的代码是：生产者线程执行lock.unlock()内部使用unparkSuccessor唤醒阻塞队列里面的Thread-0（前提假设Thread-0是此队列的第一个线程节点）</span></span><br><span class="line"><span class="comment">              		2、其他线程中断了Thread-0导致唤醒</span></span><br><span class="line"><span class="comment">              		为了找出是哪种中断，需要在checkInterruptWhileWaiting进行判断，以便让Thread-0从阻塞队列出队拿到锁资源后，补充对应的中断类型（或者抛出InterruptedException或者补一个selfInterrupt()）</span></span><br><span class="line"><span class="comment">              	*/</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">/* ②	醒来后的Thread-0使用Thread.interrupted()进行自我中断状态检查，如果自己在等待（线程休眠）过程中被中断过，则会跳出循环去到下面③的逻辑。</span></span><br><span class="line"><span class="comment">             	当然如果Thread-0没被中断过，那么回到while就满足OnSyncQueue，因此也会跳出while循环，意味着interruptMode为0</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">                <span class="keyword">if</span> ((interruptMode = checkInterruptWhileWaiting(node)) != <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">/* ③ 由于Thread-0还在阻塞队列，运行到这里，就需要使用独占不可中断的acquireQueued方法重新获取锁资源，由于生产者线程执行②所提到的unlock后，此时Thread-0一定能acquireQueued成功获取独占锁（假设没有外界线程和Thread-0抢锁）：</span></span><br><span class="line"><span class="comment">         （1）acquireQueued返回true表示线程节点在阻塞队列中又被中断过1次，因此可以将interruptMode设为REINTERRUPT表示此线程节点被中断两次</span></span><br><span class="line"><span class="comment">         （2）当然对于Thread-0已经占有独占锁来说，acquireQueued返回的interrupted=false，因此会跳过此逻辑</span></span><br><span class="line"><span class="comment">           */</span> </span><br><span class="line">            <span class="keyword">if</span> (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)</span><br><span class="line">                interruptMode = REINTERRUPT;</span><br><span class="line">          	<span class="comment">// ④ 如果interruptMode是THROW_IE那么执行流从②跳过③来到这里，说明此线程节点在条件队列中就被中断过（还未被signal），因此需要将此线程节点剔除出条件队列</span></span><br><span class="line">            <span class="keyword">if</span> (node.nextWaiter != <span class="keyword">null</span>) <span class="comment">// clean up if cancelled</span></span><br><span class="line">                unlinkCancelledWaiters();</span><br><span class="line">          	<span class="comment">// ⑤ Thread-0从条件队列到阻塞队列出队的全过程是否发生的中断，都在这里得到最终的响应：如果是在signal前被外界中断，则THROW_IE，也即抛出异常；否则就是在signal后在阻塞队列等待锁期间被外界中断过，补上一个 selfInterrupt()即可</span></span><br><span class="line">            <span class="keyword">if</span> (interruptMode != <span class="number">0</span>)</span><br><span class="line">                reportInterruptAfterWait(interruptMode);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>有了以上完整的条件队列设计原理，本文的demo的内部线程之间的协调则很好理解：</p>
<ul>
<li>（1）5个消费者线程公平模式lock.lock()之后在await执行前</li>
</ul>
<p>条件队列：空</p>
<p>独占锁资源：Thread-0</p>
<p>其他4个线程因为Thread-0占有锁资源，只能先进入lock关联的CLH阻塞队列：</p>
<p>head(null,-1)&lt;-&gt;node(Thread1,-1)&lt;-&gt;node(Thread2,-1)&lt;-&gt;node(Thread3,-1)&lt;-&gt;node(Thread4,0)-&gt;null</p>
<ul>
<li>（2）Thread-0执行await操作后，await操作包括的动作：进入条件队列、释放锁资源、阻塞自己</li>
</ul>
<p>条件队列：firstWaiter(Thread-0,-2)-&gt;null  (因为Thread-0进入条件队列)</p>
<p>独占线程：Thread-1 （因为Thread-0 fullyRelease()内部使用unparkSuccessor唤醒了阻塞队列的第一个线程节点Thread-1）</p>
<p>阻塞队列：head(null,-1)&lt;-&gt;node(Thread2,-1)&lt;-&gt;node(Thread3,-1)&lt;-&gt;node(Thread4,0)-&gt;null</p>
<ul>
<li>（3）接着Thread-1唤醒后执行await，重复以上(2)流程，直到Thread-4也执行await后有：</li>
</ul>
<p>阻塞队列：head(null,0)</p>
<p>条件队列：firstWaiter(Thread-0,-2)-&gt;node(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>独占锁资源：null</p>
<p>可以看到，此时5个消费者线程都阻塞在lock关联下的条件队列，如果main程序没有安排生产者线程去signal此条件队列，那么main程序就会这种情况下挂起。</p>
<ul>
<li>（4）接着生产者线程启动，加锁成功，执行到signal，将条件队列的中阻塞状态的Thread-0转移（transferForSignal）到阻塞队列</li>
</ul>
<p>独占锁资源：生产者线程持有</p>
<p>阻塞队列：head(null,-1)&lt;-&gt;node(Thread-0,0)&lt;-&gt;null    （Thread-0等待生产者释放锁资源）</p>
<p>条件队列：firstWaiter(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<ul>
<li>（5）生产者完成signal后，执行到lock.unlock()后，也即使用unparkSuccessor唤醒阻塞队列的Thread-0</li>
</ul>
<p>（注意：由于生产者此时可以进行第二次循环再次抢占锁但因为Thread-0已经持有锁，因此生产者线程此时会阻塞在阻塞队列）</p>
<p>独占锁资源：Thread-0持有</p>
<p>阻塞队列：head(null,-1)&lt;-&gt; 生产者线程</p>
<p>条件队列：firstWaiter(Thread1,-2)-&gt;node(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>Thread-0继续执行“消费”代码：打印”生产了一台modelY”，后unlock释放锁资源</p>
<ul>
<li>（6）生产者进行第二次循环后，因为Thread-0释放锁可以唤醒阻塞队列生产者线程，因此它能加锁成功，之后又来到signal，将条件队列的中阻塞状态的Thread-1转移到阻塞队列</li>
</ul>
<p>独占锁资源：生产者线程持</p>
<p>阻塞队列：head(null,-1)&lt;-&gt;node(Thread-1,0)&lt;-&gt;null    （等待生产者释放锁资源）</p>
<p>条件队列：firstWaiter(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<ul>
<li>（7）生产者完成signal后，执行到lock.unlock()，也即使用unparkSuccessor唤醒阻塞队列的Thread-1</li>
</ul>
<p>独占锁资源：Thread-1持有</p>
<p>（注意：由于生产者此时可以进行第二次循环再次抢占锁但因为Thread-1已经持有锁，因此生产者线程此时会阻塞在阻塞队列）</p>
<p>阻塞队列：head(null,-1)&lt;-&gt; 生产者线程</p>
<p>条件队列：firstWaiter(Thread2,-2)-&gt;node(Thread3,-2)-&gt;node(Thread4,-2)-&gt;null</p>
<p>Thread-1继续执行“消费”代码：打印”生产了一台modelY”，后unlock释放锁资源</p>
<p>之后的循环逻辑类似，这里不再累赘。</p>
<p>如果你能清晰描述上述多个消费者和生产者线程的内部协调机制，那么才算是真正理解了AQS的底层设计及其源代码实现。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>Java高级主题</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：Unsafe类实现的JUC框架free-lock设计原理讨论</title>
    <url>/2021/05/09/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9AUnsafe%E7%B1%BB%E5%AE%9E%E7%8E%B0%E7%9A%84JUC%E6%A1%86%E6%9E%B6free-lock%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E8%AE%A8%E8%AE%BA/</url>
    <content><![CDATA[<p>Unsafe类在整套JUC框架中绝对是核心的一个概念，它是实现free-lock的底层核心设计，它内部直接调用的是Java的JNI，只有理解它的CAS原子操作的内部设计原理，才能更加深入理解JUC的free-lock设计。</p>
<h4 id="使用unsafe操作数组"><a href="#使用unsafe操作数组" class="headerlink" title="使用unsafe操作数组"></a>使用unsafe操作数组</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hashmap.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> sun.misc.Unsafe;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Unsafe3</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Unsafe unsafe;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Field field = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">            field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">            unsafe = (Unsafe)field.get(<span class="keyword">null</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[] intArr1=&#123;<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span>[] intArr2=&#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// byte类型数组内存每个&quot;单元格&quot;容量是1个字节，对应Scale就是1</span></span><br><span class="line">        <span class="keyword">int</span> byteScale=unsafe.arrayIndexScale(<span class="keyword">byte</span>[].class);</span><br><span class="line">        <span class="comment">// short类型数组内存每个&quot;单元格&quot;容量是2个字节，对应Scale就是2</span></span><br><span class="line">        <span class="keyword">int</span> shortScale=unsafe.arrayIndexScale(<span class="keyword">short</span>[].class);</span><br><span class="line">        <span class="comment">// int类型数组内存每个&quot;单元格&quot;容量是4个字节，对应Scale就是4</span></span><br><span class="line">        <span class="keyword">int</span> intScale=unsafe.arrayIndexScale(<span class="keyword">int</span>[].class);</span><br><span class="line">        <span class="comment">// long类型数组内存每个&quot;单元格&quot;容量是8个字节，对应Scale就是8</span></span><br><span class="line">        <span class="keyword">int</span> longScale=unsafe.arrayIndexScale(<span class="keyword">long</span>[].class);</span><br><span class="line">        <span class="comment">// Integer类型数组内存每个&quot;单元格&quot;容量是4个字节，对应Scale就是4</span></span><br><span class="line">        <span class="keyword">int</span> integerScale=unsafe.arrayIndexScale(Integer[].class);</span><br><span class="line">        <span class="comment">// String类型数组内存每个&quot;单元格&quot;容量是4个字节，对应Scale就是4</span></span><br><span class="line">        <span class="keyword">int</span> stringScale=unsafe.arrayIndexScale(String[].class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        <span class="keyword">boolean</span> b=intArr1.getClass().equals(intArr2.getClass()) &amp;&amp; <span class="keyword">int</span>[].class.equals(intArr1.getClass()) &amp;&amp; <span class="keyword">int</span>[].class.equals(intArr2.getClass());</span><br><span class="line">        System.out.println(b); <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// intArr1数组实例和intArr2的数组实例都是同一对象，指向int[]</span></span><br><span class="line">        System.out.println(intArr1.getClass().equals(intArr2.getClass()));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取当前intArr1数组实例的相对基址</span></span><br><span class="line">        <span class="keyword">long</span> baseOffset1=unsafe.arrayBaseOffset(intArr1.getClass());</span><br><span class="line"></span><br><span class="line">        System.out.println(baseOffset1); <span class="comment">// 16</span></span><br><span class="line">        <span class="comment">// 获取当前intArr2数组实例的相对基址</span></span><br><span class="line">        <span class="keyword">long</span> baseOffset2=unsafe.arrayBaseOffset(intArr2.getClass());</span><br><span class="line">        System.out.println(baseOffset2); <span class="comment">// 16</span></span><br><span class="line">        <span class="comment">// 从baseOffset1和baseOffset2的值都是16可以看出，这是他们指向对象的相对基址，而不是指向对象绝对地址（如果是执行对象绝对地址，那么这里两个值一定不同）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        System.out.println(unsafe.getIntVolatile(intArr1,baseOffset1)); <span class="comment">//打印intArr1第1个（首地址）对应的元素，也就是intArr1[0]</span></span><br><span class="line">        System.out.println(unsafe.getIntVolatile(intArr1,baseOffset1+intScale)); <span class="comment">//打印intArr1的第2个元素，也就是intArr1[1]</span></span><br><span class="line">        System.out.println(unsafe.getIntVolatile(intArr1,baseOffset1+intScale* <span class="number">2L</span>)); <span class="comment">//打印intArr1的第3个元素，也就是intArr1[2]</span></span><br><span class="line">        System.out.println(unsafe.getIntVolatile(intArr1,baseOffset1+intScale* <span class="number">3L</span>)); <span class="comment">//打印intArr1的第4个元素，也就是intArr1[3]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取intArr2的数据的第3个元素</span></span><br><span class="line">        System.out.println(unsafe.getIntVolatile(intArr2,baseOffset2+intScale* <span class="number">2L</span>)); <span class="comment">//打印intArr2第3个对应的元素，也就是intArr2[2]</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="使用Unsafe操作自定义类（非并发条件下）"><a href="#使用Unsafe操作自定义类（非并发条件下）" class="headerlink" title="使用Unsafe操作自定义类（非并发条件下）"></a>使用Unsafe操作自定义类（非并发条件下）</h4><p>以下定义个三个类及其属性，可以清楚看到unsafe操作对象的逻辑：</p>
<p>首先使用objectFieldOffset取到该对象的字段相对基址</p>
<p>其次，getXX取值方面，也即“读”：</p>
<ul>
<li><p>A、非并发编程条件下，基础类型使用getInt、getDouble等方法获取字段值，引用类型使用getObject取值</p>
</li>
<li><p>B、并发编程条件下，基础类型使用getIntVolatile、getDoubleVolatile等方法获取字段值，引用类型使用getObjectVolatile取值</p>
</li>
</ul>
<p>putXX设置值方面（也即set字段值），也即“写”：</p>
<ul>
<li><p>A、非并发编程条件下，基础类型使用putInt、putDouble等方法对字段进行赋值，引用类型使用putObject等方法对字段进行赋值</p>
</li>
<li><p>B、并发编程条件下，由于涉及多线程对临界区的写入，因此需要使用CAS机制去写入，而不是简单的putObject方法，对于基础类型和引用类型常用的CAS方法：</p>
</li>
<li><p>```<br>compareAndSwapInt、compareAndSwapLong、compareAndSwapObject</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">对应的demo程序如下：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;java</span><br><span class="line">package hashmap.demo;</span><br><span class="line"></span><br><span class="line">import sun.misc.Unsafe;</span><br><span class="line"></span><br><span class="line">import java.lang.reflect.Field;</span><br><span class="line">import java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"></span><br><span class="line">class A &#123;</span><br><span class="line">    private int a &#x3D; 1;</span><br><span class="line">    private String aa;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class B &#123;</span><br><span class="line">    private int b &#x3D; 2;</span><br><span class="line">    private String bb &#x3D; &quot;Unsafe looks like a C Pointer&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class C &#123;</span><br><span class="line">    private int c &#x3D; 3;</span><br><span class="line">    private String cc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class Unsafe4 &#123;</span><br><span class="line">    private static Unsafe unsafe;</span><br><span class="line"></span><br><span class="line">    static &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Field field &#x3D; Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);</span><br><span class="line">            field.setAccessible(true);</span><br><span class="line">            unsafe &#x3D; (Unsafe) field.get(null);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws NoSuchFieldException &#123;</span><br><span class="line">        long baseOffset1 &#x3D; unsafe.objectFieldOffset(A.class.getDeclaredField(&quot;a&quot;));</span><br><span class="line">        long baseOffset2 &#x3D; unsafe.objectFieldOffset(B.class.getDeclaredField(&quot;b&quot;));</span><br><span class="line">        long baseOffset3 &#x3D; unsafe.objectFieldOffset(C.class.getDeclaredField(&quot;c&quot;));</span><br><span class="line">        long baseOffset4 &#x3D; unsafe.objectFieldOffset(B.class.getDeclaredField(&quot;bb&quot;));</span><br><span class="line">        long baseOffset5 &#x3D; unsafe.objectFieldOffset(C.class.getDeclaredField(&quot;cc&quot;));</span><br><span class="line"></span><br><span class="line">        System.out.println(baseOffset1); &#x2F;&#x2F; 12</span><br><span class="line">        System.out.println(baseOffset2); &#x2F;&#x2F; 12</span><br><span class="line">        System.out.println(baseOffset3); &#x2F;&#x2F; 12</span><br><span class="line">        System.out.println(baseOffset4); &#x2F;&#x2F; 16</span><br><span class="line">        System.out.println(baseOffset5); &#x2F;&#x2F; 16</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 非并发情况下，使用getInt方法获取字段值</span><br><span class="line">        System.out.println(unsafe.getInt(new A(), baseOffset1)); &#x2F;&#x2F; a&#x3D;1</span><br><span class="line">        System.out.println(unsafe.getInt(new B(), baseOffset2)); &#x2F;&#x2F; a&#x3D;1</span><br><span class="line">        System.out.println(unsafe.getInt(new C(), baseOffset3)); &#x2F;&#x2F; c&#x3D;3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 引用类型的字段需要使用getObject或者getObjectVolatile获取其值</span><br><span class="line">        System.out.println(unsafe.getObject(new B(), baseOffset4)); &#x2F;&#x2F; Unsafe looks like a C Pointer</span><br><span class="line">        System.out.println(unsafe.getObject(new C(), baseOffset5)); &#x2F;&#x2F; null</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 并发情况下，因为同一对象的同一字段可能有多线程并发get或者set，线程栈内部的字段值和主存字段值可能会不一致，因此需要使用getIntVolatile方法获取字段值</span><br><span class="line">        System.out.println(unsafe.getIntVolatile(new A(), baseOffset1)); &#x2F;&#x2F; a&#x3D;1</span><br><span class="line">        System.out.println(unsafe.getIntVolatile(new B(), baseOffset2)); &#x2F;&#x2F; a&#x3D;1</span><br><span class="line">        System.out.println(unsafe.getIntVolatile(new C(), baseOffset3)); &#x2F;&#x2F; c&#x3D;3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        System.out.println(unsafe.getObjectVolatile(new B(), baseOffset4)); &#x2F;&#x2F; Unsafe looks like a C Pointer</span><br><span class="line"></span><br><span class="line">        &#x2F;* unsafe.getObjectVolatile返回的是Object类型，那么这个Object类型具体代表是Person类型、Integer类型、C类型还是其他什么类型？</span><br><span class="line">        * 转型的依据：cc字段是什么类型就转型为什么类型，例如在ConcurrentHashMap的源码中：</span><br><span class="line">        * seg &#x3D; (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)</span><br><span class="line">        * 直接从Segments数组取出位于u下标的Object具体来说一个Segment&lt;K,V&gt;类型元素，因此需将取出的Object转型为对应的Segment&lt;K,V&gt;类型</span><br><span class="line">        * 其实这里取到的是cc字段，因此可以转型为String类型</span><br><span class="line">        * 这里需要说明的是：String类型其实不需要显式转型</span><br><span class="line">        *&#x2F;</span><br><span class="line">        String ccField &#x3D; (String) unsafe.getObjectVolatile(new C(), baseOffset5);</span><br><span class="line">        System.out.println(ccField); &#x2F;&#x2F; null</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="关于使用getObject和getObjectVolatile返回值为Object的转型说明"><a href="#关于使用getObject和getObjectVolatile返回值为Object的转型说明" class="headerlink" title="关于使用getObject和getObjectVolatile返回值为Object的转型说明"></a>关于使用<code>getObject</code>和<code>getObjectVolatile</code>返回值为Object的转型说明</h4><p>两个方法源码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">native</span> Object <span class="title">getObject</span><span class="params">(Object var1, <span class="keyword">long</span> var2)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">native</span> Object <span class="title">getObjectVolatile</span><span class="params">(Object var1, <span class="keyword">long</span> var2)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这里需要清楚一个基本常识：</p>
<p>往内存某个地址put一个“Person”类型对象，那么从相同的地址取出来也应该是相同的Person类型对象，如果取出来是Dog类型，那么就不符合设计规范了！或者取出的Object应该转型为Person类型，你给它强制转换为Dog类型，编译器当然会抛出一个：<code>ClassCastException</code></p>
<p>因此使用getObject和getObjectVolatile方法时，需显式转型为具体类型的实例，以便操作该实例相关方法，这里以在ConcurrentHashMap的某个ensureSegment方法源码作为说明：</p>
<p><code>UNSAFE.getObjectVolatile(ss, u)</code>表示直接Segments数组取出位于u下标的Object，而这个Object结合上下文可知它是一个Segment<K,V>类型的实例，因此对取出的Object将其转型为对应的Segment<K,V>类型</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u))</span><br><span class="line">       == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (UNSAFE.compareAndSwapObject(ss, u, <span class="keyword">null</span>, seg = s))</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后你会发现在ConcurrentHashMap有13个关于getObjectXX类型方法，无一例外都是这样固定转型用法，不管是上面的ensureSegment方法还是下面segmentForHash方法：</p>
<p> <code>(Segment&lt;K,V&gt;) UNSAFE.getObjectVolatile(segments, u)</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Segment&lt;K,V&gt; <span class="title">segmentForHash</span><span class="params">(<span class="keyword">int</span> h)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE;</span><br><span class="line">    <span class="keyword">return</span> (Segment&lt;K,V&gt;) UNSAFE.getObjectVolatile(segments, u);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="使用Unsafe操作自定义类（并发写和读条件下）"><a href="#使用Unsafe操作自定义类（并发写和读条件下）" class="headerlink" title="使用Unsafe操作自定义类（并发写和读条件下）"></a>使用Unsafe操作自定义类（并发写和读条件下）</h4><p>首先使用以下demo程序在单线程情况下compareAndSwapInt的用法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hashmap.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sun.misc.Unsafe;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Counter</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">volatile</span> V value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Unsafe4</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Unsafe unsafe;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Field field = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">            field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">            unsafe = (Unsafe) field.get(<span class="keyword">null</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> NoSuchFieldException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> baseOffset = unsafe.objectFieldOffset(Counter.class.getDeclaredField(<span class="string">&quot;value&quot;</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1、单线程情况，对Counter的value字段进行读和写</span></span><br><span class="line">        Counter&lt;Integer&gt; counter=<span class="keyword">new</span> Counter&lt;&gt;();</span><br><span class="line">        System.out.println(unsafe.getInt(counter, baseOffset)); <span class="comment">// 0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、使用非CAS机制的putInt方法将counter的value字段写入10</span></span><br><span class="line">        unsafe.putInt(counter,baseOffset,<span class="number">10</span>);</span><br><span class="line">        <span class="comment">// 3、取出value字段值是否为10</span></span><br><span class="line">        System.out.println(unsafe.getInt(counter, baseOffset)); <span class="comment">// 10</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4、使用CAS机制的原子写入value的值，写入条件描述为：如果主存里面的value字段值等于所期待的10，那么就将该value字段值更新为新值20</span></span><br><span class="line">        <span class="keyword">boolean</span> isSet= unsafe.compareAndSwapInt(counter,baseOffset,<span class="number">10</span>,<span class="number">20</span>);</span><br><span class="line">        <span class="comment">// 5、从第3点可知，主存里面的value字段值一定为10，因此compareAndSwapInt会对value进行更新为20</span></span><br><span class="line">        System.out.println(isSet); <span class="comment">// true</span></span><br><span class="line">        System.out.println(unsafe.getInt(counter, baseOffset)); <span class="comment">// 20</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6、使用CAS机制的原子写入value的值，写入条件描述为：如果主存里面的value字段值等于所期待的21，那么就将该value字段值使用原子操作更新为新值30</span></span><br><span class="line">        <span class="keyword">boolean</span> isSet1= unsafe.compareAndSwapInt(counter,baseOffset,<span class="number">21</span>,<span class="number">30</span>);</span><br><span class="line">        <span class="comment">// 7、从第5点可知，主存里面的value字段值一定为20，显然不符合所期待的21，因此compareAndSwapInt会放弃本次对value写入（更新）</span></span><br><span class="line">        System.out.println(isSet1); <span class="comment">// false</span></span><br><span class="line">        System.out.println(unsafe.getInt(counter, baseOffset)); <span class="comment">// 20</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里有个简单的类比：</p>
<p>counter.value就好比桌子的一张白纸，假设白纸上当前写的初始值为0，同学A此时准备走到桌子前使用compareAndSwapInt去更改白纸的数字，但要满足这样条件：</p>
<p>1、如果同学A走到桌子前发现白纸的数字是0，与他期待的数字0一致，说明当前没有其他同学抢先更改白纸数字，因此同学A能安全的原子性更改白纸的数字，对其加1，也即同学A使用CAS对白纸的数字改为1，CAS返回true。</p>
<p>2、如果同学A走到桌子前发现白纸的数字是1，与他期待的数字0一致，说明在他之前已经有另外一个同学早就把白纸数字改为1，因此同学A此时需要放弃对白纸数字的写入，CAS返回false，那么同学A只能重新再到桌子上该，并且他期待白纸的数字是1</p>
<p>再看一下说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">unsafe.compareAndSwapInt(counter, baseOffest, expect, update);</span><br></pre></td></tr></table></figure>
<p>可以这样理解：对应counter实例，value字段对应的在主存相对地址为baseOffest，如果该value在主存的值等于线程栈空间存放的expect值，说明没有其他线程去主动更新主存的value字段，此时本线程使用compareAndSwapInt原子更新value字段一定不会产生冲突，而且能成功更新主存value的值。</p>
<p>这里有个理解技巧：<code>compareAndSwapInt(counter, baseOffest, expect, update)</code> 首先expect跟update是没有任何联系的，不要混淆错看为：expect等于update时才写入，这是非常典型的误解。CAS整个用法只关注线程栈空间的expect值即可，expect会被native方法拿去跟主存中的value字段比较。</p>
<p>有了以上“只关注线程栈空间的expect去比较主存的值”，接着我们再来理解Unsafe类的getAndSet：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getAndSet</span><span class="params">(V newValue)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            V x = get();</span><br><span class="line">            <span class="keyword">if</span> (compareAndSet(x, newValue))</span><br><span class="line">                <span class="keyword">return</span> x;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// compareAndSet其实封装了`compareAndSwapInt(counter, baseOffest, expect, update)`</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">compareAndSet</span><span class="params">(V expect, V update)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> unsafe.compareAndSwapObject(<span class="keyword">this</span>, valueOffset, expect, update);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>显然getAndSet的思想是：</p>
<p>1、先去主存去取值回来：x = get()</p>
<p>2、将取出的值作为expect，也即compareAndSet(x, newValue)的x变量</p>
<p>3、按照“只关注线程栈空间的expect去比较主存的值”思路：当前期待的值即为x，它被取出来后(暂时存放到线程栈空间)，然后再被拿去跟主存的x去比较，如果两者相等，说明在它之前没有其他线程去更改主存的x，这时当前线程就可以放心用newValue写入主存的x，而且写入操作是原子性的；否则进入下一次尝试</p>
<p>有了上面的CAS机制铺垫，以下是无锁并发的编程demo:</p>
<p>使用1万个线程对Counter类的value字段进行加1的CAS原子操作，如果能正确工作，那么最终打印counter.value的是10000</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hashmap.demo;</span><br><span class="line"><span class="keyword">import</span> sun.misc.Unsafe;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Unsafe5</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Counter</span> </span>&#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">volatile</span> <span class="keyword">int</span> value; <span class="comment">// 多线程情况下爱，value字段需要使用volatile原语定义，以保证线程间对它的可见性</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、创建unsafe实例以及获取counter.value字段的相对地址，static块是按照源码组织形式的写法</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Counter counter=<span class="keyword">new</span> Counter();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Unsafe unsafe;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> baseOffset;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Field field = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">            field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">            unsafe = (Unsafe) field.get(<span class="keyword">null</span>);</span><br><span class="line">            baseOffset = unsafe.objectFieldOffset(Counter.class.getDeclaredField(<span class="string">&quot;value&quot;</span>));</span><br><span class="line">            <span class="comment">//或者 baseOffset = unsafe.objectFieldOffset(counter.getClass().getDeclaredField(&quot;value&quot;));</span></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        List&lt;Thread&gt; threadList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">// 2、创建1万个线程，每个线程对counter.value进行加1操作，这里使用的自旋+CAS，也即直到当前线程能够成功完成对主存的value进行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i++) &#123;</span><br><span class="line">            Thread t = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">for</span> (; ; ) &#123;</span><br><span class="line">                        <span class="comment">// 在每个线程内部，使用CAS无锁操作对主存中的 counter实例的value字段进行原子加1，如果写入成功，则当前线程退出</span></span><br><span class="line">                        <span class="keyword">int</span> x = counter.value;</span><br><span class="line">                        <span class="keyword">if</span> (unsafe.compareAndSwapInt(counter, baseOffset, x, x + <span class="number">1</span>)) &#123;</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            threadList.add(t);</span><br><span class="line">            t.start();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(Thread t:threadList)&#123;</span><br><span class="line">            t.join();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// value最终的值应为100000</span></span><br><span class="line">        System.out.println(counter.value);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从此demo程序对于counter.value的加1操作设计，可以总结出一套基于unsafe的原子操作类的模板，这就是<code>java.util.concurrent.atomic</code>里面所有AtomicXXX的设计原理:</p>
<p>具体来说，java.util.concurrent.atomic中的类可以分成4种：</p>
<ul>
<li><p>基本类型：AtomicBoolean，AtomicInteger，AtomicLong</p>
</li>
<li><p>引用类型：AtomicReference</p>
</li>
<li><p>数组类：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray</p>
</li>
<li><p>更新器类：AtomicLongFieldUpdater，AtomicIntegerFieldUpdater，AtomicReferenceFieldUpdater</p>
</li>
<li><p>复合变量类：AtomicMarkableReference，AtomicStampedReference</p>
</li>
</ul>
<h4 id="CAS引起的ABA问题"><a href="#CAS引起的ABA问题" class="headerlink" title="CAS引起的ABA问题"></a>CAS引起的ABA问题</h4><p>CAS的思想主要是：如果主存的值跟expect值相同，说明主存的值没有被其他线程改动过，那么当前线程（以下称为T1）当然可以去原子写入</p>
<p>这里有个重要的前提，“如果主存的值跟expect值相同”，包含以下两种情况：</p>
<p>1、主存的值为A，在被T1更新前，这个A没有被其他线程更改过，那么就是我们熟悉的compareAndSet，T1一定可以CAS操作成功，这个没有问题。</p>
<p>2、主存的值为A，中间被其他线程改为B，随后又被其他线程改回A，那么对于T1来说它看到主存的值还是A（实际上此A已经非彼A），因此T1使用compareAndSet成功操作，尽管在这里T1是操作成功了，但CAS竟然无法发现“A变为B再变为A”的特殊情况，这会引起一些潜在的bug：</p>
<p>现在考察以下暴露的隐藏问题：</p>
<p>多个线程对栈进行操作，这个栈为：A-&gt;B-&gt;C-&gt;D，现在线程T1要使用compareAndSet(A,F)对这个栈的栈顶改为B，在T1准备CAS时，线程T2被cpu优先调度执行</p>
<p>线程T2对A,B,C出栈后，再对A入栈，此时栈变为A-&gt;D，线程T1此时被cpu调度执行</p>
<p>线程T1使用compareAndSet(A,F)，此刻它认为栈顶的A和它期待的A显然是相同的，于是更新栈顶为B，T1返回的栈为为F-&gt;D，问题出现在哪里？</p>
<p>正常来说在阻塞式多线程并发情况下：T1执行compareAndSet(A,F)后，T1返回的栈为F-&gt;B-&gt;C-&gt;D</p>
<p>但是在并发CAS情况下，T1执行compareAndSet(A,F)后，T1返回的栈为F-&gt;D，B和C两个元素就这样凭空消失了！！！</p>
<p>这就是所谓的“ABA问题”，根本原因就是CAS只是简单比较主存的值和expect的值比较是否相同，它无法发现主存的值是否被改变了多次</p>
<h4 id="如何解决ABA问题？"><a href="#如何解决ABA问题？" class="headerlink" title="如何解决ABA问题？"></a>如何解决ABA问题？</h4><p>CAS为对A添加一个版本号，即[A,oldVersion]，执行CAS时，不仅要比较主存的值和A是否相同，而且也要比较主存的值的版本号oldVersion有无变化，只有当</p>
<p>[主存的值,oldVersion] 等于[A,oldVersion]时，才可以执行原子写入操作。</p>
<p>这就是AtomicStampedReference的设计原理，它有一个内部类Pair，目的就是构造一个具有值和版本号标识的”结对子对象”：Pair(reference,stamp)，它可以唯一表示出</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> T reference;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> stamp;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Pair</span><span class="params">(T reference, <span class="keyword">int</span> stamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.reference = reference;</span><br><span class="line">        <span class="keyword">this</span>.stamp = stamp;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> Pair&lt;V&gt; pair;</span><br></pre></td></tr></table></figure>
<p>还是以“A同学更改桌子上白纸上的数字（或者字符）”作为例子说明</p>
<p>1、首先白纸上已经写着一个初始值“A”，在A的旁边还写着一个版本0</p>
<p>2、A同学成功更改白纸字符对应的过程：A同学手上拿着期待值“A”和版本号0走到桌子前，看到白纸上值恰好写着A而且版本也是0，说明没有其他同学主动来更改白纸，因此A同学可以使用CAS原子更改白纸的值，对应代码如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Unsafe6</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> String paper = <span class="string">&quot;A&quot;</span>;</span><br><span class="line">    <span class="comment">//asr：模拟桌子上的白纸，白纸的初始值为A，初始版本为0</span></span><br><span class="line">    <span class="keyword">static</span> AtomicStampedReference&lt;String&gt; asr = <span class="keyword">new</span> AtomicStampedReference&lt;&gt;(paper, <span class="number">0</span>);</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="comment">// 白纸被更新前对应的值和版本号</span></span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp()); <span class="comment">// A:0</span></span><br><span class="line">                <span class="comment">// A同学手上拿着期待值A和期待版本0，以及准备写入的新值B，准备写入的新版号</span></span><br><span class="line">                <span class="keyword">boolean</span> isSet = asr.compareAndSet(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="number">0</span>, <span class="number">1</span>); </span><br><span class="line">                System.out.println(isSet); <span class="comment">//true</span></span><br><span class="line">              	<span class="comment">// 白纸被CAS成功更新后对应的值和版本号</span></span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp());<span class="comment">// B:1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3、A同学放弃此次更改白纸字符对应的过程：同学A手上拿着期待值“A”和版本号0走到桌子，看到白纸上写着A但是版本号写着2，虽然值相同，但两者版本不一样，说明其他同学主动来更改白纸的值A，A同学放弃本次更新。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Unsafe6</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> String paper = <span class="string">&quot;A&quot;</span>;</span><br><span class="line">    <span class="keyword">static</span> AtomicStampedReference&lt;String&gt; asr = <span class="keyword">new</span> AtomicStampedReference&lt;&gt;(paper, <span class="number">0</span>);</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// B同学优先使用CAS对白纸进行写入</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp()); <span class="comment">// A:0</span></span><br><span class="line">                <span class="keyword">boolean</span> isSet = asr.compareAndSet(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">                System.out.println(isSet);<span class="comment">// true</span></span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp());<span class="comment">//B:1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">        Thread.sleep(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// C同学优先使用CAS对白纸进行写入</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp()); <span class="comment">//B:1</span></span><br><span class="line">                <span class="keyword">boolean</span> isSet = asr.compareAndSet(<span class="string">&quot;B&quot;</span>, <span class="string">&quot;A&quot;</span>, <span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">                System.out.println(isSet);<span class="comment">//true</span></span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp());<span class="comment">//A:2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        Thread.sleep(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等前面两个同学对白纸完成CAS操作后，白纸的值和版本号为A:2</span></span><br><span class="line">        <span class="comment">// 此时A同学手上拿着期待值A和期待版本0，以及准备写入的新值B，准备写入的新版号</span></span><br><span class="line">        <span class="comment">// 结果发现白纸的值虽然为A，但是版本号为2，因此isSet为false也即CAS操作失败,白纸的值维持为A</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp());<span class="comment">// A:2</span></span><br><span class="line">                <span class="keyword">boolean</span> isSet = asr.compareAndSet(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">                System.out.println(isSet);<span class="comment">//false</span></span><br><span class="line">                System.out.println(asr.getReference() + <span class="string">&quot;:&quot;</span> + asr.getStamp());<span class="comment">//A:2</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>有了demo例子的铺垫，结合前面的unsafe多个demo的案例，AtomicStampedReference就是基于compareAndSwapObject方法进行设计的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);</span><br></pre></td></tr></table></figure>
<p>AtomicStampedReference在内部中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">UNSAFE.compareAndSwapObject(<span class="keyword">this</span>, pairOffset, expectPair, newPair);</span><br></pre></td></tr></table></figure>
<p>源码解析：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> java.util.concurrent.atomic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AtomicStampedReference</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> T reference; <span class="comment">// 用于被比较的引用类型（当然可以传入任意类型，因为这里是T泛型)，在以下被称为值</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> stamp;  <span class="comment">//  版本号</span></span><br><span class="line">      </span><br><span class="line">      	<span class="comment">// 内部结对类，是辅助类，用于解决ABA的关键手段之一</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">Pair</span><span class="params">(T reference, <span class="keyword">int</span> stamp)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.reference = reference;</span><br><span class="line">            <span class="keyword">this</span>.stamp = stamp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 用于构建一个值和版本号绑在一起的“结对”实例</span></span><br><span class="line">        <span class="keyword">static</span> &lt;T&gt; <span class="function">Pair&lt;T&gt; <span class="title">of</span><span class="params">(T reference, <span class="keyword">int</span> stamp)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Pair&lt;T&gt;(reference, stamp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// unsafe进行CAS操作的实例object，考虑并发场景下，需要使用volatile内存可见性原语</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> Pair&lt;V&gt; pair;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 使用促使的引用类型实例和促使版本号构造AtomicStampedReference实例，例如</span></span><br><span class="line"><span class="comment">    AtomicStampedReference&lt;String&gt; asr = new AtomicStampedReference&lt;&gt;(&quot;A&quot;, 0);</span></span><br><span class="line"><span class="comment">    那么这里initialRef就是&quot;A&quot;，initialStamp就是0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">AtomicStampedReference</span><span class="params">(V initialRef, <span class="keyword">int</span> initialStamp)</span> </span>&#123;</span><br><span class="line">        pair = Pair.of(initialRef, initialStamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 只能返回获取值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> V <span class="title">getReference</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pair.reference;</span><br><span class="line">    &#125;</span><br><span class="line">		<span class="comment">// 只能返回获取值对应的版本号</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getStamp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pair.stamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//可以同时返回值和版本号，注意这里返回的版本号是放在入参数组stampHolder[0]中,至于为何不是直接用整型变量返回版本号？使用整型数据更方便让get方法将版本号放在给定的入参数据里，算是普通思路。</span></span><br><span class="line">    <span class="comment">/*常见用法：</span></span><br><span class="line"><span class="comment">      int[] stampHolder=new int[1];</span></span><br><span class="line"><span class="comment">      这时asr.get(stampHolder)取到值，而stampHolder[0]就取到值对应的版本号</span></span><br><span class="line"><span class="comment">      System.out.println(&quot;ref:&quot;+asr.get(stampHolder)+&quot; stamp:&quot;+stampHolder[0]);</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(<span class="keyword">int</span>[] stampHolder)</span> </span>&#123;</span><br><span class="line">        Pair&lt;V&gt; pair = <span class="keyword">this</span>.pair;</span><br><span class="line">        stampHolder[<span class="number">0</span>] = pair.stamp;</span><br><span class="line">        <span class="keyword">return</span> pair.reference;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 跟compareAndSet类似，可以看做是弱CAS，不常用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">weakCompareAndSet</span><span class="params">(V   expectedReference,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     V   newReference,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">int</span> expectedStamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">int</span> newStamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> compareAndSet(expectedReference, newReference,</span><br><span class="line">                             expectedStamp, newStamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    该方法就是用户调用的CAS方法，这里做了一个优化小技巧：如果新值和新版本号恰好等于主存的的值以及主存的版本号，显然连CAS操作都省了，直接返回true，有点点耍小聪明和碰运气</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">compareAndSet</span><span class="params">(V   expectedReference,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 V   newReference,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">int</span> expectedStamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">int</span> newStamp)</span> </span>&#123;</span><br><span class="line">        Pair&lt;V&gt; current = pair;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">          <span class="comment">// 通过对比期待值和主存值、期待版本号和主存值对应的版本号，可以确保不再出现所谓的ABA问题，能“感知”值的改变次数</span></span><br><span class="line">            expectedReference == current.reference &amp;&amp;</span><br><span class="line">            expectedStamp == current.stamp &amp;&amp;</span><br><span class="line">            ((newReference == current.reference &amp;&amp;</span><br><span class="line">              newStamp == current.stamp) ||</span><br><span class="line">             casPair(current, Pair.of(newReference, newStamp)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		<span class="comment">// 没有使用CAS去更新，只是简单的更新操作，这一方法适合用在单线程或者冲突不严重的并发情况，看你对业务的经验</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(V newReference, <span class="keyword">int</span> newStamp)</span> </span>&#123;</span><br><span class="line">        Pair&lt;V&gt; current = pair;</span><br><span class="line">        <span class="keyword">if</span> (newReference != current.reference || newStamp != current.stamp)</span><br><span class="line">            <span class="keyword">this</span>.pair = Pair.of(newReference, newStamp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">   不管版本号是否相同，只要期待值和主存的值相等，就用新版本号更新主存值的旧版本号，</span></span><br><span class="line"><span class="comment">   注意：每次调用该方法都可能会更新失败，需要结合自旋直到更新成功为止</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">attemptStamp</span><span class="params">(V expectedReference, <span class="keyword">int</span> newStamp)</span> </span>&#123;</span><br><span class="line">        Pair&lt;V&gt; current = pair;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">            expectedReference == current.reference &amp;&amp;</span><br><span class="line">            (newStamp == current.stamp ||</span><br><span class="line">             casPair(current, Pair.of(expectedReference, newStamp)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unsafe mechanics</span></span><br><span class="line">		<span class="comment">// 以下就是前面我们熟悉的unsafe相关操作：创建unsafe实例、pair的相对地址</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> sun.misc.Unsafe UNSAFE = sun.misc.Unsafe.getUnsafe();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> pairOffset =</span><br><span class="line">        objectFieldOffset(UNSAFE, <span class="string">&quot;pair&quot;</span>, AtomicStampedReference.class);</span><br><span class="line"></span><br><span class="line">  	<span class="comment">// 到了这里才是真正执行对主存对象进行操作的逻辑，可以看到这里使用compareAndSwapObject(Object var1, long var2, Object var4, Object var5);</span></span><br><span class="line">    <span class="comment">// 注意对比前面的unsafe.compareAndSwapInt(被更新的对象,相对地址,期待值,新值);</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">casPair</span><span class="params">(Pair&lt;V&gt; cmp, Pair&lt;V&gt; val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> UNSAFE.compareAndSwapObject(<span class="keyword">this</span>, pairOffset, cmp, val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">long</span> <span class="title">objectFieldOffset</span><span class="params">(sun.misc.Unsafe UNSAFE,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  String field, Class&lt;?&gt; klazz)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> UNSAFE.objectFieldOffset(klazz.getDeclaredField(field));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NoSuchFieldException e) &#123;</span><br><span class="line">            <span class="comment">// Convert Exception to corresponding Error</span></span><br><span class="line">            NoSuchFieldError error = <span class="keyword">new</span> NoSuchFieldError(field);</span><br><span class="line">            error.initCause(e);</span><br><span class="line">            <span class="keyword">throw</span> error;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>与AtomicStampedReference的内部的结对对象是[值,版本]设计思想类似的还有AtomicMarkableReference，它内部是结对对象是[值,标志位]，也即[值,1]或者[值,0]，如下面所示</p>
<p>AtomicStampedReference内部是结对类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> T reference;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> stamp;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">Pair</span><span class="params">(T reference, <span class="keyword">int</span> stamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.reference = reference;</span><br><span class="line">    <span class="keyword">this</span>.stamp = stamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AtomicMarkableReference内部是结对类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> T reference;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">boolean</span> mark;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">Pair</span><span class="params">(T reference, <span class="keyword">boolean</span> mark)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.reference = reference;</span><br><span class="line">    <span class="keyword">this</span>.mark = mark;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它们两个在使用场合有什么不同呢？</p>
<p>AtomicStampedReference可以给值(reference)加上版本号后，可以追踪到这个值的整个变化过程，如：<br><code>[A,0] =&gt; [B,1] =&gt; [C,2] =&gt; [A,3]</code>，通过getStamp返回的版本号大小，可以知道值在中途被更改了3次。</p>
<p>场景另外一种并发场景：也需要你并不关心值（reference）中途更改了多少次，而是只关注这个主存的值是否有被修改过，AtomicMarkableReference显然很适合。</p>
<p>ABA 问题</p>
<p><a href="https://blog.csdn.net/tiandao321/article/details/80811103?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control">https://blog.csdn.net/tiandao321/article/details/80811103?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control</a></p>
<h4 id="ReentrantLock里面的阻塞锁和非柱塞锁的区别"><a href="#ReentrantLock里面的阻塞锁和非柱塞锁的区别" class="headerlink" title="ReentrantLock里面的阻塞锁和非柱塞锁的区别"></a>ReentrantLock里面的阻塞锁和非柱塞锁的区别</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> ReentrantLock reentrantLock= <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        reentrantLock.lock();</span><br><span class="line">        System.out.println(<span class="string">&quot;线程A在运行中&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5</span>*<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        reentrantLock.unlock();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line">    Thread.sleep(<span class="number">1</span>);<span class="comment">// 让线程A先运行</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 线程A在工作时，线程B被阻塞了，啥事都做不了，干等，浪费资源</span></span><br><span class="line">    <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            reentrantLock.lock();</span><br><span class="line">            System.out.println(<span class="string">&quot;线程B在运行中&quot;</span>);</span><br><span class="line">            reentrantLock.unlock();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从上面的运行结果来看：线程A在工作时，线程B被阻塞了，啥事都做不了，干等，浪费资源</p>
<p>如果使用非阻塞锁，那么线程A在工作时，线程B不会被阻塞，线程B可以自由做其他事情，没有白白自旋</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> ReentrantLock reentrantLock= <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        reentrantLock.lock();</span><br><span class="line">        System.out.println(<span class="string">&quot;线程A在运行中&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5</span>*<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        reentrantLock.unlock();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line">    Thread.sleep(<span class="number">1</span>);<span class="comment">// 让线程A先运行</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 线程A在工作时，使用tryLock非阻塞锁，这样线程B不会被线程A阻塞，线程A工作的同时，线程B也可以去干点别的事情！</span></span><br><span class="line">    <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (!reentrantLock.tryLock())&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;线程B在工作中，对链表扫描是否存在key节点&quot;</span>);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            reentrantLock.unlock();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：可以看到在第5秒后，线程B获取到了锁因此退出while循环，使用trylock非阻塞锁后，在这5秒时间内，至少线程B没有白白浪费，而是干了一些查询key节点的工作，这就是scanAndLockForPut的设计思想</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">线程A在运行中</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br></pre></td></tr></table></figure>
<p>对比scanAndLockForPut的<code>while (!tryLock())</code>即可理解其设计思想：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> HashEntry&lt;K,V&gt; <span class="title">scanAndLockForPut</span><span class="params">(K key, <span class="keyword">int</span> hash, V value)</span> </span>&#123;</span><br><span class="line">    HashEntry&lt;K,V&gt; first = entryForHash(<span class="keyword">this</span>, hash);</span><br><span class="line">    HashEntry&lt;K,V&gt; e = first;</span><br><span class="line">    HashEntry&lt;K,V&gt; node = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">int</span> retries = -<span class="number">1</span>; <span class="comment">// negative while locating node</span></span><br><span class="line">    <span class="keyword">while</span> (!tryLock()) &#123; <span class="comment">// 某线程没有获取锁的期间，顺便完成了链表的遍历以及新key节点的创建工作</span></span><br><span class="line"> <span class="comment">// ......         </span></span><br><span class="line">         node = <span class="keyword">new</span> HashEntry&lt;K,V&gt;(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">  			<span class="comment">// ...... </span></span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：jdk1.8 ConcurrentHashMap的TreeBin读写锁竞争机制讨论</title>
    <url>/2021/07/18/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9Ajdk1.8%20ConcurrentHashMap%E7%9A%84TreeBin%E8%AF%BB%E5%86%99%E9%94%81%E7%AB%9E%E4%BA%89%E6%9C%BA%E5%88%B6%E8%AE%A8%E8%AE%BA/</url>
    <content><![CDATA[<p>本文接前面ConcurrentHashMap文章的内容，继续深入到TreeBin这个特殊节点的读写锁竞争机制。</p>
<h4 id="7、TreeBin类设计原理"><a href="#7、TreeBin类设计原理" class="headerlink" title="7、TreeBin类设计原理"></a>7、TreeBin类设计原理</h4><p>对TreeBin类深入分析，不仅能够理解为何CHM能支持并发读的底层实现，而且也能加深jk1.8的ConcurrentHashMap整体设计原理。本文将详细深入地解析TreeBin优秀的读写锁控制设计，此部分内容在全网的相关文章很少涉及。</p>
<h5 id="7-1-保证加锁对象不改变的设计思想"><a href="#7-1-保证加锁对象不改变的设计思想" class="headerlink" title="7.1 保证加锁对象不改变的设计思想"></a>7.1 保证加锁对象不改变的设计思想</h5><p>首先看其源码的注释说明：</p>
<blockquote>
<p>TreeNodes used at the heads of bins. TreeBins do not hold user keys or values, but instead point to list of TreeNodes and their root. They also maintain a parasitic read-write lock forcing writers (who hold bin lock) to wait for readers (who do not) to complete before tree restructuring operations.</p>
</blockquote>
<p>TreeBins节点不是用于放置key和value，而是用于指向TreeNodes和TreeNodes的根节点。TreeBins内部会维护一把读写锁，用于保证在树重构前，持有锁的写线程被强制等待无锁读线程完成。</p>
<p>当然这注释也未能回答这样核心问题：为何对于桶位是红黑树的情况下，CHM放置的一个TreeBin节点，而不像HashMap那样放置一个TreeNode节点？</p>
<p>首先看看TreeBin的使用场景，在put场景下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">            <span class="comment">// </span></span><br><span class="line">          	<span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// ..</span></span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// </span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                Node&lt;K,V&gt; p;</span><br><span class="line">                binCount = <span class="number">2</span>;</span><br><span class="line">                <span class="keyword">if</span> ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,</span><br><span class="line">                                               value)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    oldVal = p.val;</span><br><span class="line">                    <span class="keyword">if</span> (!onlyIfAbsent)</span><br><span class="line">                        p.val = value;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>当key所在桶位可以放入key时，先对头节点加锁<code>synchronized (f)</code>，注意这是独占锁，要求头节点对象在锁期间不会改变，否则就不能锁住同一对象。</p>
<p>为论证f头节点是<code>TreeNode</code>类型不适用并发的CHM场景，不妨假设CHM使用TreeNode作为CHM桶位上红黑树的头节点，于是在put场景下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">            <span class="comment">// ① 假设当前头节点f是一个TreeNode节点，则执行流会进入②分支</span></span><br><span class="line"><span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">                <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">                    <span class="comment">// </span></span><br><span class="line">                  	<span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">								<span class="comment">// ..</span></span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// ② 这里已经将TreeBin换成TreeNode类型</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeNode) &#123;</span><br><span class="line">                        Node&lt;K,V&gt; p;</span><br><span class="line">                        binCount = <span class="number">2</span>;</span><br><span class="line">                      	<span class="comment">// ③ 将key和value插入到红黑树当中</span></span><br><span class="line">                        <span class="keyword">if</span> ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,</span><br><span class="line">                                                       value)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            oldVal = p.val;</span><br><span class="line">                            <span class="keyword">if</span> (!onlyIfAbsent)</span><br><span class="line">                                p.val = value;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>这里关键是第③步逻辑：将key和value插入到红黑树当中，会发生什么事情？</p>
<p>我们知道，在HashMap节点，将一个节点put入红黑树后，需要做插入平衡处理和将红黑树root节点移到双向链表的头节点位置，目的是为了保证桶位上的头节点即是红黑树的根节点也是双向链表的头结点，下面就是HashMap对应的操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">putTreeVal</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">int</span> h, K k, V v)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 插入平衡处理和将红黑树root节点移到双向链表的头节点位置</span></span><br><span class="line">            moveRootToFront(tab, balanceInsertion(root, x));</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>但对于并发CHM来说，红黑树的插入平衡处理会导致root节点发生了改变（例如插入平衡前根节点是treeNodeA，插入平衡后根节点是treeNodeB）而不是位于桶位头节点上，如果CHM桶位头节点还是TreeNode，那么就会出现以下图示的不能保证写线程独占操作的线程不安全情况。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/54c658513c0a61c8d834b71268ad8b52.png" alt="TreeBin改为TreeNode的锁情况.001"></p>
<p>如何解决以上遇到的问题？</p>
<a id="more"></a>
<p>Doug Lea为此设计了TreeBin类（节点），该节点只放在桶位头节点上，它内部“包装”一棵红黑树（有些文章会跟你说TreeBin是 TreeNode的代理结点，其实意思都一样）。加锁时，对桶位头节点上TreeBin节点进行加锁，内部的红黑树根节点root在调整平衡后不管如何变化，当前桶位的加锁对象——TreeBin节点保持不变，也即保证写操作独占性，如下图所示。此外TreeBin还支持高级特性：并发环境下的读写锁控制机制。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6dfa0d55150d2c596cca7bafb704cfc3.png" alt="TreeBin对象加锁不变.0001"></p>
<p>而且这样设计的TreeBin还有另外一个收益：无需进行类似HashMap的<code>moveRootToFront</code> 操作，因为桶头节点不再要求是</p>
<p>红黑树的根节点root，也不是链表的first头节点，而是一个包装了TreeNodes的TreeBin节点。</p>
<p>以下是HashMap的插入节点后需要做<code>moveRootToFront</code> 操作的源码片段：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// putVal方法内部代码片段    </span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">    e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line"><span class="comment">// putTreeVal方法内部代码片段    </span></span><br><span class="line">moveRootToFront(tab, balanceInsertion(root, x));</span><br></pre></td></tr></table></figure>
<p>以下是CHM插入新节点后的插入平衡操作：在插入平衡操作前，采用cas加锁机制，显然不同于HashMap，这个加锁机制有点复杂，目的是什么呢？在后面7.3小节会提到。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">      lockRoot();</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">// 仅有插入平衡操作，没有moveRootToFront操作，因为桶位头节点一直都是TreeBin节点，这个节点的hash值为-2，没有key和value</span></span><br><span class="line">          root = balanceInsertion(root, x); </span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          unlockRoot();</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>此外，在源码中的lockRoot等地方的注释中，有个短语：tree restructuring，表示：红黑树重新调整结构。</p>
<p>而tree restructuring operations则表示：红黑树重新调整结构操作，具体来说是以下两个方法</p>
<p>put方面内部的putTreeVal方法以及remove方法内部的removeTreeNode方法，也即红黑树插入一个新节点需要触发tree restructuring 操作，红黑树删除一个节点需要触发tree restructuring 操作：执行tree restructuring操作前需要使用lockRoot()获取写锁，调整完后使用unlockRoot()释放写锁。</p>
<h5 id="7-2-TreeBin类"><a href="#7-2-TreeBin类" class="headerlink" title="7.2 TreeBin类"></a>7.2 TreeBin类</h5><p>TreeBin作为CHM内部类，有部分设计是新设计，用于解决高并发条件下的读写竞争问题，而另外一部分设计则沿用HashMap红黑树部分操作方法：如<code>rotateLeft</code>、<code>rotateRight</code>、<code>balanceInsertion</code>、<code>balanceDeletion</code> 等，这几个方法是在synchronized(f)独占锁条件系进行的，因此跟HashMap原来的执行机制类似，此处不再累赘。本章重点放在新的设计上。</p>
<h6 id="7-2-1-基本的成员变量"><a href="#7-2-1-基本的成员变量" class="headerlink" title="7.2.1 基本的成员变量"></a>7.2.1 基本的成员变量</h6><p>基本成员变量具体使用在7.3小节的读写竞争设计给出，waiter、lockState以及lockState对应的三个标记值，再结合位运算方式，非常巧妙的实现了TreeBin读写锁竞争！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeBin</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    TreeNode&lt;K,V&gt; root; <span class="comment">// 桶位上红黑树根节点的引用</span></span><br><span class="line">    <span class="keyword">volatile</span> TreeNode&lt;K,V&gt; first; <span class="comment">// 因为TreeNode节点还有next属性，因此红黑树本身也是一条链表，first就是该链表的头节点，常常用在需要线性遍历节点的场景</span></span><br><span class="line">    <span class="keyword">volatile</span> Thread waiter; <span class="comment">// 参考7.3.1小节内容</span></span><br><span class="line">    <span class="keyword">volatile</span> <span class="keyword">int</span> lockState; <span class="comment">// 直译：锁状态，用于表征TreeBin对象当前的锁状态是什么，对应以下三种状态</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">// values for lockState</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> WRITER = <span class="number">1</span>; <span class="comment">// set while holding write lock 写线程给lockState进行cas设1以此获得写锁：0001</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> WAITER = <span class="number">2</span>; <span class="comment">// set when waiting for write lock  写线程将lockState设为2就会转成“等待线程”：0010</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> READER = <span class="number">4</span>; <span class="comment">// increment value for setting read lock 桶位每来一个读线程就会对lockState进行cas加4操作：0100</span></span><br></pre></td></tr></table></figure>
<p>看完后文，你可以发现为何采用1、2、4这样的值，而不是1，2，3或者0，1，2或者2，4，6等，这与读写锁竞争的算法设计有关。</p>
<h6 id="7-2-2-构造方法"><a href="#7-2-2-构造方法" class="headerlink" title="7.2.2 构造方法"></a>7.2.2 构造方法</h6><p>以下只给出部分片段代码，省略部分的源码片段，其逻辑与HashMap类似，不再给出。</p>
<p>TreeBin构造方法目的只有一个：基于桶位链表构建一棵红黑树，例如在treeifyBin方法内部的 <code>setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd))</code></p>
<blockquote>
<p>TreeBin构造方法在treeifyBin方法被调用，此外还有以下两种情况也会调用TreeBin的构造方法，在优先重点关注”TreeBin构造方法在treeifyBin方法被调用”的场景。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates bin with initial set of nodes headed by b.</span></span><br><span class="line"><span class="comment"> 调用方：treeifyBin方法里面的setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)) ，这里的hd就是桶位链表头节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">TreeBin(TreeNode&lt;K,V&gt; b) &#123;</span><br><span class="line">    <span class="keyword">super</span>(TREEBIN, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>); <span class="comment">//TREEBIN的值为-2，作为TreeBin节点的hash值：hash for roots of trees，这个特殊的hash值一般用在桶位节点类型判断上，fwd节点是-1，因此只要桶位节点的hash值&lt;0，就可以判断当前桶位节点不是链表</span></span><br><span class="line">    <span class="keyword">this</span>.first = b; <span class="comment">// 红黑树本身也是一条链表，这里的first表示链表的头节点，也即hd链表头节点</span></span><br><span class="line">    TreeNode&lt;K,V&gt; r = <span class="keyword">null</span>;</span><br><span class="line">  	<span class="comment">// ①外层for循环用于从链表取出节点</span></span><br><span class="line">    <span class="keyword">for</span> (TreeNode&lt;K,V&gt; x = b, next; x != <span class="keyword">null</span>; x = next) &#123;</span><br><span class="line">        next = (TreeNode&lt;K,V&gt;)x.next;</span><br><span class="line">        <span class="comment">// ②内层循环用于将链表遍历节点插入到红黑树合适位置，插入并完成调整则完成了链表上一个节点的转移，回到外层for循环继续转移链表下一个节点</span></span><br><span class="line">      	<span class="keyword">for</span> (TreeNode&lt;K,V&gt; p = r;;) &#123;</span><br><span class="line">             r = balanceInsertion(r, x);</span><br><span class="line">             <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然也可以使用逆向思维去找出“TreeBin构造方法”在什么场合被调用？</p>
<p>什么场景下，CHM的table桶位上才能放置一个TreeBin节点？</p>
<p>自然是桶位上已经是一棵红黑树？</p>
<p>那么桶位上的红黑树是怎么来？</p>
<p>桶位上的冲突链表达到树化阈值后，使用treeifyBin基于链表构建了红黑树。</p>
<p>找到答案：treeifyBin方法内部一定有TreeBin构造方法调用，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeifyBin</span><span class="params">(Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; b; <span class="keyword">int</span> n, sc;</span><br><span class="line">    <span class="keyword">if</span> (tab != <span class="keyword">null</span>) &#123;</span><br><span class="line">      	<span class="comment">// ①如果table长度还未达到64，优先使用扩容逻辑而不是转红黑树逻辑</span></span><br><span class="line">        <span class="keyword">if</span> ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY)</span><br><span class="line">            tryPresize(n &lt;&lt; <span class="number">1</span>);</span><br><span class="line">      	<span class="comment">// ②说明当前CHM需要树化处理</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((b = tabAt(tab, index)) != <span class="keyword">null</span> &amp;&amp; b.hash &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">          	<span class="comment">// ③锁住当前桶位链表头节点</span></span><br><span class="line">            <span class="keyword">synchronized</span> (b) &#123;</span><br><span class="line">              	 <span class="comment">// 再次检查桶位头节点前后时刻有无改动，没改动的情况下，才能进行写操作</span></span><br><span class="line">                <span class="keyword">if</span> (tabAt(tab, index) == b) &#123;</span><br><span class="line">                  	<span class="comment">// 红黑树本身也是一条链表</span></span><br><span class="line">                    TreeNode&lt;K,V&gt; hd = <span class="keyword">null</span>, tl = <span class="keyword">null</span>;</span><br><span class="line">                  	<span class="comment">// ④链表Node节点转为TreeNode节点，并拷贝到新链表hd-&gt;...-&gt;tl</span></span><br><span class="line">                    <span class="keyword">for</span> (Node&lt;K,V&gt; e = b; e != <span class="keyword">null</span>; e = e.next) &#123;</span><br><span class="line">                        TreeNode&lt;K,V&gt; p =</span><br><span class="line">                            <span class="keyword">new</span> TreeNode&lt;K,V&gt;(e.hash, e.key, e.val,</span><br><span class="line">                                              <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">                        <span class="keyword">if</span> ((p.prev = tl) == <span class="keyword">null</span>)</span><br><span class="line">                            hd = p;</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            tl.next = p;</span><br><span class="line">                        tl = p;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// ⑤这里就是TeeBin构造方法调用点！！使用cas在index这个桶位上放置一个TreeBin节点，而且可以看到TreeBin节点的入参是hd链表头结点</span></span><br><span class="line">                    setTabAt(tab, index, <span class="keyword">new</span> TreeBin&lt;K,V&gt;(hd));</span><br></pre></td></tr></table></figure>
<p>有人会问，进入②逻辑开始构建红黑树，但从代码上只看到拷贝了一条hd新链表的逻辑，完全没有构建红黑树的逻辑，这是怎么回事？</p>
<font color=red>这是因为真正构建红黑树的逻辑是在new TreeBin<K,V>(hd)时才被创建，也即CHM桶位上红黑树是由TreeBin构造方法内部代理创建的。</font>

<p>此外还有以下两种情况也会调用TreeBin的构造方法：</p>
<p>1、在扩容阶段transfer方法里面，转移桶位链表到新table上是有对应的<code>new TreeBin&lt;K,V&gt;</code> ,也即在新table的i位置上基于低位节点链表构建红黑树，在新table的i+n位置上基于高位节点链表构建红黑树。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">                    ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) :</span><br><span class="line"><span class="comment">// TreeBin构造方法的调用点</span></span><br><span class="line">                        (hc != <span class="number">0</span>) ? <span class="keyword">new</span> TreeBin&lt;K,V&gt;(lo) : t;</span><br><span class="line">                    hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) :</span><br><span class="line">                        (lc != <span class="number">0</span>) ? <span class="keyword">new</span> TreeBin&lt;K,V&gt;(hi) : t;</span><br><span class="line">                    setTabAt(nextTab, i, ln);</span><br><span class="line">                    setTabAt(nextTab, i + n, hn);</span><br><span class="line">                    setTabAt(tab, i, fwd);</span><br></pre></td></tr></table></figure>
<p>2、将文件流（本地文件或者socket流）反序列为CHM对象时，readObject也会调用TreeBin的构造方法来构建红黑树实例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Reconstitutes the instance from a stream (that is, deserializes it).</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> s the stream</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">readObject</span><span class="params">(java.io.ObjectInputStream s)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> java.io.IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * To improve performance in typical cases, we create nodes</span></span><br><span class="line"><span class="comment">     * while reading, then place in table once size is known.</span></span><br><span class="line"><span class="comment">     * However, we must also validate uniqueness and deal with</span></span><br><span class="line"><span class="comment">     * overpopulated bins while doing so, which requires</span></span><br><span class="line"><span class="comment">     * specialized versions of putVal mechanics.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 省略部分</span></span><br><span class="line">        <span class="keyword">while</span> (p != <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="comment">// 1、基于TreeNode的next属性先构建出一条链表hd		</span></span><br><span class="line">                        TreeNode&lt;K,V&gt; hd = <span class="keyword">null</span>, tl = <span class="keyword">null</span>;</span><br><span class="line">                        <span class="keyword">for</span> (q = p; q != <span class="keyword">null</span>; q = q.next) &#123;</span><br><span class="line">                            TreeNode&lt;K,V&gt; t = <span class="keyword">new</span> TreeNode&lt;K,V&gt;</span><br><span class="line">                                (q.hash, q.key, q.val, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">                            <span class="keyword">if</span> ((t.prev = tl) == <span class="keyword">null</span>)</span><br><span class="line">                                hd = t;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                tl.next = t;</span><br><span class="line">                            tl = t;</span><br><span class="line">                        &#125;</span><br><span class="line">          				<span class="comment">// 2、在桶位j上，使用TreeBin构造方法构建一棵红黑树，节点来自上面的hd链表</span></span><br><span class="line">                        setTabAt(tab, j, <span class="keyword">new</span> TreeBin&lt;K,V&gt;(hd));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<h5 id="7-2-3-putval方面-binCount-2的含义？"><a href="#7-2-3-putval方面-binCount-2的含义？" class="headerlink" title="7.2.3 putval方面 binCount = 2的含义？"></a>7.2.3 putval方面 binCount = 2的含义？</h5><p>在put操作里面，我们知道底层是调用了putval方法，有一个逻辑很难理解：</p>
<p>为何当前桶位节点是TreeBin节点时，binCount=2，而且不需要继续累加计数？ 为何不是跟链表一样从1开始计数？（binCount计算当前桶位的节点数）</p>
<p>或者这么提问：binCount是用于统计桶位链表节点数量以便判断是否达到树化阈值，但TreeBin里面已经是一棵红黑树了，那么binCount岂不是没有什么实际意义？为何还需要binCount = 2，用意何在？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">    Node&lt;K,V&gt; p;</span><br><span class="line">    binCount = <span class="number">2</span>; <span class="comment">// 这里binCount不会进行累加计数，为何还需要binCount = 2，有什么用意？</span></span><br><span class="line">    <span class="keyword">if</span> ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,</span><br><span class="line">                                   value)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        oldVal = p.val;</span><br><span class="line">        <span class="keyword">if</span> (!onlyIfAbsent)</span><br><span class="line">            p.val = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从7.2.2的构造方法的内容可知，桶位上的TreeBin节点是用下面这样的代码构建成的，以treeifyBin为例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setTabAt(tab, index, <span class="keyword">new</span> TreeBin&lt;K,V&gt;(hd));</span><br></pre></td></tr></table></figure>
<p>这里的hd是桶位链表头节点，算1个，TreeBin本身算1个，共两个，这里binCount=2不是说TreeBin只有两个节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TreeBin(TreeNode&lt;K,V&gt; hd) &#123;</span><br><span class="line">    <span class="keyword">super</span>(TREEBIN, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>); <span class="comment">// 这里就可以表示1个节点：TreeBin本身算1个，hash值为-2</span></span><br></pre></td></tr></table></figure>
<p>binCount=2的实际用意是：能够让addCount的分支2执行判断是否需要扩容，从而TreeBin的红黑树也可以被扩容</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//addCount(1L, binCount=2);</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">    	<span class="comment">// 省略部分</span></span><br><span class="line">      <span class="comment">// 能够让addCount的分支2执行判断是否需要扩容，从而TreeBin的红黑树也可以被扩容</span></span><br><span class="line">        <span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">            <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                   (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                <span class="keyword">int</span> rs = resizeStamp(n);</span><br></pre></td></tr></table></figure>
<h5 id="7-3-TreeBin读写锁的设计"><a href="#7-3-TreeBin读写锁的设计" class="headerlink" title="7.3 TreeBin读写锁的设计"></a>7.3 TreeBin读写锁的设计</h5><p>深度分析TreeBin读写锁的设计非常有利于理解CHM在红黑树结构上并发读写机制，而且TreeBin读写锁的设计也相当优秀。</p>
<p>在对TreeBin进行写操作时，由于已经使用了<code>synchronized(f)</code>独占锁语义，表示仅有一个写线程进入写操作，此设计解决了写线程与写线程之间竞争，而读线程和写线程之间的竞争如果采用<code>synchronized</code>方式，在高并发条件下性能无法接受。为了达到高性能的读写锁竞争，肯定需要熟悉的lock-free技术：CAS机制。</p>
<font color=red>TreeBin读写竞争机制容易给人一种惯性思维：如果有写线程在写，那么读线程就不能读。相信这是所有没有深入研究过TreeBin读写锁源码的人会这样理解，遗憾的是：这种理解从TreeBin底层实现来看是错误的。</font>

<font color=red>TreeBin读写竞争机制实际是这样的：即使有写线程在写，读线程依然可以并发读！具体实现参考7.3.2小节</font> ：

其他说明：

>本文提到的写线程是指：
>
>put入一个节点后（或删除一个节点后），准备执行插入平衡（删除平衡）操作的线程。或者简单说：在TreeBin节点行进行put或者remove操作的线程。同一时刻只能有一个写线程进行写操作。
>
>本文提到的读线程是指：
>
>使用get获取（find）TreeBin上指定的key的线程，同一时刻可以有多个读线程并发进行读操作。

由于Doug  Lea使用位运算设计TreeBin读写锁获取的条件，使得其源码理解上有些晦涩难懂，具体看后面的章节分析。

##### 7.3.1 TreeBin写线程竞争write lock的设计

在putTreeVal方法里面，插入节点后需要做插入平衡处理：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    lockRoot();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        root = balanceInsertion(root, x);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        unlockRoot();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

不同于HashMap的设计，这里在插入平衡前加入了lockRoot设计：put写线程竞争到当前TreeBin对象write lock才能实施插入平衡操作

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Acquires write lock for tree restructuring.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//  </span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lockRoot</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 写线程如果cas加锁成功，那么LOCKSTATE从0变为1（注意这里：只能当LOCKSTATE主存值为0时，写线程才能获得write lock），因此只要LOCKSTATE=1，说明当前TreeBin对象正在被写线程做插入平衡操作，那么此时其他读线程会被阻塞吗，惯性思维会认为读线程会被此刻的写操作阻塞？ 但实际情况不会，具体解释参考7.3.2。</span></span><br><span class="line">  	<span class="comment">// 当然，如果写线程cas不成功，就得使用自旋去竞争write lock</span></span><br><span class="line">    <span class="keyword">if</span> (!U.compareAndSwapInt(<span class="keyword">this</span>, LOCKSTATE, <span class="number">0</span>, WRITER))</span><br><span class="line">        contendedLock(); <span class="comment">// offload to separate method</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

这里提到的`自旋去竞争获取write lock` 是跟“谁在竞争”呢？当然是和读线程(find操作)竞争。两者CAS互相竞争，源码片段对比如下：

>// ①写线程竞争写锁
>if (!U.compareAndSwapInt(this, LOCKSTATE, 0, WRITER))
>          contendedLock(); // offload to separate method
>  }

> // ②读线程竞争读锁
>
> else if (U.compareAndSwapInt(this, LOCKSTATE, s,s + READER))  

contendedLock有三个分支：

分支1：写线程竞争write lock

分支2：写线程竞争write lock失败后，在此分支将自己变为“waiter等待线程”

分支3：在分支2完成后，写线程已经是waiter线程，将自己挂起不再继续自旋，避免无限for循环的cpu空转

<font color=red>另外需要注意一个细节：虽然写线程在`if (!U.compareAndSwapInt(this, LOCKSTATE, 0, WRITER))`cas不成功表明lockState那一刻在主存值不为0，但进入if后，来到contendedLock，lockState也有可能变成为0，因为同一时刻有读线程并发读，读完后会对lockState进行cas减4操作，也可能将lockState减至0。简单说来：写线程进入contendedLock时或者进入方法内部时，lockState在主存值可能为0，也可能不是0 </font>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Possibly blocks awaiting root lock.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">contendedLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// waiting为true时用于控制这样情况：如果当前写线程未能获得write lock，就不再继续自旋cas竞争，而是将自己转为挂起状态的“等待线程”（waiter），这样做的好处：避免cpu空转浪费cpu时间片</span></span><br><span class="line">    <span class="keyword">boolean</span> waiting = <span class="keyword">false</span>;</span><br><span class="line">		<span class="comment">// 当前写线程进入自旋：要么获得write lock，要么变为waiter并将自己挂起</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> s;;) &#123;</span><br><span class="line">		<span class="comment">/* 分支1：竞争write lock，分两种情况讨论</span></span><br><span class="line"><span class="comment">		① lockState不为0时，</span></span><br><span class="line"><span class="comment">		要使得 (s = lockState) &amp; ~WAITER) == 0，推出lockState=WAITER，也即</span></span><br><span class="line"><span class="comment">		WAITER &amp; ~WAITER必然等于0</span></span><br><span class="line"><span class="comment">		改为使用位运算方式，也可以推导出来（只需考虑低4位）：</span></span><br><span class="line"><span class="comment">		  xxxx</span></span><br><span class="line"><span class="comment">		&amp;~0010</span></span><br><span class="line"><span class="comment">		也即：</span></span><br><span class="line"><span class="comment">		 xxxx</span></span><br><span class="line"><span class="comment">		&amp;1101</span></span><br><span class="line"><span class="comment">		要使得结果为0，那么xxxx只能是0010这个值，也即lockState为2，也即lockState=WAITER</span></span><br><span class="line"><span class="comment">   	而lockState=WAITER，表示当前写线程已经在上一轮遍历转为了“waiter线程”，也说明此时没有读线程正在读TreeBin对象（如果还有读线程，lockState值一定大于2），那么写线程当然可以去竞争写锁：U.compareAndSwapInt(this, LOCKSTATE, 2, WRITER)</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">   </span></span><br><span class="line"><span class="comment">   ② lockState在主存值又等于0的情况：这个“又”是指什么情况呢？</span></span><br><span class="line"><span class="comment">   加锁现在有一个读线程和写线程准备读写一个TreeBin，写线程首次对lockState cas不成功说明同一时刻lockState已经被读线程从0改到4（读线程cas加4操作），当写线程进入contendedLock方法后，此时读线程退出对lockState减4，恰好使得lockState在主存值又等于0。</span></span><br><span class="line"><span class="comment">   lockState=0，显然满足(s = lockState) &amp; ~WAITER) == 0，也即</span></span><br><span class="line"><span class="comment">   此刻无读线程竞争，则写线程可以直接去cas加锁</span></span><br><span class="line"><span class="comment">  	*/</span></span><br><span class="line">        <span class="keyword">if</span> (((s = lockState) &amp; ~WAITER) == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, LOCKSTATE, s, WRITER)) &#123;</span><br><span class="line">			<span class="comment">// 如果当前写线程是因为lockState=WAITER加锁成功进来的，那么可以将自己之前被标为waiter线程的标记去除，表示“我现在不是waiter线程，而是已经获取写锁变成writer线程了”</span></span><br><span class="line">                <span class="keyword">if</span> (waiting)</span><br><span class="line">                    waiter = <span class="keyword">null</span>;</span><br><span class="line">      <span class="comment">// 写线程成功获得write lock当然可以返回，结束自旋。</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment">/* 分支2：写线程竞争write lock失败后，在此分支将自己变为“waiter等待线程”</span></span><br><span class="line"><span class="comment">        到了此分支说明此时lockStat不是0也不是2，是多少呢？</span></span><br><span class="line"><span class="comment">        要使得(s &amp; WAITER) == 0成立，也即</span></span><br><span class="line"><span class="comment">          xx00</span></span><br><span class="line"><span class="comment">        &amp; 0010</span></span><br><span class="line"><span class="comment">       结果为0，可以推出s一定是0或者4、8、12...这样的值，根据读线程能对lockStat使用cas加4操作的设计，可知：</span></span><br><span class="line"><span class="comment">       此时有一个或者多个读线程在读TreeBin对象，那么写线程当然需要等等，这就是为何有waiting、waiter这样的设计</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((s &amp; WAITER) == <span class="number">0</span>) &#123;</span><br><span class="line">   <span class="comment">/* 既然写线程发现有读线程正在读TreeBin对象，那么就将自己变成等待线程后继续下一次自旋并在分支3把自己挂起来。</span></span><br><span class="line"><span class="comment">   注意这里不是直接将lockState改为WAITER值，而是改为s | WAITER，例如s在主内存值为4，那么cas成功后例如lockState=4+2=6。容易得出如果有N个读线程并发读，那么在这里lockState就设为4*N+2</span></span><br><span class="line"><span class="comment">   */</span>       </span><br><span class="line">            <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, LOCKSTATE, s, s | WAITER)) &#123;</span><br><span class="line">                waiting = <span class="keyword">true</span>;</span><br><span class="line">                waiter = Thread.currentThread();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">  <span class="comment">// 分支3：在分支2完成后，写线程已经是waiter线程，在这里调用UnSafe的park方法将自己挂起不再继续自旋，避免无限for循环的cpu空转</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (waiting)</span><br><span class="line">            LockSupport.park(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<font color=red>小结：contendedLock的for循环其实只需关注前3轮遍历：</font>

<p>对于lockState为0时，这种情况很好理解：写线程直接进入分支1竞争write lock</p>
<p>对于lockState不为0时，流程如下：</p>
<p>1、写线程首次进入for循环会直接进入分支2，因为此时lockState已经被1个或多个并发读线程cas加4满足分支2条件，写线程在此分支把自己变成waiter线程。<br>2、第2次循环时，写线程因为在首次循环将waiting设为true，因此会进入分支3，将自己挂起</p>
<p>3、分支1执行时机是：当其他读线程读完后（find到key）使用unpark唤醒写线程，此时写线程从挂起处执行，回到for循环进入分支1再继续竞争write lock</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(;;)</span><br><span class="line"><span class="comment">// 省略部分</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (waiting)</span><br><span class="line">    LockSupport.park(<span class="keyword">this</span>);</span><br><span class="line">    <span class="comment">//写线程从挂起处恢复cpu现场,回到for循环</span></span><br></pre></td></tr></table></figure>
<p>在理解以上“基于量化的分析”后，再通过以下的“定性说明”，则能掌握contendedLock的设计用意：</p>
<p>1、写线程发现有读线程正在读TreeBin，那么写线程首次进入for循环后，把自己变为waiter线程（意思是：我在等读线程完成），接着进入第2轮循环。</p>
<p>2、写线程在第2轮循环将自己挂起，避免自旋消耗cpu时间片</p>
<p>3、当“最后一个读线程”找到key并在退出前将写线程唤醒。</p>
<p>4、写线程唤醒后，进入第3轮循环，来到分支1去竞争写锁。</p>
<h5 id="7-3-2-TreeBin读线程竞争read-lock的设计"><a href="#7-3-2-TreeBin读线程竞争read-lock的设计" class="headerlink" title="7.3.2 TreeBin读线程竞争read lock的设计"></a>7.3.2 TreeBin读线程竞争read lock的设计</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; <span class="keyword">int</span> n, eh; K ek;</span><br><span class="line">    <span class="keyword">int</span> h = spread(key.hashCode());</span><br><span class="line"><span class="comment">// 省略部分</span></span><br><span class="line">  	<span class="comment">//如果e节点hash值小于0，说明节点要么是TreeBin节点，要么是fwd节点，两者都有find方法</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (eh &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> (p = e.find(h, key)) != <span class="keyword">null</span> ? p.val : <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure>
<p>如果eh=-2，那么e节点是一个TreeBin节点，也即<code>e.find</code>方法最终调用的是TreeBin内部的find方法（对读线程来说这个find方法可称为读操作）。</p>
<p>其find方法主要设计思想：</p>
<p>1、如果TreeBin节点已经有写线程正在做写操作（插入平衡）或者有处于等待状态的写线程，那么当前读线程尝试用遍历链表的方式去读取节点。</p>
<p>2、如果当前读线程恰能在TreeBin节点cas加锁成功，那么读线程使用红黑树树查找方法快速找到节点，完了后使用unpark将“已经挂起的写线程也即waiter线程”唤醒。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns matching node or null if none. Tries to search</span></span><br><span class="line"><span class="comment"> * using tree comparisons from root, but continues linear</span></span><br><span class="line"><span class="comment"> * search when lock not available.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">find</span><span class="params">(<span class="keyword">int</span> h, Object k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (k != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; e = first; e != <span class="keyword">null</span>; ) &#123;</span><br><span class="line">            <span class="keyword">int</span> s; K ek;</span><br><span class="line">          	<span class="comment">/* 分支1：</span></span><br><span class="line"><span class="comment">          	（1）考察lockState=1以及lockState=2时，(s = lockState) &amp; (WAITER|WRITER)计算结果代表的真实含义。</span></span><br><span class="line"><span class="comment">          	首先WAITER|WRITER =&gt; 2|1 =&gt; 3 =&gt; 0011</span></span><br><span class="line"><span class="comment">          	① lockState=1</span></span><br><span class="line"><span class="comment">              0001  </span></span><br><span class="line"><span class="comment">          	&amp; 0011</span></span><br><span class="line"><span class="comment">          	结果不等于0</span></span><br><span class="line"><span class="comment">          	② lockState=2</span></span><br><span class="line"><span class="comment">          	  0010  </span></span><br><span class="line"><span class="comment">          	&amp; 0011</span></span><br><span class="line"><span class="comment">          	结果不等于0</span></span><br><span class="line"><span class="comment">          	以上两个计算结果是要解释分支1要做的事情：如果当前有写线程正在对TreeBin节点写操作（TreeBin被写线程做插入平衡、删除操作）或者当前写线程已经是一个waiter线程，读线程不会被阻塞，而且读线程用遍历链表的方式去find key</span></span><br><span class="line"><span class="comment">          	（2）第二种情况较为复杂：TreeBin同时关联读线程和写线程。当读写线程是以类似下列的CAS序列去并发读、写TreeBin对象时</span></span><br><span class="line"><span class="comment">          	读线程      写线程(waiter线程)   读线程			 读线程			读线程...</span></span><br><span class="line"><span class="comment">          	cas(s,s+4) cas(s,s|WAITER)    cas(s,s+4)	cas(s,s+4)  cas(s,s+4) ...</span></span><br><span class="line"><span class="comment">          	此时lockState=4*N+2，N=1，2，3，4...，</span></span><br><span class="line"><span class="comment">          	条件(s = lockState) &amp; (WAITER|WRITER)) != 0也能成立。</span></span><br><span class="line"><span class="comment">          	因为此情况下至少有1个读线程和1个写线程（waiter线程），而且从lockState=4*N+2的低两位为10也可以推导出：它和0011相&amp;必然不等于0，位运算如下所示</span></span><br><span class="line"><span class="comment">          	x010</span></span><br><span class="line"><span class="comment">          &amp; 0011</span></span><br><span class="line"><span class="comment">         右起第2位的1保证了结果一定不为0，这也是因为lockState值有一个特殊的“+2”，也即有waiter线程。</span></span><br><span class="line"><span class="comment">          	*/</span></span><br><span class="line">          <span class="comment">// 同时分支1在这里可以证明TreeBin支持即使有写线程在写，也不会阻塞读线程并发读！！</span></span><br><span class="line">            <span class="keyword">if</span> (((s = lockState) &amp; (WAITER|WRITER)) != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (e.hash == h &amp;&amp;</span><br><span class="line">                    ((ek = e.key) == k || (ek != <span class="keyword">null</span> &amp;&amp; k.equals(ek))))</span><br><span class="line">                    <span class="keyword">return</span> e;</span><br><span class="line">                e = e.next;</span><br><span class="line">            &#125;</span><br><span class="line">  <span class="comment">/* 分支2：若分支1不成立，说明(s = lockState) &amp; (WAITER|WRITER)) ==0</span></span><br><span class="line"><span class="comment">  由于 (WAITER|WRITER)的值为0011，要使得(s = lockState) &amp; (WAITER|WRITER))结果为0，那么lockState的低两位必须为00， 也即：</span></span><br><span class="line"><span class="comment">                       xx00</span></span><br><span class="line"><span class="comment">                     &amp; 0011</span></span><br><span class="line"><span class="comment">     结果为0说明什么？说明当前时刻lockState的值不是1和2和4*N+2这样的值，而是0，4，8，...4*N这样的值，对应的读写竞争层面含义为：当前有1个或者多个读线程正在读TreeBin节点，那么作为新来的读线程自然可以加进来，并对lockState加4</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            <span class="comment">// 这里可以证明TreeBin支持并发读，每进入一个读线程，就对lockState加4</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, LOCKSTATE, s,</span><br><span class="line">                                         s + READER)) &#123;</span><br><span class="line">                TreeNode&lt;K,V&gt; r, p;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                  <span class="comment">// 读线程在这里即可放心读取节点（find key）</span></span><br><span class="line">                    p = ((r = root) == <span class="keyword">null</span> ? <span class="keyword">null</span> :</span><br><span class="line">                         r.findTreeNode(h, k, <span class="keyword">null</span>));</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    Thread w;</span><br><span class="line">                <span class="comment">/*这里可是一个关键的设计：它的思想跟transfer判断最后一个扩容线程的思想是如出一辙：</span></span><br><span class="line"><span class="comment">                  每次有一个读线程读结束后就会对lockState加“-4”，相当于减4，注意这里用的getAndAddInt，而不是前面熟悉的CAS，用处是什么呢？</span></span><br><span class="line"><span class="comment">                  用处：若最后一个读线程读结束后它使用getAndAddInt返回的恰好是6也即等于(READER|WAITER)，同时lockState被减至2（恰好等于WAITER），表明此刻当前TreeBin对象还关联着仅剩一个waiter线程，于是最后一个读线程在return前顺便把waiter线程唤醒，好让这个写线程恢复执行去获取写锁。这一小设计相当巧妙！！</span></span><br><span class="line"><span class="comment">                 本代码片段设计思想可结合7.3.3的图3来理解。</span></span><br><span class="line"><span class="comment">                  */</span></span><br><span class="line">                    <span class="keyword">if</span> (U.getAndAddInt(<span class="keyword">this</span>, LOCKSTATE, -READER) ==</span><br><span class="line">                        (READER|WAITER) &amp;&amp; (w = waiter) != <span class="keyword">null</span>)</span><br><span class="line">                        LockSupport.unpark(w);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> p;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>到了这里，我们可以根据分支1回答以下关键问题：</p>
<p>为何TreeBin读写竞争机制可以实现：即使有写线程在写，读线程依然可以并发读，不是说好的读写竞争吗（写的时候不能读）</p>
<font color=red>这是因为：写线程在写操作时，是对红黑树做插入平衡操作，它只是用到了TreeNode的red、parent、right、left属性，别忘了TreeNode的设计中还有next属性，也即红黑树所有节点在背后已经通过next属性链成了一条链表，当红黑树调整时不会改变next属性，因此链表结构保持不变或者说写线程写操作不会影响到TreeNode的链表节点前后指向，那么就可以安排读线程用遍历链表的方式去读链表节点，所以才有了TreeBin以下的设计</font>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 写线程写时 或者写线程是waiter线程时，可使用TreeNode的next属性去遍历链表，实现读线程并发读也不会被写线程阻塞。</span></span><br><span class="line">              <span class="keyword">if</span> (((s = lockState) &amp; (WAITER|WRITER)) != <span class="number">0</span>) &#123;</span><br><span class="line">                   <span class="keyword">if</span> (e.hash == h &amp;&amp;</span><br><span class="line">                       ((ek = e.key) == k || (ek != <span class="keyword">null</span> &amp;&amp; k.equals(ek))))</span><br><span class="line">                       <span class="keyword">return</span> e;</span><br><span class="line">                   e = e.next; <span class="comment">// 使用TreeNode的next属性去遍历背后的链表结构</span></span><br><span class="line">               &#125;</span><br></pre></td></tr></table></figure>
<h5 id="7-3-3-图示理解lockState如何控制读写线程"><a href="#7-3-3-图示理解lockState如何控制读写线程" class="headerlink" title="7.3.3 图示理解lockState如何控制读写线程"></a>7.3.3 图示理解lockState如何控制读写线程</h5><p>7.3.1和7.3.2给出非常详细解释了lockState位运算满足的条件及其设计目的，在此基础上结合以下图可深入掌握TreeBin设计的读写锁竞争机制，它是一个非常优秀的并发设计！</p>
<p>图1：仅有写线程情况下，逻辑简单不再给出图示。</p>
<p>假设TreeBin节点仅关联一个写线程，那么s的状态值变化很简单，0变为1，再从1变为0：</p>
<p>① 写线程进入lockRoot：cas(lockState,0,1)，此时lockState从0变为1，表示写线程获得写锁</p>
<p>② 写线程完成写操作后调用unlockRoot：lockState = 0，此时lockState 从1变为0，表示写线程释放了写锁</p>
<p>图2：假设当前有N个读线程且无写线程的情况下并发读TreeBin节点，不妨假设它们都是同一时刻进入find方法，完成读后，也是同一时刻退出find方法，那么lockState值变化如下图所示：</p>
<p>（此设计很像transfer方法里面扩容线程计数设计：扩容线程进入扩容操作时sc+1、退出扩容操作时sc-1。）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6cb4f3e0435a69f7c9f119e3accc35c3.png" alt="N个读线程并发读TreeBin.001"></p>
<font color=red> 图3：假设当前有N个读线程和1写线程并发读写TreeBin节点，毫无疑问，此时读线程要竞争读锁、写线程要竞争写锁。不妨假设第1个读线程正在读TreeBin节点，接着来了一个写线程，再接着进来第2个读线程以及后续更多的读线程，那么观察lockState的变化过程：</font>

<p>(此设计很像transfer方法里面的cas设计：扩容线程进入扩容时sc+1、退出扩容时sc-1，而且这里有个特殊点：lockState=4*N+2中的“+2”，从图中可以知道这个“+2”表示：目前还存在与TreeBin节点关联的唯一一个waiter线程。)</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/71fed23c88b3acec285cc576a04e91c5.png" alt="读写线程并存的lockState状态图的副本"></p>
<h4 id="8、ForwardingNode节点分析"><a href="#8、ForwardingNode节点分析" class="headerlink" title="8、ForwardingNode节点分析"></a>8、ForwardingNode节点分析</h4><p>这个节点的设计相对简单：在扩容阶段，transfer方法中转移节点的那片逻辑里，当旧表桶位节点迁移到新表中后，会在旧表桶位上放置一个ForwardingNode节点，该节点的hash值为MOVED=-1，不存放key以及value。</p>
<p>还有一个nextTable属性以及内置一个读方法find，它们的设计意图是？解释如下：</p>
<p>在扩容阶段：对于桶位i，当旧表桶位节点迁移到新表中后，会在旧表桶位i上放置一个ForwardingNode节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">           		setTabAt(nextTab, i, ln);</span><br><span class="line">                  setTabAt(nextTab, i + n, hn);</span><br><span class="line"><span class="comment">// 旧表桶位i节点全部迁移到新表后，会在旧表桶位i上放置一个ForwardingNode节点</span></span><br><span class="line">                  setTabAt(tab, i, fwd); </span><br></pre></td></tr></table></figure>
<p>当有读线程定位桶位i发现是一个fwd节点，就会调用其内部方法find去找key，但是当前桶位只是一个fwd节点，它没有key和value，那么读线程去哪里找key呢？</p>
<p>这就是nextTable属性的用意：读线程会去新表nextTable相应的桶位去找节点，这一点可以在以下源码的①位置得到证明。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* ---------------- Special Nodes -------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A node inserted at head of bins during transfer operations.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ForwardingNode</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Node&lt;K,V&gt;[] nextTable;</span><br><span class="line">    ForwardingNode(Node&lt;K,V&gt;[] tab) &#123;</span><br><span class="line">        <span class="keyword">super</span>(MOVED, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">this</span>.nextTable = tab;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">Node&lt;K,V&gt; <span class="title">find</span><span class="params">(<span class="keyword">int</span> h, Object k)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// loop to avoid arbitrarily deep recursion on forwarding nodes</span></span><br><span class="line">        <span class="comment">// ① 读线程虽然定位在旧表桶上，从此处for循环条件来看，读线程会在新表nextTable去找节点</span></span><br><span class="line">        <span class="comment">// 这里的outer有特别的用意</span></span><br><span class="line">        outer: <span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = nextTable;;) &#123;</span><br><span class="line">            Node&lt;K,V&gt; e; <span class="keyword">int</span> n;</span><br><span class="line">            <span class="keyword">if</span> (k == <span class="keyword">null</span> || tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span> ||</span><br><span class="line">                (e = tabAt(tab, (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">          <span class="comment">//② 在新表的桶位上遍历链表</span></span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                <span class="keyword">int</span> eh; K ek;</span><br><span class="line">                <span class="keyword">if</span> ((eh = e.hash) == h &amp;&amp;</span><br><span class="line">                    ((ek = e.key) == k || (ek != <span class="keyword">null</span> &amp;&amp; k.equals(ek))))</span><br><span class="line">                    <span class="keyword">return</span> e;</span><br><span class="line">                <span class="keyword">if</span> (eh &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                  <span class="comment">// 如果读线程转去新表（第一个nextTable）去读节点发现该新表对应节点又是一个fwd，说明数据节点被迁移到第二个nextTable，也即读线程转去新表时，新表已经进入扩容期。这时怎么处理呢？ 很简单：告诉读线程请你去第二个nextTable继续去找数据节点。</span></span><br><span class="line">                    <span class="comment">//③</span></span><br><span class="line">                    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> ForwardingNode) &#123;</span><br><span class="line">                    	<span class="comment">// 让tab指向第二个nextTable新表，那么下一轮遍历线程就会在第二个nextTable新表去找节点</span></span><br><span class="line">                        tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable;</span><br><span class="line">                        <span class="keyword">continue</span> outer;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                      	<span class="comment">// 到打这里说明当前e节点是一个TreeBin节点，TreeBin也有find方法，用它找key即可</span></span><br><span class="line">                        <span class="keyword">return</span> e.find(h, k);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这片源码有个地方的写法比较陌生outer语法：也即 ①位置和③位置，它所要表达的逻辑如下图所示</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">outer: <span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = nextTable;;) &#123;</span><br><span class="line">            <span class="comment">//continue outer告诉线程在此处继续执行for(;;)循环</span></span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">			<span class="comment">// 省略部分</span></span><br><span class="line">              <span class="keyword">if</span> (e <span class="keyword">instanceof</span> ForwardingNode) &#123;</span><br><span class="line">                tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable;</span><br><span class="line">                <span class="keyword">continue</span> outer;</span><br><span class="line">            	&#125;</span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用于解决这样的场景：</p>
<p>1、首先读线程在旧表读桶位i0时发现是一个fwd节点，读线程会转去新表（第一个nextTable）去读节点</p>
<p>2、接着读线程发现新表对应桶位i1又是一个fwd节点，说明新表（第一个nextTable）已经进入扩容期。</p>
<p>3、读线程只能继续转去下一个新表（第二个nextTable）找，发现对应桶位i2是正常的数据节点，那么就可以使用for(;;)读取数据节点。</p>
<p>可以看出outer设计目的是避免无限递归查找：新表的e.find -&gt;递归&gt;回到新表自己的e.find-&gt;…-&gt; 无限递归</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/e0c0576900dea2795ee43035c6a347b7.png" alt="Forwarding节点分析.001"></p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：jdk1.8的ConcurrentHashMap源码深入分析（一）</title>
    <url>/2021/05/23/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9Ajdk1.8%E7%9A%84ConcurrentHashMap%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>虽然jdk1.7的CHM能够解决非阻塞高并发的读写情况，但它仍然不够理想，例如写并发度受限于Segment数组初始大小，一旦创建就无法再扩容并发度；如果key非常多且分布不均匀，例如都落在同一个Segment位置上，相当于多个线程竞争同一把“全局锁”，CHM“仿佛”退化为HashTable。此外，因为CHM的Segment上使用jdk1.7的HashMap，它的性能显然没有jdk1.8的HashMap强，jdk1.7resize扩容处理时只能单线程完成扩容操作，jdk1.7计算size方法可能要对每个Segment加阻塞锁，基于以上jdk1.7的CHM缺点，Doug Lea重新设计新版本的CHM，其源代码行数总共6312行，性能确实高了，但代价是代码逻辑的复杂度要高出很多。</p>
<p>理解本文所有源码分析以及相关技术都需要读者已经掌握1.8/1.7HashMap、1.7 CHM、Unsafe、CAS这些进阶知识，否则难以消化相关知识，可以提前阅读本博客相关文章：</p>
<p>（以下“CHM”表示ConcurrentHashMap简写）</p>
<h4 id="数据结构图示"><a href="#数据结构图示" class="headerlink" title="数据结构图示"></a>数据结构图示</h4><p>分为普通状态下（非扩容期间）和扩容期间</p>
<p>（1）CHM非扩容时，也即普通正常状态下的内部数据结构图：</p>
<p>可以看到跟jdk1.8的HashMap数据结构不同的地方：CHM用了一个称为TreeBin的节点作为桶位头节点，不是HashMap的TreeNode:<br><img src="https://img-blog.csdnimg.cn/ef0e55b37d5744cbb7f036ca5a19c6ab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>（2）CHM正在扩容时的内部数据结构图：</p>
<p>  可以看到正在扩容时的内部节点结构，数组尾部的一些桶位头节点放入了ForwardingNode节点。</p>
<a id="more"></a>
<p>  <img src="https://img-blog.csdnimg.cn/6455b8ed077d45548a4901bae17c44a5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>其实还有一个情况，以上两张图也没有展示出来：当CHM调用computeIfAbsent或者compute方法来插入系节点时，会在桶位上放置一个ReservationNode用于占位操作，本博客也有关于相关深度分析的文章</p>
<h4 id="重要成员变量说明"><a href="#重要成员变量说明" class="headerlink" title="重要成员变量说明"></a>重要成员变量说明</h4><h5 id="CHM重要常量"><a href="#CHM重要常量" class="headerlink" title="CHM重要常量"></a>CHM重要常量</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/* ---------------- Constants -------------- */</span></span><br><span class="line"><span class="comment">// CHM数组长度最大值</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CHM数组初始化默认长度</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CAPACITY = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 非最大2的整次幂数组长度：最大值-8。适用于toArray及其相关方法的场合</span></span><br><span class="line"><span class="comment">   * The largest possible (non-power of two) array size.</span></span><br><span class="line"><span class="comment">   * Needed by toArray and related methods.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_ARRAY_SIZE = Integer.MAX_VALUE - <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 为了兼容jdk1.7版本的CHM，在这里也给出相关成员变量的</span></span><br><span class="line"><span class="comment">   * The default concurrency level for this table. Unused but</span></span><br><span class="line"><span class="comment">   * defined for compatibility with previous versions of this class.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CONCURRENCY_LEVEL = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与1.8 HashMap一致，此处不再说明</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> LOAD_FACTOR = <span class="number">0.75f</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与1.8 HashMap一致，此处不再说明</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEIFY_THRESHOLD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与1.8 HashMap一致，此处不再说明</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNTREEIFY_THRESHOLD = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 与1.8 HashMap一致，此处不再说明</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TREEIFY_CAPACITY = <span class="number">64</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 由于本文重点讨论fullAddCount，与扩容相关的常量在扩容文章给出。</span></span><br></pre></td></tr></table></figure>
<h5 id="重要成员变量"><a href="#重要成员变量" class="headerlink" title="重要成员变量"></a>重要成员变量</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/* ---------------- Fields -------------- */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   HashMap的底层数组，注意它被volatile语义修饰:让table数组符合happen-before的规则，第一次put才会被初始化，长度为2的整数幂。</span></span><br><span class="line"><span class="comment">   * The array of bins. Lazily initialized upon first insertion.</span></span><br><span class="line"><span class="comment">   * Size is always a power of two. Accessed directly by iterators.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">transient</span> <span class="keyword">volatile</span> Node&lt;K,V&gt;[] table;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下4个字段需要结合每小节内容去理解才能掌握其设计含义</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Base counter value, used mainly when there is no contention,</span></span><br><span class="line"><span class="comment">   * but also as a fallback during table initialization</span></span><br><span class="line"><span class="comment">   * races. Updated via CAS.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">long</span> baseCount;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  设计最精妙的变量之一，对它实施位运算可以实现对CHM多种操作的控制</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">int</span> sizeCtl;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 理解fullAddCount方法实现后才能明白此变量的意义。Spinlock (locked via CAS) used when resizing and/or creating CounterCells.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">int</span> cellsBusy;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 理解fullAddCount方法实现后才能明白此变量的意义。Table of counter cells. When non-null, size is a power of 2.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> CounterCell[] counterCells;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="put方法"><a href="#put方法" class="headerlink" title="put方法"></a>put方法</h4><p>关于spread方法、tableSizeFor方法、comparableClassFor方法、compareComparables方法可以参考jdk1.8的HashMap，本文不再赘述。</p>
<p>put方法最适合作为源码分析入口，如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ConcurrentHashMap&lt;Integer, String&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">map.put(<span class="number">3</span>, <span class="string">&quot;foo3&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>put方法内部调用的是putVal方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> putVal(key, value, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于CHM来说，put一个key到桶桶上会有以下几种情况：<br>1、table为空  2、桶位为空  3、桶位是一条冲突链 4、桶位是一个treeBin节点  5、桶位是一个<code>ForwardingNode</code>节点（表示有其他线程正在扩容） 6、桶位是一个<code>ReservationNode</code>节点：表示有其他线程正在调用computeIfAbsent或者compute方法来插入在该桶位插入节点，注意同一桶位的同一时刻，5和6是不会同时发生在，两个情况都是表达插入节点的逻辑。</p>
<p>因此put主体逻辑设计主要分为以下7个步骤：</p>
<p>1、如果tab为空，那么进行table初始化，该方法里面使用自旋+CAS让线程自己竞争初始化权</p>
<p>2、如果key所定位的桶位头节点f为空节点，线程使用CAS竞争将key节点插入到该桶位中</p>
<p>3、如果key所定位的桶位头节点hash为-1，也即表示该桶位的节点是个fwd节点，说明当前CHM正在扩容，那么当前put线程遇到这个节点会让去帮助扩容线程。</p>
<p>4、若不是以上1~3三种情况，那么先使用synchronized锁在该桶位，如果该桶位上一条冲突链，遍历该链插入key，如果该桶位上是一个treeBin，说明桶位已经是一棵红黑树，则按红黑树插入节点方法来插入key</p>
<p>5、判断key插入的链表长度是否大于等于8，来决定是否需要树化</p>
<p>6、将CHM的节点总数size加1（采用并发计数器累加）</p>
<p>7、判断节点总数是否达到扩容阈值而进行扩容</p>
<p>本文重点分析1-6的设计，第7点是扩容逻辑，由于扩容逻辑相对复杂、内容丰富，需要单独起一篇文章研究。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/** Implementation for put and putIfAbsent */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(K key, V value, <span class="keyword">boolean</span> onlyIfAbsent)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span> || value == <span class="keyword">null</span>) <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">int</span> hash = spread(key.hashCode());</span><br><span class="line">    <span class="keyword">int</span> binCount = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = table;;) &#123;</span><br><span class="line">      	<span class="comment">// f是key所定位的first头节点，fh：first头节点的Hash值，n:当前table数组长度，i：key定位到的桶位下标 </span></span><br><span class="line">        Node&lt;K,V&gt; f; <span class="keyword">int</span> n, i, fh;</span><br><span class="line">        <span class="comment">// 1、如果tab为空，那么进行table初始化，initTable方法里面使用自旋+CAS让线程去竞争初始化权</span></span><br><span class="line">        <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">            tab = initTable();</span><br><span class="line">      	<span class="comment">// 2、如果key所定位的桶位头节点f为空节点，线程使用CAS竞争将key节点插入到该桶位中，如果成功插入，则退出循环</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; hash)) == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (casTabAt(tab, i, <span class="keyword">null</span>,<span class="keyword">new</span> Node&lt;K,V&gt;(hash, key, value, <span class="keyword">null</span>)))</span><br><span class="line">                <span class="keyword">break</span>;                   <span class="comment">// no lock when adding to empty bin</span></span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">// 3、如果key所定位的桶位头节点hash为-1，说明当前CHM正在扩容，也即表示该桶位已被其他扩容线程迁移完数据，那么当前线程要进入扩容去帮助CHM扩容，注意不是帮助此桶位扩容！</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)</span><br><span class="line">            tab = helpTransfer(tab, f);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            V oldVal = <span class="keyword">null</span>;</span><br><span class="line">          	<span class="comment">// 4、若不是以上1~3三种情况，那么先使用synchronized锁在该桶位first头节点，从这里开始看出jdk1.8的CHM锁粒度比1.7要小，锁对象就是桶位本身</span></span><br><span class="line">            <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">            <span class="comment">// 再次确认当前first头节点有没变更过，如果f已被改变，则回到for循环，重新按1-4流程再来一次</span></span><br><span class="line">                <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">            <span class="comment">// 4.1 如果当前桶位是一条冲突链表，遍历冲突链，并对binCount进行计数，如果存在key则覆盖，否则使用尾插法将key节点插入到链表尾部，此逻辑跟jdk1.8的类似</span></span><br><span class="line">                    <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                      	<span class="comment">// 对冲突链表长度进行计数，用于树化判断</span></span><br><span class="line">                        binCount = <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">for</span> (Node&lt;K,V&gt; e = f;; ++binCount) &#123;</span><br><span class="line">                            K ek;</span><br><span class="line">                            <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                                ((ek = e.key) == key ||</span><br><span class="line">                                 (ek != <span class="keyword">null</span> &amp;&amp; key.equals(ek)))) &#123;</span><br><span class="line">                                oldVal = e.val;</span><br><span class="line">                                <span class="keyword">if</span> (!onlyIfAbsent)</span><br><span class="line">                                    e.val = value;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                            Node&lt;K,V&gt; pred = e;</span><br><span class="line">                          	<span class="comment">//来到链表尾部，尾插法</span></span><br><span class="line">                            <span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                pred.next = <span class="keyword">new</span> Node&lt;K,V&gt;(hash, key,</span><br><span class="line">                                                          value, <span class="keyword">null</span>);</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// 4.2 如果当前桶位头节点是一个TreeBin节点</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                        Node&lt;K,V&gt; p;</span><br><span class="line">                      	<span class="comment">// 这里binCount是从2开始，解释参考本博客的ThreeBin读写锁机制分析的文章</span></span><br><span class="line">                        binCount = <span class="number">2</span>;</span><br><span class="line">                        <span class="keyword">if</span> ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,</span><br><span class="line">                                                       value)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            oldVal = p.val;</span><br><span class="line">                            <span class="keyword">if</span> (!onlyIfAbsent)</span><br><span class="line">                                p.val = value;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (binCount != <span class="number">0</span>) &#123;</span><br><span class="line">              	<span class="comment">// 判断冲突链表长度是否需要树化</span></span><br><span class="line">                <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD)</span><br><span class="line">                    treeifyBin(tab, i);</span><br><span class="line">                <span class="keyword">if</span> (oldVal != <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">return</span> oldVal;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">// 5、将CHM的节数总数size加1（采用并发计数器累加）</span></span><br><span class="line">    addCount(<span class="number">1L</span>, binCount);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果对jdk1.8 HashMap有深入研究的话，其实CHM的put本身设计逻辑并不难理解，而最复杂且精妙的设计反而是put方面里面的辅助方法：initTable方法、helpTransfer方法、transfer方法、addCount方法、fullAddCount方法，每一个方法无不体现JUC并发代码的精巧设计。</p>
<p>在前面的“CHM重要常量”，特意将以下三个变量放在此来说明它们的作用，分别代表三种节点的哈希值：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MOVED = -<span class="number">1</span>;<span class="comment">// ForwardingNode（扩容期间需要用到）的哈希值</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEBIN = -<span class="number">2</span>;<span class="comment">// TreeBin节点（树化需要用到）的哈希值</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> RESERVED = -<span class="number">3</span>;<span class="comment">// ReservationNode节点（在computeIfAbsent、compute方法需要到）</span></span><br></pre></td></tr></table></figure>
<p>在put一个节点需要判断当前桶位节点类型，其中用到以下逻辑，其中只要头节点的哈希值fh&gt;=0，我们就可以认为它是一条链表（或者单节点），所以可以使用遍历方式去put入key</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">		<span class="comment">// 4.1 如果当前桶位是一条冲突链表，遍历冲突链，并对binCount进行计数，如果存在key则覆盖，否则使用尾插法将key节点插入到链表尾部，此逻辑跟jdk1.8的类似</span></span><br><span class="line">        <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">          	<span class="comment">// 对冲突链表长度进行计数，用于树化判断</span></span><br><span class="line">            binCount = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 4.2 若头节点哈希值小于0，要么是fwd节点要么是ThreeBin节点</span></span><br><span class="line">   	    <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">            Node&lt;K,V&gt; p;</span><br><span class="line">            binCount = <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,</span><br><span class="line">                                           value)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                oldVal = p.val;</span><br><span class="line">                <span class="keyword">if</span> (!onlyIfAbsent)</span><br><span class="line">                    p.val = value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;      </span><br></pre></td></tr></table></figure>
<p>如果头节点哈希值小于0也即：-1，-2，-3，它们代表第三种类型节点，因此还需要进一步判断f节点是否为TreeBin节点，因此有</p>
<p><code>if (f instanceof TreeBin)</code>， 为何不设计为: <code>if(fh==TREEBIN)</code>，个人认为：</p>
<p>纯粹是为了源代码的可读性，因为我们put一个key到桶位前，更关心关心的是key所在桶位的“节点”是什么类型，当开发者看到<code>if (f instanceof TreeBin)</code>，自然就会理解为：如果当前桶位头节点是一个TreeBin节点。</p>
<p>而如果写成<code>if(fh==TREEBIN)</code>， 显然就没上面的理解来得更加“人性化、可读性高”</p>
<h4 id="initTable方法"><a href="#initTable方法" class="headerlink" title="initTable方法"></a>initTable方法</h4><p>在重要成员变量里面，有一个叫sizeCtl的东西，在这里才能很好理解的它的作用:</p>
<p>initTable总体设计逻辑：例如线程A使用自旋+CAS去竞争table初始化权，CAS成功，则sizeCtl值会被改为-1表示“已成功抢到锁”，其他线程看到sizeCtl=-1就不再争抢初始化权而是让出cpu时间片。线程A接着完成new table初始化操作，并将sizeCtl设为当前table的扩容阈值：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Node&lt;K,V&gt;[] initTable() &#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; <span class="keyword">int</span> sc;</span><br><span class="line">  	<span class="comment">// 1、如果当前线程遇到table不为空，说明已经被其他线程完成了table初始化，直接返回</span></span><br><span class="line">    <span class="keyword">while</span> ((tab = table) == <span class="keyword">null</span> || tab.length == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 2、如果当前线程遇到sizeCtl=-1，说明其他线程早于当前线程将sc设为-1并正在table初始化，那么当前线程只能放弃初始化权</span></span><br><span class="line">        <span class="keyword">if</span> ((sc = sizeCtl) &lt; <span class="number">0</span>)</span><br><span class="line">            Thread.yield(); <span class="comment">// lost initialization race; just spin</span></span><br><span class="line">    <span class="comment">// 3、当前线程成功将sizeCtl用cas改为-1，表是：“我在对table正在初始化中”，注意SIZECTL是offset内存偏移地址，区别于sizeCtl</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, -<span class="number">1</span>)) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              	<span class="comment">// 当前线程CAS成功后，再次判断tab是否为空，保证“以下操作仅由当前线程来初始化”的语义</span></span><br><span class="line">                <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || tab.length == <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">// 4、创建新table</span></span><br><span class="line">                    <span class="comment">// sc已经是-1，因此这里的n将采用DEFAULT_CAPACITY，值为16</span></span><br><span class="line">                    <span class="keyword">int</span> n = (sc &gt; <span class="number">0</span>) ? sc : DEFAULT_CAPACITY;</span><br><span class="line">                    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">                  	<span class="comment">// 创建新的节点数组</span></span><br><span class="line">                    Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node&lt;?,?&gt;[n];</span><br><span class="line">                  	table = tab = nt;</span><br><span class="line">                  	<span class="comment">// 计算新表扩容阈值，这里使用位计算，原理不变，还是newTable.length*0.75,</span></span><br><span class="line">                    sc = n - (n &gt;&gt;&gt; <span class="number">2</span>);<span class="comment">//=&gt; n-n/4=&gt;n(1-0.25)=&gt;n*0.75</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">              	<span class="comment">// 保证初始化后，sizeCtl一定是新数组的扩容阈值</span></span><br><span class="line">                sizeCtl = sc;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tab;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="addCount方法"><a href="#addCount方法" class="headerlink" title="addCount方法"></a>addCount方法</h4><p>首先回想jdk1.8 HashMap的put方法：插入key后，对size自增1，如果size超过阈值则对HashMap底层table进行2倍扩容：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,<span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// .......</span></span><br><span class="line">        ++modCount;</span><br><span class="line">        <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line">            resize();</span><br><span class="line">        afterNodeInsertion(evict);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>那么addCount方法也是类似的设计思想：</p>
<p>先对size加1，然后根据size节点总数判断是否需要扩容。</p>
<p>但这里最复杂的地方在于：多线程并发情况下，如何正确实现对size原子加1？addCount及其内部的fullAddCount方法可以完成此任务，代价是：程序设计相对复杂（设计思路十分精巧，值得在日常项目引入），设计原理如下图所示：<br><img src="https://img-blog.csdnimg.cn/dad538c261234249972fd48d844615ae.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>当线程竞争不激烈情况下，通过自旋+cas对baseCount进行加1计数</p>
<p>当线程竞争十分激烈的情况下，有一部分线程很幸运能够抢到cas权力成功对baseCount加1，而剩下对baseCount加1cas失败的线程，它们就会创建一个CounterCells计数的数组，然后线程给对应自己的桶位Cell对象进行cas加1操作，这样一来就实现了“线程分流”，减少竞争。</p>
<p>最后统计总数时，将baseCount和CounterCells数组每个桶位的计数值累加起来，就是size的大小。该原理对应的源码如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">long</span> <span class="title">sumCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    CounterCell[] as = counterCells; CounterCell a;</span><br><span class="line">    <span class="keyword">long</span> sum = baseCount;</span><br><span class="line">    <span class="keyword">if</span> (as != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; as.length; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> ((a = as[i]) != <span class="keyword">null</span>)</span><br><span class="line">              	<span class="comment">// baseCount加上每一个桶位的value，最后就是总数sum</span></span><br><span class="line">                sum += a.value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>另外一个有类似功能的类，专门设计给高并发计数的场景：LongAdder类（java.util.concurrent.atomic.LongAdder），它比AtomicLong原子计数器性能更强，本博客也给出相关性能测试。</p>
</blockquote>
<p>addCount骨架代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">    CounterCell[] as; <span class="keyword">long</span> b, s;</span><br><span class="line">  	<span class="comment">// 1、使用baseCount以及 CounterCell数组完成并发环境下的加1计数</span></span><br><span class="line">    <span class="keyword">if</span> ((as = counterCells) != <span class="keyword">null</span> ||</span><br><span class="line">        !U.compareAndSwapLong(<span class="keyword">this</span>, BASECOUNT, b = baseCount, s = b + x))&#123;...&#125;</span><br><span class="line">  	<span class="comment">// 2、判断CHM是否需要扩容</span></span><br><span class="line">    <span class="keyword">if</span> (check &gt;= <span class="number">0</span>)&#123;...&#125; </span><br></pre></td></tr></table></figure>
<p>完整源码分析：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">		<span class="comment">// 调用addCount(1L, binCount)，因此以下的入参x是1，check是binCount，</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">/*</span></span><br><span class="line"><span class="comment">      	CounterCell是一个简单的内部类，它有一个value属性，用于被线程CAS加值操作</span></span><br><span class="line"><span class="comment">      	static final class CounterCell &#123;</span></span><br><span class="line"><span class="comment">        				volatile long value;</span></span><br><span class="line"><span class="comment">        				CounterCell(long x) &#123; value = x; &#125;</span></span><br><span class="line"><span class="comment">    		&#125;</span></span><br><span class="line"><span class="comment">      	*/</span></span><br><span class="line">      	<span class="comment">// as:存放如上图所示的CounterCell数组的临时变量，不同线程会对其不同段进行cas加1操作</span></span><br><span class="line">        <span class="comment">// b:baseCount变量</span></span><br><span class="line">        CounterCell[] as; <span class="keyword">long</span> b, s;</span><br><span class="line"> <span class="comment">// 判断条件1:若counterCells数组不是空，说明已经有其他线程创建了它，也说明此刻出现线程竞争</span></span><br><span class="line"> <span class="comment">// 判断条件2: 当前线程对baseCount进行CAS加1操作失败，说明有其他线程竞争着baseCount，也说明此刻出现线程竞争</span></span><br><span class="line">        <span class="keyword">if</span> ((as = counterCells) != <span class="keyword">null</span> ||</span><br><span class="line">            !U.compareAndSwapLong(<span class="keyword">this</span>, BASECOUNT, b = baseCount, s = b + x)) &#123;</span><br><span class="line"><span class="comment">// a变量：CounterCell数组某个桶位（也称某个段），v变量：CounterCell计数器里面value变量，m变量：CounterCell数组长度</span></span><br><span class="line">            CounterCell a; <span class="keyword">long</span> v; <span class="keyword">int</span> m;</span><br><span class="line">   <span class="comment">// uncontended线程之间有无激烈竞争标志，true：表示线程之间竞争还不激烈，false：表示线程之间竞争很激烈</span></span><br><span class="line">            <span class="keyword">boolean</span> uncontended = <span class="keyword">true</span>; </span><br><span class="line">          	<span class="comment">// 注意这里4个判断条件</span></span><br><span class="line"><span class="comment">// 判断条件1：该条件执行是因为第1个if走的是判断条件2，说明多个线程在竞争cas操作baseCount，但是还没线程去创建CounterCell数组，那么当前线程可用fullAddCount完成加1操作。</span></span><br><span class="line"><span class="comment">// 判断条件2：该条件执行是因为第1个if走的是判断条件1，你想想，能调用length属性说明as是非空对象，也即as = counterCells) != null，但还未放入cell对象，此时as.length=0或者as.length-1&lt;0</span></span><br><span class="line"><span class="comment">// 判断条件3：该条件执行是因为第1个if走的是判断条件1，当前线程通过hash定位到counterCells数组对应桶位为空，说明当前线程接下来将可以在桶位上对CounterCell的value进行加1操作，将交由fullAddCount完成。</span></span><br><span class="line"><span class="comment">// 判断条件4： 当线程对它所在的桶位cas加1操作失败，说明已经有多个线程正在激烈竞争counterCells数组，接下来需要对counterCells数组扩容来分散（降低）线程竞争，将交由fullAddCount完成</span></span><br><span class="line">            <span class="keyword">if</span> (as == <span class="keyword">null</span> || (m = as.length - <span class="number">1</span>) &lt; <span class="number">0</span> ||</span><br><span class="line">                <span class="comment">// 线程定位到CounterCell数组的hash方法：ThreadLocalRandom.getProbe() &amp; m</span></span><br><span class="line">                (a = as[ThreadLocalRandom.getProbe() &amp; m]) == <span class="keyword">null</span> ||</span><br><span class="line">                !(uncontended =</span><br><span class="line">                  U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123;</span><br><span class="line"><span class="comment">// 多个线程同时竞争修改baseCount时，竞争失败的线程会执行fullAddCount，把x的值插入到counterCell数组对应的单元value里面。从这里也可以立即看出uncontended=false真实含义：表示当前多线程并发计数竞争激烈</span></span><br><span class="line"><span class="comment">// fullAddCount(1,uncontended=false),该方法能保证完成加1操作             </span></span><br><span class="line">                 fullAddCount(x, uncontended);</span><br><span class="line">              	<span class="comment">// 如果执行流执行到这里，说明fullAddCount已经完成加1操作，可以返回。</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果check（binCount）&lt;=1,显然不需要扩容，可直接return结束</span></span><br><span class="line">            <span class="keyword">if</span> (check &lt;= <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            s = sumCount(); <span class="comment">// 计算CHM的总节点数量，以便之后用s &gt;= (long)(sc = sizeCtl) 判断是否需要扩容</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="fullAddCount方法"><a href="#fullAddCount方法" class="headerlink" title="fullAddCount方法"></a>fullAddCount方法</h4><p>在源码中,第2500行，有一片代码段称为“Counter support”，作者指出它是用于分布式计数，fullAddCount方法改编自LongAdder and Striped64</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* ---------------- Counter support -------------- */</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A padded cell for distributing counts.  Adapted from LongAdder</span></span><br><span class="line"><span class="comment"> * and Striped64.  See their internal docs for explanation.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<p>fullAddCount方法的设计和实现是复杂且有精妙的，主要分为三个逻辑：</p>
<p>1、如果CounterCell数组不为空且数组里面已经有Cell对象，说明CounterCell数组已经被其他线程完成了创建，那么当前线程自然无需再创建它，而是尝试在当前下线程对应的空桶位放入Cell(1),相当于对value加1操作（如果桶位已有Cell对象，就使用cas对其加1计数）</p>
<p>2、在条件1不满足情况下，此时CounterCell数组为空表示还未被创建，那么当前线程把cellsBusy状态改为1（相当于加锁），并把CounterCell数组创建好，默认长度2，创建好后顺便在对应的桶位放入Cell(1)</p>
<p>3、桶位Cell加1操作失败、又没抢到CounterCell数组的创建权，总不能白跑一趟，因此到了这一步当前线程顺便再尝试对baseCount加1，若cas成功就可以直接break。</p>
<p>以上三个分支逻辑对应以下骨架代码：主分支1、主分支2、主分支3，其中主分支1的逻辑设计是最复杂的，它包含6个次分支：1.1到1.6。</p>
<p>这里一定要有一个基本常识：首先，线程的每次循环，只能执行3个主分支中的其中1个。另外一个基本常识：如果线程进入到主分支1，那么在1.1到1.6总共6个次分支中，线程也只能进入其中一个分支</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(;;)</span><br><span class="line">&#123;   <span class="comment">// 主分支1，对应以上第1点</span></span><br><span class="line">    <span class="keyword">if</span> ((as = counterCells) != <span class="keyword">null</span> &amp;&amp; (n = as.length) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 1.1</span></span><br><span class="line">        <span class="keyword">if</span> ((a = as[(n - <span class="number">1</span>) &amp; h]) == <span class="keyword">null</span>) &#123;...&#125;</span><br><span class="line">        <span class="comment">// 1.2</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (!wasUncontended)</span><br><span class="line">          wasUncontended = <span class="keyword">true</span>;</span><br><span class="line">        <span class="comment">// 1.3</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="comment">// 1.4</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (counterCells != as || n &gt;= NCPU)</span><br><span class="line">          collide = <span class="keyword">false</span>;</span><br><span class="line">        <span class="comment">// 1.5</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (!collide)</span><br><span class="line">          collide = <span class="keyword">true</span>;</span><br><span class="line">        <span class="comment">// 1.6</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (cellsBusy == <span class="number">0</span> &amp;&amp;</span><br><span class="line">                 U.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>)) &#123; </span><br><span class="line">        <span class="comment">// 完成CounterCell数组容量2倍扩容</span></span><br><span class="line">          collide = <span class="keyword">false</span>;</span><br><span class="line">          <span class="keyword">continue</span>;   &#125;   </span><br><span class="line">        <span class="comment">// 为当前线程生成新的hash值，用于下一次遍历，（希望它可以定位到另外一个桶位）</span></span><br><span class="line">        h = ThreadLocalRandom.advanceProbe(h);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 主分支2，对应以上第2点</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(cellsBusy==<span class="number">0</span> &amp;&amp; counterCells==as &amp;&amp; U.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>))&#123;&#125;</span><br><span class="line">    <span class="comment">// 主分支3，对应以上第3点</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(U.compareAndSwapLong(<span class="keyword">this</span>, BASECOUNT, v = baseCount, v + x))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>讲解完整源码之前，先了解cellsBusy“锁”的设计理论，cellsBusy本身只是一个volatile变量，而cellsBusy+CAS就可以用无锁方式实现“加锁功能”，线程就是用这种“轻锁机制”去完成CounterCell数组创建、将Cell对象放入桶位、CounterCell数组扩容，用法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ① 尝试“加锁”：若为true就相当于加锁成功，但该锁机制很轻量！</span></span><br><span class="line"><span class="keyword">if</span> (cellsBusy == <span class="number">0</span> &amp;&amp; U.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>))&#123;</span><br><span class="line">   <span class="keyword">try</span>&#123;</span><br><span class="line"><span class="comment">// ② 临界区代码：完成CounterCell数组创建或者将Cell对象放入桶位或者CounterCell数组扩容</span></span><br><span class="line">   &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line"><span class="comment">// ③ 释放“锁”</span></span><br><span class="line">     cellsBusy == <span class="number">0</span></span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>完整源码解析，以下for(;;)循环内部逻辑按前面提到的三个主分支进行分段解析，这是因为太多if和else if条件，需要分段解析才比较清晰其内部设计逻辑</p>
<h5 id="fullAddCount主分支1"><a href="#fullAddCount主分支1" class="headerlink" title="fullAddCount主分支1"></a>fullAddCount主分支1</h5><p>线程从addCount首次进入到fullAddCount时，假设先进入到分支1</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// fullAddCount(x=1, uncontended=false)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">fullAddCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">boolean</span> wasUncontended)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h; </span><br><span class="line"> <span class="comment">// 如果当前线程hash值为0，则强制进行线程hash初始化处理，这里为何直接用Random方法呢？因为从ThreadLocalRandom字面也可以看出，这里是每个线程生成自己用的随机数，可以理解高并发下每个线程都有自己的随机数。</span></span><br><span class="line">    <span class="keyword">if</span> ((h = ThreadLocalRandom.getProbe()) == <span class="number">0</span>) &#123;</span><br><span class="line">        ThreadLocalRandom.localInit();      <span class="comment">// force initialization</span></span><br><span class="line">        h = ThreadLocalRandom.getProbe();</span><br><span class="line">        wasUncontended = <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line"> <span class="comment">// collide：表示线程定位到的Cell桶位是否有冲突，显然当CounterCell数组最后一个桶位都不空，说明已经出现线程操作数组冲突</span></span><br><span class="line">    <span class="keyword">boolean</span> collide = <span class="keyword">false</span>;                <span class="comment">// True if last slot nonempty</span></span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;</span><br><span class="line"> <span class="comment">// as:CounterCell数组的临时变量，a：线程定位到桶位上的Cell对象，n: CounterCell数组的长度，v：桶位上的Cell对象的value属性       </span></span><br><span class="line">        CounterCell[] as; CounterCell a; <span class="keyword">int</span> n; <span class="keyword">long</span> v;</span><br><span class="line">      </span><br><span class="line"> <span class="comment">// 分支1、如果CounterCell数组已经被其他线程创建，接下来当前线程尝试占据桶位来完成加1的任务</span></span><br><span class="line">        <span class="keyword">if</span> ((as = counterCells) != <span class="keyword">null</span> &amp;&amp; (n = as.length) &gt; <span class="number">0</span>) &#123;</span><br><span class="line"> <span class="comment">// 1.1 线程所在的CounterCell桶位为空，有机会将Cell对象放入到桶位上</span></span><br><span class="line">            <span class="keyword">if</span> ((a = as[(n - <span class="number">1</span>) &amp; h]) == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 1.1.2 若cellsBusy未被加锁，则当前线程先创建一个Cell对象，准备放入桶位</span></span><br><span class="line">                <span class="keyword">if</span> (cellsBusy == <span class="number">0</span>) &#123;            <span class="comment">// Try to attach new Cell</span></span><br><span class="line">                    CounterCell r = <span class="keyword">new</span> CounterCell(x); <span class="comment">// 这里的x也就入参1，表示加1</span></span><br><span class="line">      <span class="comment">// 1.1.3 线程再次检查cellsBusy未被其他线程加锁且当前线程对cellsBusy加锁成功，那么接下来当前线程就可以尝试将Cell放入桶位</span></span><br><span class="line">                    <span class="keyword">if</span> (cellsBusy == <span class="number">0</span> &amp;&amp;</span><br><span class="line">                        U.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>)) &#123;</span><br><span class="line">      <span class="comment">// 1.1.4 Cell是否创建的标识，显然程序执行到这，Cell对象还未放入桶位</span></span><br><span class="line">                        <span class="keyword">boolean</span> created = <span class="keyword">false</span>;</span><br><span class="line">                        <span class="keyword">try</span> &#123;               <span class="comment">// Recheck under lock</span></span><br><span class="line">      <span class="comment">// cellsBusy标识锁并不是用于锁CounterCell数组本身，而是数组的某个桶位，CounterCell数组此时可能被线程扩容更改过，因此需要重新检查</span></span><br><span class="line">                            CounterCell[] rs; <span class="keyword">int</span> m, j;</span><br><span class="line">      <span class="comment">// 如果counterCells数组还是一开始的那个数组且已被创建，这时当前线程可以放心将Cell对象放入线程hash定位到对应桶位，并将创建标识created置为true</span></span><br><span class="line">                            <span class="keyword">if</span> ((rs = counterCells) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                                (m = rs.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">                                rs[j = (m - <span class="number">1</span>) &amp; h] == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                rs[j] = r;</span><br><span class="line">                                created = <span class="keyword">true</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="comment">// 当前线程操作完桶位放入cell对象后，释放cellsBusy锁</span></span><br><span class="line">                            cellsBusy = <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">      <span class="comment">// 上面的CounterCell(1)能放入桶位，当然线程完成加1计数任务，可直接返回</span></span><br><span class="line">                        <span class="keyword">if</span> (created)</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">      <span class="comment">// 源代码注释：Slot is now non-empty，说明线程未能成功将Cell(1)对象放在桶位，则再次回到for循环                        </span></span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">      <span class="comment">//  代码执行到这里时，说明分支1.1.2 的cellsBusy被其他线程加锁，自己没抢到更改权，此时先不认为是冲突，使用新hash值 h = ThreadLocalRandom.advanceProbe(h) 回到for循环重试</span></span><br><span class="line">                collide = <span class="keyword">false</span>;</span><br><span class="line">      <span class="comment">// 执行流下一条语句是执行h = ThreadLocalRandom.advanceProbe(h) 然后回到for循环，而不继续执行1.2次分支，这里很容易误解。</span></span><br><span class="line">            &#125;</span><br><span class="line">                </span><br></pre></td></tr></table></figure>
<h5 id="次分支1-2的设计目的："><a href="#次分支1-2的设计目的：" class="headerlink" title="次分支1.2的设计目的："></a>次分支1.2的设计目的：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="comment">// 1.2  // fullAddCount(long x=1, boolean wasUncontended=false)  </span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!wasUncontended)       <span class="comment">// CAS already known to fail</span></span><br><span class="line">                wasUncontended = <span class="keyword">true</span>;      <span class="comment">// Continue after rehash</span></span><br></pre></td></tr></table></figure>
<p>当前线程第1次遇到CounterCell数组不为空的情况下，且线程定位到的桶位不是空，并不急着安排线程对桶位上Cell对象进行cas加1计数。由于addCount方法调用fullAddCount入参wasUncontended时被设为false，因此线程会进入分支1.2，此时wasUncontended设为true表示当前线程暂时还没有遇到竞争，更新hash值（<code>h = ThreadLocalRandom.advanceProbe(h)</code>），再直接回到for进行第2次遍历重试。</p>
<font color=red>如果第2次遍历时，线程所定位的桶位又不为空，由于wasUncontended是true,因此会跳过次分支1.2，来到分支1.3：既然第2次定位到桶位又是不空，那么可以尝试去对桶位上Cell对象直接cas加1操作</font>

<blockquote>
<p>这里为何设计2次遍历？ 因为CounterCell首次被创建时容量为2，也即有<code>rs[0]</code>,<code>rs[1]</code>两个空桶位，假设线程第1次定位到<code>rs[0]</code>不为空（此时不能认为存在线程竞争导致的，只是<code>rs[0]</code>里面已经有Cell对象，这也是wasUncontende设为true的原因）接下来当然是再换线程的hash看看能不能定位到rs[1]桶位，如果恰好定位到rsp[1]是空桶位，那么线程就可以放入Cell对象。如果2次重试线程定位到的桶位都不为空，线程第3次遍历重试就会进入下文的次分支1.3（当然换完hash值也有可能又定位到rs[0]桶位）</p>
</blockquote>
<h5 id="次分支1-3的设计目的："><a href="#次分支1-3的设计目的：" class="headerlink" title="次分支1.3的设计目的："></a>次分支1.3的设计目的：</h5><p>如果第2次遍历时，线程所定位的桶位又不为空，由于wasUncontended是true，因此会跳过次分支1.2，尝试去对桶位上Cell对象直接cas加1操作，如果加1成功，线程就可以退出for循环了，如果失败就来到次分支1.4</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">		<span class="comment">// 1.3</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))</span><br><span class="line">                <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<h5 id="次分支1-5以及1-6的设计目的："><a href="#次分支1-5以及1-6的设计目的：" class="headerlink" title="次分支1.5以及1.6的设计目的："></a>次分支1.5以及1.6的设计目的：</h5><p>不妨先考察次分支1.4两个条件都不成立时的执行流：</p>
<p>这时线程就会跳过分支1.4去执行次分支1.5（因为collide默认为false，此时线程会进入分支1.5），将collide设为true，更新hash值，再直接回到for进行第3次遍历：</p>
<font color=green>**实际含义为：我（当前线程）第1次循环定位桶位不为空，第2次循环定位桶位也不为空，且对桶位上Cell对象cas加1失败，说明出现了明显线程竞争，那么第3次循环重试我就选择去扩容CounterCell数组**</font>

<font color=red>此时线程进行第3次循环时，因为collide已经在上一轮设为true，所以线程在第3次重试时会跳过次分支1.5，来到次分支1.6代码区，此时线程会将CounterCell数组做2倍扩容，然后将collide设为false，表示：既然都扩容了，说明有新空桶位可供操作，线程间的冲突得以解决，当前线程只需直接continue回到for重试，也即源码注释提到的： Retry with expanded table</font>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="comment">// 1.4</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (counterCells != as || n &gt;= NCPU)</span><br><span class="line">            collide = <span class="keyword">false</span>;            <span class="comment">// At max size or stale</span></span><br><span class="line"><span class="comment">// 1.5</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (!collide)</span><br><span class="line">            collide = <span class="keyword">true</span>;</span><br><span class="line"><span class="comment">// 1.6 占有cellsBusy锁</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (cellsBusy == <span class="number">0</span> &amp;&amp;</span><br><span class="line">                 U.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>)) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (counterCells == as) &#123;<span class="comment">// Expand table unless stale</span></span><br><span class="line">                    <span class="comment">// 两倍扩容</span></span><br><span class="line">                    CounterCell[] rs = <span class="keyword">new</span> CounterCell[n &lt;&lt; <span class="number">1</span>];</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i)</span><br><span class="line">                        rs[i] = as[i];</span><br><span class="line">                    counterCells = rs;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                cellsBusy = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">             <span class="comment">// 冲突得以解决</span></span><br><span class="line">            collide = <span class="keyword">false</span>;</span><br><span class="line">             <span class="comment">// 扩容后肯定有空桶位，当然可直接回到for循环重试 </span></span><br><span class="line">          	<span class="keyword">continue</span>;                   <span class="comment">// Retry with expanded table</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h5 id="次分支1-4的设计目的（最难理解的次分支）"><a href="#次分支1-4的设计目的（最难理解的次分支）" class="headerlink" title="次分支1.4的设计目的（最难理解的次分支）"></a>次分支1.4的设计目的（最难理解的次分支）</h5><p>该分支必须结合分支1.5和分支1.6的设计意义来理解：</p>
<p>分支1.6说明：线程一旦遇到明显的冲突，也即多次for循环定位到桶位不为空且对非空桶位的Cell对象cas加1失败（collide=true），那它会执行分支1.6逻辑：对CounterCell数组进行2倍扩容。那么会不会出现这样的情况：<font color=red>如果不断有后续更多线程也遭遇“冲突”，那么就有线程会无休止对CounterCell数组进行2倍扩容，有无停止扩容的条件</font>？</p>
<p>当然是有的，这就是次分支1.4的设计目的：防止CounterCell数组无限被扩容，如何实现？</p>
<p>次分支1.4有2个判断条件：</p>
<p>先看条件2：n &gt;= NCPU，也即CounterCell数组长度达到cpu个数时，就将collide 改为false，更新hash值，再直接回到for重试</p>
<p>而分支1.5、1.6逻辑都被会跳过不执行，下一次循环如果又来到分支1.4，因为n &gt;= NCPU成立，所以分支1.5、1.6逻辑再次被跳过不执行，之后的循环也类似处理。</p>
<p>所以：CounterCell数组停止的条件是，只要它的长度达到cpu个数，当前线程就不会再执行次分支1.6扩容的代码，而是从分支1.4将collide改为false，然后直接回到for循环、</p>
<h5 id="当CounterCell数组不再扩容时，这些线程该如何执行多个分支代码呢"><a href="#当CounterCell数组不再扩容时，这些线程该如何执行多个分支代码呢" class="headerlink" title="当CounterCell数组不再扩容时，这些线程该如何执行多个分支代码呢"></a><font color=red>当CounterCell数组不再扩容时，这些线程该如何执行多个分支代码呢</font></h5><p>显然，当CounterCell数组不再扩容时，那么空桶位很快会被线程们放满Cell对象，也即CounterCell数组数组没有空桶位，之后线程是这么执行的：</p>
<p>次分支1.1 <code>if ((a = as[(n - 1) &amp; h]) == null)</code>不再成立，所以会跳到次分支1.2，由于wasUncontended早已被设为true（请回去看次分支1.2的设计目的），次分支1.2也会被跳过，线程继续来到次分支1.3，此时线程会使用cas对Cell对象加1操作，若加成功就可以break，否则就来到次分支1.4，因为n=&gt;NCPU一直为true，将collide改为false，更新hash值，再直接回到for重试。</p>
<p><font color=red>简单总结：当CounterCell数组不再扩容时，且没有空桶位时，这些更新线程其实就会简化成以下的执行逻辑</font>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(;;)</span><br><span class="line">&#123;  </span><br><span class="line">  	<span class="comment">// 当前线程使用cas对Cell对象加1操作，加成功就结束循环！，否则更新hash值重试</span></span><br><span class="line">		<span class="keyword">if</span> (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))&#123;</span><br><span class="line">				<span class="keyword">break</span>;</span><br><span class="line">				&#125;</span><br><span class="line">  	<span class="comment">// 更新当前线程hash值，以便下一次重试能够定位不同于本次的桶位</span></span><br><span class="line">        h = ThreadLocalRandom.advanceProbe(h);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从以上简化代码可以看出，线程一定能在某个时刻在对应桶位的Cell对象上加1成功，然后退出循环，这也是fullAddCOunt设计真实目的：保证每个线程进入到fullAddCount到离开fullAddCount之前一定能够cas加1成功</p>
<p>此时，我们再来考察次分支1.4的条件1：</p>
<p>有了条件2的铺垫，其他条件1很好理解：counterCells != as，表明恰好有其他线程正在扩容CounterCell数组（导致as指向新的CounterCells数组对象），接下来可能在扩容后的CounterCell数组有更多的空桶位，那么当前线程就没必要去竞争扩容了，只需将collide 改为false，更新自己hash值，然后直接回到for循环重试看看能否运气好拿到新的空桶位。</p>
<h5 id="fullAddCount主分支2："><a href="#fullAddCount主分支2：" class="headerlink" title="fullAddCount主分支2："></a>fullAddCount主分支2：</h5><p>若CounterCell数组为空时（还未被创建），也即fullAddCount的主分支1会被跳过，当前线程会进入主分支2代码区，那么当前线程把cellsBusy状态改为1（相当于加锁），创建好一个默认长度为2的CounterCell数组，并顺便在对应的桶位放入Cell(1)完成加1计数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (cellsBusy == <span class="number">0</span> &amp;&amp; counterCells == as &amp;&amp;</span><br><span class="line">           U.compareAndSwapInt(<span class="keyword">this</span>, CELLSBUSY, <span class="number">0</span>, <span class="number">1</span>)) &#123;</span><br><span class="line">      <span class="keyword">boolean</span> init = <span class="keyword">false</span>;</span><br><span class="line">      <span class="keyword">try</span> &#123;                           <span class="comment">// Initialize table</span></span><br><span class="line"><span class="comment">// 再次确认counterCells数组没有被其他线程改动过，如果被改动过，那么就跳过初始化并释放cellsBusy“锁”</span></span><br><span class="line">          <span class="keyword">if</span> (counterCells == as) &#123;</span><br><span class="line"><span class="comment">//  CounterCell数组就是在这里首次被创建</span></span><br><span class="line">              CounterCell[] rs = <span class="keyword">new</span> CounterCell[<span class="number">2</span>];</span><br><span class="line"><span class="comment">// h&amp;1结果：要么0要么1，也即将Cell(1)对象放在两个空桶位rs[0]、rs[1]中的任一个</span></span><br><span class="line">              rs[h &amp; <span class="number">1</span>] = <span class="keyword">new</span> CounterCell(x);</span><br><span class="line">              counterCells = rs;</span><br><span class="line">              init = <span class="keyword">true</span>;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"><span class="comment">// 既然当前线程已经完成创建CounterCell数组，当然可以释放cellsBusy锁</span></span><br><span class="line">          cellsBusy = <span class="number">0</span>;</span><br><span class="line">      &#125;</span><br><span class="line"><span class="comment">//当前线程完成CounterCell数组初始化后，因为在创建期间就完成Cell(1)加1计数，故可直接break退出。</span></span><br><span class="line">      <span class="keyword">if</span> (init)</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="fullAddCount主分支3："><a href="#fullAddCount主分支3：" class="headerlink" title="fullAddCount主分支3："></a>fullAddCount主分支3：</h5><p> 主分支3：线程执行fullAddCount的主分支1桶位Cell加1操作失败，接着执行fullAddCount主分支2时又没抢到CounterCell数组的创建权，</p>
<p>那么当前线程总不能白跑一趟，因此到了这一步当前线程顺便尝试对baseCount做加加1计数（或者加x计数，x代表正负数都可以），若成功就退出，失败就回到for循环重试</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   </span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapLong(<span class="keyword">this</span>, BASECOUNT, v = baseCount, v + x))</span><br><span class="line">            <span class="keyword">break</span>;                          <span class="comment">// Fall back on using base</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>小结：从三个主分支的设计来看，主分支1无疑是最复杂的，因为判断多，条件多，理解起来象比较绕，需要用全局、逆向思维去分析，最好是先找到一个容易理解的分支，然后根据这个分支的实际含义来顺藤摸瓜再找到其他分支的实际含义</p>
<h5 id="从for循环退出的条件也可理解fullAddCount的内在设计逻辑"><a href="#从for循环退出的条件也可理解fullAddCount的内在设计逻辑" class="headerlink" title="从for循环退出的条件也可理解fullAddCount的内在设计逻辑"></a>从for循环退出的条件也可理解fullAddCount的内在设计逻辑</h5><p>有以上详细的源码解释后，考察线程for自旋退出的条件（成功计数加1的条件），其实就是对应前面提到的3个主分支。</p>
<p>1、第2552行，主分支1-次分支1.1，当前线程对应的桶位为null然后成功放入Cell对象时可退出for自旋</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (created)</span><br><span class="line">    <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>2、第2561行，主分支1-次分支1.3，进入CounterCell数组的操作逻辑，当前线程对非空桶位的Cell对象，使用CAS加1成功时可退出for自旋</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))</span><br><span class="line">    <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>3、第2597行，主分支2：当前线程创建默认长度2的CounterCell数组并选其中一个桶位放入Cell对象后可退出for自旋</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (init)</span><br><span class="line">    <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>4、第2600行，主分支3：当前线程以上尝试三种逻辑都失败时，总不能白跑一趟，再尝试去CAS对baseCount加1，成功时可退出for循环</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapLong(<span class="keyword">this</span>, BASECOUNT, v = baseCount, v + x))</span><br><span class="line">    <span class="keyword">break</span>;                          <span class="comment">// Fall back on using base</span></span><br></pre></td></tr></table></figure>
<p>关于addcount完成计数加1后的CHM扩容逻辑分析，则单独在另外一篇博客给出，因为它的设计相对复杂，需要分析的内容较多。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度讨论官方关于jdk1.8ConcurrentHashMap的computeIfAbsent源代码修复逻辑</title>
    <url>/2021/10/10/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%AE%A8%E8%AE%BA%E5%AE%98%E6%96%B9%E5%85%B3%E4%BA%8Ejdk1.8ConcurrentHashMap%E7%9A%84computeIfAbsent%E6%BA%90%E4%BB%A3%E7%A0%81%E4%BF%AE%E5%A4%8D%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>在文章中《深度解析官方关于jdk1.8的resizeStamp的bug处理过程》，我们讨论关于CHM的核心设计——resizeStam需要修复的处理过程，本文再次基于openJDK的bugs讨论组提出的CHM源代码另外一个会造成死循环的bug，默认读者已经掌握CHM的核心源代码实现，否则无法从本文的讨论中获益。文章前部分先把computeIfAbsent的bug成因分析清楚，再来介绍官网<code>ConcurrentHashMap.computeIfAbsent stuck in an endless loop</code>的讨论过程，这样更容易看懂相关内容。</p>
<p>研究openJDK官方公布的相关源码bug有何“收益”：</p>
<p>虽然这些bug不是特别严重，修复起来也即几行代码，但如果想要解决这种看似“简单的bug”，要求对CHM设计原理、类、方法实现细节足够熟悉，也就是说，你要具备（至少在这个bug上下文的类、方法范围内）和源代码设计者同等思考视角才能去挖掘bug的本质原因并提出合理的修复建议。换句话说，你研究的不是这个bug本身，而是深入精通整个类的源代码实现，这种高级收益在日常业务开发几乎无法获得。</p>
<p><img src="https://img-blog.csdnimg.cn/e850dc9870704077af0bb9da6e7810c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>《gitee 博客文章封面》</p>
<h4 id="认识computeIfAbsent用法"><a href="#认识computeIfAbsent用法" class="headerlink" title="认识computeIfAbsent用法"></a>认识computeIfAbsent用法</h4><p>理解computeIfAbsent在一些场合下的用法，有助于帮助切入源代码分析。</p>
<h5 id="computeIfAbsent使用场景1："><a href="#computeIfAbsent使用场景1：" class="headerlink" title="computeIfAbsent使用场景1："></a>computeIfAbsent使用场景1：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo1</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeKeyLength</span><span class="params">(String key)</span></span>&#123; <span class="comment">// 计算key的长度，将其作为该key对应的value</span></span><br><span class="line">        <span class="keyword">return</span> key.length();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ConcurrentHashMap&lt;String,Integer&gt; map=<span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">        map.put(<span class="string">&quot;foo&quot;</span>,<span class="number">1</span>);</span><br><span class="line">        map.computeIfAbsent(<span class="string">&quot;foobar&quot;</span>,key-&gt;computeKeyLength(key));</span><br><span class="line">        System.out.println(map); <span class="comment">//输出 &#123;foobar=6, foo=1&#125;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>computeIfAbsent字面意思：如果key不在map里面，那么就使用给定的匿名函数（也叫映射函数）将key对应的value“计算出来”。（匿名函数也即lambda语法是jdk1.8语法新特性，这一点不必多说）</p>
<p>按这个思路可以有以下解释：</p>
<a id="more"></a>
<p>因为字符串”foobar”这个key不在map里面，因此把它放入map的同时，对应的value要用给定的函数computeKeyLength计算出来，例如这里调用computeKeyLength计算值为6，因此有key=foobar，value=6，将其放入map中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">map.put(<span class="string">&quot;bar&quot;</span>,<span class="number">10</span>);</span><br><span class="line">map.computeIfAbsent(<span class="string">&quot;bar&quot;</span>,key-&gt;computeKeyLength(key));</span><br><span class="line">System.out.println(map); <span class="comment">// 输出：&#123;bar=10&#125;</span></span><br></pre></td></tr></table></figure>
<p>若key已经在map时，value不会被<code>computeKeyLength(key)</code>的计算值6所覆盖。</p>
<p>当然此demo做了一个不优雅的示范：既然可用匿名函数的写法去写逻辑，就没必要基于方法computeKeyLength去封装多一层，最简便写法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">map.computeIfAbsent(<span class="string">&quot;foobar&quot;</span>,key-&gt;key.length());</span><br></pre></td></tr></table></figure>
<p>注意这个key可以作为匿名函数的入参去参与到计算value，也可以不作为匿名函数的入参，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map.computeIfAbsent(&quot;foobar&quot;,key-&gt;10);</span><br></pre></td></tr></table></figure>
<p>显然foobar=10。</p>
<h5 id="computeIfAbsent使用场景2："><a href="#computeIfAbsent使用场景2：" class="headerlink" title="computeIfAbsent使用场景2："></a>computeIfAbsent使用场景2：</h5><p>并发场景下的频率统计：该demo方法其实在并发计数器LongAdder这个类的源码注释里面，Doug Lea已经告诉我们一个经典的场景恰好需要使用computeIfAbsent方法</p>
<blockquote>
<p>LongAdders can be used with a java.util.concurrent.ConcurrentHashMap to maintain a scalable frequency map (a form of histogram or multiset). For example, to add a count to a ConcurrentHashMap<String,LongAdder> freqs, initializing if not already present, you can use freqs.computeIfAbsent(k -&gt; new LongAdder()).increment();</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.LongAdder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ConcurrentHashMap&lt;String,LongAdder&gt; map=<span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">        String[] strings=&#123;<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;foo&quot;</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span> (String key : strings) &#123;</span><br><span class="line">           map.computeIfAbsent(key,k-&gt; <span class="keyword">new</span> LongAdder()).increment();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(map); <span class="comment">//输出 &#123;bar=1, foo=3&#125;</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该demo虽然只是用单个线程去执行computeIfAbsent，但逻辑是清晰的：实现对字符串出现次数进行统计</p>
<ul>
<li>关于LongAdder的分析，它内部其实有一个像ConcurrentHashMap的fullAddCount并发计数逻辑，这里不再讨论，有关研究可参考本博客的文章《Java并发进阶系列：LongAdder高并发计数性能分析》</li>
</ul>
<h5 id="computeIfAbsent方法源码解析"><a href="#computeIfAbsent方法源码解析" class="headerlink" title="computeIfAbsent方法源码解析"></a>computeIfAbsent方法源码解析</h5><p>这部内容要求读者已经掌握jdk1.8的ConcurrentHashMap设计及其关键方法的源代码实现逻辑，否则将难以理解其含义。本节所提的computeIfAbsent是未修复前的版本，这里并不会详细解析computeIfAbsent每一行代码，因为它跟putVal方法逻辑几乎一样，而不同地方可参考以下数字标记的说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">computeIfAbsent</span><span class="params">(K key, Function&lt;? <span class="keyword">super</span> K, ? extends V&gt; mappingFunction)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span> || mappingFunction == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException()</span><br><span class="line">    <span class="keyword">int</span> h = spread(key.hashCode()); </span><br><span class="line">    V val = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">int</span> binCount = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = table;;) &#123; <span class="comment">//看到这个写法应该很熟悉了：自旋+cas机制，为啥要自旋，因为线程不保证自己一次cas就成功，如果和其他线程竞争失败，则需要重试cas，这就是“自旋+cas机制”的黄金搭配。</span></span><br><span class="line">        Node&lt;K,V&gt; f; <span class="keyword">int</span> n, i, fh;</span><br><span class="line">        <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">            tab = initTable();</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>) &#123; </span><br><span class="line">          	<span class="comment">//① 如果key对应的桶位为空，先创建一个保留节点用于接下里的占位逻辑</span></span><br><span class="line">            Node&lt;K,V&gt; r = <span class="keyword">new</span> ReservationNode&lt;K,V&gt;();</span><br><span class="line">          	<span class="comment">// ②当前线程用保留节点占位当然需要借用独占锁对r对象进行加锁</span></span><br><span class="line">            <span class="keyword">synchronized</span> (r) &#123;</span><br><span class="line">              	<span class="comment">// 在当前桶位放置保留节点用于占位，占位之后就可以给该桶位放入新建的node节点</span></span><br><span class="line">                <span class="keyword">if</span> (casTabAt(tab, i, <span class="keyword">null</span>, r)) &#123;</span><br><span class="line">                    binCount = <span class="number">1</span>;</span><br><span class="line">                    Node&lt;K,V&gt; node = <span class="keyword">null</span>;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">   <span class="comment">/*putVal在桶位为空时的逻辑，可看到非常简单，直接使用cas给当前桶位设置新节点，value是给定的value，不需要通过函数计算出value</span></span><br><span class="line"><span class="comment">  if (casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) &#123;</span></span><br><span class="line"><span class="comment"> 				 break;  </span></span><br><span class="line"><span class="comment">  			&#125;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">                      <span class="comment">// 对于computeIfAbsent，value是需要用给定的匿名函数计算出的，正如前面场景1的“bar”这个key对应的“value”就是使用computeKeyLength(key)计算处理的值</span></span><br><span class="line">                        <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>)</span><br><span class="line">                            node = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val, <span class="keyword">null</span>);</span><br><span class="line">                    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                      	<span class="comment">//虽然在②步骤那里已经在桶位i放置了一个ReservationNode用于占位，到了这个步骤才是真正把数据节点node放入桶位i当中</span></span><br><span class="line">                        setTabAt(tab, i, node);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// 显然②步骤一定能成功在桶位i放入node节点（binCount=1），既然已经将key和value放入map，那么任务完成，当前线程退出自旋</span></span><br><span class="line">            <span class="keyword">if</span> (binCount != <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//③如果key定位到的桶位i恰好是一个ForwardingNode占位节点，那么当前线程要去参与“帮助扩容”的逻辑，这里跟putval一样。</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)</span><br><span class="line">            tab = helpTransfer(tab, f);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//代码若执行到这，说明桶位i是一个链表或者一棵红色树 </span></span><br><span class="line">            <span class="keyword">boolean</span> added = <span class="keyword">false</span>;</span><br><span class="line">            <span class="comment">// 当前线程先给头节点加独占锁，保证当前线程写入节点操作时的独占性</span></span><br><span class="line">            <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">      <span class="comment">//并发环境，这里当然还要二次检查头节点是不是刚刚加锁前的头节点（也即检查加锁前后的头节点有无被改动过）</span></span><br><span class="line">                <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">                     <span class="comment">// ④f节点是链表的情况</span></span><br><span class="line">                    <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                        binCount = <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">for</span> (Node&lt;K,V&gt; e = f;; ++binCount) &#123;</span><br><span class="line">                            K ek; V ev;</span><br><span class="line">                            <span class="comment">// 在链表中遇到相同的key，那么就不做更新value操作，返回旧value</span></span><br><span class="line">                            <span class="keyword">if</span> (e.hash == h &amp;&amp;</span><br><span class="line">                                ((ek = e.key) == key ||</span><br><span class="line">                                 (ek != <span class="keyword">null</span> &amp;&amp; key.equals(ek)))) &#123;</span><br><span class="line">                                val = e.val;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                            Node&lt;K,V&gt; pred = e;</span><br><span class="line">                          	<span class="comment">// 尾插法：来到链表尾部</span></span><br><span class="line">                            <span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                               <span class="comment">// key不在链表时，value则由给定的匿名函数计算而来</span></span><br><span class="line">                                <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                                    added = <span class="keyword">true</span>;</span><br><span class="line">                                    <span class="comment">//尾插法</span></span><br><span class="line">                                    pred.next = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val, <span class="keyword">null</span>);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// ④f节点是TreeBin的情况（该桶位是一棵红黑树）</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                        binCount = <span class="number">2</span>;</span><br><span class="line">                        TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span><br><span class="line">                        TreeNode&lt;K,V&gt; r, p;</span><br><span class="line">                     </span><br><span class="line">                        <span class="keyword">if</span> ((r = t.root) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                            (p = r.findTreeNode(h, key, <span class="keyword">null</span>)) != <span class="keyword">null</span>)</span><br><span class="line">                            val = p.val;</span><br><span class="line">                      	 <span class="comment">// key不在树里面时，value则由给定的匿名函数计算而来</span></span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            added = <span class="keyword">true</span>;</span><br><span class="line">                            t.putTreeVal(h, key, val);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (binCount != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD)</span><br><span class="line">                    treeifyBin(tab, i);</span><br><span class="line">                <span class="keyword">if</span> (!added)</span><br><span class="line">                    <span class="keyword">return</span> val;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  	//以下两行是putVal的逻辑：CHM添加一个节点后，需要对size加1计数</span></span><br><span class="line"><span class="comment">    addCount(1L, binCount);</span></span><br><span class="line"><span class="comment">    return null;</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">if</span> (val != <span class="keyword">null</span>)</span><br><span class="line">        addCount(<span class="number">1L</span>, binCount);</span><br><span class="line">    <span class="keyword">return</span> val;<span class="comment">// </span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>相信经过以上“可理解的场景使用和源代码分析”，computeIfAbsent应该能掌握了，下面进入官方bug解析的流程，具有较高水平的知识点，值得阅读！</p>
<h4 id="an-endless-loop"><a href="#an-endless-loop" class="headerlink" title="an endless loop"></a>an endless loop</h4><p>具体提交页面参考<a href="https://bugs.openjdk.java.net/browse/JDK-8062841">官方bug描述页面</a></p>
<p>问题：ConcurrentHashMap.computeIfAbsent stuck in an endless loop</p>
<p>该提交者提交方式是那种示范性的提问方式，提供较为详细的辅助信息：jdk版本、操作系统信息、问题描述、以及复现bug的测试代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> at.irian.misc.javabug;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Map&lt;String, Integer&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">        map.computeIfAbsent(</span><br><span class="line">                <span class="string">&quot;AaAa&quot;</span>,</span><br><span class="line">          			<span class="comment">// 建议改写为 k1-&gt; map.computeIfAbsent(&quot;BBBB&quot;,k2-&gt;42)，相对直观易懂</span></span><br><span class="line">                key -&gt; &#123;</span><br><span class="line">                    <span class="keyword">return</span> map.computeIfAbsent(<span class="string">&quot;BBBB&quot;</span>,key2 -&gt; <span class="number">42</span>);</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">               </span><br></pre></td></tr></table></figure>
<p>有了前面computeIfAbsent用法的介绍，提交者提交的测试代码其实很好理解：</p>
<p>“AaAa”对应的value需要根据<code>map.computeIfAbsent(&quot;BBBB&quot;,key2 -&gt; 42)</code>计算出来，易知value返回的是42</p>
<p>本来期待map的结果是<code>&#123;AaAa=42,BBBB=42&#125;</code>，当执行时，发现程序“卡住了”不能结束，说明bug已复现，但如何一步一步追踪它呢？</p>
<p>考虑到提交者的测试代码写得比较一般，比如匿名函数写个return显然是多余的，再例如为了制造两个key哈希冲突，写了个“AaAa”、“BBBB”，这当然无大碍，只是看起来有点“Counterintuitive”，因此这里给出相对容易接受的测试代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        ConcurrentHashMap&lt;Name, Integer&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">        map.computeIfAbsent(</span><br><span class="line">                <span class="keyword">new</span> Name(<span class="string">&quot;foo&quot;</span>),</span><br><span class="line">                k1-&gt; map.computeIfAbsent(<span class="keyword">new</span> Name(<span class="string">&quot;bar&quot;</span>),k2-&gt;<span class="number">10</span>)</span><br><span class="line">          			<span class="comment">/*或者使用 k1-&gt; map.computeIfAbsent(new Name(&quot;bar&quot;),k2-&gt; k2.key.length()) </span></span><br><span class="line"><span class="comment">          			这里的k2是指new Name(&quot;bar&quot;)，那么k2.key就是“bar”这个字符串，那么k2.key.length()就是</span></span><br><span class="line"><span class="comment">          			计算k2里面字符串的长度，预期输出结果为&#123;bar=3,foo=3&#125;</span></span><br><span class="line"><span class="comment">          			*/</span></span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 定义一个Name对象，用于作为map的key</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Name</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String key;</span><br><span class="line">        Name(String key)&#123;</span><br><span class="line">            <span class="keyword">this</span>.key=key;</span><br><span class="line">        &#125;</span><br><span class="line">     </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          	<span class="comment">// 重写hashCode方法，保证每个不同key计算的哈希值都一样，目的是让不同的key直接在同一桶位上发生哈希冲突，以便观察bug的执行流程。显然根据桶位计算方法：i = (16 - 1) &amp; 1，由此可知，不同Name的key都会在桶位1上发生冲突。</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> key;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于<code>new Name(&quot;foo&quot;)</code>和<code>new Name(&quot;bar&quot;)</code>这两个不同key，其哈希值都是1，期待运行结果：<code>&#123;foo=10,bar=10&#125;</code>,实际运行结果：程序陷入了死循环，复现了源代码的bug。</p>
<h4 id="深挖原因"><a href="#深挖原因" class="headerlink" title="深挖原因"></a>深挖原因</h4><p>借助IDEA debug即可完成此任务。在computeIfAbsent的以下代码处打上断点：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>) &#123; <span class="comment">//这行不是断点位置 </span></span><br><span class="line">    Node&lt;K,V&gt; r = <span class="keyword">new</span> ReservationNode&lt;K,V&gt;(); <span class="comment">// 这行是断点位置</span></span><br></pre></td></tr></table></figure>
<p>根据Name的固定hash值可知道，当首次执行将<code>new Name(&quot;foo&quot;)</code>放入桶位时，程序第一循环先完成table的初始化创建，即如下逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = table;;) &#123;</span><br><span class="line">    Node&lt;K,V&gt; f; <span class="keyword">int</span> n, i, fh;</span><br><span class="line">  	<span class="comment">// ① 第1次循环，table为空，因此需要执行初始化逻辑</span></span><br><span class="line">    <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">        tab = initTable();</span><br><span class="line">  	<span class="keyword">else</span> <span class="keyword">if</span>&#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>table的初始化之后，进行第2次循环，会进入到以下逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ② 第2次循环，会进入此逻辑，这好理解，因为new Name(&quot;foo&quot;)这个key的哈希值为1，首次放入key之前，桶位1的头节点f一定是null的</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>) &#123;</span><br><span class="line">            	Node&lt;K,V&gt; r = <span class="keyword">new</span> ReservationNode&lt;K,V&gt;(); <span class="comment">//这行是断点位置</span></span><br><span class="line">            	<span class="keyword">synchronized</span> (r) &#123; <span class="comment">// ③</span></span><br><span class="line">                	<span class="keyword">if</span> (casTabAt(tab, i, <span class="keyword">null</span>, r)) &#123;</span><br><span class="line">                    	binCount = <span class="number">1</span>;</span><br><span class="line">                    	Node&lt;K,V&gt; node = <span class="keyword">null</span>;</span><br><span class="line">                    	<span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="comment">//</span></span><br><span class="line">                        	<span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>)</span><br></pre></td></tr></table></figure>
<p>以上两点是要说明：运行debug时虽然执行流会马上停在断点代码位置，但在暂停前，代码已经执行完第1次循环（初始化操作）和正在进行第2次循环</p>
<p>因为CHM是并发的，因此进入加锁区后不是先把new Name(“foo”)这个节点直接放入桶位1中，而是先放一个保留节点用于占位（好让其他线程看到该桶位是一个保留节点后就转去做其他事情），然后才进入以下try代码区</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>)<span class="comment">// ④</span></span><br><span class="line">        node = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val, <span class="keyword">null</span>);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    setTabAt(tab, i, node);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>标记④位置的代码就是揭开bug的关键入口点，在try代码块里面，new Name(“foo”)这个key的value是用指定的匿名函数（或者称为映射函数）计算出来，因此④位置代码是调用给定匿名函数去计算value，如果Step Into到这个位置，那么接下来再继续Step Into就会来测试代码这一位置<code>k1-&gt; map.computeIfAbsent(new Name(&quot;bar&quot;),k2-&gt;10)</code>：</p>
<p>说明new Name(“foo”)这个key要想放入桶位1，得先等<code>map.computeIfAbsent(new Name(&quot;bar&quot;),k2-&gt;10)</code>逻辑执行完成。</p>
<p>从Frames的线程方法调用栈也可以看出相关逻辑：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6c404db5015738104b55964eeeb0db55.png" alt="computeIfAbsent_1"></p>
<p>继续Step Into操作当然是再次进入computeIfAbsent方法内部，接下来，对于key为<code>new Name(&quot;bar&quot;)</code>，显然它也能定位到桶位1，接下来好办，进入for循环（自旋）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> V <span class="title">computeIfPresent</span><span class="params">(K key, BiFunction&lt;? <span class="keyword">super</span> K, ? <span class="keyword">super</span> V, ? extends V&gt; remappingFunction)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (key == <span class="keyword">null</span> || remappingFunction == <span class="keyword">null</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">      <span class="keyword">int</span> h = spread(key.hashCode());</span><br><span class="line">      V val = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">int</span> delta = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">int</span> binCount = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = table;;) &#123;</span><br><span class="line">          Node&lt;K,V&gt; f; <span class="keyword">int</span> n, i, fh;</span><br><span class="line">          <span class="comment">//① table已不为空，所以执行流不会进入此逻辑</span></span><br><span class="line">          <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">              tab = initTable();</span><br><span class="line">        	<span class="comment">//② 执行第一次computeIfAbsent时，桶位1被放置了一个保留节点，因此桶位1不再为空，所以执行流不会进入此逻辑。注意注意：此桶位1还未放入new Name(&quot;foo&quot;)这个节点！因为new Name(&quot;foo&quot;)还在等new Name(&quot;bar&quot;)这个key计算value</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>)&#123;&#125;</span><br><span class="line">        	<span class="comment">//③ 桶位1放的是一个保留节点ReservationNode，显然不是ForwardingNode节点，所以执行流不会进入此逻辑。</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)&#123;&#125;</span><br><span class="line">        	<span class="comment">//④ 前面三个条件不满足，执行流最终会进入此逻辑，这里解析bug的关键逻辑！</span></span><br><span class="line">        	<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//4.1</span></span><br><span class="line">            <span class="comment">//4.2</span></span><br><span class="line">            <span class="comment">//4.3</span></span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>
<p>继续深入④逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//④ 前面三个条件不满足，执行流最终会进入此逻辑，这里解析bug的关键逻辑！</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">boolean</span> added = <span class="keyword">false</span>;</span><br><span class="line">    <span class="comment">//对桶位上的头节点f加独占锁（这个f节点显然还是new Name(&quot;foo&quot;)放入的保留节点ReservationNode）</span></span><br><span class="line">  	<span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">    		<span class="comment">//4.1 f是ReservationNode节点显然不是链表，所以执行流不会进入此逻辑</span></span><br><span class="line">            <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                binCount = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">for</span> (Node&lt;K,V&gt; e = f;; ++binCount) &#123;</span><br><span class="line">                    K ek; V ev;</span><br><span class="line">                    <span class="keyword">if</span> (e.hash == h &amp;&amp;</span><br><span class="line">                        ((ek = e.key) == key ||</span><br><span class="line">                         (ek != <span class="keyword">null</span> &amp;&amp; key.equals(ek)))) &#123;</span><br><span class="line">                        val = e.val;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    Node&lt;K,V&gt; pred = e;</span><br><span class="line">                    <span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            added = <span class="keyword">true</span>;</span><br><span class="line">                            pred.next = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val, <span class="keyword">null</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">//4.2 f是ReservationNode节点,显然也不是TreeBin节点，所以执行流不会进入此逻辑</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                binCount = <span class="number">2</span>;</span><br><span class="line">                TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span><br><span class="line">                TreeNode&lt;K,V&gt; r, p;</span><br><span class="line">                <span class="keyword">if</span> ((r = t.root) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                    (p = r.findTreeNode(h, key, <span class="keyword">null</span>)) != <span class="keyword">null</span>)</span><br><span class="line">                    val = p.val;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    added = <span class="keyword">true</span>;</span><br><span class="line">                    t.putTreeVal(h, key, val);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">//4.3 程序执行到这里发现都无法满足以上条件来放入new Name(&quot;bar&quot;)节这个节点，而new Name(&quot;foo&quot;)这个key又在等待new Name(&quot;bar&quot;)计算value的返回值，也即当前桶位还未放入这两个key节点，因此binCount还是初始值0，所以执行流不会进入此逻辑</span></span><br><span class="line">    <span class="keyword">if</span> (binCount != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD)</span><br><span class="line">            treeifyBin(tab, i);</span><br><span class="line">        <span class="keyword">if</span> (!added)</span><br><span class="line">            <span class="keyword">return</span> val;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//⑤ 执行流回到for循环继续，接下来就是一直死循环了</span></span><br></pre></td></tr></table></figure>
<p>总结以上流程，用骨架代码解释死循环发生的过程：</p>
<p>对于new Name(“bar”)这个节点进入computeIfAbsent后，发生以下循环问题：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = table;;) &#123;</span><br><span class="line">          Node&lt;K,V&gt; f; <span class="keyword">int</span> n, i, fh;</span><br><span class="line">          <span class="comment">//① table已不为空，所以执行流不会进入此逻辑</span></span><br><span class="line">          <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">              tab = initTable();</span><br><span class="line">        	<span class="comment">//② 执行第一次computeIfAbsent时，该桶位已经是ReservationNode节点，所以执行流不会进入此逻辑！</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>)&#123;&#125;</span><br><span class="line">        	<span class="comment">//③ f是ReservationNode节点不是fwd节点，所以执行流不会进入此逻辑</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)&#123;&#125;</span><br><span class="line">        	<span class="comment">//④ 前面三个条件不满足，执行流最终会进入此逻辑，这里解析bug的关键逻辑！</span></span><br><span class="line">        	<span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//4.1 f是ReservationNode节点不是链表，所以执行流不会进入此逻辑</span></span><br><span class="line">            <span class="comment">//4.2 f是ReservationNode节点不是TreeBin节点，所以执行流不会进入此逻辑</span></span><br><span class="line">            <span class="comment">//4.3 binCount还是初始值0，所以执行流不会进入此逻辑</span></span><br><span class="line">            <span class="comment">// 回到for循环：接下里即陷入死循环</span></span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>
<p>有没发现当准备为new Name(“bar”)节点找满足条件来插入节点时，发现for循环里面的7个条件（①、②、③、④、4.1、4.2、4.3)）都不满足，那执行流接下里怎么办？ 只能继续下一轮循环，下一轮循依旧出现7个条件都不满足只能再继续循环，因此进入了死循环陷阱。</p>
<p>本质原因是map写入节点的操作恰好同时满足以下两个条件：</p>
<p>（1）两个key定位到相同的桶位（hash冲突）</p>
<p>（2）一个key调用<code>map.computeIfAbsent</code>计算value的过程中又递归调用<code>map.computeIfAbsent</code>计算另外一个key的value</p>
<h4 id="使用jstack定位死循环位置"><a href="#使用jstack定位死循环位置" class="headerlink" title="使用jstack定位死循环位置"></a>使用jstack定位死循环位置</h4><p>既然已经掌握computeIfAbsent出现无限循环的原理，那么当发生无限循环时，可以使用jstack快速定位位置。</p>
<p>使用top -u 查看cpu使用率最高的进程号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ top -u </span><br><span class="line">PID    COMMAND      %CPU  TIME     #TH   #WQ  #PORTS MEM    PURG   CMPRS  PGRP  PPID  STATE</span><br><span class="line">40929  java         90.5  00:31.31 18&#x2F;1  1    75     17M    0B     0B     8828  8828  running</span><br></pre></td></tr></table></figure>
<p>再使用jps打印所有java进程执行，进程号40929对应的主类是Demo3，因此进程号40929就是目标处理对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> kevent@MacBookPro  ~  jps</span><br><span class="line">40928 Launcher</span><br><span class="line">40929 Demo3</span><br><span class="line">40967 Jps</span><br><span class="line">8828</span><br></pre></td></tr></table></figure>
<p>使用jstack打印40929进程里面的所有线程方法调用栈信息并输出文件中（把信息放在文本中方便查阅，否则直接在terminal打印不方便分析）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jstack -l 40929 &gt; 40929.txt  # -l选项表示长列表，会打印出更为详细关于锁的信息（如果有死锁被被监测到并打印出来）</span><br></pre></td></tr></table></figure>
<p>如果程序fork了很多线程，还需要使用<code>top -H -p pid</code>来定位哪个线程的cpu使用率最高，然后再用线程号去jstack 输出文本里面grep到对应线程的调用栈信息。</p>
<p>打印出信息如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ jstack -l 40929</span><br><span class="line">Full thread dump OpenJDK 64-Bit Server VM (25.252-b09 mixed mode):</span><br><span class="line"></span><br><span class="line"># 省略其他输出</span><br><span class="line"></span><br><span class="line">&quot;main&quot; #1 prio&#x3D;5 os_prio&#x3D;31 tid&#x3D;0x00007fc89300b000 nid&#x3D;0x1003 runnable [0x0000700009038000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">	at concurrent.demo.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1747)</span><br><span class="line">	at concurrent.demo.Demo3.lambda$main$1(Demo3.java:7)</span><br><span class="line">	at concurrent.demo.Demo3$$Lambda$1&#x2F;250421012.apply(Unknown Source)</span><br><span class="line">	at concurrent.demo.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1688)</span><br><span class="line">	- locked &lt;0x0000000795864ea0&gt; (a concurrent.demo.ConcurrentHashMap$ReservationNode)</span><br><span class="line">	at concurrent.demo.Demo3.main(Demo3.java:5)</span><br><span class="line"></span><br><span class="line">   Locked ownable synchronizers:</span><br><span class="line">	- None</span><br><span class="line"># 省略其他输出</span><br></pre></td></tr></table></figure>
<p>Demo3执行后一直“卡着”并不会结束运行，上面显示main主线程，它的状态是RUNNABLE，这个正是说明当前Demo3一直在运行中没有结束。以下内容是主线程的方法调用栈，可以看到栈顶的computeIfAbsent方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">at concurrent.demo.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1747)</span><br><span class="line">at concurrent.demo.Demo3.lambda$main$1(Demo3.java:7)</span><br><span class="line">at concurrent.demo.Demo3$$Lambda$1&#x2F;250421012.apply(Unknown Source)</span><br><span class="line">at concurrent.demo.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1688)</span><br><span class="line">- locked &lt;0x0000000795864ea0&gt; (a concurrent.demo.ConcurrentHashMap$ReservationNode)</span><br><span class="line">at concurrent.demo.Demo3.main(Demo3.java:5)</span><br></pre></td></tr></table></figure>
<p>(ConcurrentHashMap.java:1747)信息很关键，它指出死循环发生在ConcurrentHashMap.java源代码文件的1747行，接着你可以在源代码文件相应位置加个打印语句<code>System.out.println(&quot;dead loop&quot;);</code>看看是否是这个是位置有死循环``，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6817b340064cd6fe400a5846be97c467.png" alt="computeIfAbsent_2"></p>
<p>源代码能编辑的前提是你对IDEA的Sourcepath做了设置，至于如何设置，这些小trick不再说明。</p>
<p>再次执行Demo3时，则会一直打印<code>dead loop</code>，这种方式看起来也让其死循环行为更为直观。</p>
<h4 id="openJDK官方的讨论过程"><a href="#openJDK官方的讨论过程" class="headerlink" title="openJDK官方的讨论过程"></a>openJDK官方的讨论过程</h4><p>有了以上对computeIfAbsent全面的解析，则可以更好理解openJDK的官方讨论过程，<a href="https://bugs.openjdk.java.net/browse/JDK-8062841">参考此提交页面</a></p>
<p>1、<a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dl">Doug Lea</a> added a comment - 2014-11-04 11:21</p>
<p>Doug Lea说自己没有一个好的方式去诊断CHM就此出现的问题，他猜测是否是线程在执行<code>computing value</code>逻辑时stuck住了，但从提交者的描述来看又无法确定这点。</p>
<blockquote>
<p>I do not see a way to diagnose if there is a CHM problem here. The reservation mentioned may exist if some other thread is stuck while computing value, but there is no way to determine this from description.</p>
</blockquote>
<p>你可以理解为Doug Lea对此bug暂无解决思路（未能在短时间内想起哪个逻辑出现问题），这是因为提交者首次提bug时没有附上“可复现bug的代码文件”，仅根据贴上的基本描述无法清晰指出问题所在。</p>
<p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=pardesha">Pardeep Sharma</a> added a comment - 2014-12-03 03:58</p>
<blockquote>
<p>Response from the submitter:<br>“I’ve been investigating the bug further in the mean time and I have a<br>minimal example (see attachment). </p>
<p>The problem is that we’re doing a computeIfAbsent within another<br>computeIfAbsent with an object that has accidentally the same hashCode<br>(in the attached example “AaAa” and “BBBB” also have same hashCode). </p>
<p>The documentation states that this is forbidden (mistake on our side)<br>but it also states that this should throw “IllegalStateException - if<br>the computation detectably attempts a recursive update to this map<br>that would otherwise never complete”. This is not the case. </p>
<p>I would suggest that either<br>a) the documentation is adjusted to make it more clear that the<br>IllegalStateException is thrown on best effort basis.<br>or<br>b) the implementation is adjusted so that the IllegalStateException is<br>really thrown.”</p>
</blockquote>
<p>这部分内容很关键，提交者补充了准确的描述、修复意见和可复现bug的代码文件，补充的内容大概意思如下：</p>
<p>（1）bug产生的原因：map.computeIfAbsent里面的value再次调用computeIfAbsent，而且两个computeIfAbsent对应的key恰好hash冲突</p>
<p>（2）提出computeIfAbsent方法官方注释虽然提到禁止使用这种递归调用computeIfAbsent的用法，提交者认为更合理的方式是抛出：IllegalStateException</p>
<p>（3）提出源代码修复建议  a)注释里面尽可能对IllegalStateException抛出情况写清楚点   b)调整代码逻辑使得遇到此类情况可抛出<code>IllegalStateException</code></p>
<p>3、<a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dl">Doug Lea</a> added a comment - 2014-12-22 05:46</p>
<blockquote>
<p>Ignore my previous comment. We discovered some feasible diagnostic improvements that cover more user errors involving recursive map updates by functions supplied in computeIfAbsent, including the case attached in this report. </p>
<p>Pending any further discussion on concurrency-interest, we should integrate to JDK9, then 8u.</p>
</blockquote>
<p>在距离该bug提交时间（11-04）48天后（当然这里不是指Doug Lea每天思考该问题总共用了48天才找出如何解决），<a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dl">Doug Lea</a> 终于搞懂bug的发生逻辑，也谦虚说忽略他之前给出的评论意见，说他们能找到问题的所在，并有该bug复现的case。提到之后如果有关于concurrency相关的讨论（修复）都应该优先在jdk9完成，然后再去处理jdk8u版本。</p>
<h4 id="源代码修复说明"><a href="#源代码修复说明" class="headerlink" title="源代码修复说明"></a>源代码修复说明</h4><p>在这里不妨假设还是按照上面<code>an endless loop</code>章节在分析递归调用<code>new Name(&quot;bar&quot;)</code>的computeIfAbsent发生的循环逻辑，把整个执行流程放到以下修复后代码中去考察：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (Node&lt;K,V&gt;[] tab = table;;) &#123;</span><br><span class="line">         Node&lt;K,V&gt; f; <span class="keyword">int</span> n, i, fh;</span><br><span class="line">         <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">             tab = initTable();</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i = (n - <span class="number">1</span>) &amp; h)) == <span class="keyword">null</span>)&#123;&#125;</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)&#123;&#125;      </span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">             <span class="keyword">boolean</span> added = <span class="keyword">false</span>;</span><br><span class="line">             <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">                 <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">                   	<span class="comment">//4.1 f是ReservationNode节点不是链表，所以执行流不会进入此逻辑</span></span><br><span class="line">                     <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                         binCount = <span class="number">1</span>;</span><br><span class="line">                         <span class="keyword">for</span> (Node&lt;K,V&gt; e = f;; ++binCount) &#123;</span><br><span class="line">                             K ek;</span><br><span class="line">                             <span class="keyword">if</span> (e.hash == h &amp;&amp;</span><br><span class="line">                                 ((ek = e.key) == key ||</span><br><span class="line">                                  (ek != <span class="keyword">null</span> &amp;&amp; key.equals(ek)))) &#123;</span><br><span class="line">                                 val = e.val;</span><br><span class="line">                                 <span class="keyword">break</span>;</span><br><span class="line">                             &#125;</span><br><span class="line">                             Node&lt;K,V&gt; pred = e;</span><br><span class="line">                             <span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                 <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                         <span class="comment">// 新增的修复代码，链表这边也可能会出现递归调用，也需要直接抛出异常。为何这里也会出现</span></span><br><span class="line">                                     <span class="keyword">if</span> (pred.next != <span class="keyword">null</span>)</span><br><span class="line">                                         <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Recursive update&quot;</span>);</span><br><span class="line">                                     added = <span class="keyword">true</span>;</span><br><span class="line">                                     pred.next = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val);</span><br><span class="line">                                 &#125;</span><br><span class="line">                                 <span class="keyword">break</span>;</span><br><span class="line">                             &#125;</span><br><span class="line">                         &#125;</span><br><span class="line">                     &#125;</span><br><span class="line">                   <span class="comment">//4.2 f是ReservationNode节点不是TreeBin节点，所以执行流不会进入此逻辑</span></span><br><span class="line">                     <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                         binCount = <span class="number">2</span>;</span><br><span class="line">                         TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span><br><span class="line">                         TreeNode&lt;K,V&gt; r, p;</span><br><span class="line">                         <span class="keyword">if</span> ((r = t.root) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                             (p = r.findTreeNode(h, key, <span class="keyword">null</span>)) != <span class="keyword">null</span>)</span><br><span class="line">                             val = p.val;</span><br><span class="line">                         <span class="keyword">else</span> <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                             added = <span class="keyword">true</span>;</span><br><span class="line">                             t.putTreeVal(h, key, val);</span><br><span class="line">                         &#125;</span><br><span class="line">                     &#125;</span><br><span class="line">                   <span class="comment">// 4.3 新增的修复代码</span></span><br><span class="line">                   <span class="comment">// 在第一次computeIfAbsent执行时new Name(&quot;foo&quot;) 就是通过独占锁在桶位放置了一个ReservationNode然后等待new Name(&quot;bar&quot;) 的返回，而new Name(&quot;bar&quot;) 执行computeIfAbsent时来到相同桶位，此时桶位显然是一个ReservationNode，满足判断条件，故直接抛出异常使得线程终止运行，而不是继续下一轮遍历，解决了死循环。</span></span><br><span class="line">                     <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> ReservationNode)</span><br><span class="line">                         <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Recursive update&quot;</span>);</span><br><span class="line">                 &#125;</span><br></pre></td></tr></table></figure>
<p>当然这种有递归更新操作的修复不单单只在computeIfAbsent方法修改，还有其他方法内部都需要修改，具体参考他们给出的<a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?r1=1.258&amp;r2=1.259&amp;sortby=date">修复前后diff链接</a>。</p>
<p>Doug Lea除了修复这个死循环的问题，他还给出了一个提高computeIfAbsent性能的修复，<a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?r1=1.295&amp;r2=1.296&amp;sortby=date">修复前后diff连接</a></p>
<p>Improve already-present performance in computeIfAbsent, putIfAbsent</p>
<blockquote>
<p>Revision <strong>1.296</strong> - (<a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?revision=1.296&amp;view=markup&amp;sortby=date">view</a>) (<a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?annotate=1.296&amp;sortby=date">annotate</a>) - <strong>[selected]</strong><br><em>Sun Jul 17 12:09:12 2016 UTC</em> (5 years ago) by <em>dl</em><br>Branch: <a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?view=log&amp;sortby=date&amp;pathrev=MAIN"><strong>MAIN</strong></a><br>Changes since <strong>1.295: +10 -2 lines</strong><br>Diff to <a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?r1=1.295&amp;r2=1.296&amp;sortby=date">previous 1.295</a></p>
</blockquote>
<p>其实也很简单，正如注释所说：check first node without acquiring lock，在无需加锁情况下，快速判断头节点是否和给定key相同，如相同则无须更新value，直接返回头节点原value。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//...</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)</span><br><span class="line">    tab = helpTransfer(tab, f);</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (fh == h    <span class="comment">// check first node without acquiring lock</span></span><br><span class="line">         &amp;&amp; ((fk = f.key) == key || (fk != <span class="keyword">null</span> &amp;&amp; key.equals(fk)))</span><br><span class="line">         &amp;&amp; (fv = f.val) != <span class="keyword">null</span>)</span><br><span class="line">    <span class="keyword">return</span> fv;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">boolean</span> added = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line"> <span class="comment">//...</span></span><br></pre></td></tr></table></figure>
<p>有了以上全文内容的理解，再来回顾computeIfAbsent的源代码注释则能理解它要强调的重点！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * If the specified key is not already associated with a value,</span></span><br><span class="line"><span class="comment"> * attempts to compute its value using the given mapping function</span></span><br><span class="line"><span class="comment"> * and enters it into this map unless &#123;<span class="doctag">@code</span> null&#125;.  The entire</span></span><br><span class="line"><span class="comment"> * method invocation is performed atomically.  The supplied</span></span><br><span class="line"><span class="comment"> * function is invoked exactly once per invocation of this method</span></span><br><span class="line"><span class="comment"> * if the key is absent, else not at all.  (key不在map的情况下，给定的映射函数只会调用一次用来计算key对应的value)</span></span><br><span class="line"><span class="comment"> *Some attempted update operations on this map by other threads may be blocked while</span></span><br><span class="line"><span class="comment"> * computation is in progress, so the computation should be short</span></span><br><span class="line"><span class="comment"> * and simple.（给key调用映射函数计算value的过程中可能阻塞其他线程，因此这个“映射函数”的计算逻辑尽可能短、简单，例如key对应的value需要用10秒才计算完那么put这个key时显然会导致其他线程需要等待10秒才能继续后续写操作）</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The mapping function must not modify this map during computation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> key key with which the specified value is to be associated</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> mappingFunction the function to compute a value</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the current (existing or computed) value associated with</span></span><br><span class="line"><span class="comment"> *         the specified key, or null if the computed value is null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> NullPointerException if the specified key or mappingFunction</span></span><br><span class="line"><span class="comment"> *         is null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalStateException if the computation detectably</span></span><br><span class="line"><span class="comment"> *         attempts a recursive update to this map that would</span></span><br><span class="line"><span class="comment"> *         otherwise never complete</span></span><br><span class="line"><span class="comment"> （抛出IllegalStateException就是修复的内容：如果检测到递归更新，如本文提供Demo3案例，则会抛出这个异常错误）</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> RuntimeException or Error if the mappingFunction does so,</span></span><br><span class="line"><span class="comment"> *         in which case the mapping is left unestablished</span></span><br><span class="line"><span class="comment"> *</span></span><br></pre></td></tr></table></figure>
<p>掌握computeIfAbsent实现以及bug修复原理，你可以猜到HashMap、TreeMap、Hashtable等computeIfAbsent是如何处理吗？</p>
<p>这是一个陷阱式提问，显然HashMap、TreeMap、Hashtable的computeIfAbsent是不可能发生死循环，因为它们的computeIfAbsent源代码实现里面就不存在<code>for循环（自旋）+CAS</code>这套操作，而且HashMap、TreeMap这些也不是设计用于并发环境。</p>
<p>此外jdk1.8的ConcurrentHashMap的computeIfAbsent死循环的bug还引起阿里的分布式事务Seata框架发生同样的bug，文章为<a href="https://seata.io/zh-cn/blog/seata-dsproxy-deadlock.html">《ConcurrentHashMap导致的Seata死锁问题》</a>，文章发布时间很新，2021年3月13日。写这篇深度分析文章的作者是Seata开发者之一，很不错，该文适合已经具有微服务下的分布式事务开发经验的同学研究。</p>
<h4 id="One-more-thing"><a href="#One-more-thing" class="headerlink" title="One more thing"></a>One more thing</h4><p>在“源代码修复diff说明”章节，代码修复只解释4.3新增的修复代码，眼尖的同学也许发现了4.1的链表也新增的修复代码而4.2的TreeBin里面确无需修改，这两部分的修复逻辑为何是这么处理呢？</p>
<h5 id="讨论4-1位置指出的链表片段代码的修复思路"><a href="#讨论4-1位置指出的链表片段代码的修复思路" class="headerlink" title="讨论4.1位置指出的链表片段代码的修复思路"></a>讨论4.1位置指出的链表片段代码的修复思路</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">boolean</span> added = <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">                <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">                  	<span class="comment">//4.1 f是ReservationNode节点不是链表，所以执行流不会进入此逻辑</span></span><br><span class="line">                    <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                        binCount = <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">for</span> (Node&lt;K,V&gt; e = f;; ++binCount) &#123;</span><br><span class="line">                            K ek;</span><br><span class="line">                            <span class="keyword">if</span> (e.hash == h &amp;&amp;</span><br><span class="line">                                ((ek = e.key) == key ||</span><br><span class="line">                                 (ek != <span class="keyword">null</span> &amp;&amp; key.equals(ek)))) &#123;</span><br><span class="line">                                val = e.val;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                            Node&lt;K,V&gt; pred = e;</span><br><span class="line">                            <span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 新增的修复代码，链表这边也可能会出现递归调用，也需要直接抛出异常。为何这里也会出现</span></span><br><span class="line">                                    <span class="keyword">if</span> (pred.next != <span class="keyword">null</span>)</span><br><span class="line">                                        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Recursive update&quot;</span>);</span><br><span class="line">                                    added = <span class="keyword">true</span>;</span><br><span class="line">                                    pred.next = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val);</span><br><span class="line">                                &#125;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br></pre></td></tr></table></figure>
<p>首先应该可以猜到，既然Doug Lea 修复了它，说明链表里面肯定存在递归更新（且哈希冲突下的递归更新），但会出现类似ReservationNode的死循环bug吗？还是有其他不一样的情况？这里再次设计相关复现代码以解释之，Demo4如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.LinkedBlockingQueue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo4</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;();</span><br><span class="line">        ConcurrentHashMap&lt;Name, Integer&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">        map.put(<span class="keyword">new</span> Name(<span class="string">&quot;a0&quot;</span>),<span class="number">1</span>);</span><br><span class="line">        map.put(<span class="keyword">new</span> Name(<span class="string">&quot;a1&quot;</span>),<span class="number">2</span>);</span><br><span class="line">        map.computeIfAbsent(<span class="keyword">new</span> Name(<span class="string">&quot;a2&quot;</span>),k1-&gt;map.computeIfAbsent(<span class="keyword">new</span> Name(<span class="string">&quot;a3&quot;</span>),k2-&gt;k2.key.length()));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        System.out.println(map);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Name</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String key;</span><br><span class="line">        Name(String key)&#123;</span><br><span class="line">            <span class="keyword">this</span>.key=key;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> key;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上预期打印结果：{a0=1,a1=2,a3=2,a2=2}</p>
<p>实际输出为：{a0=1, a1=2, a2=2}</p>
<p>原因分析如下图所示：<br><img src="https://img-blog.csdnimg.cn/f37ea7b269f04b09b9dc2e867aeeb67d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>以上图示说明的是在链表中出现computeIfAbsent递归调用的bug并不会出现类似前面ReservationNode引起的死循环，而是出现“节点被覆盖”的bug，因此需要修复，修复的代码也很好处理：<font color=red>依据上图思路，在Time4时刻先判断此刻<code>pred.next</code>是否还是Time2时刻时的情况，即<code>pred.next=null</code>，如果不为空，说明在Time3时刻，递归调用了computeIfAbsent导致pred.next指向一个新增节点，出现了“inconsistent read”，这种情况很像mysql的“不可重复读”：</font></p>
<p>事务A在第一次读和第二次读的结果不一样，是因为在第一次和第二次读的中间时刻，有事务B对目标记录修改，导致事务A在一个事务内两次读到的数据不一样，也即不可重复读。</p>
<p>按这里的思路套入“上图中链表的不可重复读”：线程A第一次computeIfAbsent读取的<code>pred.next</code>为空，接着有线程A第一次computeIfAbsent内部再调用第二个computeIfAbsent对<code>pred.next</code>修改指向了一个非空新增节点，之后当线程A返回到执行第一次computeIfAbsent中断位置继续执行，发现第二次读取的<code>pred.next</code>不为空，那么线程A此时应该抛出异常，这就是Doug Lea对链表出现递归调用computeIfAbsent的修复策略，修复源代码片段如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Node&lt;K,V&gt; pred = e;</span><br><span class="line"><span class="keyword">if</span> ((e = e.next) == <span class="keyword">null</span>) &#123; <span class="comment">//这里可以看做是线程A第一次读取pred.next,显然此刻pred.next是指向null</span></span><br><span class="line">    <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">		<span class="comment">// 新增的修复代码：线程A第二次读pred.next，如果此刻pred.next指向为null，说明本次computeIfAbsent内部没递归调用computeIfAbsent，是合法插入节点的操作，此时可以放心让pred.next指向新增节点</span></span><br><span class="line">        <span class="keyword">if</span> (pred.next != <span class="keyword">null</span>)</span><br><span class="line">          <span class="comment">// 如果线程A第二次读pred.next不为空，说明本次computeIfAbsent递归调用computeIfAbsent，直接抛出IllegalStateException</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Recursive update&quot;</span>);</span><br><span class="line">        <span class="comment">// 线程A第二次读pred.next，如果此刻pred.next指向为null，说明本次computeIfAbsent内部没递归调用computeIfAbsent是合法插入节点的操作，此时可以放心让pred.next指向新增节点new Node。</span></span><br><span class="line">        added = <span class="keyword">true</span>;</span><br><span class="line">        pred.next = <span class="keyword">new</span> Node&lt;K,V&gt;(h, key, val);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>多线程下的computeIfAbsent递归结果其实跟单线程一样，但肯定不会出现死循环，有余力的同学可以自行分析或者debug。</p>
<h5 id="讨论4-2-位置TreeBin部分的修复思路"><a href="#讨论4-2-位置TreeBin部分的修复思路" class="headerlink" title="讨论4.2 位置TreeBin部分的修复思路"></a>讨论4.2 位置TreeBin部分的修复思路</h5><p>对比jdk1.8和jdk16的TreeBin片段前后，发现源代码无需修复，这是又是为何呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">            <span class="comment">//4.2 f是ReservationNode节点不是TreeBin节点，所以执行流不会进入此逻辑</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                  binCount = <span class="number">2</span>;</span><br><span class="line">                  TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span><br><span class="line">                  TreeNode&lt;K,V&gt; r, p;</span><br><span class="line">                  <span class="keyword">if</span> ((r = t.root) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                      (p = r.findTreeNode(h, key, <span class="keyword">null</span>)) != <span class="keyword">null</span>)</span><br><span class="line">                      val = p.val;</span><br><span class="line">                  <span class="keyword">else</span> <span class="keyword">if</span> ((val = mappingFunction.apply(key)) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                      added = <span class="keyword">true</span>;</span><br><span class="line">                      t.putTreeVal(h, key, val);</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line"><span class="comment">//4.3 新增的修复，处理ReservationNode引起的死循环bug</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> ReservationNode)</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Recursive update&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>有了前面链表的分析积累，很好理解为何TreeBin无需担心递归更新带来的问题，因为它不会发生这样的bug，下面给出Demo5以及详细可理解的分析：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> concurrent.demo;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo5</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        ConcurrentHashMap&lt;Name, Integer&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">      </span><br><span class="line">      	<span class="comment">// put64个节点触发扩容并会树化红黑树</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">64</span>;i++)&#123;</span><br><span class="line">            map.put(<span class="keyword">new</span> Name(<span class="string">&quot;a&quot;</span>+i),i);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">// 测试b2节点会不会像链表那边被b1节点覆盖</span></span><br><span class="line">        map.computeIfAbsent(<span class="keyword">new</span> Name(<span class="string">&quot;b1&quot;</span>),k1-&gt;map.computeIfAbsent(<span class="keyword">new</span> Name(<span class="string">&quot;b2&quot;</span>),k2-&gt;k2.key.length()));</span><br><span class="line">        System.out.println(map);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Name</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String key;</span><br><span class="line">        Name(String key)&#123;</span><br><span class="line">            <span class="keyword">this</span>.key=key;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> key;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>详细图解：<br><img src="https://img-blog.csdnimg.cn/e850dc9870704077af0bb9da6e7810c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h4><p>相信能读懂本文的同学，应该会受益匪浅，此外，网上也有coder能写出关于官方处理computeIfAbsent的修复过程解析，但他们基本停留在<code>ReservationNode</code>这一点上，而关于链表片段代码的修复以及红黑树部分片段维持不修复的本质原因却没有给出深度解读，这两点作为computeIfAbsent核心设计，肯定需要深挖为何如何修复或不修复的原因。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度讨论官方关于jdk1.8ConcurrentHashMap的resizeStamp源代码修复逻辑</title>
    <url>/2021/09/12/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%AE%A8%E8%AE%BA%E5%AE%98%E6%96%B9%E5%85%B3%E4%BA%8Ejdk1.8ConcurrentHashMap%E7%9A%84resizeStamp%E6%BA%90%E4%BB%A3%E7%A0%81%E4%BF%AE%E5%A4%8D%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>首先给出以下open JDK版本的序号说明和Oracle JDK序号说明</p>
<p>（1）对于JDK8或者Java 8</p>
<p>即可指代openjdk-8-jdk或者java-1.8.0-openjdk，</p>
<p>也可指代Oracle家的Java SE 8或者JDK 8u211 and later</p>
<p>（1）对于JDK16或者Java 16</p>
<p>即可指代openjdk的JDK 16.0.2 ，也可指代Oracle家的Java SE 16或者 jdk16.0.1，这里为何给出Java 8和Java 16版本说明？</p>
<p>首先resizeStamp的bug在Java8出现，并在Java 12被修复，因此本文直接给出最新版Java 16作为bug修复前后对比即可。</p>
<p>以下做个约定：统一以Java X形式作为版本称号，CHM：ConcurrentHashMap的简称，以此减少阅读障碍。</p>
<p>在前面的文章中，关于Java 8 的CHM addCount方法里面分支2：<code>resizeStamp</code>和<code>sc==rs+1、sc==rs+MAX_RESIZERS</code>的讨论中，已经指出其bug嫌疑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 分支1 省略...</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">// 分支2 </span></span><br><span class="line"><span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">            <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                   (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                <span class="keyword">int</span> rs = resizeStamp(n); <span class="comment">// 注意这里计算出的rs是正值</span></span><br><span class="line">                <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">// sc是负值，怎么会等于rs+1或者rs + MAX_RESIZERS这个正值呢？ 有可能是个bug</span></span><br><span class="line">                    <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                        sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                        transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                        transfer(tab, nt);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                             (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">                    transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">                s = sumCount();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h4 id="官方bug描述"><a href="#官方bug描述" class="headerlink" title="官方bug描述"></a>官方bug描述</h4><p>其实这个bug在open jdk的官方bugs主页已经给出相关解释和修复过程，链接：<a href="https://bugs.openjdk.java.net/browse/JDK-8214427">官方bug描述页面</a>：<br><img src="https://img-blog.csdnimg.cn/img_convert/eb2c433432625b947c9577f66143aa89.png" alt="开发者在官方提交的resizeStamp的bug描述"></p>
<p>从Detail这一块描述得到信息如下：</p>
<p>bug的描述：ConcurrentHashMap.addCount()设计逻辑中可能存在bug。（这里虽然提到addCount()方法，但本人更想强调的是扩容分支的resizeStamp的bug）</p>
<p>级别是：bug</p>
<p>当前状态：已经修复</p>
<p>影响的版本：Java 11、Java12</p>
<p>在哪个版本得到修复：Java 12</p>
<p>使用操作系统平台：所有</p>
<p>bug页面创建时间：2018-11-26</p>
<p>解决bug的最后时间：2018-12-11</p>
<p>bug所属库：Java的核心库——core-libs</p>
<h4 id="提交者的修复建议"><a href="#提交者的修复建议" class="headerlink" title="提交者的修复建议"></a>提交者的修复建议</h4><blockquote>
<p>In the above code, condition of (sc == rs + 1 || sc == rs + MAX_RESIZERS ) would never be true , since the value of rs is positive and the value of sc is negative .</p>
<p>译：条件 (sc == rs + 1 || sc == rs + MAX_RESIZERS )永远不可能true，因为rs的值为正数，而sc值为负数</p>
</blockquote>
<p>并建议修改为：</p>
<blockquote>
<p>The correct condition should be (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + 1 || (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + MAX_RESIZERS, which can be used to dedect if resizing process finished or resizing threads reaches maxmium limitation</p>
<p>译：正常的条件应该是这样： (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + 1 || (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + MAX_RESIZERS，这两个条件表示扩容任务已结束或者参与扩容的线程总数达到最大值</p>
</blockquote>
<a id="more"></a>
<p>确实，这个bug非常明显，以分支2作为说明</p>
<p>int rs = resizeStamp(n)，以容量n=16作为说明，rs计算为下面的值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000 0000 0000 0000 1000 0000 0001 1011</span><br></pre></td></tr></table></figure>
<p>考察低16位，rs+1的结果显然是一个正数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000 0000 0000 0000 1000 0000 0001 1100</span><br></pre></td></tr></table></figure>
<p>rs+MAX_RESIZERS同理也是一个正数，接着判断条件if(sc&lt;0)成立才能进入rs+1等条件，也即此时sc是一个负数（其实是因为首个扩容线程会将sc设为<code>(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2</code>的一个基础负数），基于此，有提交者在这里发现的了bug：sc是负数，而rs+1是正数，因此sc==rs+1永远不会成立</p>
<h4 id="官方关于此bug的讨论过程"><a href="#官方关于此bug的讨论过程" class="headerlink" title="官方关于此bug的讨论过程"></a>官方关于此bug的讨论过程</h4><p>JCP  JSR-166 Expert Group （关于Java并发编程的规范提案的专家组）几个相关成员的对话过程即可知道他们对问题的思考和处理方式。</p>
<p>在Activity这个栏目就是用于提交者已经相关专家bug讨论过程，“All”是显示所有他们的活动记录，一般无需关注，“Comments”显示他们的对话过程，bug的讨论过程就在这里，因此需要重点关注，具体如下：</p>
<p>以下是来自“Comments”区域的内容：</p>
<p>(最开始由Webbug Group 这个小组提交了该issue - 2018-11-26 00:53)</p>
<p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=smarks">Stuart Marks</a> 说：</p>
<blockquote>
<p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=smarks">Stuart Marks</a> added a comment - 2018-11-28 09:10</p>
<p>Martin, can you take a look at this?</p>
<p>Martin，来，帮我看看这个bug？</p>
</blockquote>
<p>Martin的回答，主要意思是：bug提交者在查一个确实是由addCount产生错误计数，但Martin说他们也没有可以使用的压测案例，并建议使用者用多线程做压测来让addCount的这个bug复现，但这个bug不好复现。</p>
<blockquote>
<p>Resizing the internal bucket array is hairy race-prone code, and hard to stress test because resizes are relatively rare.</p>
<p>The reporter probably investigated an actual occurrence of incorrect count (we could ask!), but we don’t have a stress test reproduction that could be used.</p>
<p>One should be able to construct a stress test using multiple threads to trigger concurrent attempts to addCount, but it won’t be easy.</p>
</blockquote>
<p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dholmes">David Holmes</a> 对Martin说：</p>
<blockquote>
<p>What is your analysis just based on the code and the report? It certainly appears incorrect to me.</p>
<p>你的分析只是基于代码以及提交的报告？这个bug在我看来显然是不正确的。</p>
</blockquote>
<p>Martin回答<a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dholmes">David Holmes</a> ：</p>
<p>Martin说自己也看了看源码但研究时间不够长，自己还没能搞懂其设计，然后说Doug应该记得这个设计！</p>
<blockquote>
<p>I stared at the code for a while, but not long enough to understand it. Doug will remember!</p>
</blockquote>
<p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dl">Doug Lea</a> added a comment - 2018-11-28 15:53:</p>
<p>Doug Lea看到这个bug，做了基本的分析：这个bug会影响到CHM性能也即有些线程不能参与到扩容任务中，并指出这个bug只是影响性能而不是一个引起map发生错误的bug，指出这个bug需要修复。</p>
<blockquote>
<p>Yes. Some of this check now includes dead code, because of a change of representation at one point that wasn’t adjusted for. With the possible effect of some threads not helping resize (a performance, not map correctness bug) This should be fixed (and is committed in jdr166 repo):</p>
</blockquote>
<p>然后他贴出修复前后的源码diff</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">--- ConcurrentHashMap.java.~<span class="number">1.314</span>.~ <span class="number">2018</span>-<span class="number">10</span>-<span class="number">05</span> <span class="number">13</span>:<span class="number">42</span>:<span class="number">39.860409607</span> -<span class="number">0400</span></span><br><span class="line">+++ ConcurrentHashMap.java <span class="number">2018</span>-<span class="number">11</span>-<span class="number">28</span> <span class="number">18</span>:<span class="number">48</span>:<span class="number">55.998082379</span> -<span class="number">0500</span></span><br><span class="line">@@ -<span class="number">2307</span>,<span class="number">9</span> +<span class="number">2307</span>,<span class="number">9</span> @@</span><br><span class="line">                    (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                 <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">                 <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">- <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">- sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">- transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">+ <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + <span class="number">1</span> ||</span><br><span class="line">+ (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + MAX_RESIZERS ||</span><br><span class="line">+ (nt = nextTable) == <span class="keyword">null</span> || transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                         <span class="keyword">break</span>;</span><br><span class="line">                     <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                         transfer(tab, nt);</span><br></pre></td></tr></table></figure>
<p>可以看到判断条件变为:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + <span class="number">1</span> ||(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + MAX_RESIZERS ||(nt = nextTable) == <span class="keyword">null</span> || transferIndex &lt;= <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>根据修复的条件，可知rs是负数，因此<code>(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs</code> 已经没有实际意义，因为(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT)是一个正数，rs是一个负数，显然是不相等，因此<code>(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs</code>是多余的。</p>
<p>这就是最终定稿修复源码吗？  继续看后面的讨论</p>
<p>Pallavi Sonal (Inactive)](<a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=psonal">https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=psonal</a>) added a comment - 2018-11-29 01:09</p>
<p>Pallavi Sonal收到提交者新的描述，</p>
<blockquote>
<p>Additional Information from submitter:<br>I need to change the correct conditions given by me in the bug description ï¼š</p>
<p>The correct condition shuold be</p>
<p>sc == ( rs&lt;&lt;&lt;RESIZE_STAMP_SHIFT ) +1 || sc == ( rs&lt;&lt;&lt;RESIZE_STAMP_SHIFT ) + MAX_RESIZERS</p>
</blockquote>
<p>其实就是说一开始提交bug的描述中，两个条件应该是这样的形式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + <span class="number">1</span> || (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) == rs + MAX_RESIZERS</span><br></pre></td></tr></table></figure>
<p>现在提交者应该是自己对扩容移位理解深入后，发现按下面这样写更能准确表达：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//int rs = resizeStamp(n);</span></span><br><span class="line">sc == ( rs&lt;&lt;&lt;RESIZE_STAMP_SHIFT ) +<span class="number">1</span> || sc == ( rs&lt;&lt;&lt;RESIZE_STAMP_SHIFT ) + MAX_RESIZERS</span><br></pre></td></tr></table></figure>
<p>本人也赞同这种建议：因为该if条件考察主体是以rs扩容戳的角度出发，因为rs由resizeStamp(n)计算出是正值:</p>
<p><code>0000 0000 0000 0000 1000 0000 0001 1011</code></p>
<p>因此将rs左移RESIZE_STAMP_SHIFT位后，扩容戳关键信息移动到高16位：</p>
<p><code>1000 0000 0001 1011 0000 0000 0000 000</code> （此时是一个负数）</p>
<p>然后再去判断<code>sc == ( rs&lt;&lt;&lt;RESIZE_STAMP_SHIFT ) +1</code> 是否成立，这样才能符合rs的移位设计逻辑：高16位存放扩容印记信息，低16位用于存放扩容线程数量。</p>
<p>而<code>sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT</code> 的写法无法表达出“rs的高16位存放扩容印记信息，rs低16位用于存放扩容线程数量。”这种移位设计理念。</p>
<h4 id="bug的复现方法"><a href="#bug的复现方法" class="headerlink" title="bug的复现方法"></a>bug的复现方法</h4><p>Pallavi Sonal最关键的贡献是提供了bug的复现方法，如下所示：</p>
<p>这里有个tricky：将MAX_RESIZERS设为2以及在transfer中挂起进入transfer线程 suspend Threads</p>
<blockquote>
<p>This bug could be verifed by a small example :</p>
<p>1、将ConcurrentHashMap源码拷贝自己测试包目录下</p>
<p>First, copy the ConcurrentHashMap source code to your own package</p>
<p>2、还有其他相关的ThreadLocalRandom源码也要拷到自己项目包</p>
<p>Second, do some necessary modification to make it can be compiled (eg: change package declaration, make Unsafe instance works, copy ThreadLocalRandom to your package as well, since the ConcurrentHashMap used ThreadLocalRandom.probe() function, which is not public )</p>
<p>3、写个demo测试代码，以及在源码加一些修改（注意源码无法直接修改，可以选择拷到自己包下，个人推荐：最好的方式是在IDEA将SDKs 的<code>Sourcepath</code>替换自定义的源码目录，这样JDK源码就可以直接编辑，比上面提的方式要方便）</p>
<p>Third, reduce MAX_RESIZERS to 2, as the documentation shows, this should ensure there are at most 2 threads can do resizing concurrently</p>
<p>// CHM源码将MAX_RESIZERS数量设为2，以便观察进入transfer的线程数量</p>
<p>private static final int MAX_RESIZERS = 2;</p>
<p>Fourth, add the following code snippet into the customized ConcurrentHashMap class</p>
<p>public static void main(String[] args) {</p>
<p>ConcurrentHashMap hashMap = new ConcurrentHashMap(8);</p>
<p>for(int i = 0; i&lt; 300; i++)<br>{<br>new Thread() {<br>@Override<br>public void run() {<br>hashMap.put(Thread.currentThread().getId(),”id: “+Thread.currentThread().getId());<br>}<br>}.start();<br>}<br>}</p>
<p>5、在transfer方法指定埋入一些让线程挂起的代码</p>
<p>Fifth, add the following code snippet into the transfer function of ConcurrentHashMap . To suspend any thread that entered into transfer</p>
<p>if (nextTab == null) { // initiating<br>try {<br>@SuppressWarnings(“unchecked”)<br>Node<K,V>[] nt = (Node<K,V>[])new Node&lt;?,?&gt;[n &lt;&lt; 1];<br>nextTab = nt;<br>} catch (Throwable ex) { // try to cope with OOME<br>sizeCtl = Integer.MAX_VALUE;<br>return;<br>}<br>nextTable = nextTab;<br>transferIndex = n;<br>}</p>
<p>// The following added code here is to suspend Threads !!!! 在这里挂起线程<br>try {<br>String s = new String();<br>synchronized (s)<br>{<br>s.wait();<br>}<br>} catch (InterruptedException e) {<br>e.printStackTrace();<br>}</p>
<p>6、在以下 addCount 代码片段加入断点，并使用IDEA的 “Thread” option来测试<br>Six, add the Thread break point in the following code line in addCount function</p>
<p>( Tip: I used Idea Intellij , choose “Thread” option can suspend each thread in your application , otherwise it will only suspend only the first Thread which executed to the break point)</p>
<p>​      if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))<br>​      transfer(tab, nt);<br>enter image description here</p>
<p>debug后会发现进入transfer方法的线程数量超过2个，这就可以证明<code>sc==rs+MAX_RESIZERS</code>完全没有起效，因为按原始写法：sc是负值，rs是正值，<code>sc==rs+MAX_RESIZERS</code> 本身不会成立，当然无法限制进入扩容逻辑线程的总数量</p>
<p>Then run the main function, you will see more than 2 threads entered transfer function, which means MAX_RESIZERS does not take any effect.</p>
</blockquote>
<p>复现这个bug设计思路确实操作性很高！</p>
<p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=dl">Doug Lea</a> added a comment - 2018-12-02 08:00</p>
<p>其实由于上面已经给出bug复现方法，Doug Lea肯定更加清楚问题所在，因此他说helpTransfer方法也有这个问题，最后他用了更加简化的移位表达式来修复这个bug，并且Doug Lea也亲自验证修复后条件是否能起到相关效果。</p>
<blockquote>
<p>A similar change is also necessary in helpTransfer. These together with a simplification of the shift expressions are now in jsr166 repo. I also verified that limits are maintained.</p>
</blockquote>
<h4 id="bug修复的代码最终提交到仓库："><a href="#bug修复的代码最终提交到仓库：" class="headerlink" title="bug修复的代码最终提交到仓库："></a>bug修复的代码最终提交到仓库：</h4><p><a href="https://bugs.openjdk.java.net/secure/ViewProfile.jspa?name=hgupdate">HG Updates</a> added a comment - 2018-12-11 20:16</p>
<blockquote>
<p>URL: <a href="http://hg.openjdk.java.net/jdk/jdk/rev/b4eaf570a588">http://hg.openjdk.java.net/jdk/jdk/rev/b4eaf570a588</a><br>User: martin<br>Date: 2018-12-12 04:13:15 +0000</p>
</blockquote>
<p>8214427: probable bug in logic of ConcurrentHashMap.addCount() Reviewed-by: martin, dholmes</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right">author</th>
<th>dl</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">date</td>
<td>Tue, 11 Dec 2018 19:55:27 -0800 (2018-12-12)</td>
</tr>
<tr>
<td style="text-align:right">parents</td>
<td><a href="http://hg.openjdk.java.net/jdk/jdk/rev/c7c285b0b640">c7c285b0b640</a></td>
</tr>
<tr>
<td style="text-align:right">children</td>
<td><a href="http://hg.openjdk.java.net/jdk/jdk/rev/a35f7a452257">a35f7a452257</a></td>
</tr>
<tr>
<td style="text-align:right">files</td>
<td><a href="http://hg.openjdk.java.net/jdk/jdk/file/b4eaf570a588/src/java.base/share/classes/java/util/concurrent/ConcurrentHashMap.java">src/java.base/share/classes/java/util/concurrent/ConcurrentHashMap.java</a></td>
</tr>
<tr>
<td style="text-align:right">diffstat</td>
<td>1 files changed, 7 insertions(+), 9 deletions(-) <a href="javascript:toggleDiffstat(">[<code>+</code>]</a>)</td>
</tr>
</tbody>
</table>
</div>
<p>修复代码提交者应该是Doug Lea，因为dl是(D)oug (L)ea的缩写，Reviewed-by：martin, dholmes</p>
<p>具体为diff如下：</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">+++ b/src/java.base/share/classes/java/util/concurrent/ConcurrentHashMap.java	Tue Dec 11 19:55:27 2018 -0800</span></span><br><span class="line">// 以下是addCount方法的源码修复 </span><br><span class="line"><span class="meta">@@ -2334,17 +2334,15 @@</span></span><br><span class="line">             Node&lt;K,V&gt;[] tab, nt; int n, sc;</span><br><span class="line">             while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp;</span><br><span class="line">                    (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                 // 删除原先的写法   </span><br><span class="line"><span class="deletion">-                int rs = resizeStamp(n);</span></span><br><span class="line">								 // 直接在这里对rs进行左移位操作</span><br><span class="line"><span class="addition">+                int rs = resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT;</span></span><br><span class="line">                 if (sc &lt; 0) &#123;</span><br><span class="line"><span class="deletion">-                    if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||</span></span><br><span class="line"><span class="deletion">-                        sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||</span></span><br><span class="line"><span class="deletion">-                        transferIndex &lt;= 0)</span></span><br><span class="line">									// rs左移后，(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs是多余也是无实际意义条件，直接删除</span><br><span class="line">									</span><br><span class="line"><span class="addition">+                    if (sc == rs + MAX_RESIZERS || sc == rs + 1 ||</span></span><br><span class="line"><span class="addition">+                        (nt = nextTable) == null || transferIndex &lt;= 0)</span></span><br><span class="line">                         break;</span><br><span class="line">                     if (U.compareAndSetInt(this, SIZECTL, sc, sc + 1))</span><br><span class="line">                         transfer(tab, nt);</span><br><span class="line">                 &#125;</span><br><span class="line"><span class="deletion">-                else if (U.compareAndSetInt(this, SIZECTL, sc,</span></span><br><span class="line"><span class="deletion">-                                             (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2))</span></span><br><span class="line">				//显然这样看起来更容易理解：首个进入扩容逻辑的线程，将sizeCtl设为基础值rs+2</span><br><span class="line"><span class="addition">+                else if (U.compareAndSetInt(this, SIZECTL, sc, rs + 2))</span></span><br><span class="line">                     transfer(tab, null);</span><br><span class="line">                 s = sumCount();</span><br><span class="line">             &#125;</span><br><span class="line"><span class="meta">@@ -2358,11 +2356,11 @@</span></span><br><span class="line">				 // 以下是helpTransfer方法的源码修复 </span><br><span class="line">         Node&lt;K,V&gt;[] nextTab; int sc;</span><br><span class="line">         if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp;</span><br><span class="line">             (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123;</span><br><span class="line">             // 同上</span><br><span class="line"><span class="deletion">-            int rs = resizeStamp(tab.length);</span></span><br><span class="line"><span class="addition">+            int rs = resizeStamp(tab.length) &lt;&lt; RESIZE_STAMP_SHIFT;</span></span><br><span class="line">             while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp;</span><br><span class="line">                    (sc = sizeCtl) &lt; 0) &#123;</span><br><span class="line"><span class="deletion">-                if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||</span></span><br><span class="line"><span class="deletion">-                    sc == rs + MAX_RESIZERS || transferIndex &lt;= 0)</span></span><br><span class="line"><span class="addition">+                if (sc == rs + MAX_RESIZERS || sc == rs + 1 ||</span></span><br><span class="line"><span class="addition">+                    transferIndex &lt;= 0)</span></span><br><span class="line">                     break;</span><br><span class="line">                 if (U.compareAndSetInt(this, SIZECTL, sc, sc + 1)) &#123;</span><br><span class="line">                     transfer(tab, nextTab);</span><br></pre></td></tr></table></figure>
<h4 id="在Java-16源码验证其修复的代码"><a href="#在Java-16源码验证其修复的代码" class="headerlink" title="在Java 16源码验证其修复的代码"></a>在Java 16源码验证其修复的代码</h4><p>上面提到bug在Java 12就被修复了，考虑到当前最新的jdk版本为Java 16，因此可在JDK16验证其修复的源码，修改处为下面的三个更改：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">  CounterCell[] cs; <span class="keyword">long</span> b, s;   </span><br><span class="line">  		<span class="comment">// 省略部分...</span></span><br><span class="line">  		<span class="comment">// 分支2</span></span><br><span class="line">			<span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">            <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                   (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                <span class="comment">// 更改1</span></span><br><span class="line">                <span class="keyword">int</span> rs = resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT;</span><br><span class="line">                <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">// 更改2</span></span><br><span class="line">                    <span class="keyword">if</span> (sc == rs + MAX_RESIZERS || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                        (nt = nextTable) == <span class="keyword">null</span> || transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                        transfer(tab, nt);</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">// 更改3</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc, rs + <span class="number">2</span>))</span><br><span class="line">                    transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">                s = sumCount();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>可以看到，rs移位操作设计简化了，逻辑容易理解，这里再次给出分支2的解释：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">  CounterCell[] cs; <span class="keyword">long</span> b, s;   </span><br><span class="line">  		<span class="comment">// 省略部分...</span></span><br><span class="line">  		<span class="comment">// 分支2</span></span><br><span class="line">			<span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">            <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                   (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                <span class="comment">// 更改1:将扩容印记先左移16位，以便低16位用于线程数量累计。显然此时rs是一个负数</span></span><br><span class="line">                <span class="keyword">int</span> rs = resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT;</span><br><span class="line">                <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">// 更改2：原来的条件1已被删除，原因已在前面给出。</span></span><br><span class="line">                  	<span class="comment">// 若一个线程遇到以下四种情况之一就会自行break结束：扩容的线程总数达到最大限制值或者扩容任务已结束（所有扩容线程已退出）或者nextTable为空，或者已经没有可分配的桶位。条件1和条件2参考下图加深理解。</span></span><br><span class="line">                    <span class="keyword">if</span> (sc == rs + MAX_RESIZERS || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                        (nt = nextTable) == <span class="keyword">null</span> || transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="comment">// 除去首个扩容线程，以后每来一个扩容线程就对sc加1，参考下图</span></span><br><span class="line">                    <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                        transfer(tab, nt);</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">// 更改3：首个扩容线程对sc设为基础值为：rs+2后再进入transfer方法参与扩容，这种写法比之前版本要清晰很多，从这里也可以推导出：当sc的值为rs+1时，就能说明当前所有扩容线程都退出了扩容逻辑，CHM扩容完成。</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSetInt(<span class="keyword">this</span>, SIZECTL, sc, rs + <span class="number">2</span>))</span><br><span class="line">                    transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">                <span class="comment">// 当前线程再次获取CHM的节点总数，然后回到上面while循环检查s有无达下一个阶段扩容阈值sizeCtl，如果需要进行下一阶段扩容，那么当前线程又会回到更改3的位置：作为下一节点扩容的首个扩容线程。</span></span><br><span class="line">                s = sumCount();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/img_convert/97e83ffc401b68273676330e258d2250.png" alt="sc线程计数示意图.001"></p>
<h4 id="解析本次bug的带来的技术收益"><a href="#解析本次bug的带来的技术收益" class="headerlink" title="解析本次bug的带来的技术收益"></a>解析本次bug的带来的技术收益</h4><p>为何要如此详细的分析官方修复的这个bug呢？ </p>
<p>最直观明显的技术收益：当你能对源代码的bug有深刻认识，并且知道专家组成员对bug的讨论以及修复过程，那么其实你对CHM的设计理念和源码实现能掌握得相当深入，感觉像是你也参与了CHM的部分源码编写，无形中对提高个人高级开发能力有一定帮助。</p>
<p>其次，当你知道有这么一个“open JDK bugs官方提交系统”，也许当你在研究难度比较高的源码设计时，若你能发现bug，你也可以提交，以此证明自身掌握高级研发能力程度。</p>
<p>最后，这个官方bugs提交系统也可以帮助个人快速找到想要深入解析的包或者类的关键设计原理，因为页面含有非常详细的bug描述、bug复现方法、bug出现原因分析、bug的代码修复。</p>
<p>查询页面如下：</p>
<p>基本用法也简单：在Projects里面选择你关注的领域，例如JDK，若不想加其他条件，可以直接使用关键字去检索你想要研究的源码</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/1166b7e443f330f3baeb476bee50696c.png" alt="openJDKbug查询页面"></p>
<h4 id="bugs系统还给出另外与resizeStamp相关的bug"><a href="#bugs系统还给出另外与resizeStamp相关的bug" class="headerlink" title="bugs系统还给出另外与resizeStamp相关的bug"></a>bugs系统还给出另外与resizeStamp相关的bug</h4><p>不过是这个bug是提交者自己闹的乌龙，<a href="https://bugs.openjdk.java.net/browse/JDK-8242464">bug页面链接</a></p>
<p>提交者提交的描述：Bug in the logic of ConcurrentHashMap.addCount() when used in Threads</p>
<p>他认为主要问题不是sc==rs+1这边负数那边正数，而是应该关注当前数组的容量要两倍于原表</p>
<p>因此他认为if条件应该这么该：</p>
<p>将<code>sc==rs+1</code> 应该改为<code>(sc &gt;&gt;&gt;RESIZE_STAMP_SHIFT) == (rs&gt;&gt;&gt;RESIZE_STAMP_SHIFT) + 1</code></p>
<blockquote>
<p>A DESCRIPTION OF THE PROBLEM :<br>At java.util.concurrent.ConcurrentHashMap#addCount:2339<br>i think the condition is if the thread is reach maximum or the new table size is twice as before or the nextTable is null or the transferIndex is lesss than zero; the bug in jdk8 is “<a href="https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8214427">https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8214427</a>“ , i think the main thing is not one is positive another is negtive, but the new table size is twice as before.<br>at jdk12 i think is not fix it, “sc == rs + 1” compare the work thread but not array size, i think the code should be </p>
<p>if (sc == rs + MAX_RESIZERS || (sc &gt;&gt;&gt;RESIZE_STAMP_SHIFT) == (rs&gt;&gt;&gt;RESIZE_STAMP_SHIFT) + 1 ||<br>           (nt = nextTable) == null || transferIndex &lt;= 0)<br>           break;<br>rather than </p>
<p>if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||<br>           sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||<br>           transferIndex &lt;= 0)<br>           break; </p>
<p>REGRESSION : Last worked in version 8 </p>
<p>FREQUENCY : always </p>
</blockquote>
<p>到后面，提交者发现原来是自己理解错了，并再次加了以下comments：</p>
<p>他说很抱歉提交这样的bug，是他自己想错了，他以为<code>(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1</code> 这两个条件是用于描述检查数组size，其实不是的。他最终理解了<code>sc == rs + 1</code> 这样的条件目的在于对参与线程线程数量的计数</p>
<blockquote>
<p>Additional Information from Submiiter:<br>I am so sorry for submiting it. at jdk 1.8 “java.util.concurrent.ConcurrentHashMap#helpTransfer:2304”, i think “(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 “ is check the array size ,but yesterday i found that thinking of “ sc == rs + 1” as checking the count of thread was better.</p>
</blockquote>
<p>个人评价：</p>
<p>首先这个提交者没有真正理解和掌握resizeStamp的设计理念，以及这if中相关条件的用意，</p>
<p>既然这个提交者说将<code>sc == rs + 1</code> 改为<code>(sc &gt;&gt;&gt;RESIZE_STAMP_SHIFT) == (rs&gt;&gt;&gt;RESIZE_STAMP_SHIFT) + 1</code>才合理，为何他又不会将<code>sc == rs + MAX_RESIZERS</code> 改为:</p>
<p><code>(sc &gt;&gt;&gt;RESIZE_STAMP_SHIFT) == (rs&gt;&gt;&gt;RESIZE_STAMP_SHIFT) + MAX_RESIZERS</code></p>
<p>基于此，说明提交者没有真正理解这两个条件的实际目的， 所以他提的描述以及修复是不合理的，从Comments也可以看到那些专家对提交者提到的bug不感冒也可看出，这次提交的bug修复申请似乎意义不大：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">We need a clearer explanation of what we&#39;re fixing here</span><br><span class="line">...</span><br><span class="line">but we need better input from the reporter.</span><br></pre></td></tr></table></figure>
<p>最后由提交者回复<code>I am so sorry for submiting it.</code> 结束。</p>
<p>从这个案例也告诉大家，对于JDK级别这种bug的提交，首先提交者自己能理解相关源码设计和实现，最好有测试case，然后给出详细的而准确的描述，如果提交者自己还未搞懂相关逻辑就急着提交，最后可能闹个笑话。</p>
<h4 id="关于sizeCtl一个bug"><a href="#关于sizeCtl一个bug" class="headerlink" title="关于sizeCtl一个bug"></a>关于sizeCtl一个bug</h4><p>这个bug比较简单，<a href="https://bugs.openjdk.java.net/browse/JDK-8202422">描述页面链接</a></p>
<p>它指出：sizeCtl在构造方法初始化时，选用同一个容量，但用以下不同的构造方法，结果发现两者计算出的sizeCtl不一样</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConcurrentHashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity,<span class="keyword">float</span> loadFactor, <span class="keyword">int</span> concurrencyLevel)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConcurrentHashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span></span></span><br></pre></td></tr></table></figure>
<p>具体bug例子：</p>
<blockquote>
<p>The following two statements:</p>
<p>new ConcurrentHashMap(22,0.75f,1);<br>new ConcurrentHashMap(22). </p>
<p>The first construct method makes sizeCtl field value to 32, but the second one makes sizeCtl to 64.Both the two construct methods use the same parameter value. I think they should make the ‘sizeCtl’ value to be the same.</p>
</blockquote>
<p>对于给定的初始容量22来说，易知计算后sizeCtl正常值为32，但前一个构造方法计算出的sizeCtl是32，后一个构造方法计算出的sizeCtl是64，因此出现bug。</p>
<p>感兴趣的同学，可以自行研究其修复过程，本文不再给出相关说明。</p>
<h4 id="resizeStamp的bug复现方法（非常关键）"><a href="#resizeStamp的bug复现方法（非常关键）" class="headerlink" title="resizeStamp的bug复现方法（非常关键）"></a>resizeStamp的bug复现方法（非常关键）</h4><h5 id="源代码修复前的复现过程"><a href="#源代码修复前的复现过程" class="headerlink" title="源代码修复前的复现过程"></a>源代码修复前的复现过程</h5><p>文章最前面提到的JDK-8214427提交者确实是一个有水平且能深入理解源码的人，他还给Doug Lea他们提供了复现问题的6个步骤，思路清晰，因此本节也按其步骤给出IDEA debug过程，如下：</p>
<p>1、使用IDEA创建一个普通（Java或者Maven）项目，并在java目录下创建包concurrent.demo（或者自行命名），将源码文件ConcurrentHashMap.java、ThreadLocalRandom.java拷贝到包concurrent.demo下，创建ResizeStampBugTest用于测试，最终项目结构如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── src</span><br><span class="line">│   ├── main</span><br><span class="line">│   │   ├── java</span><br><span class="line">│   │   │   └── concurrent</span><br><span class="line">│   │   │       └── demo</span><br><span class="line">│   │   │           ├── ConcurrentHashMap.java</span><br><span class="line">│   │   │           ├── ResizeStampBugTest.java</span><br><span class="line">│   │   │           └── ThreadLocalRandom.java</span><br><span class="line">│   │   └── resources</span><br></pre></td></tr></table></figure>
<p>这两个源码文件在哪里找？简单问题不再回答。</p>
<p>2、修改ConcurrentHashMap.java和ThreadLocalRandom.java里面的Unsafe代码，使得Unsafe类在自己项目上可以使用</p>
<p>因为源码文件已经拷贝到自己项目下，因此可以对其进行编辑</p>
<p>对于ConcurrentHashMap.java的Unsafe代码修改（注意有两个Unsfafe地方都需要修改）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以下三行是新增，通过反射获得的Unsafe实例      </span></span><br><span class="line">      Field field = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">      field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">      U = (Unsafe) field.get(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下是源码获取Unsafe的写法，注释它即可</span></span><br><span class="line">    <span class="comment">// U = sun.misc.Unsafe.getUnsafe();</span></span><br></pre></td></tr></table></figure>
<p>对于ThreadLocalRandom.java的Unsafe代码修改方法也同上：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// 以下三行是新增，通过反射获得的Unsafe实例      </span></span><br><span class="line">       Field field = Unsafe.class.getDeclaredField(<span class="string">&quot;theUnsafe&quot;</span>);</span><br><span class="line">       field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">       UNSAFE = (Unsafe) field.get(<span class="keyword">null</span>);</span><br><span class="line"><span class="comment">// 以下是源码获取Unsafe实例的写法，注释它即可</span></span><br><span class="line"><span class="comment">// UNSAFE = sun.misc.Unsafe.getUnsafe();</span></span><br></pre></td></tr></table></figure>
<p>3、在transfer方法的代码片段位置加上能让当然线程挂起的代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="keyword">if</span> (nextTab == <span class="keyword">null</span>) &#123;            <span class="comment">// initiating</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">                Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node&lt;?,?&gt;[n &lt;&lt; <span class="number">1</span>];</span><br><span class="line">                nextTab = nt;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Throwable ex) &#123;      <span class="comment">// try to cope with OOME</span></span><br><span class="line">                sizeCtl = Integer.MAX_VALUE;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            nextTable = nextTab;</span><br><span class="line">            transferIndex = n;</span><br><span class="line">    <span class="comment">// 打印挂起的线程      </span></span><br><span class="line">            System.out.println(Thread.currentThread().getName()+<span class="string">&quot;:线程挂起&quot;</span>);</span><br><span class="line">    <span class="comment">// 本文使用的是LockSupport的park方法将当前线程挂起，其实内部调用了UNSAFE.park。this表示当前代码块。这种写法在TreeBin的读写锁竞争设计里面有被运用过。    </span></span><br><span class="line">            LockSupport.park(<span class="keyword">this</span>);</span><br><span class="line"><span class="comment">// 以下是提交者提供让线程挂起的写法之一，建议使用park方法来简直明了        </span></span><br><span class="line"><span class="comment">// The following added code here is to suspend Threads !</span></span><br><span class="line"><span class="comment">//            try &#123;</span></span><br><span class="line"><span class="comment">//                String s = new String();</span></span><br><span class="line"><span class="comment">//                synchronized (s)</span></span><br><span class="line"><span class="comment">//                &#123;</span></span><br><span class="line"><span class="comment">//                    System.out.println(Thread.currentThread().getName()+&quot;:线程暂停&quot;);</span></span><br><span class="line"><span class="comment">//                    s.wait();</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//            &#125; catch (InterruptedException e) &#123;</span></span><br><span class="line"><span class="comment">//                e.printStackTrace();</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br></pre></td></tr></table></figure>
<p>4、更改其他常量属性</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> 	<span class="comment">// 将桶位分配步长改为2，也即对一个容量为16的CHM，可以同时4个线程并发迁移各种桶位，也即至多有个4个线程能进入transfer方法</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TRANSFER_STRIDE = <span class="number">4</span>;</span><br><span class="line"><span class="comment">// 注释对应源码</span></span><br><span class="line">  <span class="comment">//private static final int MIN_TRANSFER_STRIDE = 16;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">//将MAX_RESIZERS设为3，由之前的CHM文章可知，实际参与到扩容线程数量为MAX_RESIZERS-1个，</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_RESIZERS = <span class="number">3</span>;</span><br><span class="line">	<span class="comment">//注释对应源码</span></span><br><span class="line">	<span class="comment">//private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;</span></span><br></pre></td></tr></table></figure>
<p>5、使用断点位置1复现bug</p>
<p>JDK-8214427的提交者给出可以在AddCount的分支2以下两行打上断点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))&#123;</span><br><span class="line">    transfer(tab, nt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在IDEA执行debug可以查看到以下每个线程方法调用栈的情况：（以下默认读者已经熟悉IDEA多线程调试操作以及相关界面的含义。）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ae8107dce306a9b66a3c2f7cf5d41b85.png" alt="sc+1位置断点debug图1"></p>
<p>而且是多个线程被观测：从线程Thread-12到Thread63都可以被观测。</p>
<p>这里需要解释为何是从IDEA捕抓到RUNNING状态的是线程Thread-12开始，而是不是从Thread-0开始？</p>
<p>因为前面第0号到10号线程总共put 入了11个节点（put结束后，这些线程发现不用扩容故结束），接着线程Thread-11去put节点完后发现此时CHM节点数量达到扩容阈值12（16*0.75），线程Thread-11就开始进入以下代码，可见线程Thread-11是作为首个进入扩容线程，但这里不是IDEA断点位置，这就是前面0到11线程不会在IDEA Frames或者Threads界面出现。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                             (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))&#123;</span><br><span class="line">    transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>又因为后续来了线程Thread-12put如节点后也发生s达到扩容阈值，会进入以下代码对sc加1计数，而这恰好是断点位置，因此线程Thread-12会IDEA 放入观测Frames中，而且线程Thread-12还是处于RUNNING状态</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))&#123;</span><br><span class="line">    transfer(tab, nt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同理后续的线程Thread-13、Thread-14…… 直到线程Thread-63都会被IDEA放入观测Frames中，如下图所示：<img src="https://img-blog.csdnimg.cn/img_convert/dea07fce20d299c5818f908ec2acada4.png" alt="sc+1位置断点debug图2"><br>有了以上铺垫，现在如何在Debug界面将bug复现？</p>
<p>现在回顾文章开头提出源码（如下代码片段）中出现的问题：<code>sc=rs+1</code>以及<code>sc=rs+MAX_RESIZERS</code>不会成立，因此sc本身为负数，rs这边为正数，因此if中的<code>sc == rs + MAX_RESIZERS</code> 限制参与扩容线程的总数量的条件不会起效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) !&#x3D; rs || sc &#x3D;&#x3D; rs + 1 ||</span><br><span class="line">    sc &#x3D;&#x3D; rs + MAX_RESIZERS || (nt &#x3D; nextTable) &#x3D;&#x3D; null ||</span><br><span class="line">    transferIndex &lt;&#x3D; 0)&#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName()+&quot;因无桶位可分配，此线程直接退出&quot;);</span><br><span class="line">    break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么如何证明呢？很简单，因为我们前面已经将 MAX_RESIZERS设为3，表示最多只允许 (MAX_RESIZERS-1)也即最多只能有2个线程进入扩容逻辑transfer方法，如果在IDEA界面观测到3个以上线程进入扩容逻辑transfer方法，说明bug成功复现，操作过程如下：</p>
<p>（1）在Frames界面，线程选择下拉框中选中一个线程，例如Thread-12，点击<code>Step Over</code> 跳过断点位置1的sc+1计数，接着点击<code>Step Into</code> 让线程在断点位置2进入transfer方法 ，此时在Thread-12的方法调用栈上出现transfer方法帧：<br><img src="https://img-blog.csdnimg.cn/img_convert/a045b2a6bb8c69277c4c7c465050d5d8.png" alt="sc+1位置断点debug图3"></p>
<p>（2）同理选中其他线程按（1）的“Step Over—&gt;（多次）Step Over—&gt;Step Into”操作,你发现超过2个线程都能进入transfer方法，其实后面的线程都可以进入transfer方法。这里不再一一给出图示。</p>
<p>这里为何说是（多次）Step Over呢，因此Thread-12 抢到CAS对sc加1，那么Thread-13只能回到<code>While</code>处再次来到断点位置去竞争CAS，所以需要对Thread-13（多次）Step Over。</p>
<p>（3）经过以上环节可以看到总共有64号到11号共24个线程能进入扩容逻辑，bug得到完美复现。</p>
<p>6、使用断点位置2复现bug</p>
<p>在5提到的方法中，需要手动Step Over—&gt;（多次）Step Over—&gt;Step Into操作将线程执行流进入到transfer方法，线程数量多，这么一个个操作去观察，显然方式很不smart。考虑另外一种方式：</p>
<p>去掉原来两个断点，新增一个断点，打在以下位置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = tab.length, stride;  <span class="comment">// 断点位置</span></span><br></pre></td></tr></table></figure>
<p>给断点设置线程“Suspend条件”：<br><img src="https://img-blog.csdnimg.cn/img_convert/d4ac26eb1ebad97848cdb06a4845158b.png" alt="transfer内部位置断点debug图1"></p>
<p>断点条件：<code>tab.length==16 &amp;&amp; nextTab !=null</code> ，表示CHM还在容量为16阶段的扩容流程中，那么此时一定会有线程进入到transfer方法里面，通过查看Frames，你可以发现有很多扩容线程，也再次使得bug完美复现，这里不再累赘。</p>
<h5 id="源代码修复后的验证过程"><a href="#源代码修复后的验证过程" class="headerlink" title="源代码修复后的验证过程"></a>源代码修复后的验证过程</h5><p>本小节尝试将源代码修复后，观测进入transfer线程的数量是否与（MAX_RESIZERS-1）设定的总数一致，如果一致，说明源码修改的逻辑可接受：将AddCount方法的分支2改为正常的写法后，断点位置如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>)) &#123; <span class="comment">// 断点</span></span><br><span class="line">    transfer(tab, nt); <span class="comment">// 断点</span></span><br></pre></td></tr></table></figure>
<p>在transfer方法中加入线程挂起代码<code>LockSupport.park(this)</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (nextTab == <span class="keyword">null</span>) &#123;            <span class="comment">// initiating</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">        Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node&lt;?,?&gt;[n &lt;&lt; <span class="number">1</span>];</span><br><span class="line">        nextTab = nt;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable ex) &#123;      <span class="comment">// try to cope with OOME</span></span><br><span class="line">        sizeCtl = Integer.MAX_VALUE;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    nextTable = nextTab;</span><br><span class="line">    transferIndex = n;</span><br><span class="line">    LockSupport.park(<span class="keyword">this</span>);</span><br></pre></td></tr></table></figure>
<p>观测过程：根据前面分析可知，从线程Thread-12到Thread-63都会同一时刻来到sc+1计数逻辑进行CAS竞争，接下来的IDEA Debug步骤如下：</p>
<p>（1）选中Thread-12，对其执行sc+1计数逻辑进行CAS竞争，因此Thread-12优先来sc+1这个位置并CAS竞争成功，它可进入transfer内部。</p>
<p>（2）接着选中Thread-13，对其执行sc+1计数逻辑进行CAS竞争，因为竞争失败，因此回到While循环继续：</p>
<p>图中可以清晰看见：sc==rs+MAX_RESIZERS 为true，表名当前参与到扩容线程的数量达到最大的限定值，Thread-13将会进入break然后退出</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/33aa00f8570788a8f611b7a38b8b4ed0.png" alt="rs修复后debug图1"></p>
<p>（3）同理选中Thread-14等后续线程也会跟Thread-13同一逻辑，</p>
<p>也即，Thread-13到Thread-63线程都会break掉</p>
<p>（4）在（1）中只有1个线程Thread-12进入transfer内部，不是说好会有（MAX_RESIZERS-1），也即2个线程能进入transfer内部吗？  </p>
<p>别忘记线程Thread-11已经通过以下逻辑作为第1个扩容线程进入了transfer方法内部</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, rs + <span class="number">2</span>))&#123;</span><br><span class="line">    transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此总共有2个线程：Thread-11和线程Thread-12进入到transfer方法内部，说明修复之后的代码逻辑正确。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度讨论高并发跳表数据结构ConcurrentSkipListMap的源代码实现（下）</title>
    <url>/2022/01/30/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%AE%A8%E8%AE%BA%E9%AB%98%E5%B9%B6%E5%8F%91%E8%B7%B3%E8%A1%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84ConcurrentSkipListMap%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<p>文章说明：因为CSM解析内容较多，因此全文分为“深度讨论高并发跳表数据结构ConcurrentSkipListMap的源代码实现（上）”和“深度讨论高并发跳表数据结构ConcurrentSkipListMap的源代码实现（下）”两篇文章<br>上篇：CSM数据结构设计原理、doGet、doPut核心方法解析<br>下篇：doRemove核心方法解析、总结</p>
<p><img src="https://img-blog.csdnimg.cn/015a4701c67246769fa08e411c3c9896.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>《gitee 博客文章封面》</p>
<h4 id="remove方法："><a href="#remove方法：" class="headerlink" title="remove方法："></a>remove方法：</h4><h5 id="删除操作的设计原理"><a href="#删除操作的设计原理" class="headerlink" title="删除操作的设计原理"></a>删除操作的设计原理</h5><p>这里先介绍Doug Lea在源代码注释给出算法设计说明：n.helpDelete(b,f)的设计原理</p>
<p>上一篇文章的doGet、doPut方法中都有涉及到遇到被标记”删除“的节点时都会加入<code>n.helpDelete(b,f)</code>的处理逻辑，</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">* In addition to using deletion markers, the lists also use</span><br><span class="line">* nullness of value fields to indicate deletion, in a style</span><br><span class="line">* similar to typical lazy-deletion schemes.  If a node&#x27;s value is</span><br><span class="line">* null, then it is considered logically deleted and ignored even</span><br><span class="line">* though it is still reachable. This maintains proper control of</span><br><span class="line">* concurrent replace vs delete operations -- an attempted replace</span><br><span class="line">* must fail if a delete beat it by nulling field, and a delete</span><br><span class="line">* must return the last non-null value held in the field. (Note:</span><br><span class="line">* Null, rather than some special marker, is used for value fields</span><br><span class="line">* here because it just so happens to mesh with the Map API</span><br><span class="line">* requirement that method get returns null if there is no</span><br><span class="line">* mapping, which allows nodes to remain concurrently readable</span><br><span class="line">* even when deleted. Using any other marker value here would be</span><br><span class="line">* messy at best.)</span><br><span class="line">* n是当前要删除的节点，b是n的前驱节点，f是n的后继节点，开始删除前，(b,n,f)的指向关系如下</span><br><span class="line">* Here&#x27;s the sequence of events for a deletion of node n with</span><br><span class="line">* predecessor b and successor f, initially:</span><br><span class="line">*</span><br><span class="line">*        +------+       +------+      +------+</span><br><span class="line">*   ...  |   b  |------&gt;|   n  |-----&gt;|   f  | ...</span><br><span class="line">*        +------+       +------+      +------+</span><br><span class="line">*</span><br><span class="line">* 1. CAS n&#x27;s value field from non-null to null.</span><br><span class="line">*    From this point on, no public operations encountering</span><br><span class="line">*    the node consider this mapping to exist. However, other</span><br><span class="line">*    ongoing insertions and deletions might still modify</span><br><span class="line">*    n&#x27;s next pointer.</span><br><span class="line">*</span><br><span class="line">* 1、 将删除节点n的value使用cas设成null，表示此刻起，n节点处于待删除状态，虽然其value为null，对于public相关方法如get等，在执行遇到此类型节点时不会将该节点视为有效节点（也即会被跳过处理），但是如果对于正在执行的插入和删除操作的方法来说，可能也可以去更改“此待删除节点n”的后继节点。</span><br><span class="line">* 2. CAS n&#x27;s next pointer to point to a new marker node.</span><br><span class="line">*    From this point on, no other nodes can be appended to n.</span><br><span class="line">*    which avoids deletion errors in CAS-based linked lists.</span><br><span class="line">*</span><br><span class="line">*        +------+       +------+      +------+       +------+</span><br><span class="line">*   ...  |   b  |------&gt;|   n  |-----&gt;|marker|------&gt;|   f  | ...</span><br><span class="line">*        +------+       +------+      +------+       +------+</span><br><span class="line">* 2、使用cas将n节点的next字段指向一个marker节点后，从此刻起，任何节点都不会被放在n的后继节点位置，也即不可能出现n-&gt;n1、n-&gt;n2等，只有n-&gt;marker</span><br><span class="line">* 3. CAS b&#x27;s next pointer over both n and its marker.</span><br><span class="line">*    From this point on, no new traversals will encounter n,</span><br><span class="line">*    and it can eventually be GCed.</span><br><span class="line">*        +------+                                    +------+</span><br><span class="line">*   ...  |   b  |-----------------------------------&gt;|   f  | ...</span><br><span class="line">*        +------+                                    +------+</span><br><span class="line">* 3、通过cas将b的next指针（越过n节点以及n的后继marker节点）指向f节点，从此刻起，n节点不会被相关操作遍历到，n最终会被GC。</span><br></pre></td></tr></table></figure>
<p>可以看出CSM删除操作方面，借用marker节点来实现，将待删除节点的value设为null值来表示该节点处在删除状态但还未真正删除，这种设计风格就像“惰性删除语义(lazy-deletion schemes)”，先标记删，等某个时机再真正执行之。如果CSM的一个节点的value是null，说明该节点在逻辑上已经被删除。</p>
<a id="more"></a>
<p>以下是remove方法的源代码解析</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> doRemove(key, <span class="keyword">null</span>);<span class="comment">//注意只需要比较key即可，不需要考虑value相等才删除！</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="具体逻辑由doRemove实现"><a href="#具体逻辑由doRemove实现" class="headerlink" title="具体逻辑由doRemove实现"></a>具体逻辑由doRemove实现</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">doRemove</span><span class="params">(Object key, Object value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line">    outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    		<span class="comment">// 1、和doGet、findNode类似设计，首先找到key的前驱节点</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">        <span class="comment">// </span></span><br><span class="line">            Object v; <span class="keyword">int</span> c;</span><br><span class="line">            <span class="keyword">if</span> (n == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">          <span class="comment">// 经典的(b,n,f)三指针</span></span><br><span class="line">            Node&lt;K,V&gt; f = n.next;</span><br><span class="line">          <span class="comment">// 2、n已不是b的后继节点，也即读n节点前后不一致，则重试</span></span><br><span class="line">            <span class="keyword">if</span> (n != b.next)                    <span class="comment">// inconsistent read</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">          <span class="comment">// 3、数据节点n节点被标记为删除状态，那么使用helpDelete把n节点删除，然后重试。</span></span><br><span class="line">            <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123;        <span class="comment">// n is deleted</span></span><br><span class="line">                n.helpDelete(b, f);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">// 4、key的前驱节点b被标记删除状态，只能重试，读取新的b</span></span><br><span class="line">            <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n)      <span class="comment">// b is deleted</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//5、 给定的key不在数据链表里面，直接结束</span></span><br><span class="line">            <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">          	<span class="comment">//6、 给定的key比当前数据节点n还大，那么更新b、n向右继续检索</span></span><br><span class="line">            <span class="keyword">if</span> (c &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                b = n;</span><br><span class="line">                n = f;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// 以下找到与key相等的n节点</span></span><br><span class="line">            <span class="comment">//7、doRemove的入参value默认是null，因此以下会被跳过，若指定value，则还需判断给定的value和当前找到n.value是否相等</span></span><br><span class="line">            <span class="keyword">if</span> (value != <span class="keyword">null</span> &amp;&amp; !value.equals(v))</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">            <span class="comment">//8、执行流运行到这里，说明满足删除n节点的条件也即key=n.key,n就是要删除的目标数据节点，因此将n.value设为null，cas失败则重试，注意这是是标记删除，不是直接把n节点删除。</span></span><br><span class="line">            <span class="keyword">if</span> (!n.casValue(v, <span class="keyword">null</span>))</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">/*9、执行流运行到这里，说明n成功被标记删除状态</span></span><br><span class="line"><span class="comment">          	条件1：将 b → n → f 变成  b → n → marker → f</span></span><br><span class="line"><span class="comment">          	条件2：将 b → n → f 变成 b → f</span></span><br><span class="line"><span class="comment">          	如果条件1CAS失败或者条件2CAS失败，都会调用findNode(key)来删除数据节点n</span></span><br><span class="line"><span class="comment">          	*/</span></span><br><span class="line">            <span class="keyword">if</span> (!n.appendMarker(f) || !b.casNext(n, f))</span><br><span class="line">                findNode(key);                  <span class="comment">// retry via findNode</span></span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">/*10、不妨假设条件1成立，也即将 b → n → f 变成  b → n → marker → f</span></span><br><span class="line"><span class="comment">              需要n节点上方的索引节点都清除（包括清除索引节点的前后指向关系），恰好findPredecessor就是在索引层干这事。</span></span><br><span class="line"><span class="comment">              */</span> </span><br><span class="line">                findPredecessor(key, cmp);      <span class="comment">// clean index</span></span><br><span class="line">                <span class="keyword">if</span> (head.right == <span class="keyword">null</span>)</span><br><span class="line">                    tryReduceLevel();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">            <span class="keyword">return</span> vv;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>虽然在doGet方法中有给出findPredecessor，它是用在查询场景中，而在本小节中findPredecessor被用来删除n节点对应的上方索引节点场景中，现在结合上方doRmove的源代码理解，将更能掌握其中设计逻辑。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Node&lt;K,V&gt; <span class="title">findPredecessor</span><span class="params">(Object key, Comparator&lt;? <span class="keyword">super</span> K&gt; cmp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(); <span class="comment">// don&#x27;t postpone errors</span></span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">      	</span><br><span class="line">        <span class="keyword">for</span> (Index&lt;K,V&gt; q = head, r = q.right, d;;) &#123;</span><br><span class="line">          	<span class="comment">//在当前索引层检索，只要不是来到链表尾部，就继续在该层里面遍历</span></span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                Node&lt;K,V&gt; n = r.node;</span><br><span class="line">                K k = n.key;</span><br><span class="line">                <span class="comment">// 此处对应的是上方doRemove内部将n.value标记为null的情况</span></span><br><span class="line">                <span class="keyword">if</span> (n.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                  	<span class="comment">/* 既然数据层的n节点被删除，那么n节点对应上方关联的索引节点r也要删除:</span></span><br><span class="line"><span class="comment">                    索引层：将 q → r → r.right 变成  q → r.right </span></span><br><span class="line"><span class="comment">                    */</span></span><br><span class="line">                    <span class="keyword">if</span> (!q.unlink(r))</span><br><span class="line">                        <span class="keyword">break</span>;           <span class="comment">// restart</span></span><br><span class="line">                  	<span class="comment">// 索引节点r成功删除后，将r指向q的新right节点，此时q → r 两个索引节点都处于正常状态，继续下一轮遍历。</span></span><br><span class="line">                    r = q.right;         <span class="comment">// reread r</span></span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">// 继续在该层索引向右检索，直到cpr(cmp, key, k) =0</span></span><br><span class="line">                <span class="keyword">if</span> (cpr(cmp, key, k) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    q = r;</span><br><span class="line">                    r = r.right;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// 当q位于第1层索引层位置时，此时q.down指向的是idx=null，因此可退出，到此从入口head到出口q.down=null沿途找到的与n对应的每层索引节点idx都被删除掉。可直接退出</span></span><br><span class="line">            <span class="keyword">if</span> ((d = q.down) == <span class="keyword">null</span>) </span><br><span class="line">                <span class="keyword">return</span> q.node;</span><br><span class="line">          	<span class="comment">// 执行流运行到这里，说明还未到达第1层索引层，以上逻辑完成当前层的idx节点删除，那么继续需要处理n节点对应的更低层的索引节点idx。</span></span><br><span class="line">            q = d;</span><br><span class="line">            r = d.right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此在doRemove方法的角度来看， findPredecessor(key, cmp) 实际意义是单纯用于clean index，而不是为key找到前驱节点b这么简单，这里的clean index功能就是Doug Lea提到的“side-effect”，也即下面这句话的含义：</p>
<blockquote>
<p>Callers rely on this side-effect of clearing indices to deleted nodes.</p>
</blockquote>
<p>doRemove作为调用方，能够从调用findPredecessor过程获得额外收益：清除那些已被标记为“删除状态”的数据节点上方对应的每层索引节点idx。</p>
<p>关于doRemove的图解这里不再给出，想要深入理解的同学务必自行将删除逻辑对应的图做出来，否则将难以理解其过程的动态处理。</p>
<h5 id="tryReduceLevel-方法"><a href="#tryReduceLevel-方法" class="headerlink" title="tryReduceLevel()方法"></a>tryReduceLevel()方法</h5><p>在doRemove方法的第10个逻辑中，清理完索引节点后，还需要检查是否需要“降层处理”：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">/*10、不妨假设条件1成立，也即将 b → n → f 变成  b → n → marker → f</span></span><br><span class="line"><span class="comment">  需要n节点上方的索引节点都清除（包括清除索引节点的前后指向关系），恰好findPredecessor就是在索引层干这事。</span></span><br><span class="line"><span class="comment">  */</span> </span><br><span class="line">    findPredecessor(key, cmp);      <span class="comment">// clean index</span></span><br><span class="line">    <span class="keyword">if</span> (head.right == <span class="keyword">null</span>)</span><br><span class="line">        tryReduceLevel();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在tryReduceLevel设计，在一个if里面塞进去8个条件，而且只有h.level超过3层才进行“降层”处理，等于3层不对索引层“减层”，以下不妨考察h.level=4的情况</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">tryReduceLevel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    HeadIndex&lt;K,V&gt; h = head; </span><br><span class="line">    HeadIndex&lt;K,V&gt; d;</span><br><span class="line">    HeadIndex&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">if</span> (h.level &gt; <span class="number">3</span> &amp;&amp;</span><br><span class="line">        <span class="comment">// h</span></span><br><span class="line">        (d = (HeadIndex&lt;K,V&gt;)h.down) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        (e = (HeadIndex&lt;K,V&gt;)d.down) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        e.right == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        d.right == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        h.right == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        casHead(h, d) &amp;&amp; <span class="comment">// try to set</span></span><br><span class="line">        h.right != <span class="keyword">null</span>) <span class="comment">// recheck</span></span><br><span class="line">        casHead(d, h);   <span class="comment">// try to backout</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将具有level=4的HeadIndex结构图：<br>h（原位于level=4的HeadIndex） → null<br>↓<br>d → null<br>↓<br>e → null<br>变成level=3的HeadIndex结构：<br>h(d) (原位于level=3的d节点作为新的HeadIndex)→ idx<br>↓<br>e → idx</p>
<h5 id="关于n-helpDelete-b-f-的解析："><a href="#关于n-helpDelete-b-f-的解析：" class="headerlink" title="关于n.helpDelete(b, f)的解析："></a>关于n.helpDelete(b, f)的解析：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Helps out a deletion by appending marker or unlinking from</span></span><br><span class="line"><span class="comment"> * predecessor. This is called during traversals when value</span></span><br><span class="line"><span class="comment"> * field seen to be null.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> b predecessor</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> f successor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// b是n的前驱节点，n是待删除节点，f是n的后继节点</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">helpDelete</span><span class="params">(Node&lt;K,V&gt; b, Node&lt;K,V&gt; f)</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Rechecking links and then doing only one of the</span></span><br><span class="line"><span class="comment">     * help-out stages per call tends to minimize CAS</span></span><br><span class="line"><span class="comment">     * interference among helping threads.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 当前线程前后读取的b、n、f保持不变时，就可以实施cas操作</span></span><br><span class="line">    <span class="keyword">if</span> (f == <span class="keyword">this</span>.next &amp;&amp; <span class="keyword">this</span> == b.next) &#123;</span><br><span class="line">     <span class="comment">/* </span></span><br><span class="line"><span class="comment">     条件1：对于b → n → null这种情况，f=n.next,显然f为null</span></span><br><span class="line"><span class="comment">     条件2：对于b → n → f，且f节点不是marker节点，那么给n添加一个marker节点：</span></span><br><span class="line"><span class="comment">     b → n → marker → f，注意：marker指向f节点指向关系不是因为next指针，而是由marker的value指针指向f完成。</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">        <span class="keyword">if</span> (f == <span class="keyword">null</span> || f.value != f) <span class="comment">// not already marked</span></span><br><span class="line">            casNext(f, <span class="keyword">new</span> Node&lt;K,V&gt;(f));</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">        <span class="comment">/*否则f节点一定是marker节点，且b、n、f以及marker的指向关系如下：</span></span><br><span class="line"><span class="comment">          b → n → f(marker)→ f.next 变成 b → f.next</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">             b.casNext(<span class="keyword">this</span>, f.next);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="理解marker节点的实际设计意义"><a href="#理解marker节点的实际设计意义" class="headerlink" title="理解marker节点的实际设计意义"></a>理解marker节点的实际设计意义</h5><p>解析器设计意义需要基于图解，因为涉及到多线程并发删除的非原子性操作。</p>
<ul>
<li>无maker节点的删除设计</li>
</ul>
<p>假设线程的cpu时间片分配顺序如下：<br><img src="https://img-blog.csdnimg.cn/7a0fb41cb348491aa6ed71a852d6f746.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>结果是明显的，线程B以为自己成功插入了该节点，但对于读线程C来说，新插入节点newNode不可能被从b开始遍历到。</p>
<p>核心原因在于在第4个cpu时间片时，线程B使用<code>n.casNext(f,new Node&lt;&gt;(k,v,next=f))</code>,只用f节点作为CAS比较并不能感知到链表结构已经发生变化，导致线程B前后读取f是一致，然后casNext成功插入的新节点。</p>
<p>这种删除设计使得插入操作不能正确执行，因此不采用。</p>
<ul>
<li>有maker节点的删除设计</li>
</ul>
<p>只要让插入线程能根据<code>n.casNext(cmp,new Node&lt;&gt;(k,v,next=cmp))</code> 感知cmp作为cas比较的节点已经发生改变，那么插入线程自然会cas失败，然后继续下一轮重试，直到读取的n节点不是删除状态，方可能正确插入节点。<br><img src="https://img-blog.csdnimg.cn/015a4701c67246769fa08e411c3c9896.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>对于图中的3和4的说明，因为cpu时间片执行顺序不确定，注意面临两种情况：</p>
<p>1、3执行CAS成功，4失败，迫使插入线程B的CAS插入失败后重新循环读取新的n节点</p>
<p>2、4执行CAS成功，3失败，迫使删除线程A的添加marker节点失败，只能重新循环读取新的n节点</p>
<p>只需一个marker即可实现写线程和写线程之间是线程安全的竞争。</p>
<h5 id="findFirst方法"><a href="#findFirst方法" class="headerlink" title="findFirst方法"></a>findFirst方法</h5><p>需要知道的一点是：数据层链表的HeadIndex的构成如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//ConcurrentSkipListMap()构造方法里面initialize()</span></span><br><span class="line"><span class="keyword">new</span> HeadIndex&lt;K,V&gt;(<span class="keyword">new</span> Node&lt;K,V&gt;(<span class="keyword">null</span>, BASE_HEADER, <span class="keyword">null</span>),</span><br><span class="line">                                  <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>因此对于数据链表层的“头节点”可不是数据节点Node，而是索引头节点HeadIndex，因此构造阶段时：</p>
<p>HeadIndex(BASE_HEADER) → null，也即h.node → null，也即h.node =next=null</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Specialized variant of findNode to get first valid node.</span></span><br><span class="line"><span class="comment"> * @return first node or null if empty</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">findFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt; b, n;;) &#123;</span><br><span class="line">      	<span class="comment">//如上面所解析，如果当前数据层链表仅有一个HeadIndex节点，还没有任何数据节点时，显然只能返回null，这里的b=new Node&lt;K,V&gt;(null, BASE_HEADER, null)</span></span><br><span class="line">        <span class="keyword">if</span> ((n = (b = head.node).next) == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      	<span class="comment">// n作为数据节点第一个头节点没有被标记位删除，那么n节点就是所早的FirstNode</span></span><br><span class="line">        <span class="keyword">if</span> (n.value != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> n;</span><br><span class="line">      	<span class="comment">// 如果发现n已经被标记位删除，那么当前线程就去帮助删除该n节点。</span></span><br><span class="line">        n.helpDelete(b, n.next);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>findFirst方法一般被其他方法调用，例如size()方法，通过找到数据层链表的数据头节点，然后即可在当成链表去遍历：</p>
<p>不过可以确定的是，此size方法的返回值不能正确反映当前数据层链表的长度，因为是并发环境，其他删除线程可以删除数据节点节点<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">  	<span class="comment">// 通过findFirst返回首个数据节点，然后在此头节点遍历链表去统计即可。</span></span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt; n = findFirst(); n != <span class="keyword">null</span>; n = n.next) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n.getValidValue() != <span class="keyword">null</span>)</span><br><span class="line">            ++count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (count &gt;= Integer.MAX_VALUE) ? Integer.MAX_VALUE : (<span class="keyword">int</span>) count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>removeFirstEntry类似doRemove，基于(b,n,f)三节点去操作。</p>
<p>只要你掌握CSM的数据结构设计原理以及上面解析的doGet、doPut、doRemove、findPredecessor、findNode的具体实现逻辑，那么CSM其他方法都可以非常快的突破。</p>
<h4 id="CSM的小结内容"><a href="#CSM的小结内容" class="headerlink" title="CSM的小结内容"></a>CSM的小结内容</h4><h5 id="1、空间复杂度计算"><a href="#1、空间复杂度计算" class="headerlink" title="1、空间复杂度计算"></a>1、空间复杂度计算</h5><p>按照本文给出的图示，不妨假设当前数据层链表节点个数为$n$， 根据源代码文件的注释说明，一个新插入的数据节点它被作为上方一层的索引节点的概率为0.5，因此有以下非严格数学归纳过程（考虑理想情况下的分布）</p>
<p>level=1，该层索引层对应的索引节点数量 $\frac{n}{2}$</p>
<p>level=2，该层索引层对应的索引节点数量$\frac{n}{2^2}$</p>
<p>level=3，该层索引层对应的索引节点数量$\frac{n}{2^2}$</p>
<p>…</p>
<p>level=k，该层索引层对应的索引节点数量$\frac{n}{2^k}$</p>
<p>不妨假设第k层就是该跳表的最顶层索引层且假设仅有1个索引节点</p>
<p>那么有$\frac{n}{2^k}=1$ ，可算出k的值为：$k=log_2n$ ，习惯性写法为$k=log(n)$</p>
<p>到这里应该可以联想k的含义类似二叉树的高度h，h的推算也是类似的思路。</p>
<p>那么k层的跳表需要额外存储多少索引节点呢？（这里说的额外是指不计数据层节点）总共k层的索引节点相加如下</p>
<p>$\frac{n}{2}+\frac{n}{2^2}+\frac{n}{2^3}+\frac{n}{2^4}+…+4+2+1$ 共有k项，等比为0.5，求和：$sum=n-1$，也即当成为索引节点的概率取值为0.5时，需要额外存储n-1个索引节点，例如数据层节点16个，那么对应四层索引层的索引节点数为8，4，2 ，1</p>
<p>同理若成为索引节点的概率取值为$\dfrac{1}{4}$时，需要存储$\frac{n-1}{3}$  个索引节点。例如数据层节点16个，那么对应两层索引层的索引节点数为4，1 。</p>
<p>容易推出：当成为索引节点的概率取值为0.5时，需要额外存储n-1个索引节点，再加数据节点数量n个，也即CSM总共存储2n-1个节点，故空间复杂度为$o(n)$，当前去其他概率值也有类似结论，这里不再给出说明。一般使用概率p=0.5，作为CSM数据节点被选中作为索引节点的概率，如果概率选太小例如$\frac{1}{128}$，那么每次查询都几乎都要下沉到数据层去查找，那么查询时间复杂度就会退化到解决$o(n)$</p>
<h5 id="2、时间复杂度"><a href="#2、时间复杂度" class="headerlink" title="2、时间复杂度"></a>2、时间复杂度</h5><p>根据空间复杂度的索引节点：</p>
<p>最好情况：最高层索引节点就是目的节点，搜索次数1</p>
<p>最坏情况：若一个新插入的数据节点它被作为上方一层的索引节点的概率为0.5，那么每一层索引层都需要遍历两次索引节点（最高层仅有一个索引节点就1次，若有两个索引节点就两次），不妨假设现在有跳表结构如下，共4层索引：</p>
<p>1 （level=4）</p>
<p>1，9  （level=3）</p>
<p>1，5，9，13 （level=2）</p>
<p>1，3，5，7，9，11，13，15 （level=1）</p>
<p>1，2，3，4，，，，，，，，15，16 （数据层链表）</p>
<p>现在需要查找16这个节点，从level=4开始，</p>
<p>（1）level=4，只需查找1次：比较1，下移</p>
<p>（2）level=3，只需查找2次：比较1，比较9，从9垂直下移到level=2的索引9节点</p>
<p>（3）level=2，只需查找2次：比较9，比较13，从13垂直下移到level=1的索引13节点</p>
<p>（3）数据层：因为每两个节点就有一个是索引节点，因此在数据层检索比较也是最多比较2两次</p>
<p>由上面空间复杂度计算过程可知，若数据层节点有n个前提下，索引层高度为$k=log(n)$ ,</p>
<p>那么时间复杂度的计算为：$k层*（每层索引搜索次数2）+数据层搜索次数2$，也即$log (n) \times 2 +2$，简化最终可知，跳表的查询复杂度为$log(n)$</p>
<p>同理，插入、删除节点都需要先从最顶层索引层开始检索定位，最后再下移到数据层链表进行插入和删除操作，因此插入、删除的时间复杂度也是和查找get的复杂一致：$log(n)$</p>
<p>关于SkipList的严格数学证明，学有余力的同学可以去翻阅跳表发明者William Pugh原著论文，<a href="https://klevas.mif.vu.lt/~ragaisis/ADS2006/skiplists.pdf">论文地址</a><br>以下是Dung Lea对SkipList性能的简要说明，可以看出其实跟TreeMap性能很接近。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* Indexing uses skip list parameters that maintain good search</span><br><span class="line">* performance while using sparser-than-usual indices: The</span><br><span class="line">* hardwired parameters k&#x3D;1, p&#x3D;0.5 (see method doPut) mean</span><br><span class="line">* that about one-quarter of the nodes have indices. Of those that</span><br><span class="line">* do, half have one level, a quarter have two, and so on (see</span><br><span class="line">* Pugh&#39;s Skip List Cookbook, sec 3.4).  The expected total space</span><br><span class="line">* requirement for a map is slightly less than for the current</span><br><span class="line">* implementation of java.util.TreeMap.</span><br></pre></td></tr></table></figure></p>
<h5 id="3、对比ConcurrentHashMap"><a href="#3、对比ConcurrentHashMap" class="headerlink" title="3、对比ConcurrentHashMap"></a>3、对比ConcurrentHashMap</h5><p>CSM：ConcurrentSkipListMap，CHM：ConcurrentHashMap</p>
<ul>
<li>场景方面：</li>
</ul>
<p>CSM适用于要求key有序存放的场景，CHM适用于key无序存放的场景</p>
<ul>
<li>写操作性能</li>
</ul>
<p>高并发写（插入、删除）操作方面，CSM采用CAS无锁+更改节点指向实现写操作，插入不会阻塞删除操作，而CHM采用CAS+Synchronized独占锁方式，对于同一桶位进行插入操作会阻塞删除操作（反向也成立），那么这两种数据结构的插入、删除新能如何呢？</p>
<p>关于插入和删除的占用内存的比较如下：对象占用比较方面，这里使用lucene的一个工具类RamUsageEstimator</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.lucene<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lucene-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>demo代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentSkipListMap;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.util.RamUsageEstimator;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SkipListMapPerf</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> ConcurrentSkipListMap&lt;Integer,Integer&gt; csm=<span class="keyword">new</span> ConcurrentSkipListMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">static</span> ConcurrentHashMap&lt;Integer,Integer&gt; chm=<span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] scales=&#123;<span class="number">1</span>,<span class="number">4</span>,<span class="number">16</span>,<span class="number">64</span>,<span class="number">128</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> loops=<span class="number">1000</span>*<span class="number">1000</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> scale : scales) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;线程数：&quot;</span>+scale);</span><br><span class="line">            System.out.println(<span class="string">&quot;put:&quot;</span>);</span><br><span class="line">            csmPut(scale,loops);</span><br><span class="line">            chmPut(scale,loops);</span><br><span class="line">            System.out.println(<span class="string">&quot;ramUsage:&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;csm:&quot;</span>+RamUsageEstimator.humanReadableUnits(RamUsageEstimator.sizeOfMap(csm)));</span><br><span class="line">            System.out.println(<span class="string">&quot;chm:&quot;</span>+RamUsageEstimator.humanReadableUnits(RamUsageEstimator.sizeOfMap(chm)));</span><br><span class="line">            System.out.println(<span class="string">&quot;remove:&quot;</span>);</span><br><span class="line">            csmRemove(scale,loops);</span><br><span class="line">            chmRemove(scale,loops);</span><br><span class="line">            System.out.println(<span class="string">&quot;ramUsage:&quot;</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;csm&quot;</span>+RamUsageEstimator.humanReadableUnits(RamUsageEstimator.sizeOfMap(csm)));</span><br><span class="line">            System.out.println(<span class="string">&quot;chm&quot;</span>+RamUsageEstimator.humanReadableUnits(RamUsageEstimator.sizeOfMap(chm)));</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">csmPut</span><span class="params">(<span class="keyword">int</span> threadNums,<span class="keyword">int</span> loops)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        List&lt;Thread&gt; threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> start= System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadNums; i++) &#123;</span><br><span class="line">            Thread t= <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> loop = <span class="number">0</span>; loop &lt; loops; loop++) &#123;</span><br><span class="line">                    csm.put(loop,loop);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.join();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> duration=System.currentTimeMillis()-start;</span><br><span class="line">        String s= String.format(<span class="string">&quot;csmPut：线程数为%s,计算%s次,总用时%sms&quot;</span>,threadNums,loops,duration);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">chmPut</span><span class="params">(<span class="keyword">int</span> threadNums, <span class="keyword">int</span> loops)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        List&lt;Thread&gt; threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> start= System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadNums; i++) &#123;</span><br><span class="line">            Thread t= <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> loop = <span class="number">0</span>; loop &lt; loops; loop++) &#123;</span><br><span class="line">                    chm.put(loop,loop);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.join();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> duration=System.currentTimeMillis()-start;</span><br><span class="line">        String s= String.format(<span class="string">&quot;chmPut：线程数为%s,计算%s次,总用时%sms&quot;</span>,threadNums,loops,duration);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">csmRemove</span><span class="params">(<span class="keyword">int</span> threadNums, <span class="keyword">int</span> loops)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        List&lt;Thread&gt; threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> start= System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadNums; i++) &#123;</span><br><span class="line">            Thread t= <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> loop = <span class="number">0</span>; loop &lt; loops; loop++) &#123;</span><br><span class="line">                    csm.remove(loop);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.join();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> duration=System.currentTimeMillis()-start;</span><br><span class="line">        String s= String.format(<span class="string">&quot;csmRemove：线程数为%s,计算%s次,总用时%sms&quot;</span>,threadNums,loops,duration);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">chmRemove</span><span class="params">(<span class="keyword">int</span> threadNums, <span class="keyword">int</span> loops)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        List&lt;Thread&gt; threadList=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">long</span> start= System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadNums; i++) &#123;</span><br><span class="line">            Thread t= <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> loop = <span class="number">0</span>; loop &lt; loops; loop++) &#123;</span><br><span class="line">                    chm.remove(loop);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            threadList.add(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : threadList) &#123;</span><br><span class="line">            thread.join();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> duration=System.currentTimeMillis()-start;</span><br><span class="line">        String s= String.format(<span class="string">&quot;chmRemove：线程数为%s,计算%s次,总用时%sms&quot;</span>,threadNums,loops,duration);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">线程数：1</span><br><span class="line">put:</span><br><span class="line">csmPut：线程数为1,计算1000000次,总用时2285ms</span><br><span class="line">chmPut：线程数为1,计算1000000次,总用时1446ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm:511.2 MB</span><br><span class="line">chm:511.2 MB</span><br><span class="line">remove:</span><br><span class="line">csmRemove：线程数为1,计算1000000次,总用时217ms</span><br><span class="line">chmRemove：线程数为1,计算1000000次,总用时111ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm48 bytes</span><br><span class="line">chm64 bytes</span><br><span class="line">线程数：4</span><br><span class="line">put:</span><br><span class="line">csmPut：线程数为4,计算1000000次,总用时676ms</span><br><span class="line">chmPut：线程数为4,计算1000000次,总用时520ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm:511.2 MB</span><br><span class="line">chm:511.2 MB</span><br><span class="line">remove:</span><br><span class="line">csmRemove：线程数为4,计算1000000次,总用时522ms</span><br><span class="line">chmRemove：线程数为4,计算1000000次,总用时296ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm48 bytes</span><br><span class="line">chm64 bytes</span><br><span class="line">线程数：16</span><br><span class="line">put:</span><br><span class="line">csmPut：线程数为16,计算1000000次,总用时1873ms</span><br><span class="line">chmPut：线程数为16,计算1000000次,总用时1687ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm:511.2 MB</span><br><span class="line">chm:511.2 MB</span><br><span class="line">remove:</span><br><span class="line">csmRemove：线程数为16,计算1000000次,总用时789ms</span><br><span class="line">chmRemove：线程数为16,计算1000000次,总用时234ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm48 bytes</span><br><span class="line">chm64 bytes</span><br><span class="line">线程数：64</span><br><span class="line">put:</span><br><span class="line">csmPut：线程数为64,计算1000000次,总用时5564ms</span><br><span class="line">chmPut：线程数为64,计算1000000次,总用时2879ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm:511.2 MB</span><br><span class="line">chm:511.2 MB</span><br><span class="line">remove:</span><br><span class="line">csmRemove：线程数为64,计算1000000次,总用时2505ms</span><br><span class="line">chmRemove：线程数为64,计算1000000次,总用时321ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm48 bytes</span><br><span class="line">chm64 bytes</span><br><span class="line">线程数：128</span><br><span class="line">put:</span><br><span class="line">csmPut：线程数为128,计算1000000次,总用时9665ms</span><br><span class="line">chmPut：线程数为128,计算1000000次,总用时4396ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm:511.2 MB</span><br><span class="line">chm:511.2 MB</span><br><span class="line">remove:</span><br><span class="line">csmRemove：线程数为128,计算1000000次,总用时4548ms</span><br><span class="line">chmRemove：线程数为128,计算1000000次,总用时880ms</span><br><span class="line">ramUsage:</span><br><span class="line">csm48 bytes</span><br><span class="line">chm64 bytes</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>从输出结果可以得到以下推论：</p>
<p>1、插入1百万个节点后，CSM和CHM在jvm的大小都很接近</p>
<p>2、在高并发插入场景下，CHM的性能比CSM快两倍左右，解释：</p>
<p>CSM插入大量节点时，每插入一个数据节点都判断是否要在此数据节点上方建立多层索引节点，若需要添加索引节点，那么创建索引节点后将索引节点链入该层索引层中。而CHM插入大量节点时，如果key的hash足够分散，那么大量节点其实是分布在底层的table桶位上，或者在部分桶位形成冲突链链表，在同一桶位上需要升级为红黑树结构的概率小，因此CHM通过不断扩容底层table让节点直接插入在桶位上，显然比CSM的插入逻辑要快很多。</p>
<p>3、在高并发删除方面，CHM的性能比CSM快5到6倍左右。</p>
<p>考察CSM并发删除，虽然它是基于CAS+marker的无锁方式去删除节点，但CSM删除一个节点前，首先需要去多层索引去检索该key的前驱节点，然后再删除该节点以及它对应的索引节点（调整所在层的前后指向），还得判断是否需要“降层处理”，而CHM方面，由于key是数值型，百万的key分布在桶位上相对均匀，因此对于底层table的桶位节点删除操作时非常快速的，少量桶位会涉及到冲突链表删除、红黑树转为链表或者红黑树删除平衡调整等但这些情况不多，因此整体上CHM的删除效率高很多。</p>
<p>4、查找效率方面对比</p>
<p>不用对比也知道，CHM因为是hashMap的$o(1)$查找效率，显然比CSM的$log(n)$要快，如下面数据对比：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">csmGet：线程数为64,计算1000000次,总用时3949ms</span><br><span class="line">chmGet：线程数为64,计算1000000次,总用时572ms</span><br><span class="line"></span><br><span class="line">csmGet：线程数为128,计算1000000次,总用时8276ms</span><br><span class="line">chmGet：线程数为128,计算1000000次,总用时1150ms</span><br></pre></td></tr></table></figure>
<h5 id="4、CSM初始化时在new阶段还是在put阶段完成？"><a href="#4、CSM初始化时在new阶段还是在put阶段完成？" class="headerlink" title="4、CSM初始化时在new阶段还是在put阶段完成？"></a>4、CSM初始化时在new阶段还是在put阶段完成？</h5><p>CSM是在new阶段完成初始化：<code>initialize()</code>，而CHM是在第一次put才会初始化：<code>tab = initTable()</code></p>
<p>首先在new阶段已经完成初始化，CSM初始化逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    keySet = <span class="keyword">null</span>;</span><br><span class="line">    entrySet = <span class="keyword">null</span>;</span><br><span class="line">    values = <span class="keyword">null</span>;</span><br><span class="line">    descendingMap = <span class="keyword">null</span>;</span><br><span class="line">    head = <span class="keyword">new</span> HeadIndex&lt;K,V&gt;(<span class="keyword">new</span> Node&lt;K,V&gt;(<span class="keyword">null</span>, BASE_HEADER, <span class="keyword">null</span>),</span><br><span class="line">                              <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到这个new HeadIndex 是位于level=1的索引层，而这个<code>new Node&lt;K,V&gt;(null, BASE_HEADER, null)</code> 是位于最底层数据层链表，对应的图示如下：<br><img src="https://img-blog.csdnimg.cn/8e3faf34ba424df6aff13fa578779005.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<h5 id="5、CSM最高有多少层索引"><a href="#5、CSM最高有多少层索引" class="headerlink" title="5、CSM最高有多少层索引"></a>5、CSM最高有多少层索引</h5><p>根据size方法，CSM的数据层链表最多可以链接个节点<code>Integer.MAX_VALUE</code>个节点，也即$2^{31}-1$</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt; n = findFirst(); n != <span class="keyword">null</span>; n = n.next) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n.getValidValue() != <span class="keyword">null</span>)</span><br><span class="line">            ++count;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (count &gt;= Integer.MAX_VALUE) ? Integer.MAX_VALUE : (<span class="keyword">int</span>) count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>根据前面的上面1提到的空间复杂度计算，p=0.5，也即新插入的数据节点上升为索引节点的概率，那么最大索引层数为$k= log_2(2^{31}-1)$，计算结果为k=31，注意这个31层是指最大索引层数量（第31层有2个索引节点），不包括最底层的数据链表这一层！</p>
<p>那么CSM的节点数量和底层数组最大是多少呢？</p>
<p>节点数量最大值也是$2^{31}-1$</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> n = sumCount();</span><br><span class="line">    <span class="keyword">return</span> ((n &lt; <span class="number">0L</span>) ? <span class="number">0</span> :</span><br><span class="line">            (n &gt; (<span class="keyword">long</span>)Integer.MAX_VALUE) ? Integer.MAX_VALUE :</span><br><span class="line">            (<span class="keyword">int</span>)n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>底层数组最大为$2^{30}$</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The largest possible table capacity.  This value must be</span></span><br><span class="line"><span class="comment">   * exactly 1&lt;&lt;30 to stay within Java array allocation and indexing</span></span><br><span class="line"><span class="comment">   * bounds for power of two table sizes, and is further required</span></span><br><span class="line"><span class="comment">   * because the top two bits of 32bit hash fields are used for</span></span><br><span class="line"><span class="comment">   * control purposes.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// top two bits of 32bit hash fields are used for control purposes ，第31和32位被用于相关“控制”，因此底层table的桶位数量最大值为2^30次方</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>gevent与协程</title>
    <url>/2019/12/28/gevent%E4%B8%8E%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<h4 id="1、-yield-实现协程"><a href="#1、-yield-实现协程" class="headerlink" title="1、 yield 实现协程"></a>1、 yield 实现协程</h4><h5 id="1-1-yield-同步执行"><a href="#1-1-yield-同步执行" class="headerlink" title="1.1 yield 同步执行"></a>1.1 yield 同步执行</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>():</span></span><br><span class="line">    send_msg=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer</span></span><br><span class="line">        output=<span class="keyword">yield</span> send_msg</span><br><span class="line">        print(<span class="string">&#x27;[consumer] consuming &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(output))</span><br><span class="line">        send_msg=<span class="string">&#x27;ok&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">consumer_obj,num</span>):</span></span><br><span class="line">    <span class="comment"># 1、启动consumer()生成器</span></span><br><span class="line">    <span class="built_in">next</span>(consumer_obj)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,num+<span class="number">1</span>):</span><br><span class="line">        print(<span class="string">&#x27;[producer] producing &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、通过send()切换到consumer()执行</span></span><br><span class="line">        receive_msg=consumer_obj.send(i)</span><br><span class="line">        print(<span class="string">&#x27;[producer] received a message &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(receive_msg))</span><br><span class="line">    consumer_obj.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    c=consumer()</span><br><span class="line">    producer(c,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[producer] producing <span class="number">1</span></span><br><span class="line">[consumer] consuming <span class="number">1</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">2</span></span><br><span class="line">[consumer] consuming <span class="number">2</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">3</span></span><br><span class="line">[consumer] consuming <span class="number">3</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">4</span></span><br><span class="line">[consumer] consuming <span class="number">4</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">5</span></span><br><span class="line">[consumer] consuming <span class="number">5</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>整个过程无锁，由一个线程执行，producer和consumer协作完成任务，但是以上无法实现并发，生产1个，消费1个，也即1个生产者对应1个消费者</p>
<h5 id="1-2-启动多个yield模拟consumer并发"><a href="#1-2-启动多个yield模拟consumer并发" class="headerlink" title="1.2 启动多个yield模拟consumer并发"></a>1.2 启动多个yield模拟consumer并发</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>(<span class="params">consumer_index</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;consumer-&#123;&#125; started at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(consumer_index,get_time()))</span><br><span class="line">    send_msg=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 消费者保持监听producer的发来的信息</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer</span></span><br><span class="line">        output=<span class="keyword">yield</span> send_msg</span><br><span class="line">        print(<span class="string">&#x27;[consumer-&#123;&#125;] consuming &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(consumer_index,output,get_time()))</span><br><span class="line">        time.sleep(<span class="number">1</span>) <span class="comment"># 模拟IO耗时操作</span></span><br><span class="line">        send_msg=<span class="string">&#x27;ack&#x27;</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">consumer_obj,consumer_num,count</span>):</span></span><br><span class="line">    <span class="comment"># 1、启动n个consumer()生成器，相当于用协程方式模拟并发</span></span><br><span class="line">    consumers=[consumer_obj(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(consumer_num) ]</span><br><span class="line">    <span class="keyword">for</span> each_cons <span class="keyword">in</span> consumers:</span><br><span class="line">        <span class="built_in">next</span>(each_cons)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count):</span><br><span class="line">        print(<span class="string">&#x27;[producer] producing &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i,get_time()))</span><br><span class="line">        <span class="comment"># 2、对每个consumer_obj使用send()切换到consumer()执行</span></span><br><span class="line">        <span class="keyword">for</span> index,each_cons <span class="keyword">in</span> <span class="built_in">enumerate</span>(consumers):</span><br><span class="line">            receive_msg=each_cons.send(i)</span><br><span class="line">            print(<span class="string">&#x27;[producer] received &#123;&#125; from consumer-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(receive_msg,index,get_time()))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">for</span> each_cons <span class="keyword">in</span> consumers:</span><br><span class="line">        each_cons.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    producer(consumer,<span class="number">5</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>1个producer，5个consumer</p>
<p>输出<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">consumer-0 started at 21:12:45</span><br><span class="line">consumer-1 started at 21:12:45</span><br><span class="line">consumer-2 started at 21:12:45</span><br><span class="line">consumer-3 started at 21:12:45</span><br><span class="line">consumer-4 started at 21:12:45</span><br><span class="line">[producer] producing 0 at 21:12:45</span><br><span class="line">[consumer-0] consuming 0 at 21:12:45</span><br><span class="line">[producer] received ack from consumer-0</span><br><span class="line">[consumer-1] consuming 0 at 21:12:46</span><br><span class="line">[producer] received ack from consumer-1</span><br><span class="line">[consumer-2] consuming 0 at 21:12:47</span><br><span class="line">[producer] received ack from consumer-2</span><br><span class="line">[consumer-3] consuming 0 at 21:12:48</span><br><span class="line">[producer] received ack from consumer-3</span><br><span class="line">[consumer-4] consuming 0 at 21:12:49</span><br><span class="line">[producer] received ack from consumer-4</span><br><span class="line">[producer] producing 1 at 21:12:50</span><br><span class="line">[consumer-0] consuming 1 at 21:12:50</span><br><span class="line">[producer] received ack from consumer-0</span><br><span class="line">[consumer-1] consuming 1 at 21:12:51</span><br><span class="line">[producer] received ack from consumer-1</span><br><span class="line">[consumer-2] consuming 1 at 21:12:52</span><br><span class="line">[producer] received ack from consumer-2</span><br><span class="line">[consumer-3] consuming 1 at 21:12:53</span><br><span class="line">[producer] received ack from consumer-3</span><br><span class="line">[consumer-4] consuming 1 at 21:12:54</span><br><span class="line">[producer] received ack from consumer-4</span><br></pre></td></tr></table></figure><br>以上运行过程确实是协程运行，但yield无法自动切换协程，上面的运行过程打印出的实际可以发现代码同步执行：<br>5个consumer同时启动，当producer生产1个数据，consumer-0消费数据，而consumer内部有IO耗时操作（time.sleep(1)模拟IO），此时代码逻辑没有把线程当前控制权从consumer-0自动切换到consumer-1，consumer-1等待前面1秒后，才能接着干活。</p>
<h4 id="2-、greenlet实现的协程"><a href="#2-、greenlet实现的协程" class="headerlink" title="2 、greenlet实现的协程"></a>2 、greenlet实现的协程</h4><h5 id="2-1-简单gevent协程例子"><a href="#2-1-简单gevent协程例子" class="headerlink" title="2.1 简单gevent协程例子"></a>2.1 简单gevent协程例子</h5><p>&#8195;&#8195;greenlet是一个用C实现的协程模块，相比于上面使用python的yield实现协程，greenlet可以无需将函数声明为generator的前提下，用手动方式在任意函数之间切换。但在实际使用，往往不会直接使用greenlet，因为它遇到有IO地方是不会自动切换，而Gevent库可以实现这个需求，gevent是对greenlet的封装，实现自动切换，大体的设计逻辑如下：<br>&#8195;&#8195;当一个greenlet（你可以认为这个greenlet是一个协程对象，类比于线程对象thread）遇到IO操作时，比如访问读取文件或者网络socket连接，它会自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换原来位置继续执行。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent,datetime</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f1 started at&#x27;</span>,get_time())</span><br><span class="line">    <span class="comment"># gevent模拟IO耗时操作，并且gevent会在此保留现场后自动切换到其它函数</span></span><br><span class="line">    gevent.sleep(<span class="number">4</span>) </span><br><span class="line">    print(<span class="string">&#x27;f1 done at&#x27;</span>,get_time())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f2 started at&#x27;</span>,get_time())</span><br><span class="line">    gevent.sleep(<span class="number">2</span>) </span><br><span class="line">    print(<span class="string">&#x27;f2 done at&#x27;</span>,get_time())</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f3</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f3 started at&#x27;</span>,get_time())</span><br><span class="line">    gevent.sleep(<span class="number">3</span>) </span><br><span class="line">    print(<span class="string">&#x27;f3 done at&#x27;</span>,get_time())</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gevent.joinall([gevent.spawn(f1),gevent.spawn(f2),gevent.spawn(f3)])</span><br></pre></td></tr></table></figure></p>
<p>打印结果</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">f1 started at  09:21:17.066251</span><br><span class="line">f2 started at  09:21:17.066355</span><br><span class="line">f3 started at  09:21:17.066388</span><br><span class="line">f2 done at  09:21:19.067741</span><br><span class="line">f3 done at  09:21:20.067747</span><br><span class="line">f1 done at  09:21:21.067812</span><br></pre></td></tr></table></figure>
<p>可以看到，gevent在同一时刻运行3个函数，并且，f2先完成，接着f3完成，最后IO耗时最长的f1完成，三个函数共同完成耗时为4秒，说明三个函数并发执行了。如果是同步运行，整个过程耗时为4+2+3=9秒耗时，协程优势凸显。</p>
<h5 id="2-2-gevent-高并发测试"><a href="#2-2-gevent-高并发测试" class="headerlink" title="2.2 gevent 高并发测试"></a>2.2 gevent 高并发测试</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>(<span class="params">task_index</span>):</span></span><br><span class="line">    gevent.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at &#123;&#125; &#x27;</span>.<span class="built_in">format</span>(task_index,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">syn</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        task(i)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    syn(<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>同步情况下，耗时4秒</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-0 done at  09:41:14.075001 </span><br><span class="line">task-1 done at  09:41:15.076049 </span><br><span class="line">task-2 done at  09:41:16.077101 </span><br><span class="line">task-3 done at  09:41:17.078055 </span><br></pre></td></tr></table></figure>
<p>gevent实现的协程异步<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">    coroutines=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    gevent.joinall(coroutines)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    asyn(<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p>
<p>原本需要4秒的执行流，现在只需1秒完成所有任务。<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-0 done at  09:44:30.535495 </span><br><span class="line">task-1 done at  09:44:30.535749 </span><br><span class="line">task-2 done at  09:44:30.535801 </span><br><span class="line">task-3 done at  09:44:30.535833 </span><br></pre></td></tr></table></figure></p>
<p>尝试启动10万个任务，用line_profiler 查看函数中耗时操作（line_profiler 目前不兼容3.7，最好用pyenv 切换到3.6进行测试）。只需要在asyn函数上加@profile装饰器即可</p>
<p>创建asyn.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent,datetime,time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>(<span class="params">task_index</span>):</span></span><br><span class="line">    gevent.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(&#x27;task-&#123;&#125; done at &#123;&#125; &#x27;.format(task_index,datetime.datetime.now()))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@profile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">    threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    gevent.joinall(threads)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start=time.time()</span><br><span class="line">    asyn(<span class="number">100000</span>)</span><br><span class="line">    cost=time.time()-start</span><br><span class="line">    print(cost)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(spk) [root@nn spv]# kernprof -l -v asyn.py </span><br><span class="line">5.805598974227905</span><br><span class="line">Wrote profile results to asyn.py.lprof</span><br><span class="line">Timer unit: 1e-06 s</span><br><span class="line"></span><br><span class="line">Total time: 5.73735 s</span><br><span class="line">File: asyn.py</span><br><span class="line">Function: asyn at line 6</span><br><span class="line"></span><br><span class="line">Line #      Hits         Time  Per Hit   % Time  Line Contents</span><br><span class="line">==============================================================</span><br><span class="line">     6                                           @profile</span><br><span class="line">     7                                           def asyn(n):</span><br><span class="line">     8         1    1204525.0 1204525.0     21.0      threads=[gevent.spawn(task,i) for i in range(n)]</span><br><span class="line">     9         1    4532823.0 4532823.0     79.0      gevent.joinall(threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>非常清晰看到，理论上：10万个任务使用协程实现并发运行，总耗时1秒，但实际上，因为需要创建大量greenlet对象，列表创建10万个项耗时1.2秒，gevent joinall 10万个greenlet对象耗时4.5秒，所以整个程序完成总耗时实际为5.7秒左右。</p>
<p>使用memory_profiler库查看asyn.py内存使用情况，使用也简单与line_profiler相似，使用@profile装饰器来标识需要追踪的函数即可。使用协程，10万个对象消耗300多M，鉴于其并发效率高，而且所有的执行都只在一个线程实现 了，因此内存消耗可接受。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(spk) [root@nn spv]<span class="comment"># python -m memory_profiler asyn.py</span></span><br><span class="line">Filename: asyn.py</span><br><span class="line"></span><br><span class="line">Line <span class="comment">#    Mem usage    Increment   Line Contents</span></span><br><span class="line">================================================</span><br><span class="line">     <span class="number">6</span>   <span class="number">36.137</span> MiB   <span class="number">36.137</span> MiB   @profile</span><br><span class="line">     <span class="number">7</span>                             <span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">     <span class="number">8</span>  <span class="number">229.531</span> MiB    <span class="number">0.773</span> MiB       threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">     <span class="number">9</span>  <span class="number">366.270</span> MiB  <span class="number">136.738</span> MiB       gevent.joinall(threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：内存的单位MiB，表示的mebibyte，</p>
<p>MB/s的意思是每秒中传输10^6 byte的数据，以10为底数的指数</p>
<p>MiB/s的意思是每秒中传输2^20 byte的数据，以2为底数的指数</p>
<p>1 MiB =0.9765625 MB</p>
<p>创建100万个task，再看看kernprof -l -v asyn.py ，内存方面使用top可以直观看到asyn.py 占用了2G*0.816=1632 MiB</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">KiB Swap:  2097148 total,  1039272 free,  1057876 used.    14348 avail Mem </span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                </span><br><span class="line">    30 root      20   0       0      0      0 S 45.1  0.0   0:47.13 kswapd0                                </span><br><span class="line"> 29380 root      20   0 2319772   1.4g     56 R 42.8 81.6   1:04.53 kernprof</span><br></pre></td></tr></table></figure>
<p>将协程并发数设为100万，总共耗时为534秒，时间略长，使用gevent在单台服务器上，并发数不要设太离谱，1000个并发足以应付普通项目的需求，例如爬虫，例如做服务端接收客户端发来的socket流数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(spk) [root@nn spv]<span class="comment"># kernprof -l -v asyn.py           </span></span><br><span class="line"><span class="number">534.7752296924591</span></span><br><span class="line">Wrote profile results to asyn.py.lprof</span><br><span class="line">Timer unit: <span class="number">1e-06</span> s</span><br><span class="line"></span><br><span class="line">Total time: <span class="number">532.165</span> s</span><br><span class="line">File: asyn.py</span><br><span class="line">Function: asyn at line <span class="number">6</span></span><br><span class="line"></span><br><span class="line">Line <span class="comment">#      Hits         Time  Per Hit   % Time  Line Contents</span></span><br><span class="line">==============================================================</span><br><span class="line">     <span class="number">6</span>                                           @profile</span><br><span class="line">     <span class="number">7</span>                                           <span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">     <span class="number">8</span>         <span class="number">1</span>   <span class="number">82927089.0</span> <span class="number">82927089.0</span>     <span class="number">15.6</span>      threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">     <span class="number">9</span>         <span class="number">1</span>  <span class="number">449238172.0</span> <span class="number">449238172.0</span>     <span class="number">84.4</span>      gevent.joinall(threads)</span><br></pre></td></tr></table></figure>
<h5 id="2-3-理解gevent的monkey-patch-all"><a href="#2-3-理解gevent的monkey-patch-all" class="headerlink" title="2.3  理解gevent的monkey.patch_all()"></a>2.3  理解gevent的monkey.patch_all()</h5><p>&#8195;&#8195;在接下有关gevent的实际项目中，py程序都会引用monkey.patch_all()这个方法，它的作用是用非阻塞模块替换python自带的阻塞模块，这就是所谓”猴子补丁”，原理是运行时用非阻塞的对象属性替换对应阻塞对象的属性，或者用自己实现的同名非阻塞模块，替换对应的阻塞模块。<br>&#8195;&#8195;注意：这里说的模块就是“有完整功能的一个.py文件“或者”由多个py文件组成的一个完整功能的模块“<br>&#8195;&#8195;例如下面要实现这么一个需求：server.py运行时，将thread.py模块的synfoo函数替换为自定义的mythread.py模块里面asynfoo函数</p>
<p>thread.py 模块<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;builtin method synfoo of thread.py&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>自定义的mythread.py模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;builtin method asynfoo of mythread.py&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>server.py程序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> thread</span><br><span class="line"><span class="comment"># 在本程序的modules字典里面删除原thread模块</span></span><br><span class="line"><span class="keyword">del</span> sys.modules[<span class="string">&#x27;thread&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用异步的模块替换当前thread模块</span></span><br><span class="line">sys.modules[<span class="string">&#x27;thread&#x27;</span>] = <span class="built_in">__import__</span>(<span class="string">&#x27;mythread&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载thread</span></span><br><span class="line"><span class="keyword">import</span> thread</span><br><span class="line">thread.foo() <span class="comment">#这里的thread已经是mythread模块</span></span><br><span class="line">print(thread)</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line"># server.py运行时已经成功将内建同步模块替换为异步的模块</span><br><span class="line">builtin method asynfoo of mythread.py</span><br><span class="line"># thread的指向自定义模块mythread</span><br><span class="line">&lt;module &#x27;mythread&#x27; from &#x27;/opt/spv/mythread.py&#x27;&gt;</span><br></pre></td></tr></table></figure></p>
<p>以上就是monkey.patch_all()大致逻辑，gevent可以把python内建的多个模块在程序运行时替换为它写的异步模块，默认是把内建的socket、thread、queue等模块替换为非阻塞同名模块</p>
<p>（site-packages/gevent/monkey.py）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_all</span>(<span class="params">socket=<span class="literal">True</span>, dns=<span class="literal">True</span>, time=<span class="literal">True</span>, select=<span class="literal">True</span>, thread=<span class="literal">True</span>, os=<span class="literal">True</span>, ssl=<span class="literal">True</span>, httplib=<span class="literal">False</span></span></span></span><br><span class="line"><span class="function"><span class="params">              subprocess=<span class="literal">True</span>, sys=<span class="literal">False</span>, aggressive=<span class="literal">True</span>, Event=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              builtins=<span class="literal">True</span>, signal=<span class="literal">True</span></span>):</span></span><br><span class="line">    _warnings, first_time = _check_repatching(**<span class="built_in">locals</span>())</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _warnings <span class="keyword">and</span> <span class="keyword">not</span> first_time:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> os:</span><br><span class="line">        patch_os()</span><br><span class="line">    <span class="keyword">if</span> time:</span><br><span class="line">        patch_time()</span><br><span class="line">    <span class="keyword">if</span> thread:</span><br><span class="line">        patch_thread(Event=Event, _warnings=_warnings)</span><br><span class="line">    <span class="comment"># sys must be patched after thread. in other cases threading._shutdown will be</span></span><br><span class="line">    <span class="comment"># initiated to _MainThread with real thread ident</span></span><br><span class="line">    <span class="keyword">if</span> sys:</span><br><span class="line">        patch_sys()</span><br><span class="line">    <span class="keyword">if</span> socket:</span><br><span class="line">        patch_socket(dns=dns, aggressive=aggressive)</span><br><span class="line">    <span class="keyword">if</span> select:</span><br><span class="line">        patch_select(aggressive=aggressive)</span><br><span class="line">    <span class="keyword">if</span> ssl:</span><br><span class="line">        patch_ssl()</span><br><span class="line">    <span class="keyword">if</span> httplib:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;gevent.httplib is no longer provided, httplib must be False&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> subprocess:</span><br><span class="line">        patch_subprocess()</span><br><span class="line">    <span class="keyword">if</span> builtins:</span><br><span class="line">        patch_builtins()</span><br></pre></td></tr></table></figure>
<p>如果不想gevent对某个内建模块覆盖为非阻塞，可以将该模块设为False：monkey.patch_all(thread=False)<br>或者在monkey.patch_all(thread=False) 语句后面追加import threading<br>目的是内建同步模块再次覆盖前面的gevent异步模块，例如server.py文件<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">monkey.patch_all(thread=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 或者 import threading</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>():</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">syn</span>():</span></span><br><span class="line">	t=threading.thread(target=task,args=())</span><br><span class="line">	t.start()</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span></span><br><span class="line">	gevent.joinall...</span><br></pre></td></tr></table></figure></p>
<p>由此可知，gevent的patch_all针对模块的覆盖是有顺序的，因为当使用gevent时，import的模块顺序很重要，内建模块在patch_all前面或者在patch_all后面，对应是同步还是异步模块导入。</p>
<h6 id="2-2-1-locals-方法"><a href="#2-2-1-locals-方法" class="headerlink" title="2.2.1 locals()方法"></a>2.2.1 locals()方法</h6><p>&#8195;&#8195;locals()返回一个字典，它可以获取当前模块所有的局部变量以及当前模块引入的其他模块，例如myFoo.py模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">local_module_dict=<span class="built_in">locals</span>()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>():</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line">print(local_module_dict[<span class="string">&#x27;threading&#x27;</span>])</span><br><span class="line">print(local_module_dict[<span class="string">&#x27;socket&#x27;</span>])</span><br><span class="line">输出</span><br><span class="line"><span class="comment"># &lt;module &#x27;threading&#x27; from &#x27;/root/.pyenv/versions/3.7.5/lib/python3.7/threading.py&#x27;&gt;</span></span><br><span class="line"><span class="comment"># &lt;module &#x27;socket&#x27; from &#x27;/root/.pyenv/versions/3.7.5/lib/python3.7/socket.py&#x27;&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>gevent通过locals()获取这些模块后，将需要替换的模块都替换gevent自己实现的非阻塞模块</p>
<h6 id="2-2-2-gevent-替换非阻塞的模块的思路"><a href="#2-2-2-gevent-替换非阻塞的模块的思路" class="headerlink" title="2.2.2 gevent 替换非阻塞的模块的思路"></a>2.2.2 gevent 替换非阻塞的模块的思路</h6><p>&#8195;&#8195;在前面的例子中，使用gevent.sleep()可以让协程自动切换实现异步方式执行，如果使用内建的time.sleep()，则变成同步执行，下面看看gevent如何使用patch_time()方法为sleep打补丁：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_time</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Replace :func:`time.sleep` with :func:`gevent.sleep`.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> gevent.hub <span class="keyword">import</span> sleep</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="comment"># 用gevent.hub.sleep方法替换内建的sleep方法</span></span><br><span class="line">    patch_item(time, <span class="string">&#x27;sleep&#x27;</span>, sleep)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_item</span>(<span class="params">module, attr, newitem</span>):</span></span><br><span class="line">    olditem = <span class="built_in">getattr</span>(module, attr, _NONE)</span><br><span class="line">    <span class="keyword">if</span> olditem <span class="keyword">is</span> <span class="keyword">not</span> _NONE:</span><br><span class="line">        saved.setdefault(module.__name__, &#123;&#125;).setdefault(attr, olditem)</span><br><span class="line">    <span class="built_in">setattr</span>(module, attr, newitem)</span><br></pre></td></tr></table></figure>
<p>实现原理跟2.2提到monke.patch_all()一样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent.hub <span class="keyword">import</span> sleep <span class="comment"># 先导入gevent的sleep</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment"># 再导入内建time模块</span></span><br><span class="line">print(<span class="built_in">getattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>)) <span class="comment"># 获取原内建sleep</span></span><br><span class="line"><span class="comment"># &lt;function time.sleep&gt;</span></span><br><span class="line"><span class="built_in">setattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>,sleep) <span class="comment"># 将gevent的异步sleep方法替换原sleep方法</span></span><br><span class="line">print(<span class="built_in">getattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>)) <span class="comment"># 打印运行是sleep方法看看是内建的还是gevent实现的</span></span><br><span class="line"><span class="comment"># &lt;function gevent.hub.sleep(seconds=0, ref=True)&gt;</span></span><br></pre></td></tr></table></figure>
<p>对于模块的导入，则需要使用<code>getattr自省模式创建一个对象，然后通过__import__引入</code></p>
<p>以gevent替换os为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_os</span>():</span></span><br><span class="line">    patch_module(<span class="string">&#x27;os&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>具体实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用gevent的os替换内建的os模块，这里的name就是&#x27;os&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_module</span>(<span class="params">name, items=<span class="literal">None</span></span>):</span></span><br><span class="line">	<span class="comment"># 通过__import__方法导入用gevent的os</span></span><br><span class="line">    gevent_module = <span class="built_in">getattr</span>(<span class="built_in">__import__</span>(<span class="string">&#x27;gevent.&#x27;</span> + name), name)</span><br><span class="line">    module_name = <span class="built_in">getattr</span>(gevent_module, <span class="string">&#x27;__target__&#x27;</span>, name)</span><br><span class="line">    module = <span class="built_in">__import__</span>(module_name)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    	<span class="comment"># 获取gevent_module里面跟os相关的方法</span></span><br><span class="line">        items = <span class="built_in">getattr</span>(gevent_module, <span class="string">&#x27;__implements__&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> AttributeError(<span class="string">&#x27;%r does not have __implements__&#x27;</span> % gevent_module)</span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> items:</span><br><span class="line">    	<span class="comment"># 用 gevent自己实现的os里面方法替换内建os指定方法</span></span><br><span class="line">        patch_item(module, attr, <span class="built_in">getattr</span>(gevent_module, attr))</span><br><span class="line">    <span class="keyword">return</span> module</span><br></pre></td></tr></table></figure>
<p>相信到了这里，已经可以理解 monkey.patch_all()为何要在gevent的程序头部引入，常见“模板”如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">monkey.patch_all()</span><br></pre></td></tr></table></figure>
<p>很多文章在讨论gevent的协程时，基本都是一句话“这是打补丁，将阻塞模块替换为非阻塞模块”简单带过，对于大部分人来说，这种说明一般会感到疑惑。</p>
<h4 id="3、gevent-examples"><a href="#3、gevent-examples" class="headerlink" title="3、gevent examples"></a>3、gevent examples</h4><p>&#8195;&#8195;本章主要结合一些场景给出gevent用法，参考了gevent官网给出的examples：<a href="http://www.gevent.org/examples/index.html">地址</a></p>
<h5 id="3-1-使用协程高并发爬网页"><a href="#3-1-使用协程高并发爬网页" class="headerlink" title="3.1 使用协程高并发爬网页"></a>3.1 使用协程高并发爬网页</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> random,datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line"></span><br><span class="line"><span class="comment"># patches stdlib (including socket and ssl modules) to cooperate with other greenlets</span></span><br><span class="line"><span class="comment"># 将标准lib打补丁，例如下面的https请求需要用到ssl模块，将该内建的ssl模块替换为gevent的ssl</span></span><br><span class="line">monkey.patch_all()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里给出的https协议来说明gevent可进行SSL的相关任务处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span> (<span class="params">workers=<span class="number">1000</span></span>):</span></span><br><span class="line">    start=time.time()</span><br><span class="line">    url_pool = [</span><br><span class="line">        <span class="string">&#x27;https://www.baidu.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.apple.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.qq.com/&#x27;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    urls=[ random.choice(url_pool) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(workers)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_head</span>(<span class="params">url</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;Starting &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(url,datetime.datetime.now()))</span><br><span class="line">        data = requests.get(url).text <span class="comment"># gevent会在发生IO的位置实现协程自动切换</span></span><br><span class="line">        <span class="comment">#print(&#x27;%s: %s bytes: %r&#x27; % (url, len(data), data[:2]))</span></span><br><span class="line">    jobs = [gevent.spawn(print_head, _url) <span class="keyword">for</span> _url <span class="keyword">in</span> urls]</span><br><span class="line">    gevent.wait(jobs) <span class="comment"># 阻塞主线程，让所有协程得以持续运行</span></span><br><span class="line">    cost=time.time()-start</span><br><span class="line">    print(<span class="string">&#x27;cost:&#x27;</span>,cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>
<p>以上1000个请求，只需运行一个线程，非常轻量且“低功耗”，而多线程方式，则需创建1000个线程，这就是协程的优势。</p>
<h5 id="3-2-gevent实现的socket高并发"><a href="#3-2-gevent实现的socket高并发" class="headerlink" title="3.2 gevent实现的socket高并发"></a>3.2 gevent实现的socket高并发</h5><p><strong>server.py端逻辑</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket,datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecodeErr</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Server</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host=<span class="string">&#x27;0.0.0.0&#x27;</span>,port=<span class="number">8090</span>,conns=<span class="number">100</span></span>):</span></span><br><span class="line">        self._s=socket.socket()</span><br><span class="line">        self._s.bind((host,port))</span><br><span class="line">        self._s.listen(conns)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_request</span>(<span class="params">self,conn</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 接收客户端发送的是比特字节，需要decode为str类型</span></span><br><span class="line">            msg=conn.recv(<span class="number">1024</span>).decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> msg:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            print(<span class="string">&#x27;got the msg:&#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(msg,self.recv_time()))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 发送给client需要byte类型</span></span><br><span class="line">            conn.send(<span class="built_in">bytes</span>(msg,encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">            <span class="keyword">if</span> msg ==<span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">                conn.shutdown(socket.SHUT_RDWR)</span><br><span class="line">                conn.close()</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line"><span class="meta">    @staticmethod            </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recv_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">serve_forever</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                client_conn,client_ip=self._s.accept()</span><br><span class="line">                <span class="comment"># 创建一个新的Greenlet服务新的请求</span></span><br><span class="line">                g=gevent.spawn(self.parse_request,client_conn)</span><br><span class="line">                print(<span class="string">&#x27;new client connected:&#123;&#125; &#123;&#125; serving...&#x27;</span>.<span class="built_in">format</span>(client_ip,g.name))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conns=<span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    server=Server(conns=conns)</span><br><span class="line">    server.serve_forever()                   </span><br></pre></td></tr></table></figure></p>
<p>另外打开2个终端使用 telnet 188.0.0.10 8090</p>
<p>输出：<br>可以看到每个client请求都是由新的greenlet来服务，这个Greenlet就是协程对象<code>&lt;Greenlet at 0x7f7dc87efa70: parse_request(&lt;gevent._socket3.socket object, fd=7, family=2, ty)&gt;</code>，而非多线程对象。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python server.py 100</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21042) Greenlet-0 serving...</span><br><span class="line">got the msg:foo</span><br><span class="line"> at 10:8:3</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21044) Greenlet-1 serving...</span><br><span class="line">got the msg:bar</span><br><span class="line"> at 10:8:37</span><br></pre></td></tr></table></figure>
<p>这里需要注意：<br>在parse_request里面关闭client的连接用conn.shutdown(socket.SHUT_WR)<br>shutdown 方法的 how 参数接受如下参数值： </p>
<ul>
<li>SHUT_RD：关闭 socket 的输入部分，程序还可通过该 socket 输出数据。(tcp半开状态)</li>
<li>SHUT_WR： 关闭该 socket 的输出部分，程序还可通过该 socket 读取数据。（(tcp半开)状态）</li>
<li>SHUT_RDWR：全关闭。该 socket 既不能读取数据，也不能写入数据。</li>
</ul>
<p>conn.close():关闭完整tcp连接通道<br>close方法不是立即释放，如果想立即释放，需在close之前使用shutdown方法<br>server.py 使用gevent实现可接受高并发连接，下面给出gevent版的client，高并发socket请求</p>
<p><strong>client.py端代码*</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,server_ip,port,workers=<span class="number">10</span></span>):</span></span><br><span class="line">        self.server_ip=server_ip</span><br><span class="line">        self.port=port</span><br><span class="line">        self.workers=workers</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @staticmethod            </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recv_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">asyn_sock</span>(<span class="params">self,msg</span>):</span></span><br><span class="line">        client=socket.socket()</span><br><span class="line">        client.connect((self.server_ip,self.port))</span><br><span class="line">        bmsg=<span class="built_in">bytes</span>(msg,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        client.sendall(bmsg)</span><br><span class="line">        recv_data=client.recv(<span class="number">1024</span>).decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;gevent object:&#123;&#125; data:&#123;&#125; at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gevent.getcurrent(),recv_data,self.recv_time()))</span><br><span class="line">        client.close()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        threads=[gevent.spawn(self.asyn_sock,<span class="built_in">str</span>(i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.workers)]</span><br><span class="line">        gevent.joinall(threads)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    workers=<span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    c=Client(server_ip=<span class="string">&#x27;188.0.0.10&#x27;</span>,port=<span class="number">8090</span>,workers=workers)</span><br><span class="line">    c.start()</span><br></pre></td></tr></table></figure></p>
<p>服务器端启动100个连接数，客户端并发10个请求。</p>
<p>服务端打印如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python server.py 100</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21246) Greenlet-0 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21248) Greenlet-1 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21250) Greenlet-2 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21252) Greenlet-3 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21254) Greenlet-4 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21256) Greenlet-5 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21258) Greenlet-6 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21260) Greenlet-7 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21262) Greenlet-8 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21264) Greenlet-9 serving...</span><br><span class="line">got the msg:0 at 10:11:5</span><br><span class="line">got the msg:1 at 10:11:5</span><br><span class="line">got the msg:2 at 10:11:5</span><br><span class="line">got the msg:3 at 10:11:5</span><br><span class="line">got the msg:4 at 10:11:5</span><br><span class="line">got the msg:5 at 10:11:5</span><br><span class="line">got the msg:6 at 10:11:5</span><br><span class="line">got the msg:7 at 10:11:5</span><br><span class="line">got the msg:8 at 10:11:5</span><br><span class="line">got the msg:9 at 10:11:5</span><br></pre></td></tr></table></figure>
<p>客户端打印如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python asyn.py 10</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a85f0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;9&#x27;)&gt; data:9 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a84d0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;8&#x27;)&gt; data:8 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a83b0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;7&#x27;)&gt; data:7 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8290: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;6&#x27;)&gt; data:6 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8170: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;5&#x27;)&gt; data:5 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8050: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;4&#x27;)&gt; data:4 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586ef0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;3&#x27;)&gt; data:3 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586b90: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;2&#x27;)&gt; data:2 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586cb0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;1&#x27;)&gt; data:1 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586a70: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;0&#x27;)&gt; data:0 at:10:11:5</span><br></pre></td></tr></table></figure>
<p>可以看到不管是服务器和客户端，都是由多个greenlet协程对象负责请求或者负责服务。<br>如果server.py端并发数设为10000，client.py并发也设为10000，那么会出现以下情况：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">init__</span><br><span class="line">OSError: [Errno 24] Too many open files</span><br><span class="line">During handling of the above exception, another exception occurred:</span><br></pre></td></tr></table></figure><br>这里因为centos限制用户级别在打开文件描述符的数量，可查看默认值：限制至多打开1024个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# ulimit -n</span><br><span class="line">1024</span><br></pre></td></tr></table></figure>
<p>linux 一般会在以下几个文件对系统资源做限制，例如用户级别（此外还有系统级别）的限制： /etc/security/limits.conf，和/etc/security/limits.d/目录，/etc/security/limits.d/里面配置会覆盖/etc/security/limits.conf的配置：</p>
<blockquote>
<p>系统限制用户的资源有：所创建的内核文件的大小、进程数据块的大小、Shell<br>进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell<br>进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。</p>
</blockquote>
<p>提升并发性能：临时修改：ulimit -n 100000；永久性修改：root权限下，在/etc/security/limits.conf中添加如下两行，*表示所有用户，重启/或者注销重登陆生效</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">* soft nofile 102400</span><br><span class="line">* hard nofile 104800</span><br></pre></td></tr></table></figure>
<p>注意hard limit必须大于soft limit</p>
<p>这里将linux设为ulimit -n 100000，10万个描述符！ python server.py 20000个并发，python client.py 10000并发请求打过去，3秒内完成，而且这是因为程序加入print打印语句影响性能，去掉所有print语句，2万个客户端并发不到2秒内完成，gevent或者说底层Greenlet的并发性能非常强。</p>
<h5 id="3-3-gevent数据库操作"><a href="#3-3-gevent数据库操作" class="headerlink" title="3.3 gevent数据库操作"></a>3.3 gevent数据库操作</h5><p>&#8195;&#8195;这里将给出协程方式、多线程方式连接mysql数据库某实际项目备份表，15个字段，2万多条数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">        start=time.time()</span><br><span class="line">        func(*args,**kwargs)</span><br><span class="line">        cost=time.time()-start</span><br><span class="line">        print(<span class="string">&#x27;&#123;&#125; cost:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func.__name__,cost))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_db</span>(<span class="params">index</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;负责读数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(&#x27;start:&#x27;,index)</span></span><br><span class="line">    db = pymysql.connect(host = <span class="string">&#x27;****&#x27;</span>, user = <span class="string">&#x27;****&#x27;</span>, passwd = <span class="string">&#x27;****&#x27;</span>, db= <span class="string">&#x27;****&#x27;</span>)</span><br><span class="line">    cursor = db.cursor()</span><br><span class="line">    sql=<span class="string">&#x27;select count(1) from `article &#x27;</span></span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    nums = cursor.fetchall()</span><br><span class="line">    <span class="comment">#print(&#x27;total itmes:&#x27;,nums)</span></span><br><span class="line">    cursor.close()</span><br><span class="line">    db.close()</span><br><span class="line">    <span class="comment">#print(&#x27;end:&#x27;,index)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gevent_read</span>(<span class="params">workers</span>):</span></span><br><span class="line">    <span class="comment"># 创建多个greenlets协程对象</span></span><br><span class="line">    greenlets = [gevent.spawn(read_db,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(workers)]</span><br><span class="line">    gevent.joinall(greenlets)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	<span class="comment"># 5次测试。这里每次间隔1秒，让客户端连接mysql的connections及时关闭，避免释放不及时导致超过数据库端的允许连接数</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        gevent_read(<span class="number">100</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure></p>
<p>从代码逻辑可以看出，gevent使用协程非常简单，在头部引入相关模块，再使用gevent.spawn创建多个greenlets对象，最后joinall。以下是测试结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python asyn_mysql.py </span><br><span class="line">gevent_read cost:2.702486276626587</span><br><span class="line">gevent_read cost:2.120276689529419</span><br><span class="line">gevent_read cost:2.1487138271331787</span><br><span class="line">gevent_read cost:2.61714243888855</span><br><span class="line">gevent_read cost:2.1180896759033203</span><br></pre></td></tr></table></figure>
<h5 id="3-4-gevent-多线程"><a href="#3-4-gevent-多线程" class="headerlink" title="3.4 gevent 多线程"></a>3.4 gevent 多线程</h5><p>gevent也有自己线程池，使用的python的thread，两者没区别，如果用了多线程，那么gevent其实就没多大意义了，因为不是协程模式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent.threadpool <span class="keyword">import</span> ThreadPool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">        start=time.time()</span><br><span class="line">        func(*args,**kwargs)</span><br><span class="line">        cost=time.time()-start</span><br><span class="line">        print(<span class="string">&#x27;&#123;&#125; cost:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func.__name__,cost))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpool</span>(<span class="params">workers</span>):</span></span><br><span class="line">    pool = ThreadPool(workers)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        pool.spawn(time.sleep, <span class="number">1</span>)</span><br><span class="line">    gevent.wait()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gpool(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>输出3秒，10个任务，线程池只有4个worker，因此需分三轮工作，因为耗时3秒</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python gpool.py </span><br><span class="line">gpool cost:3.006455183029175</span><br></pre></td></tr></table></figure>
<h5 id="3-5-gevent-其他examples"><a href="#3-5-gevent-其他examples" class="headerlink" title="3.5 gevent 其他examples"></a>3.5 gevent 其他examples</h5><p>这里不再一一列出，可以参考gevent github的<a href="https://github.com/gevent/gevent/tree/master/examples">example目录</a></p>
<p>不过建议看看<strong>geventsendfile.py</strong>和<strong>wsgiserver_ssl.py</strong><br>第一是零拷贝技术的协程，第二个是基于https的协程webserver</p>
<h4 id="4、greenlet-eventlet-gevent的关系"><a href="#4、greenlet-eventlet-gevent的关系" class="headerlink" title="4、greenlet/eventlet/gevent的关系"></a>4、greenlet/eventlet/gevent的关系</h4><p>&#8195;&#8195;Greelent实现了一个比较易用(相比yeild)的协程切换的库。但是greenlet没有自己的调度过程，所以一般不会直接使用。<br>&#8195;&#8195;Eventlet在Greenlet的基础上实现了自己的GreenThread，实际上就是greenlet类的扩展封装，而与Greenlet的不同是，Eventlet实现了自己调度器称为Hub，Hub类似于Tornado的IOLoop，是单实例的。在Hub中有一个event loop，根据不同的事件来切换到对应的GreenThread。同时Eventlet还实现了一系列的补丁来使Python标准库中的socket等等module来支持GreenThread的切换。Eventlet的Hub可以被定制来实现自己调度过程。<br>&#8195;&#8195;Gevent基于libev和Greenlet。不同于Eventlet的用python实现的hub调度，Gevent通过Cython调用libev来实现一个高效的event loop调度循环。同时类似于Eventlet，Gevent也有自己的monkey_patch，在打了补丁后，完全可以使用python线程的方式来无感知的使用协程，减少了开发成本。<br>&#8195;&#8195;这里也顺便给出greenlet/eventlet/gevent和其他可以实现协程模式库的对比表格，该表来自Gruvi作者的项目介绍页。Gruvi是一个轻量且特别的协程库，项目作者因为不太认同常见python协程库的实现方式，而且也不认同不推荐使用monkey patch方式，所有他写了Gruvi，专注green thread：<a href="https://gruvi.readthedocs.io/en/latest/rationale.html">项目地址</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Feature</th>
<th style="text-align:left">Gruvi</th>
<th style="text-align:left">Asyncio</th>
<th style="text-align:left">Gevent</th>
<th style="text-align:left">Eventlet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">IO library</td>
<td style="text-align:left"><a href="https://github.com/joyent/libuv">libuv</a></td>
<td style="text-align:left">stdlib</td>
<td style="text-align:left"><a href="http://libev.schmorp.de/">libev</a></td>
<td style="text-align:left">stdlib / <a href="http://libevent.org/">libevent</a></td>
</tr>
<tr>
<td style="text-align:left">IO abstraction</td>
<td style="text-align:left">Transports / Protocols</td>
<td style="text-align:left">Transports / Protocols</td>
<td style="text-align:left">Green sockets</td>
<td style="text-align:left">Green sockets</td>
</tr>
<tr>
<td style="text-align:left">Threading</td>
<td style="text-align:left"><a href="https://pypi.python.org/pypi/fibers">fibers</a></td>
<td style="text-align:left"><code>yield from</code></td>
<td style="text-align:left"><a href="https://pypi.python.org/pypi/greenlet">greenlet</a></td>
<td style="text-align:left"><a href="https://pypi.python.org/pypi/greenlet">greenlet</a></td>
</tr>
<tr>
<td style="text-align:left">Resolver</td>
<td style="text-align:left">threadpool</td>
<td style="text-align:left">threadpool</td>
<td style="text-align:left">threadpool / <a href="http://c-ares.haxx.se/">c-ares</a></td>
<td style="text-align:left">blocking / <a href="http://www.dnspython.org/">dnspython</a></td>
</tr>
<tr>
<td style="text-align:left">Python: 2.x</td>
<td style="text-align:left">YES (2.7)</td>
<td style="text-align:left">YES (2.6+, via <a href="https://bitbucket.org/enovance/trollius">Trollius</a>)</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
</tr>
<tr>
<td style="text-align:left">Python: 3.x</td>
<td style="text-align:left">YES (3.3+)</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">Python: PyPy</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
</tr>
<tr>
<td style="text-align:left">Platform: Linux</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
</tr>
<tr>
<td style="text-align:left">Platform: Mac OSX</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
</tr>
<tr>
<td style="text-align:left">Platform: Windows</td>
<td style="text-align:left">FAST (IOCP)</td>
<td style="text-align:left">FAST (IOCP)</td>
<td style="text-align:left">SLOW (select)</td>
<td style="text-align:left">SLOW (select)</td>
</tr>
<tr>
<td style="text-align:left">SSL: Posix</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
<td style="text-align:left">FAST</td>
</tr>
<tr>
<td style="text-align:left">SSL: Windows</td>
<td style="text-align:left">FAST (IOCP)</td>
<td style="text-align:left">FAST (IOCP 3.5+)</td>
<td style="text-align:left">SLOW (select)</td>
<td style="text-align:left">SLOW (select)</td>
</tr>
<tr>
<td style="text-align:left">SSL: Contexts</td>
<td style="text-align:left">YES (also Py2.7)</td>
<td style="text-align:left">YES (also Py2.6+)</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
</tr>
<tr>
<td style="text-align:left">HTTP</td>
<td style="text-align:left">FAST (via <a href="https://github.com/joyent/http-parser">http-parser</a>)</td>
<td style="text-align:left">NO (external)</td>
<td style="text-align:left">SLOW (stdlib)</td>
<td style="text-align:left">SLOW (stdlib)</td>
</tr>
<tr>
<td style="text-align:left">Monkey Patching</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">NO</td>
<td style="text-align:left">YES</td>
<td style="text-align:left">YES</td>
</tr>
</tbody>
</table>
</div>
<p>本博客也会为Gruvi写一篇文章，主要是欣赏作者阐述的设计理念。从对比表格来看，Asyncio各方面都出色，而且完全由Python标准库实现，后面也有关于Asyncio深入讨论的文章。</p>
<h4 id="5、gevent-不适用的场合"><a href="#5、gevent-不适用的场合" class="headerlink" title="5、gevent 不适用的场合"></a>5、gevent 不适用的场合</h4><p>这里参考Stack Overflow的文章<a href="https://stackoverflow.com/questions/54254252/asyncio-vs-gevent">《Asyncio vs. Gevent 》</a></p>
<p>it wasn’t perfect:</p>
<ul>
<li>Back then, it didn’t work well on Windows (and it still has some limitations today). gevent在Windows 表现不佳</li>
<li>It couldn’t monkey-patch C extensions, so we coudn’t use MySQLdb,  for example. Luckily, there were many pure Python alternatives, like  PyMySQL. 由于gevent的 monkey-patch替换原理，参考上面2.2，它只支持对存python库打补丁，对于C语言实现的python库，例如MySQLdb，则不支持。</li>
</ul>
<p>这里篇文章大致意思是建议用asyncio，因为它是标准库，有着非常详细的文档以及稳定的python官方维护。gevent也可以用，但是自己要清楚项目演进的后续维护情况。</p>
<p>Supported Platforms</p>
<blockquote>
<p><a href="https://pypi.org/project/gevent/whatsnew_1_3.html">gevent 1.3</a> runs on Python 2.7 and Python 3. Releases 3.4, 3.5 and<br>3.6 of Python 3 are supported. (Users of older versions of Python 2<br>need to install gevent 1.0.x (2.5), 1.1.x (2.6) or 1.2.x (&lt;=2.7.8);<br>gevent 1.2 can be installed on Python 3.3.) gevent requires the<br><a href="https://greenlet.readthedocs.io">greenlet</a> library and will install<br>the <a href="https://cffi.readthedocs.io">cffi</a> library by default on Windows.</p>
</blockquote>
<h4 id="6、协程原理解析"><a href="#6、协程原理解析" class="headerlink" title="6、协程原理解析"></a>6、协程原理解析</h4><p>&#8195;&#8195;前面具体的gevent代码示例，对深入理解协程有一定帮助，因为在本文中，把原理性的讨论放在最后一节显得更为合理。谈到协程又不得不把进程、线程以及堆、栈相关概念抛出，以便从全局把握协程、线程和进程。</p>
<h5 id="6-1-进程与内存分配"><a href="#6-1-进程与内存分配" class="headerlink" title="6.1 进程与内存分配"></a>6.1 进程与内存分配</h5><p>&#8195;&#8195;进程是系统资源分配的最小单位，Linux系统由一个个在后台运行process提供所有功能的组成，你可以用<code>ll /proc |wc -l</code>或者<code>ps aus|less</code>查看系统运行的进程。进程自己是需要占用系统资源的，例如cpu、内存、网络，这里我们关注其<br>程序的内存分配。<br>这里以一个由C /C++编译的程序占用的内存分为以下几个部分为例说明，这段内容参考文章<a href="https://blog.csdn.net/ZXR_LJ/article/details/79440577">《堆栈的区别》</a>：</p>
<ul>
<li><p>栈区（stack）： 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p>
</li>
<li><p>堆区（heap）：一般由程序员（在代码里面自行申请内存）分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。</p>
</li>
<li><p>全局区（静态区）（static）：全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放</p>
</li>
<li><p>文字常量区 ：常量字符串就是放在这里的。 程序结束后由系统释放</p>
</li>
<li><p>程序代码区：存放函数体的二进制代码</p>
</li>
</ul>
<p>&#8195;&#8195;可以想象，系统创建一个新的进程都进行以上的复杂内存分配工作，而进程结束后系统还得进行大量内存回收清理工作，如果系统有成千上万个进程创建、切换以及销毁，可想而知，非常消耗资源，”疲于奔命，顾不上其他重要请求“（这就是Apache服务器的并发性的劣势，看看Nginx有多强大）。所以多进程做并发业务，显然不是一个理想方案。</p>
<h5 id="6-2-线程"><a href="#6-2-线程" class="headerlink" title="6.2 线程"></a>6.2 线程</h5><p>&#8195;&#8195;关于进程的描述，其实很多文章可以找到相关讨论，这里以线程和进程的区别作为说明：</p>
<ul>
<li><p>本质区别：进程是操作系统资源分配（分配CPU、内存、网络）的基本单位，而线程是任务（进行某种代码逻辑）调度和执行的基本单位</p>
</li>
<li><p>资源占用区别：每个进程都有独立的代码和程序上下文环境，进程之间的切换消耗较大系统资源（投入大，代价较高）；这里顺便说明为何代价高？因为进程之间切换涉及到用户空间（用户态）和内核空间（内核态）的切换。一个进程里面可以有多个线程运行，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的消耗的是当前进程占有的资源，代价较小，但也不低。</p>
</li>
<li><p>内存分配方面：系统在运行的时候会为每个进程分配不同的内存空间；而对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源。</p>
</li>
<li><p>所处环境：在操作系统中能同时运行多个进程（程序）；而在同一个进程中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行）</p>
</li>
</ul>
<h5 id="6-3-协程"><a href="#6-3-协程" class="headerlink" title="6.3 协程"></a>6.3 协程</h5><p>&#8195;&#8195;终于谈到本章的主角：协程，英文coroutine，它比线程更加轻量，你可以这样认为：一个进程可以拥有多个线程一样，而一个线程也可以拥有多个协程。<br>==<strong>协程与进程的区别</strong>==：</p>
<ul>
<li>执行流的调度者不同，进程是内核调度，而协程是在用户态调度，也就是说进程的上下文是在内核态保存恢复的，而协程是在用户态保存恢复的，很显然用户态的代价更低</li>
<li>进程会被强占，而协程不会，也就是说协程如果不主动让出CPU，那么其他的协程，就没有执行的机会。</li>
<li>对内存的占用不同，实际上协程可以只需要4K的栈就足够了，而进程占用的内存要大的多<ul>
<li>从操作系统的角度讲，多协程的程序是单进程，单协程</li>
</ul>
</li>
</ul>
<p>==<strong>协程与线程的区别</strong>==<br>&#8195;&#8195;一个线程里面可以包含多个协程，线程之间需要上下文切换成本相对协程来说是比较高的，尤其在开启线程较多时，线程的切换更多的是靠操作系统来控制，而协程之间的切换和运行由用户程序代码自行控制或者类似gevent这种自动切换，因此协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行），这将为用户可以设计出非常高性能的并发编程模式。如下图所示一个主线程负责使用gevent自动调度（自动切换运行）2个协程，大致逻辑如下：</p>
<ul>
<li>主线程（MainThread，也是根协程或者当前线程）创建（spawn）两个协程，只有有协程遇到IO event时候就把控制权交给当前线程，直到这个协程的IO event已经完成，主线程将控制权给这个协程。<br><img src="https://img-blog.csdnimg.cn/20191228112520473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h4 id="7、小结"><a href="#7、小结" class="headerlink" title="7、小结"></a>7、小结</h4>&#8195;&#8195;本文开启了Python的异步编程文章讨论篇章，算是比较进阶的内容，因为异步模式可让实际项目确实受益不少，在本博客之后有关异步的内容有：asyncio、文件描述符与IO多路复用。</li>
</ul>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>gevent</tag>
        <tag>协程</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Hadoop HA集群部署HBase HA集群（详细版）</title>
    <url>/2019/10/28/%E5%9F%BA%E4%BA%8EHadoop%20HA%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2HBase%20HA%E9%9B%86%E7%BE%A4%EF%BC%88%E8%AF%A6%E7%BB%86%E7%89%88%EF%BC%89/</url>
    <content><![CDATA[<h4 id="1、前言"><a href="#1、前言" class="headerlink" title="1、前言"></a>1、前言</h4><p>&#8195;&#8195;前面的博客中<a href="https://blog.csdn.net/pysense/article/details/102490212">链接</a>已经给出Hadoop3.1.2和yarn的完整部署（但还不是高可用），此篇博客将给出Hadoop的高可用部署，以及HBase高可用，为之后应用数据层开发提供底层的BigTable支持。前面的文章，我们已经深入讨论的ZooKeeper这个中间件的原理以及分布式锁的实现，事实上zookeeper使用最广泛的场景是“选举”主从角色，Hadoop以及Hbase的高可用（主从架构）正是通过ZooKeeper的临时节点机制实现。<br>以下的配置会跳过Hadoop3.1.2的部署，仅给出ZooKeeper分布式物理方式部署、以及HBase的部署过程、测试结果。</p>
<h4 id="2、ZooKeeper与Hadoop、HBase的关系"><a href="#2、ZooKeeper与Hadoop、HBase的关系" class="headerlink" title="2、ZooKeeper与Hadoop、HBase的关系"></a>2、ZooKeeper与Hadoop、HBase的关系</h4><p>&#8195;&#8195;ZooKeeper作为协调器，在大数据组件中提供：管理Hadoop集群中的NameNode、HBase中HBaseMaster的选举，节点之间状态同步等。例如在HBase中，存储HBase的Schema，实时监控HRegionServer，存储所有Region的寻址入口，当然还有最常见的功能就是保证HBase集群中只有一个Master。</p>
<h4 id="3、Hadoop与HBase的关系"><a href="#3、Hadoop与HBase的关系" class="headerlink" title="3、Hadoop与HBase的关系"></a>3、Hadoop与HBase的关系</h4><p>&#8195;&#8195;完整的hadoop组件环境架构图<br><img src="https://img-blog.csdnimg.cn/20191019105428256.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>
<p>&#8195;&#8195;首先HBase是一个分布式的、面向列的开源数据库，正是业务数据需要列数据库的支持以及该数据库能够支持业务超大数据集扩展存储的需求，HBase当然作为中间件选型的首选。上图描述Hadoop组件生态中的各层系统。其中，HBase位于结构化存储层，Hadoop的HDFS为HBase提供了高可靠性、分布式的底层存储支持，Hadoop MapReduce、Spark为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定选举服务和failover机制。<br>此外，Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。<br>==（当然本blog也会针对Hbase的架构原理做出一篇文章讨论）==<br>&#8195;&#8195;其实，本博客有关大数据的多篇文章的内容，都是为了能够梳理出大数据多个组件全流程部署、架构原理、业务数据应用开发到BI的呈现的完整技术内容，以实现大数据库开发工程师必备的项目经历。</p>
<h4 id="4、架构资源规划"><a href="#4、架构资源规划" class="headerlink" title="4、架构资源规划"></a>4、架构资源规划</h4><div class="table-container">
<table>
<thead>
<tr>
<th>nn</th>
<th>dn1</th>
<th>dn2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1vcpu，2G内存</td>
<td>1vcpu，1G内存</td>
<td>1vcpu，1G内存</td>
</tr>
<tr>
<td>NameNode</td>
<td></td>
<td>NameNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DFSZKFailoverController</td>
<td>DFSZKFailoverController</td>
<td>DFSZKFailoverController</td>
</tr>
<tr>
<td>ResourceManager</td>
<td></td>
<td>ResourceManager</td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
<tr>
<td>JobHistoryServer</td>
<td></td>
<td>JobHistoryServer</td>
</tr>
<tr>
<td>ZooKeeper</td>
<td>ZooKeeper</td>
<td>ZooKeeper</td>
</tr>
<tr>
<td>HBase master</td>
<td></td>
<td>HBase master</td>
</tr>
<tr>
<td>RegionServer</td>
<td>RegionServer</td>
<td>RegionServer</td>
</tr>
</tbody>
</table>
</div>
<p>Hadoop版本、JDK版本、HBase版本、ZooKeeper版本参考如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">hadoop-3.1.2   jdk1.8.0_161   </span><br><span class="line">hbase-2.1.7    zookeeper-3.4.14</span><br></pre></td></tr></table></figure>
<p>关于Hadoop与HBase的兼容性，官方已经给出<a href="http://hbase.apache.org/book.html#_configuration_files">Hadoop version support matrix</a>，目前HBase2.1.x、HBase2.2.x支持Hadoop 3.1.1+（Tested to be fully-functional）。关于版本兼容分析，很多类似文章也有提到，但他们所提到的兼容比对情况已过时，故最佳途径应及时查阅官网最新发布的内容。</p>
<h4 id="5、ZooKeeper集群设置"><a href="#5、ZooKeeper集群设置" class="headerlink" title="5、ZooKeeper集群设置"></a>5、ZooKeeper集群设置</h4><h5 id="5-1-设置nn节点的zoo-conf"><a href="#5-1-设置nn节点的zoo-conf" class="headerlink" title="5.1 设置nn节点的zoo.conf"></a>5.1 设置nn节点的zoo.conf</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;conf</span><br><span class="line"># 拷贝zoo_sample.cfg并重命名为zoo.cfg ：</span><br><span class="line">[root@nn conf]# cp zoo_sample.cfg zoo.conf</span><br><span class="line"></span><br><span class="line"># 修改 zoo.cfg</span><br><span class="line">[root@nn conf] vi zoo.cfg</span><br><span class="line"># data目录需自行创建，添加：</span><br><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;data</span><br><span class="line"># 在该文件最后添加，指定zookeeper集群主机及端口，节点数必须为奇数</span><br><span class="line">server.1&#x3D;nn:2888:3888</span><br><span class="line">server.2&#x3D;dn1:2888:3888</span><br><span class="line">server.3&#x3D;dn2:2888:3888</span><br><span class="line"></span><br><span class="line"># 在&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;data目录下创建myid文件</span><br><span class="line">[root@nn data]# pwd</span><br><span class="line">&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;data</span><br><span class="line">[root@nn data]# touch myid</span><br><span class="line">[root@nn data]# ls</span><br><span class="line">myid</span><br><span class="line"># 文件内容为1，即表示当前节点为在zoo.cfg中指定的server.1</span><br></pre></td></tr></table></figure>
<h5 id="5-2-将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid"><a href="#5-2-将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid" class="headerlink" title="5.2 将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid"></a>5.2 将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# scp -r zookeeper-3.4.14&#x2F; dn1:&#x2F;opt</span><br><span class="line">[root@nn opt]# scp -r zookeeper-3.4.14&#x2F; dn2:&#x2F;opt</span><br><span class="line"># 更改dn1 myid内容为2，dn2 myid内容为3</span><br></pre></td></tr></table></figure>
<h5 id="5-3-设置zk的全局环境变量"><a href="#5-3-设置zk的全局环境变量" class="headerlink" title="5.3 设置zk的全局环境变量"></a>5.3 设置zk的全局环境变量</h5><p>在三个节点上都要配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt] vi  &#x2F;etc&#x2F;profile</span><br><span class="line"># 新增</span><br><span class="line">ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeper-3.4.14 </span><br><span class="line">export   PATH&#x3D;$ZOOKEEPER_HOME&#x2F;bin:$PATH  </span><br></pre></td></tr></table></figure><br>source ~/.bash_profile</p>
<h5 id="5-4-启动zk集群"><a href="#5-4-启动zk集群" class="headerlink" title="5.4 启动zk集群"></a>5.4 启动zk集群</h5><p>在每个节点上运行<code>zkServer.sh start</code><br>查看三个节点的zk角色<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># nn节点</span><br><span class="line">[root@nn ~]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"># dn1节点</span><br><span class="line">[root@dn1 opt]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br><span class="line"># dn2节点</span><br><span class="line">[root@dn2 opt]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><br>使用jps查看zk的进程QuorumPeerMain<br>QuorumPeerMain是zookeeper集群的启动入口类，用来加载配置启动QuorumPeer线程<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# jps</span><br><span class="line">1907 Jps</span><br><span class="line">1775 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">[root@dn1 opt]# jps</span><br><span class="line">16226 Jps</span><br><span class="line">16201 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">[root@dn2 opt]# jps</span><br><span class="line">5824 QuorumPeerMain</span><br><span class="line">5861 Jps</span><br></pre></td></tr></table></figure><br>注意：如果某个节点使用jps命令后，没有QuorumPeerMain进程，一般是因为zk的端口号2181被占用，在<code>/opt/zookeeper-3.4.14</code>目录中，zookeeper.out执行日志会给相应的提示。<br>[root@nn zookeeper-3.4.14]# ls<br>bin              ivy.xml      README.md                 zookeeper-3.4.14.jar.sha1  zookeeper.out<br>….<br>以下为之前docker方式部署zk时占用了2181端口<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn2 zookeeper-3.4.14]# ss -tnlp |grep 2181</span><br><span class="line">LISTEN     0      128         :::2181                    :::*                   users:((&quot;docker-proxy&quot;,pid&#x3D;1499,fd&#x3D;4))</span><br><span class="line"># kill docker占用的2181进程，再重新启动zk即可。</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h4 id="6、Hadoop-HA配置详细说明"><a href="#6、Hadoop-HA配置详细说明" class="headerlink" title="6、Hadoop HA配置详细说明"></a>6、Hadoop HA配置详细说明</h4><p>首先配置hadoop的jdk路径：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi hadoop-env.sh</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br></pre></td></tr></table></figure></p>
<h5 id="6-1-core-site-xml-加入zk服务"><a href="#6-1-core-site-xml-加入zk服务" class="headerlink" title="6.1 core-site.xml 加入zk服务"></a>6.1 core-site.xml 加入zk服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- hdfs地址，单点模式值为namenode主节点名，本测试为HA模式，需设置为nameservice  的名字--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hdapp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">  &lt;!-- 这里的路径默认是NameNode、DataNode、JournalNode等存放数据的公共目录，也可以单独指定 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/hadoop-3.1.2/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--加入zk服务，不少于三个节点--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--设置web访问用户，否则web端浏览hdfs文件目录会提权限不足--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="6-2-hdfs-site-xml"><a href="#6-2-hdfs-site-xml" class="headerlink" title="6.2  hdfs-site.xml"></a>6.2  hdfs-site.xml</h5><p>因为要配置hadoop HA，因此这部分的属性项比较多，这部分内容参考<br><a href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">Apache官网HA配置</a>，官网已经给出非常详细且易懂的描述。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- hadoop HA 配置开始 --&gt;</span><br><span class="line">  &lt;!-- 为namenode集群起一个services name，名字和core-site.xml的fs.defaultFS指定的一致 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.nameservices&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdapp&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">    &lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.hdapp&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nn,dn2&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定nn的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.hdapp.nn&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nn:9000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">      &lt;!-- 指定dn2的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.hdapp.dn2&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;dn2:9000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">    &lt;!--名为nn的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.hdapp.nn&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nn:50070&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 名为dn2的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.hdapp.dn2&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;dn2:50070&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- namenode间用于共享编辑日志的journal节点列表&#x2F;hdapp是表示日志存储的在hdfs上根路径，为多个HA可公用服务器进行数据存储，节约服务器成本 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;qjournal:&#x2F;&#x2F;nn:8485;dn1:8485;dn2:8485&#x2F;hdapp&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- journalnode 上用于存放edits日志的目录 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.journalnode.edits.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;tmp&#x2F;dfs&#x2F;journalnode&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.automatic-failover.enabled.hdapp&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.hdapp&lt;&#x2F;name&gt;  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">    </span><br><span class="line">  &lt;!-- 一旦需要NameNode切换，使用两方式进行操作，优先使用sshfence --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.methods&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;sshfence</span><br><span class="line">shell(&#x2F;bin&#x2F;true)</span><br><span class="line">&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 如果使用ssh进行故障切换，使用ssh通信时指定私钥所在位置 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;root&#x2F;.ssh&#x2F;id_rsa&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- ssh连接超时超时时间，30s --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;30000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"> &lt;!-- HA配置结束 --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 设置 hdfs 副本数量，这里跟节点数量一致 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt; </span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<p>注意在sshfence设置中，若ssh用户名不是root，且ssh端口不是默认22，则需改为<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;sshfence([[my_hadoop][:31900]])&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br></pre></td></tr></table></figure><br>==以上设置非常重要，涉及sshfence能否正常切换主备hadoop服务==</p>
<p>官网给出自定义shell脚本去切换namenode进程<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">shell - run an arbitrary shell <span class="built_in">command</span> to fence the Active NameNode</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">The shell fencing method runs an arbitrary shell <span class="built_in">command</span>. It may be configured like so:</span></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;shell(/path/to/my/script.sh arg1 arg2 ...)&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<h5 id="6-3-mapred-site-xml"><a href="#6-3-mapred-site-xml" class="headerlink" title="6.3 mapred-site.xml"></a>6.3 mapred-site.xml</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;!-- 采用yarn作为mapreduce的资源调度框架 --&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定nn作为jobhistory服务器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放已完成job的历史日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放正在运行job的历史日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done_intermediate&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放yarn stage的日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/history/staging&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> web上默认最多显示20000个历史的作业记录信息，这里设为1000个。</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.jobhistory.joblist.cache.size&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;mapreduce.jobhistory.cleaner.enable&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 一天清理一次 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.jobhistory.cleaner.interval-ms&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;86400000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 仅保留最近1周的job日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.jobhistory.max-age-ms&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;432000000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意，这里加入yarn执行application（job）的日志记录进程，因为nn和dn2做了HA，所以nn、dn2节点都配上该jobhistory服务，dn1节点不需要。</p>
<h5 id="6-4-yarn-site-xml配置"><a href="#6-4-yarn-site-xml配置" class="headerlink" title="6.4  yarn-site.xml配置"></a>6.4  yarn-site.xml配置</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 启用yarn HA高可用性 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定resourcemanager的名字，自行命名，跟服务器hostname无关 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hayarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定nn节点为rm1 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定dn2节点为rm2  --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;dn2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定当前机器nn作为主rm1 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定zookeeper集群机器 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 禁止启动一个线程检查每个任务正使用的物理内存量、虚拟内存量是否可用 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">   	&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">   	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">   	&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">   	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h5 id="6-5-指定worker"><a href="#6-5-指定worker" class="headerlink" title="6.5 指定worker"></a>6.5 指定worker</h5><p>三个节点都设为datanode，在生产环境中，DD不要跟DN放在同一台服务器<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi etc&#x2F;hadoop&#x2F;workers </span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure><br>6.5 将/opt/hadoop-3.1.2/目录拷贝到dn1、dn2节点<br>(因为dn1不作为resourcemanager standby角色，因此在其yarn-site.xml删除)<br>nn节点作为resourcemanager 角色，因此在其yarn-site.xml，RM设为自己<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>而dn2节点作为resourcemanager standby角色，因此在其yarn-site.xml，RM设为自己<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<h5 id="6-6-修改start-dfs-sh和-stop-dfs-sh文件"><a href="#6-6-修改start-dfs-sh和-stop-dfs-sh文件" class="headerlink" title="6.6  修改start-dfs.sh和 stop-dfs.sh文件"></a>6.6  修改start-dfs.sh和 stop-dfs.sh文件</h5><p>在/opt/hadoop-3.1.2/sbin/中，分别在 start-dfs.sh 和 stop-dfs.sh文件开始处添加如下内容<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">HDFS_ZKFC_USER=root</span><br><span class="line">HDFS_JOURNALNODE_USER=root</span><br><span class="line"><span class="meta">#</span><span class="bash"> 以上内容在三个节点上配置</span></span><br><span class="line"></span><br><span class="line">在start-yarn.sh和stop-yarn.sh文件头部添加如下命令</span><br><span class="line">​```shell</span><br><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br><span class="line"><span class="meta">#</span><span class="bash"> 以上内容在三个节点上配置</span></span><br></pre></td></tr></table></figure><br>至此，以及完成hadoop层面的HA的配置文件，因为属性项很多，配置过程务必仔细核对，否则启动各种出错。下面将逐步验证各项组件启动情况</p>
<h4 id="7、集群启动"><a href="#7、集群启动" class="headerlink" title="7、集群启动"></a>7、集群启动</h4><p>在第5节内容中，三个节点zk的QuorumPeerMain进程已正常启动</p>
<h5 id="7-1-启动JournalNode进程"><a href="#7-1-启动JournalNode进程" class="headerlink" title="7.1 启动JournalNode进程"></a>7.1 启动JournalNode进程</h5><p>JournalNode服务三个节点启动都启动，因此，需在每个节点上单独运行启动命令，建议使用全局命令hdfs，否则得去每个节点的sbin目录下使用hadoop-daemon.sh<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ]# hdfs --daemon start journalnode</span><br><span class="line">[root@nn opt]# jps</span><br><span class="line">4889 JournalNode</span><br><span class="line">4987 Jps</span><br><span class="line">1775 QuorumPeerMain</span><br></pre></td></tr></table></figure><br>若启动正常，jps可以看三个节点都启动了相应进程</p>
<h5 id="7-2-格式化-NameNode和zkfc"><a href="#7-2-格式化-NameNode和zkfc" class="headerlink" title="7.2 格式化 NameNode和zkfc"></a>7.2 格式化 NameNode和zkfc</h5><p>这里的zkfc指：ZK Failover Controller daemon<br>==在NameNode的nn主节点上进行==<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# hdfs namenode -format</span><br><span class="line"><span class="meta">#</span><span class="bash"> 成功提示</span></span><br><span class="line">Storage directory /opt/hadoop-3.1.2/tmp/dfs/name has been successfully formatted.</span><br><span class="line"></span><br><span class="line">[root@nn opt]# hdfs zkfc -formatZK</span><br><span class="line"><span class="meta">#</span><span class="bash"> 成功提示</span></span><br><span class="line">: Successfully created /hadoop-ha/hdapp in ZK.</span><br></pre></td></tr></table></figure><br>==重要==<br>在备机dn2节点上执行fsimge元数据同步命令<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 sbin]# hdfs namenode -bootstrapStandby</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到namenode主从服务信息</span></span><br><span class="line">=====================================================</span><br><span class="line">About to bootstrap Standby ID dn2 from:</span><br><span class="line">           Nameservice ID: hdapp</span><br><span class="line">        Other Namenode ID: nn</span><br><span class="line">  Other NN&#x27;s HTTP address: http://nn:50070</span><br><span class="line">  Other NN&#x27;s IPC  address: nn/192.188.0.4:9000</span><br><span class="line">             Namespace ID: 778809532</span><br><span class="line">            Block pool ID: **</span><br><span class="line">               Cluster ID:**</span><br><span class="line">           Layout version: -64</span><br><span class="line">       isUpgradeFinalized: true</span><br><span class="line">=====================================================</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dn2节点通过http get去nn节点下载FsImage以便实现元数据同步</span></span><br><span class="line">namenode.TransferFsImage: Opening connection to http://nn:50070/imagetransfer?getimage=1&amp;txid=0&amp;storageInfo=-64:778809532:***:CID-594d1106-a909-4e60-8a2d-d54e264beee2&amp;bootstrapstandby=true</span><br><span class="line">***</span><br><span class="line">namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 391 bytes.</span><br></pre></td></tr></table></figure></p>
<h5 id="7-3-启动ZookeeperFailoverController、HDFS、YARN"><a href="#7-3-启动ZookeeperFailoverController、HDFS、YARN" class="headerlink" title="7.3 启动ZookeeperFailoverController、HDFS、YARN"></a>7.3 启动ZookeeperFailoverController、HDFS、YARN</h5><p>启动HA服务是有顺序的，需先启动ZKFC再启动HDFS，该服务管理hadoop的namenode主备切换；若先启动HDFS，则在未启动ZKFC进程之前，两个namenode都是standby模式，直到ZKFC启动后，HDFS才会正常进入HA主备模式。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、启动主备切换服务，在nn、dn2分别执行</span></span><br><span class="line">[root@nn sbin]# hdfs --daemon start zkfc</span><br><span class="line">[root@dn2 sbin]# hdfs --daemon start zkfc</span><br><span class="line"><span class="meta">#</span><span class="bash"> DFSZKFailoverController进程</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、只需在主节点nn上操作执行</span></span><br><span class="line">[root@nn sbin]# ./start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 验证:nn,dn2显示NN、DN、JN，dn1显示DN、JN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意namenode节点的tmp/dfs目录必须具有以下三个目录，否则namenode启动失败，提示相关目录不存在</span></span><br><span class="line">[root@dn2 dfs]# pwd</span><br><span class="line">/opt/hadoop-3.1.2/tmp/dfs</span><br><span class="line">[root@dn2 dfs]# ls</span><br><span class="line">data  journalnode  name</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、只需在主节点nn上执行</span></span><br><span class="line">[root@nn sbin]# /start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 验证：nn,dn2显示RN、NM，dn1显示NM</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h5 id="7-4-启动Application（job）History进程服务"><a href="#7-4-启动Application（job）History进程服务" class="headerlink" title="7.4  启动Application（job）History进程服务"></a>7.4  启动Application（job）History进程服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动JobHistoryServer</span></span><br><span class="line">mapred --daemon start  historyserver</span><br></pre></td></tr></table></figure>
<p>JobHistoryServer的作用：<br>可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。</p>
<h5 id="7-5-使用命令或者web页面查看集群组件服务情况"><a href="#7-5-使用命令或者web页面查看集群组件服务情况" class="headerlink" title="7.5 使用命令或者web页面查看集群组件服务情况"></a>7.5 使用命令或者web页面查看集群组件服务情况</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在NameNode主节点nn上，用命令查看Namenode</span></span><br><span class="line">[root@nn opt]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn opt]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure>
<p>在<code>http://nn:50070</code>查看nn状态（最好使用Chrome查看，用Firefox查看Utilities栏目-Browse the file system没响应）<br><img src="https://img-blog.csdnimg.cn/20191020121813446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>http://dn2:50070</code>查看dn2状态<br><img src="https://img-blog.csdnimg.cn/20191020122008421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在NameNode主节点nn上，用命令查看RM节点主备状态</span><br><span class="line">[root@nn opt]# yarn rmadmin -getServiceState rm1 </span><br><span class="line">standby</span><br><span class="line">[root@nn opt]# yarn rmadmin -getServiceState rm2</span><br><span class="line">active</span><br></pre></td></tr></table></figure>
<p>注意：这里显示是dn2（rm2）节点为active状态，当在浏览器输入<code>http://nn:8088</code>时，会自动被重定向到dn2的web服务：<code>http://dn2:8088/cluster</code><br><img src="https://img-blog.csdnimg.cn/2019102012262488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="7-6-hadoop主备切换测试"><a href="#7-6-hadoop主备切换测试" class="headerlink" title="7.6 hadoop主备切换测试"></a>7.6 hadoop主备切换测试</h5><p><strong>1）主备切换失败情况：</strong><br>在切换测试之前，请先检查Linux系统上有无安装一个fuser的工具<br>fuser：fuser可用于查询文件、目录、socket端口和文件系统的使用进程，并且可以使用fuser关闭进程，当文件系统umount报device busy时，常用到fuser查询并关闭使用相应文件系统的进程。<br>在6.2章节hdfs-site.xml配置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &lt;!-- 一旦需要NameNode切换，使用ssh方式或者shell进行操作 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.methods&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">sshfence</span><br><span class="line">        shell(&#x2F;bin&#x2F;true)&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><br>以上表示fencing的方法目前有两种，sshfence和shell<br>sshfence方法是指通过ssh登陆到active namenode节点并kill了该namenode进程，因此需设置ssh免密登陆，还要保证有杀掉namenode进程的权限，以保证hadoop集群在任何时候只有一个namenode节点处于active状态。<br>如果Linux系统没有fuser工具，那么sshfence执行会提示提示<br>==fuser: command not found==<br>==Fencing method org.apache.hadoop.ha.SshFenceByTcpPort(null) was unsuccessful.==<br>该日志路径是在dn2作为standby节点的日志目录下：<br><code>/opt/hadoop-3.1.2/logs/hadoop-root-zkfc-dn2.log</code><br>导致主备无法正常切换，可能出现脑裂（例如两个namenode都是active模式或者standby模式）</p>
<p><strong>2）解决方案：</strong><br>只需安装fuser工具即可<br><code>yum install psmisc -y</code>，安装之后，查看<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn2 logs]# ls &#x2F;usr&#x2F;sbin&#x2F;fuser </span><br><span class="line">&#x2F;usr&#x2F;sbin&#x2F;fuser</span><br></pre></td></tr></table></figure></p>
<p><strong>3）主备切换nn，dn2状态变更：</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>在主节点nn上，手动kill掉namenode进程，可以看到dn2立即变为active状态<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# jps</span><br><span class="line">7424 NameNode</span><br><span class="line">5585 JournalNode</span><br><span class="line">5347 DataNode</span><br><span class="line">6024 ResourceManager</span><br><span class="line">5001 QuorumPeerMain</span><br><span class="line">25322 JobHistoryServer</span><br><span class="line">8922 Jps</span><br><span class="line">6652 DFSZKFailoverController</span><br><span class="line">6157 NodeManager</span><br><span class="line">[root@nn sbin]# kill -9 7424</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">active</span><br></pre></td></tr></table></figure><br>==以上kill了nn的namenode进程，再启动该进程，看看nn能否变为standby模式==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# hdfs --daemon start namenode</span><br><span class="line">[root@nn ~]# hdfs haadmin -getServiceState nn</span><br><span class="line">standby</span><br><span class="line"># 或者使用强制转换主备命测试，注意因为是是测试环境，所以可以强制测试，如果已经在生产环境，请做好fsimage备份，否则可能主备的元数据不同步导致数据丢失。</span><br><span class="line">[root@nn ~]# hdfs haadmin -getAllServiceState                   </span><br><span class="line">nn:9000                                            active    </span><br><span class="line">dn2:9000                                           standby   </span><br><span class="line">[root@nn ~]# hdfs haadmin -transitionToStandby --forcemanual  nn</span><br><span class="line">[root@nn ~]# hdfs haadmin -getAllServiceState                   </span><br><span class="line">nn:9000                                            standby   </span><br><span class="line">dn2:9000                                           active   </span><br></pre></td></tr></table></figure><br>可以看到启动NN服务后，nn自身成功转为standby模式。<br>同理，RM的主备切换和恢复的过程跟上述一致，这里不再赘述。</p>
<p>==4）在zookeeper目录下查看hadoop HA建立的znode及其内容==<br>注意：以下说的zk节点是指znode，是一种类似目录的路径，不是指hadoop节点（服务器），注意区分。<br>[root@nn opt]# zkCli.sh<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看zk的根节点/有哪些节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /</span><br><span class="line">[zookeeper, yarn-leader-election, hadoop-ha]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看hadoop-ha节点,可以看到子节点hdapp就是我们在hdfs-site.xml里配置的集群nameservices名称，若有多个集群，那么/hadoop-ha节点将有多个子节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 9] ls /hadoop-ha</span><br><span class="line">[hdapp]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 继续查看子节点hdapp是否还有子节点:可以看到这是都active状态节点的信息</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 11] ls /hadoop-ha/hdapp</span><br><span class="line">[ActiveBreadCrumb, ActiveStandbyElectorLock]</span><br><span class="line"><span class="meta">#</span><span class="bash"> ABC是持久节点，ASE是临时节点，nn、dn2都在ASE注册监听临时节点删除事件</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看主备选举的锁节点存放在哪个节点地址</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 7] get /hadoop-ha/hdapp/ActiveStandbyElectorLock</span><br><span class="line"></span><br><span class="line">hdappdn2dn2 �F(�&gt;</span><br><span class="line">***</span><br></pre></td></tr></table></figure><br>这里可以看到dn2节点抢到了ActiveStandbyElectorLock，因此作为active节点。<br>ActiveBreadCrumb持久节点用来防止脑裂设计，通过注册事件回调sshfence方法在另外一个节点上kill 掉NN进程<br>具体逻辑参考这篇文章的讨论：<a href="https://www.jianshu.com/p/8a6cc2d72062">文章链接</a>，内容还不错。</p>
<p>同样的yarn-leader-election选举处理逻辑也是借用zk节点特性和注册事件回调方法来实现，大体差不多。</p>
<p>至此负责底层分布式存储的Hadoop HA高可用已经完整实现，这部分是重点和难点，因此占了较大篇幅。此外这里还没给出HA管理员命令的使用以及理解：hdfs haadmin，不过这部分内容一般是hadoop集群运维部负责的工作，作为开发者的我们，也需要了解其中一部分内容。<br>接下来的关于HBase的主备配置则相对简单。</p>
<h4 id="8、HBase的HA配置"><a href="#8、HBase的HA配置" class="headerlink" title="8、HBase的HA配置"></a>8、HBase的HA配置</h4><h5 id="8-1-配置conf"><a href="#8-1-配置conf" class="headerlink" title="8.1  配置conf"></a>8.1  配置conf</h5><p>进入hbase-1.3.1/conf/目录，修改配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/hbase-2.1.7/conf</span><br><span class="line">[root@nn conf]# pwd vi hbase-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> The java implementation to use.  Java 1.8+ required.要求1.8以上的JDK</span></span><br><span class="line">export JAVA_HOME=/opt/jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 禁用HBase自带的Zookeeper，使用独立部署Zookeeper</span></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>
<p>以上配置在三个节点上配置（其实只需在nn和dn2 HMaster节点配置），为了避免以后需将dn1作为主节点时因之前漏了配置导致启动服务各种报错。</p>
<h5 id="8-2-配置hbase-site-xml"><a href="#8-2-配置hbase-site-xml" class="headerlink" title="8.2 配置hbase-site.xml"></a>8.2 配置hbase-site.xml</h5><p>关于hbase-site的详细内容，可以参考:<br>Apache HBase <a href="http://hbase.apache.org/book.html">Getting Started</a>里面内容。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 设置HRegionServers共享的HDFS目录，必须设为在hdfs-site中dfs.nameservices的值：hdapp，而且不能有端口号，该属性会让hmaster在hdfs集群上建一个/hbase的目录 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hdapp/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 启用分布式模式 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 启用分布式模式时，以下的流能力加强需设为false --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定Zookeeper集群位置，值可以是hostname或者hostname:port --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn,dn1,dn2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定独立Zookeeper安装路径 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/zookeeper-3.4.14&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定ZooKeeper集群端口 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><br>以上配置在三个节点配上</p>
<p>==注意：有部分有关HBaseHA配置技术博客文章中，有人会把hbase.rootdir配成以下形式==：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 直接指定hdfs的主节点</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;nn:9000&#x2F;hbase&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 直接指定主的HMaster服务 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.master&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;nn:60000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>接着他们还会在<code>/opt/hbase-2.1.7/conf</code>创建一个文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi backup-master</span><br><span class="line"># 内容为hbase的备机服务器，例如本文的dn2节点</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure><br>这种配法不是hbase HA的方式，是<a href="http://hbase.apache.org/book.html#standalone_dist">官方配置</a>给出的单机模式<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">5.1.1. Standalone HBase over HDFS</span><br><span class="line"></span><br><span class="line">A sometimes useful variation on standalone hbase has all daemons running inside the one JVM but rather than persist to the local filesystem, instead they persist to an HDFS instance.</span><br><span class="line">....</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;namenode.example.org:8020&#x2F;hbase&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>这种配法容易导致HBase服务退出：一旦nn节点从active状态切换为standby或者宕机，即使dn2对外提供hdfs服务，但hbase只认nn为active状态，并且会提示出错：<br> Operation category READ is not supported in state standby，也即没有可用的hdfs文件服务提供给HMaster进程去读，最后导致hbase异常退出。</p>
<h5 id="8-3-编辑regionservers"><a href="#8-3-编辑regionservers" class="headerlink" title="8.3 编辑regionservers"></a>8.3 编辑regionservers</h5><p>修改regionservers文件，因为当前是使用独立的Zookeeper集群，所以要指定RegionServers所在机器，按规划，三个节点都是RS角色：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/hbase-2.1.7/conf</span><br><span class="line">[root@nn conf]# vi regionservers </span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure><br>以上配置在三个节点配上</p>
<h5 id="8-4-创建hdfs-site-xml的软链到hbase的conf目录下"><a href="#8-4-创建hdfs-site-xml的软链到hbase的conf目录下" class="headerlink" title="8.4 创建hdfs-site.xml的软链到hbase的conf目录下"></a>8.4 创建hdfs-site.xml的软链到hbase的conf目录下</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# ln -s /opt/hadoop-3.1.2/etc/hadoop/hdfs-site.xml /opt/hbase-2.1.7/conf/hdfs-site.xml</span><br><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/hbase-2.1.7/conf</span><br><span class="line">[root@nn conf]# ll hdfs-site.xml </span><br><span class="line">lrwxrwxrwx. 1 root root 42 ** hdfs-site.xml -&gt; /opt/hadoop-3.1.2/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>
<p>该操作在三个节点上都要执行，这一环节的配置非常关键，HBase团队也给出相关解释：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Procedure: HDFS Client Configuration</span><br><span class="line">    Of note, if you have made HDFS client configuration changes on your Hadoop cluster, such as configuration directives for HDFS clients, as opposed to server-side configurations, you must use one of the following methods to enable HBase to see and use these configuration changes:</span><br><span class="line">        Add a pointer to your HADOOP_CONF_DIR to the HBASE_CLASSPATH environment variable in hbase-env.sh.</span><br><span class="line">        Add a copy of hdfs-site.xml (or hadoop-site.xml) or, better, symlinks, under $&#123;HBASE_HOME&#125;&#x2F;conf, or</span><br><span class="line">        if only a small set of HDFS client configurations, add them to hbase-site.xml.</span><br><span class="line">An example of such an HDFS client configuration is dfs.replication. If for example, you want to run with a replication factor of 5, HBase will create files with the default of 3 unless you do the above to make the configuration available to HBase.</span><br></pre></td></tr></table></figure><br>目的是为了HBase能够同步hdfs配置变化，例如上面提到当hdfs副本数改为5时，如果不创建这种配置映射，那么HBase还是按默认的3份去执行。</p>
<p>若缺少这个软链接，HBase启动集群服务有问题，部分RS无法启动！</p>
<h5 id="8-5-启动HBase集群遇到的问题"><a href="#8-5-启动HBase集群遇到的问题" class="headerlink" title="8.5 启动HBase集群遇到的问题"></a>8.5 启动HBase集群遇到的问题</h5><p>1） HMaster是否有顺序<br>首先，hbase的HA模式是工作在hdfs HA模式下，因此首先保证hdfs HA为正常状态。其次，HMaster无需在hdfs主节点上先启动，在standby节点也可以先启动，但每个HMaster的节点需独立运行start-hbase.sh。</p>
<p>2） 在启动HBase期间，相关出错的解决</p>
<p>A、HMaster进程启动正常，但是提示slf4j jar包存在多重绑定<br><code>SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hbase-2.1.7/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</code><br>解决：其实该提示不影响HMaster和HRegionServer进程，可以选择忽略</p>
<p>B、启动HMaster短暂几秒后异常退出，日志提示找不到相关class：java.lang.NoClassDefFoundError: org/apache/htrace/SamplerBuilder<br>==解决办法==，将<code>htrace-core-3.1.0-incubating.jar</code>拷到lib目录下，<br> <code>$HBASE_HOME/</code>为环境变量配置HBase路径:<br><code>cp $HBASE_HOME/lib/client-facing-thirdparty/htrace-core-3.1.0-incubating.jar $HBASE_HOME/lib/</code></p>
<p>C、两个节点的HMaster进程都正常运行，但所有HRegionServer进程会自动退出<br>原因：集群服务器之间的时间不同步导致，<br>解决办法：时间做同步<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"> #</span><span class="bash"> 将硬件时间写到系统时间</span></span><br><span class="line">[root@dn1 ~]# hwclock -s </span><br><span class="line">保存时钟</span><br><span class="line">[root@dn1 ~]# clock -w</span><br></pre></td></tr></table></figure><br>或者增加与master之间的时钟误差宽容度（不建议）<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;</span><br><span class="line">&lt;value&gt;150000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<h5 id="8-6-查看HBase集群信息"><a href="#8-6-查看HBase集群信息" class="headerlink" title="8.6  查看HBase集群信息"></a>8.6  查看HBase集群信息</h5><p>1）首先查看各个节点已经启动的服务<br>nn节点(HMaster、HRegionServer)：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# jps</span><br><span class="line">5585 JournalNode</span><br><span class="line">5347 DataNode</span><br><span class="line">23844 NameNode</span><br><span class="line">26839 Jps</span><br><span class="line">6024 ResourceManager</span><br><span class="line">25322 JobHistoryServer</span><br><span class="line">5001 QuorumPeerMain</span><br><span class="line">26026 HMaster</span><br><span class="line">6652 DFSZKFailoverController</span><br><span class="line">6157 NodeManager</span><br><span class="line">26191 HRegionServer</span><br></pre></td></tr></table></figure><br>dn1节点(HRegionServer)：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 opt]# jps</span><br><span class="line">12917 HRegionServer</span><br><span class="line">8074 JournalNode</span><br><span class="line">7979 DataNode</span><br><span class="line">7886 QuorumPeerMain</span><br><span class="line">8191 NodeManager</span><br><span class="line">13183 Jps</span><br></pre></td></tr></table></figure><br>dn2节点(HMaster、HRegionServer)：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 conf]# jps</span><br><span class="line">3200 NodeManager</span><br><span class="line">18416 Jps</span><br><span class="line">16339 NameNode</span><br><span class="line">25322 JobHistoryServer</span><br><span class="line">17827 HMaster</span><br><span class="line">2869 DataNode</span><br><span class="line">3973 DFSZKFailoverController</span><br><span class="line">17686 HRegionServer</span><br><span class="line">2698 QuorumPeerMain</span><br><span class="line">2970 JournalNode</span><br></pre></td></tr></table></figure></p>
<p>2）可通过web页面查看hbase主备情况<br>A、<code>http://nn:16010</code>显示nn为master，dn2为backup master，拥有3个RS<br><img src="https://img-blog.csdnimg.cn/20191020231354724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">B、<code>http://dn2:16010</code>显示dn2为backup master，当前active master为nn节点<br><img src="https://img-blog.csdnimg.cn/20191020231738392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">C、在zookeeper上查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# zkCli.sh</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[zookeeper, yarn-leader-election, hadoop-ha, hbase]</span><br></pre></td></tr></table></figure>
<p>以上可以看到hbase znode以及其他znode</p>
<p>继续查看/hbase子路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;hbase    </span><br><span class="line">[meta-region-server, rs, splitWAL,</span><br><span class="line">backup-masters, table-lock, flush-table-proc,</span><br><span class="line">master-maintenance, online-snapshot,</span><br><span class="line">switch, master, running, draining,</span><br><span class="line">namespace, hbaseid, table]</span><br></pre></td></tr></table></figure><br>以上可以看出hbase的集群服务器非常依赖zookeeper组件！！</p>
<p>查看hbase节点的RS路径列表<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] ls &#x2F;hbase&#x2F;rs</span><br><span class="line">[dn2,16020,1571571550293, nn,16020,1571571549066, dn1,16020,1571571535046]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] get &#x2F;hbase&#x2F;rs&#x2F;dn1,16020,1571571535046</span><br><span class="line">�regionserver:16020&amp;�Q��PBU�&#125;�</span><br></pre></td></tr></table></figure><br>注意：在zkCli客户端get 相关path的内容因编码问题查看时会显示乱码，可通过hbase web端口查看zk内容<img src="https://img-blog.csdnimg.cn/20191020233357702.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20191020233644573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="8-7-HBase-主备切换测试"><a href="#8-7-HBase-主备切换测试" class="headerlink" title="8.7 HBase 主备切换测试"></a>8.7 HBase 主备切换测试</h5><p>1）kill掉 nn节点的HMaster，查看dn2是否转为active master<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# jps</span><br><span class="line">26026 HMaster</span><br><span class="line">[root@nn ~]# kill -9 26026</span><br></pre></td></tr></table></figure><br>在<code>http://dn2:16010</code>查看主备情况，可以看到nn节点down后，dn2成为master hbase节点<br><img src="https://img-blog.csdnimg.cn/20191021000759842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">2）将nn恢复Hbase服务，查看nn的HMaster是否为backup状态<br>在nn节点上执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn bin]# pwd</span><br><span class="line">/opt/hbase-2.1.7/bin</span><br><span class="line">[root@nn bin]# ./hbase-daemon.sh start master</span><br></pre></td></tr></table></figure>
<p>查看<code>http://nn:16010</code>，可以看到nn节点已经作为backup master，dn2节点为active master<br><img src="https://img-blog.csdnimg.cn/20191021001137709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>http://dn2:16010</code>也可查看。</p>
<p>3）强制转换底层hdfs主备状态，查看hbase HA状态<br>把nn强制变为standby，hbase 主：nn，hbase：dn2<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# hdfs haadmin -transitionToStandby --forcemanual  nn</span><br></pre></td></tr></table></figure><br>把dn2强制变为standby，hbase 主：nn，hbase：dn2<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# hdfs haadmin -transitionToStandby --forcemanual  dn2</span><br></pre></td></tr></table></figure><br>hbase的主从状态不受底层hdfs主从变化影响，因为对于hbase来说，它只知道集群hdfs服务： hdfs://hdapp/hbase并没有改变。</p>
<p>至此，本文已经成功搭建了hadoop HA、yarn HA以及HBase HA服务，过程详细，积累不少经验，为之后本人给出大数据应用最核心内容之一——“数据应用开发”铺了很好的基础。</p>
<p>小结：所有服务的启动顺序如下所示<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">完整启动顺序</span><br><span class="line">1、分别在三台服务器上启动zookeeper</span><br><span class="line">[root@nn sbin]# zkServer.sh start</span><br><span class="line"># 三个服务器均可看到QuorumPeerMain进程</span><br><span class="line"></span><br><span class="line">2、在nn和dn2主节点上，启动zkfc</span><br><span class="line">[root@nn sbin]# hdfs --daemon start zkfc</span><br><span class="line"># 在nn和dn2主节点上，均可看到DFSZKFailoverController进程</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3、在主nn节点上，运行start-dfs.sh，无需在其他节点再运行该命令</span><br><span class="line">[root@nn sbin]# .&#x2F;start-dfs.sh </span><br><span class="line"># 可以看到NameNode、DataNode、journalnode服务</span><br><span class="line"></span><br><span class="line">4、在主nn节点上，运行start-yarn.sh，无需在其他节点再运行该命令</span><br><span class="line">[root@nn sbin]# .&#x2F;start-yarn.sh </span><br><span class="line"># 可以看到ResourceManager、NodeManager服务</span><br><span class="line"></span><br><span class="line">5、在nn、dn2主节点上，启动JobHistoryServer服务</span><br><span class="line">[root@nn bin]#  mapred --daemon start  historyserver</span><br><span class="line"># jps看到JobHistoryServer服务</span><br><span class="line"></span><br><span class="line">6、在nn、dn2主节点上，启动hbase服务</span><br><span class="line"></span><br><span class="line"># nn节点启动hbase</span><br><span class="line">[root@nn bin]# .&#x2F;start-hbase.sh</span><br><span class="line"></span><br><span class="line"># 在dn2节点上启动hbase，</span><br><span class="line">[root@dn2 bin]# .&#x2F;start-hbase.sh</span><br><span class="line"># HMaster\HRegionServer</span><br></pre></td></tr></table></figure></p>
<h4 id="9、HBase在hdfs创建的目录"><a href="#9、HBase在hdfs创建的目录" class="headerlink" title="9、HBase在hdfs创建的目录"></a>9、HBase在hdfs创建的目录</h4><p><img src="https://img-blog.csdnimg.cn/201910202357587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">/hbase目录下的内容<br><img src="https://img-blog.csdnimg.cn/20191021000048443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">关于这些文件的解释以及作用，将在下一篇HBase架构原理给出。</p>
<h4 id="10、在HBase创建table测试"><a href="#10、在HBase创建table测试" class="headerlink" title="10、在HBase创建table测试"></a>10、在HBase创建table测试</h4><p>这部分内容回到大家相对熟悉的数据库知识领域，本节内容仅提供基础demo用法，关于HBase的数据结构以及架构原理，本博客将在另外一篇文章进行深入讨论。<br>以下company表为例进行基本操作，该表包含staff_info和depart_info两个列簇，表结构如下所示：<br><img src="https://img-blog.csdnimg.cn/20191023223724743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">以下为基本的hbase使用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入HBase进行交互式操作</span></span><br><span class="line">[root@nn ~] hbase shell</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建company表，从这里即可看出列数据库的优势，无需预先定义列，表结构是松散灵活的，之后想加多少列都行。</span></span><br><span class="line">hbase(main):&gt; create &#x27;company&#x27;,&#x27;staff_info&#x27;,&#x27;depart_info&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前HBase有哪些表</span></span><br><span class="line">hbase(main):&gt; list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">获得表company的描述信息</span></span><br><span class="line">hbase(main):&gt; describe &#x27;company&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看全表数据,相当于selec * from company</span></span><br><span class="line">hbase(main):&gt; scan &#x27;t_user&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 往表插入数据，语法 put  <span class="string">&#x27;t&#x27;</span> ,<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;v&#x27;</span>  (表名，行rowkey，列簇：具体列，值)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第1行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:name&#x27;,&#x27;Aery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:age&#x27;,&#x27;25&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:sex&#x27;,&#x27;Male&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:name&#x27;,&#x27;Develop&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:level&#x27;,&#x27;10&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;109&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第2行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:name&#x27;,&#x27;Bery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:age&#x27;,&#x27;23&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:sex&#x27;,&#x27;Female&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:name&#x27;,&#x27;HR&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:level&#x27;,&#x27;9&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;108&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第3行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:name&#x27;,&#x27;Cery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:age&#x27;,&#x27;26&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:name&#x27;,&#x27;Market&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:level&#x27;,&#x27;9&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;107&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第4行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:name&#x27;,&#x27;Dery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:age&#x27;,&#x27;27&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:sex&#x27;,&#x27;Male&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:name&#x27;,&#x27;Finance&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:level&#x27;,&#x27;9&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;106&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取数据，有以下几种方式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里cell已经包含timestamp和value</span></span><br><span class="line">hbase(main):038:0&gt; get &#x27;company&#x27;,&#x27;R1&#x27;</span><br><span class="line">COLUMN              cell                                                         </span><br><span class="line"> depart_info:inner_tel  timestamp=**, value=108                              </span><br><span class="line"> depart_info:level      timestamp=**, value=9                                </span><br><span class="line"> depart_info:name       timestamp=**, value=HR                               </span><br><span class="line"> staff_info:age         timestamp=**, value=23                               </span><br><span class="line"> staff_info:name        timestamp=**, value=Bery                             </span><br><span class="line"> staff_info:sex         timestamp=**, value=Female                           </span><br><span class="line">1 row(s)</span><br><span class="line">Took 0.0427 seconds    </span><br><span class="line">                        </span><br><span class="line"></span><br><span class="line">hbase(main):039:0&gt; get &#x27;company&#x27;,&#x27;R2&#x27;,&#x27;staff_info&#x27;</span><br><span class="line">COLUMN                  CELL  </span><br><span class="line"> staff_info:age         timestamp=**  value=27                               </span><br><span class="line"> staff_info:name        timestamp=**, value=Dery                             </span><br><span class="line"> staff_info:sex         timestamp=**, value=Male                             </span><br><span class="line">1 row(s)</span><br><span class="line"></span><br><span class="line">hbase(main):&gt; get &#x27;company&#x27;,&#x27;R1&#x27;,&#x27;staff_info:age&#x27;</span><br><span class="line"></span><br><span class="line">COLUMN                  CELL                                                          </span><br><span class="line"> staff_info:age         timestamp=**, value=23                               </span><br><span class="line">1 row(s)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 计算company rowkey数目</span></span><br><span class="line">hbase(main):041:0&gt; count &#x27;company&#x27;</span><br><span class="line">2 row(s)</span><br><span class="line">Took 0.1696 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除R2中的部门信息的level列                                                                    hbase(main):072:0&gt; delete <span class="string">&#x27;company&#x27;</span>,<span class="string">&#x27;R2&#x27;</span>,<span class="string">&#x27;depart_info:level&#x27;</span></span> </span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空表内容</span></span><br><span class="line"><span class="meta">hbase(main)&gt;</span><span class="bash"> truncate <span class="string">&#x27;company&#x27;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 禁用表，之后drop表才有效</span></span><br><span class="line">hbase(main):&gt; disable &#x27;company&#x27;</span><br><span class="line">hbase(main):&gt; drop &#x27;company&#x27;</span><br><span class="line">hbase(main):&gt; exists &#x27;company&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>company表的信息也可以在HMaster web端查看<br><img src="https://img-blog.csdnimg.cn/20191024003258472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">     &#8195;&#8195;以上为基本的hbase表操作，如果用shell的方式开发hbase数据应用，效率是非常低的，就像直接在mysql的shell写复杂sql、在Oracle的shell写sql那样，虽然很raw，但不友好。而对于mysql的sql开发，大家会用workbench或者DBeaver；对于Oracle的开发，大家会用PL/SQL Developer；在程序业务逻辑开发层面，大家会引入DB-API 第三方库实现业务逻辑开发。<br>那么对于HBase分布式数据库的开发，需要用到Hive工具。<br>&#8195;&#8195;Hive是建立在 Hadoop 上的数据仓库基础构架，它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），通过Hive，用户可以类 SQL 查询语言（又称 HQL）去“操作Hbase上的数据”，省去独自开发mapper 和 reducer 来处理计算任务（当然复杂的业务逻辑还是需要开发mapper和reducer）。<br>&#8195;&#8195;本博客将在后面的文章中，引入Hive组件，配合HBase进行某个主题的大数据实际项目开发。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>Hadoop集群</tag>
        <tag>HBase集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于PySpark和ALS算法实现基本的电影推荐流程</title>
    <url>/2020/01/11/%E5%9F%BA%E4%BA%8EPySpark%E5%92%8CALS%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文内容第一部分给出Pyspark常见算子的用法，第二部分则参考书籍《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。本文内容为大数据实时分析项目提供基本的入门知识。</p>
<h4 id="1、PySpark简介"><a href="#1、PySpark简介" class="headerlink" title="1、PySpark简介"></a>1、PySpark简介</h4><p>&#8195;&#8195;本节内容的图文一部分参考了这篇文章<a href="http://sharkdtu.com/posts/pyspark-internal.html">《PySpark 的背后原理 》</a>，个人欣赏此博客作者，博文质量高，看完受益匪浅！Spark的内容不再累赘，可参考本博客<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》</a>。PySpark的工作原理图示如下：<br><img src="https://img-blog.csdnimg.cn/20200108220627968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>
<p>&#8195;&#8195;在这里，Py4J 是一个用 Python 和 Java 编写的库，它可以让Python代码实现动态访问JVM的Java对象，同时JVM也能够回调 Python对象。因此PySpark就是在Spark外围包装一层Python API，借助Py4j实现Python和Java的交互（这里的交互就是通过socket实现，传字节码），进而实现通过Python编写Spark应用程序。<br><img src="https://img-blog.csdnimg.cn/20200108215814813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;在Driver端，PySparkContext通过Py4J启动一个JVM并产生一个JavaSparkContext；在Executor端，则不需要借助Py4j，因为Executor端运行的是由Driver传过来的Task业务逻辑（其实就是java的字节码）。</p>
<h4 id="2、Pyspark接口用法"><a href="#2、Pyspark接口用法" class="headerlink" title="2、Pyspark接口用法"></a>2、Pyspark接口用法</h4><h5 id="读取数据源"><a href="#读取数据源" class="headerlink" title="读取数据源"></a>读取数据源</h5><p>PySpark支持多种数据源读取，常见接口如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc.pickleFile() <span class="comment"># &lt;class &#x27;pyspark.rdd.RDD&#x27;&gt;</span></span><br><span class="line">sc.textFile() <span class="comment"># &lt;class &#x27;pyspark.rdd.RDD&#x27;&gt;</span></span><br><span class="line">spark.read.json() <span class="comment"># &lt;class &#x27;pyspark.sql.dataframe.DataFrame&#x27;&gt;</span></span><br><span class="line">spark.read.text() <span class="comment"># &lt;class &#x27;pyspark.sql.dataframe.DataFrame&#x27;&gt;</span></span><br></pre></td></tr></table></figure><br>例如读取本地要注意，格式为<code>file://+文件绝对路径</code><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;file:///home/mparsian/dna_seq.txt&quot;</span>)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取hdfs上文件数据</span></span><br><span class="line">sc.textFile(<span class="string">&quot;your_hadoop/data/moves.txt&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h5 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h5><p>Spark的算子分为两类：Transformation和Action。<br>Transformation仅仅是定义逻辑，并不会立即执行，有lazy特性，目的是将一个RDD转为新的RDD，可以基于RDDs形成lineage（DAG图）；<br>Action：触发Job运行，真正触发driver运行job；</p>
<p><strong>第一类算子：Transformation</strong></p>
<ul>
<li>map(func): 返回一个新的RDD，func会作用于每个map的key，例如在wordcount例子要<code>rdd.map(lambda a, (a, 1))</code>将数据转换成(a, 1)的形式以便之后做reduce<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">   )</span><br><span class="line">word_map_rdd = word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w, <span class="number">1</span>))</span><br><span class="line">mapping = word_map_rdd.collect()</span><br><span class="line">print(mapping)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;pyspark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>mappartitions(func, partition):  Return a new RDD by applying a function to each partition of this RDD.和map不同的地方在于map的func应用于每个元素，而这里的func会应用于每个分区，能够有效减少调用开销，减少func初始化次数。减少了初始化的内存开销。<br>例如将一个数据集合分成2个区，再对每个区进行累加，该方法适合对超大数据集合的分区累加处理，例如有1亿个item，分成100个分区，有10台服务器，那么每台服务器就可以负责自己10个分区的数据累加处理。<br>官方也提到mappartitions中如果一个分区太大，一次计算的话可能直接导致内存溢出。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">10</span>, <span class="number">22</span>, <span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">each_partition</span>):</span> </span><br><span class="line"><span class="keyword">yield</span> <span class="built_in">sum</span>(each_partition)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[[<span class="number">10</span>, <span class="number">22</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">rdd.mapPartitions(f).glom().collect()</span><br><span class="line">[[<span class="number">32</span>], [<span class="number">7</span>]]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>filter(func): 返回一个新的RDD，func会作用于每个map的key，用于筛选数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize ([<span class="string">&quot;fooo&quot;</span>, <span class="string">&quot;bbbar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;Aoo&quot;</span>])</span><br><span class="line">rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="string">&#x27;foo&#x27;</span> <span class="keyword">in</span> x).collect()</span><br><span class="line"><span class="comment"># [&#x27;fooo&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>flatMap(func): 返回一个新的RDD，func用在每个item，并把item切分为多个元素返回，例如wordcount例子的分类<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize ([<span class="string">&quot;this is pyspark&quot;</span>, <span class="string">&quot;this is spark&quot;</span>])</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27; &#x27;</span>)).collect()</span><br><span class="line"><span class="comment">#可以看到每个item为一句话，经过func后，分解为多个单词（多个元素）</span></span><br><span class="line"><span class="comment"># [&#x27;this&#x27;, &#x27;is&#x27;, &#x27;pyspark&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;spark&#x27;]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize ((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> x:(<span class="number">2</span>*x,<span class="number">3</span>*x)).collect()</span><br><span class="line"><span class="comment"># 对原来每个item分别乘2乘3，func返回两个item</span></span><br><span class="line"><span class="comment"># [2, 3, 4, 6, 6, 9]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>flatMapValues(func)：flatMapValues类似于mapValues，不同的在于flatMapValues应用于元素为key-value对的RDD中Value。每个一kv对的Value被输入函数映射为一系列的值，然后这些值再与原RDD中的Key组成一系列新的KV对。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;aoo&quot;</span>]), (<span class="string">&quot;age&quot;</span>, [<span class="string">&quot;12&quot;</span>, <span class="string">&quot;20&quot;</span>])])</span><br><span class="line">rdd.flatMapValues(<span class="keyword">lambda</span> x:x).collect()</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;aoo&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;12&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;20&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>mapValues(func): 返回一个新的RDD，对RDD中的每一个value应用函数func。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;aoo&quot;</span>]), (<span class="string">&quot;age&quot;</span>, [<span class="string">&quot;12&quot;</span>, <span class="string">&quot;20&quot;</span>])])</span><br><span class="line">rdd.mapValues(<span class="keyword">lambda</span> value:<span class="built_in">len</span>(value)).collect()</span><br><span class="line"><span class="comment"># [(&#x27;name&#x27;, 3), (&#x27;age&#x27;, 2)]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>distinct(): 去除重复的元素</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.distinct().collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 10), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>subtractByKey(other): 删除在RDD1与RDD2的key相同的项</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.subtractByKey(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>subtract(other): 取差集<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.subtract(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>intersection(other): 交集运算，保留在两个RDD中都有的元素</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.intersection(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 10)]</span></span><br></pre></td></tr></table></figure>
<p>有关key-value类型的处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="comment"># 取出所有item的key</span></span><br><span class="line">rdd.keys().collect() <span class="comment"># [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;]</span></span><br><span class="line"><span class="comment"># 取出所有的values</span></span><br><span class="line">rdd.values().collect() <span class="comment"># [1, 10, 1, 1]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>foldByKey</code>(<em>zeroValue</em>, <em>func</em>, <em>numPartitions=None</em>)</p>
<p>Merge the values for each  key using an associative function “func” and a neutral “zeroValue” which  may be added to the result an arbitrary number of times, and must not  change the result (e.g., 0 for addition, or  1 for multiplication.).<br>其实foldByKey也像reduceBykey，对同一key中的value进行合并，例如对相同key进行value累加，zeroValue=0表示累加：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.foldByKey(<span class="number">0</span>, <span class="keyword">lambda</span> x,y:x+y).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对相同key进行value累乘，注意zeroValue=1代表累乘：</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line">rdd.foldByKey(<span class="number">1</span>, <span class="keyword">lambda</span> x,y:x*y).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 4), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>groupByKey(numPartitions=None): 将(K, V)数据集上所有Key相同的数据聚合到一起，得到的结果是(K, (V1, V2…))<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">len</span>).collect())</span><br><span class="line"><span class="comment"># 统计数据集每个key的个数总和</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 3), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">list</span>).collect())</span><br><span class="line"><span class="comment"># 将每个key的v聚合到一个list里面</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, [1, 10, 1]), (&#x27;b&#x27;, [1])]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>reduceByKey(func, numPartitions=None):此算子最常用， 将(K,  V)数据集上所有Key相同的数据聚合到一起，func的参数即是每两个K-V中的V。可以使用这个函数来进行计数，例如reduceByKey(lambda  a,b:a+b)就是将key相同数据的Value进行相加。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;foo&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;bar&quot;</span>, <span class="number">3</span>)])</span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> x, y : x + y).collect() <span class="comment"># [(&#x27;foo&#x27;, 3), (&#x27;bar&#x27;, 3)]  </span></span><br><span class="line">x.reduceByKey(<span class="built_in">max</span>).collect() <span class="comment">#  [(&#x27;foo&#x27;, 2), (&#x27;bar&#x27;, 3)]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>join(other, numPartitions=None): 将(K, V)和(K, W)类型的数据进行JOIN操作，得到的结果是这样(K, (V, W))</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;bar&quot;</span>, <span class="number">10</span>) , (<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;bar&quot;</span>, <span class="number">12</span>) , (<span class="string">&quot;foo&quot;</span>, <span class="number">12</span>)])</span><br><span class="line">rdd1.join(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;bar&#x27;, (10, 12)), (&#x27;foo&#x27;, (1, 12))]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>union(other): 并集运算，合并两个RDD</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rdd1 &#x3D; sc.parallelize([(&quot;a&quot;, 10) ,(&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">rdd2 &#x3D; sc.parallelize([(&quot;a&quot;, 10) ,(&quot;c&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">rdd1.union(rdd2).collect()</span><br><span class="line"># [(&#39;a&#39;, 10), (&#39;b&#39;, 1), (&#39;a&#39;, 1), (&#39;a&#39;, 10), (&#39;c&#39;, 1), (&#39;a&#39;, 1)]</span><br></pre></td></tr></table></figure>
<p>还有更多的transmission算子这里不再一一列举，可以参考官网PySpark API文档。</p>
<p>第二类算子：Action</p>
<ul>
<li><p>collect(): 以数组的形式，返回数据集中所有的元素。在数据探索阶段常用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line">word_map_rdd = word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w, <span class="number">1</span>))</span><br><span class="line">word_map_rdd.collect()</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[(<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;pyspark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>collectAsMap将k-v数据rdd集合转为python字典类型，同一key的项，只取第一项，其他的项被忽略</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.collectAsMap() <span class="comment"># &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure></li>
<li>count(): 返回数据集中元素的个数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line">word_rdd.count() <span class="comment"># 8</span></span><br></pre></td></tr></table></figure>
<ul>
<li>take(n): 返回数据集的前N个元素</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">word_rdd.take(<span class="number">3</span>) <span class="comment"># [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>takeOrdered(n): 升序排列，取出前N个元素<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;zoo&quot;</span>, <span class="string">&quot;aoo&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>) <span class="comment"># [&#x27;aoo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>takeOrdered(n, key=lambda num: -num): 降序排列，取出前N个元素<br>key=lambda num: -num只适用数值型的rdd，其实就将每项数值变为负数再排列</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>).takeOrdered(<span class="number">3</span>,key=<span class="keyword">lambda</span> num:-num)</span><br><span class="line">print(rdd)</span><br></pre></td></tr></table></figure>
<p>字符串的rdd排序，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;fooo&quot;</span>, <span class="string">&quot;bbbar&quot;</span>, <span class="string">&quot;ffoo&quot;</span>, <span class="string">&quot;zoo&quot;</span>, <span class="string">&quot;aoo&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按字符长度降序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>,key=<span class="keyword">lambda</span> item:-<span class="built_in">len</span>(item))</span><br><span class="line"><span class="comment"># 按字符长度升序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>,key=<span class="built_in">len</span>)</span><br><span class="line"><span class="comment">#按字母升序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>countByKey(): 对同一key值累计其计数，例如wordcount</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;bar&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.countByKey().items()</span><br><span class="line"><span class="comment"># dict_items([(&#x27;foo&#x27;, 2), (&#x27;bar&#x27;, 1)])以元组的方式返回</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>countByValue():对值分组统计</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">9</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">rdd.countByValue().items()</span><br><span class="line"><span class="comment"># dict_items([(9, 2), (10, 3)])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li>Persistence(持久化)<br>persist(): 将数据按默认的方式进行持久化<br> unpersist(): 取消持久化<br>saveAsTextFile(path): 将数据集保存至文件</li>
</ul>
<ul>
<li>创建rdd对象时指定分区，<br><code>parallelize(c, numSlices=None)</code><br>对每个元素都分区<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc.parallelize([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>], <span class="number">5</span>).glom().collect()</span><br><span class="line"><span class="comment"># [[0], [2], [3], [4], [6]]</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>glom方法：Return an RDD created by coalescing all elements within each partition into a list<br>指定两个分区<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line">[[<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>广播rdd<br>给定一个key为id的字段数据集合，给定其id，求字段对应的value</li>
</ul>
<p>非广播方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apples = sc.parallelize([(<span class="number">1</span>, <span class="string">&#x27;iPhone X&#x27;</span>),(<span class="number">2</span>, <span class="string">&#x27;iPhone 8&#x27;</span>),(<span class="number">5</span>, <span class="string">&#x27;iPhone 11&#x27;</span>)])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将该数据集合转为字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apples_dict=apples.collectAsMap()</span><br><span class="line"><span class="comment"># &#123;1: &#x27;iPhone X&#x27;, 2: &#x27;iPhone 8&#x27;, 5: &#x27;iPhone 11&#x27;&#125;</span></span><br></pre></td></tr></table></figure>
<p>给定id集合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids = sc.parallelize([<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>通过map方法取出ids对应的value</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:apples_dict[x]).collect()</span><br><span class="line"><span class="comment"># [&#x27;iPhone 8&#x27;, &#x27;iPhone X&#x27;, &#x27;iPhone 11&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>这种方式，在ids与apples_dict之间的映射转换，每一个id查找映射，都需要将ids和apples_dict传到worker节点上计算，如果有100万个id，而且apples_dict是个超大字典，那么就需要进行100万次上传worker再计算结果，显然效率极低，也不合理。</p>
<p>使用广播方式可避免这种情况<br>将apples_dict转为广播变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apples_dict_bc=sc.broadcast(apples_dict)</span><br><span class="line">print(<span class="built_in">type</span>(apples_dict_bc))</span><br><span class="line"><span class="comment"># &lt;class &#x27;pyspark.broadcast.Broadcast&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>给定id集合<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids = sc.parallelize([<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure></p>
<p>id对应的value，使用apples_dict_bc.value[x]这个广播变量，获取id对应的value</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:apples_dict_bc.value[x]).collect()</span><br><span class="line"><span class="comment"># [&#x27;iPhone 8&#x27;, &#x27;iPhone X&#x27;, &#x27;iPhone 11&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>在开始计算时，apples_dict_bc会传到worker node的内存上（如果数据集合太大，有部分数据则存在磁盘）。之后worker 可以一直使用这个“常驻内存广播变量”处理映射任务，即使有100万个id，客户端只需要把id传到worker即可，这个大apples_dict_bc数据集合则无需再传送到worker，大大减少时间。</p>
<ul>
<li>累加器accumulator：</li>
</ul>
<p>创建测试数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>创建accumulator累加器total，用于累加数集合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total=sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>创建accumulator累加器counter，用于计数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">counter=sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>使用foreach，对每一项都使用total累计该元素的值，counter累加已处理的元素个数，注意：counter这个accumulator变量是自增1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd.foreach(<span class="keyword">lambda</span> item:[total.add(item),counter.add(<span class="number">1</span>)])</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">total.value # 15.0</span><br><span class="line">counter.value 5</span><br></pre></td></tr></table></figure></p>
<h5 id="完整的wordcount示例"><a href="#完整的wordcount示例" class="headerlink" title="完整的wordcount示例"></a>完整的wordcount示例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="keyword">as</span> sc</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_spark_context</span>()</span></span><br><span class="line">    conf=SparkConf().setAppName(&quot;word_count&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    spark_context=sc.getOrCreate(conf)    </span><br><span class="line">    <span class="keyword">return</span> spark_context</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_count</span>(<span class="params">spark_sc,input_file,output_dir,delimiter=<span class="string">&#x27; &#x27;</span></span>):</span></span><br><span class="line">    data_rdd=spark_sc.textFile(input_file) <span class="comment"># </span></span><br><span class="line">    word_rdd=text_rdd.flatMap(<span class="keyword">lambda</span> line:line.split(delimiter))</span><br><span class="line">    count_rdd=word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> v1,v2:v1+v2)</span><br><span class="line">    count_rdd.saveAsTextFile(output_dir) <span class="comment">#注意这里参数为文件夹 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    sc_obj=create_spark_context()</span><br><span class="line">    word_count(sc_obj,<span class="string">&quot;file:///opt/data.txt&quot;</span>,<span class="string">&quot;file:///opt/word_count_output&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>查看存放的输出结果，计算结果的输出文件放在part-00000这个文件，而_SUCCESS文件是无内容的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@nn opt]<span class="comment"># ls word_count_output/</span></span><br><span class="line">part-<span class="number">00000</span>  _SUCCESS</span><br><span class="line"></span><br><span class="line">[root@nn word_count_output]<span class="comment"># cat part-00000 </span></span><br><span class="line">(<span class="string">&#x27;linux&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;is&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;the&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;best&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;centos&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;macos&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;redhat&#x27;</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3、基于PySpark和ALS的电影推荐流程"><a href="#3、基于PySpark和ALS的电影推荐流程" class="headerlink" title="3、基于PySpark和ALS的电影推荐流程"></a>3、基于PySpark和ALS的电影推荐流程</h4><p>&#8195;&#8195;本节内容参考书籍pdf版本《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。<br>&#8195;&#8195;(有一点需要指出的是：该书的作者似乎为出书而出书，在前面十来章内容，冗长且基础，大量截图以及table，其实大部分内容可言简意赅。但他们似乎为了出书为了销量，需把这本书打造“很厚，页数多，专业技术书籍”的印象，但其精华只有后面关于pyspark.mllib机器学习示例的内容。)</p>
<h5 id="数据集背景"><a href="#数据集背景" class="headerlink" title="数据集背景"></a>数据集背景</h5><p>数据源：<code>https://grouplens.org/datasets/movielens/</code><br>这里有非常详细的电影训练数据，适合项目练手<br>数据信息：<br>MovieLens 100K<br>movie ratings.<br>Stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies</p>
<p>数据样例结构：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ml-100k]# ls</span><br><span class="line">allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base  u.genre  u.occupation</span><br><span class="line">mku.sh     u1.test  u3.base  u4.test  ua.base  ub.test  u.info   u.user</span><br><span class="line">README     u2.base  u3.test  u5.base  ua.test  u.data   u.item</span><br></pre></td></tr></table></figure></p>
<p>有关数据结构的说明，可以查看README文件，例如u.data:4个字段，user id | item id | rating | timestamp.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">196     242     3       881250949</span><br><span class="line">186     302     3       891717742</span><br></pre></td></tr></table></figure>
<h5 id="读取用户数据"><a href="#读取用户数据" class="headerlink" title="读取用户数据"></a>读取用户数据</h5><p>探索基本数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_rdd=sc.textFile(<span class="string">&quot;file:///opt/ml-100k/u.data&quot;</span>)</span><br><span class="line">user_rdd.count()<span class="comment"># 100000</span></span><br><span class="line">user_rdd.first() <span class="comment"># &#x27;196\t242\t3\t881250949&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因ALS入参为3个字段，故只需取出user_rdd前3个字段的:用户id，产品id以及评分:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_rating_rdd=user_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27;\t&#x27;</span>)[:<span class="number">3</span>]) <span class="comment"># 每行分割后为一个包含4个元素的列表，取前3项即可</span></span><br><span class="line">raw_rating_rdd.take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[[<span class="string">&#x27;196&#x27;</span>, <span class="string">&#x27;242&#x27;</span>, <span class="string">&#x27;3&#x27;</span>],[<span class="string">&#x27;186&#x27;</span>, <span class="string">&#x27;302&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]] <span class="comment"># 注意，每个item是列表</span></span><br></pre></td></tr></table></figure></p>
<p>ALS训练数据格式的入参为一组元组类型的数据：Rating(user,product,rating)，过还需做以下转换<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rating_rdd=raw_rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]))<span class="comment"># x[0],x[1],x[2]对应用户id，电影id，评分</span></span><br><span class="line">rating_rdd.take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;196&#x27;</span>, <span class="string">&#x27;242&#x27;</span>, <span class="string">&#x27;3&#x27;</span>), (<span class="string">&#x27;186&#x27;</span>, <span class="string">&#x27;302&#x27;</span>, <span class="string">&#x27;3&#x27;</span>)]<span class="comment"># rdd的每个item为元组类型</span></span><br></pre></td></tr></table></figure></p>
<p>查看不重复的用户总量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_users=rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>]).distinct().count()</span><br><span class="line">total_users <span class="comment"># 943</span></span><br></pre></td></tr></table></figure>
<p>查看不重复的电影总量（同上）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">total_moves&#x3D;rating_rdd.map(lambda x:x[1]).distinct().count()</span><br><span class="line">total_moves # 1682</span><br></pre></td></tr></table></figure>
<h5 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h5><p>大致处理流程：读取文件=&gt;user_rdd=&gt;raw_rating_rdd=&gt;rating_rdd，这里rating_rdd的格式就是ALS训练数据的格式Rating(user,product,rating)，然后再用ALS.train，训练结束后，就会创建模型对象MatrixFactorizationModel</p>
<p><strong>这里简单介绍ALS算法</strong>：Alternating Least Squares matrix factorization，其实就是（交替）最小二乘法，这里为何使用ALS？因为它同时考虑了User和Item两个方面，即即可基于用户进行推荐又可基于物品，所以适合推荐型的场景，模型一般如下：<br><img src="https://img-blog.csdnimg.cn/20200109205253156.png" alt="Am×n=Um×k×Vk×n"><br>原始协同矩阵是一个<code>m*n</code>的矩阵，是由m<em>k和k</em>n两个矩阵相乘得到的，其中k&lt;&lt;m,n，U表示用户矩阵，V表示商品矩阵，k为U、V矩阵的的秩。学过线性代数应该知道<code>A*B=C</code>，两个矩阵相乘的结果，这就是所谓协同矩阵。<br><img src="https://img-blog.csdnimg.cn/20200109204624664.png" alt="在这里插入图片描述"><br>协同推荐就等同于<code>C=A*B</code>矩阵分解，矩阵分解（协同推荐矩阵是一个稀疏矩阵，因为不是所有的用户都对产品评分）最终又可以转换成了一个优化问题。将用户u对商品V的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。在这个矩阵分解的训练过程中，评分缺失项得到了填充，那么这个填充的项就可以根据用户ID进行推荐。<br>更详细内容可以参考这两篇文章：<a href="https://blog.csdn.net/YMPzUELX3AIAp7Q/article/details/85241209">文章1</a>、<a href="https://www.cnblogs.com/xiguage119/p/10813393.html">文章2</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"><span class="comment"># 注意ALS算法是基于矩阵运算，因此需要环境安装numpy库</span></span><br></pre></td></tr></table></figure>
<p><code>ALS.train(ratings,rank,iterations=5,lambda_=0.01)</code><br>ratings:训练数据集合，就是上面提到的Rating(user,product,rating)，也即是rating_rdd这个经过预处理的数据集</p>
<p>一句完成训练：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=ALS.train(rating_rdd,<span class="number">10</span>,<span class="number">10</span>,<span class="number">0.01</span>)</span><br><span class="line">model<span class="comment"># &lt;pyspark.mllib.recommendation.MatrixFactorizationModel at 0x7f3159bc8048&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>该模型对象有几个属性：<br>model.rank # 10 分解为稀疏矩阵的秩<br>userFeatures 为分解后的用户矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.userFeatures().take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="number">1</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.7229161262512207</span>, <span class="number">0.036963045597076416</span>, <span class="number">0.23517486453056335</span>, -<span class="number">0.18118669092655182</span>, -<span class="number">1.4776617288589478</span>, -<span class="number">1.0425325632095337</span>, <span class="number">0.3823653757572174</span>, -<span class="number">0.3569445312023163</span>, -<span class="number">0.2874303162097931</span>, <span class="number">0.0020452593453228474</span>])),</span><br><span class="line"> (<span class="number">2</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.3199065327644348</span>, <span class="number">0.41293472051620483</span>, <span class="number">0.12430011481046677</span>, -<span class="number">0.42582616209983826</span>, -<span class="number">0.4546814560890198</span>, -<span class="number">1.496929407119751</span>, <span class="number">0.6246935725212097</span>, <span class="number">0.49794384837150574</span>, -<span class="number">0.3813674747943878</span>, <span class="number">0.7599969506263733</span>]))]</span><br></pre></td></tr></table></figure>
<p>productFeatures为分解后的电影（产品）矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.productFeatures().take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="number">1</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.9663546681404114</span>, <span class="number">0.0724567249417305</span>, <span class="number">0.22562265396118164</span>, -<span class="number">0.14772379398345947</span>, -<span class="number">1.3601692914962769</span>, -<span class="number">1.1434344053268433</span>, <span class="number">1.0299423933029175</span>, -<span class="number">0.17817920446395874</span>, -<span class="number">1.0483288764953613</span>, <span class="number">0.4326847195625305</span>])),</span><br><span class="line"> (<span class="number">2</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.701686441898346</span>, -<span class="number">0.44971194863319397</span>, <span class="number">0.36079081892967224</span>, -<span class="number">0.1727607101202011</span>, -<span class="number">0.4821830689907074</span>, -<span class="number">1.1037342548370361</span>, <span class="number">0.8413264155387878</span>, -<span class="number">0.08249323815107346</span>, -<span class="number">1.0539320707321167</span>, <span class="number">0.6040329337120056</span>]))]</span><br></pre></td></tr></table></figure>
<h5 id="调用已训练的模型"><a href="#调用已训练的模型" class="headerlink" title="调用已训练的模型"></a>调用已训练的模型</h5><p>model已经封装好几个常用的方法，api使用简便<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Signature: model.recommendProducts(user, num)</span><br><span class="line">Docstring:</span><br><span class="line">Recommends the top &quot;num&quot; number of products for a given user and</span><br><span class="line">returns a list of Rating objects sorted by the predicted rating in</span><br><span class="line">descending order.</span><br></pre></td></tr></table></figure></p>
<p>例如给用户199推荐前5部电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.recommendProducts(<span class="number">199</span>,<span class="number">5</span>)</span><br><span class="line">[Rating(user=<span class="number">199</span>, product=<span class="number">854</span>, rating=<span class="number">10.774026140227157</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">962</span>, rating=<span class="number">9.30074590770409</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">1176</span>, rating=<span class="number">8.813180359193545</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">1280</span>, rating=<span class="number">8.11317788460314</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">718</span>, rating=<span class="number">7.8722593701756995</span>)]</span><br></pre></td></tr></table></figure>
<p>这个结果表示，rating值越大，越排在越前面，代表更为优先推荐，首先推荐给用户199的为854这部电影<br>根据用户ID:199和电影ID:854，查询预测评分:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(<span class="number">199</span>,<span class="number">854</span>) <span class="comment"># 10.774026140227157</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用用得更多的场合是：将某部电影推荐给感兴趣的用户，可通过model.recommendUsers得出这些用户，例如，将电影ID为154，推荐给前10个用户</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.recommendUsers(<span class="number">154</span>,<span class="number">10</span>)</span><br><span class="line">输出：</span><br><span class="line">[Rating(user=<span class="number">133</span>, product=<span class="number">154</span>, rating=<span class="number">6.346890714591231</span>),</span><br><span class="line"> Rating(user=<span class="number">866</span>, product=<span class="number">154</span>, rating=<span class="number">6.10978058348641</span>),</span><br><span class="line"> Rating(user=<span class="number">50</span>, product=<span class="number">154</span>, rating=<span class="number">6.018355541192427</span>),</span><br><span class="line"> Rating(user=<span class="number">783</span>, product=<span class="number">154</span>, rating=<span class="number">5.991043569104054</span>),</span><br><span class="line"> Rating(user=<span class="number">310</span>, product=<span class="number">154</span>, rating=<span class="number">5.658875199814674</span>),</span><br><span class="line"> Rating(user=<span class="number">809</span>, product=<span class="number">154</span>, rating=<span class="number">5.636975519395109</span>),</span><br><span class="line"> Rating(user=<span class="number">78</span>, product=<span class="number">154</span>, rating=<span class="number">5.4898250475467725</span>),</span><br><span class="line"> Rating(user=<span class="number">762</span>, product=<span class="number">154</span>, rating=<span class="number">5.47223950904501</span>),</span><br><span class="line"> Rating(user=<span class="number">273</span>, product=<span class="number">154</span>, rating=<span class="number">5.318862413529849</span>),</span><br><span class="line"> Rating(user=<span class="number">264</span>, product=<span class="number">154</span>, rating=<span class="number">5.295430734770273</span>)]</span><br></pre></td></tr></table></figure>
<p>可以快速得出对电影ID为154最感兴趣的前10个用户，不过在推荐的信息里面，看不到电影名称，还需关联电影名的数据，从而形成完整的推荐信息。</p>
<p>加载电影详情数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">move_info_rdd=sc.textFile(<span class="string">&quot;file:///opt/ml-100k/u.item&quot;</span>)</span><br><span class="line">move_info_rdd.take(<span class="number">3</span>)</span><br><span class="line">输出：</span><br><span class="line">[<span class="string">&#x27;1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>查看u.item电影详情表的字段说明，总共有19个字段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u.item     -- Information about the items (movies); this <span class="keyword">is</span> a tab separated</span><br><span class="line">              <span class="built_in">list</span> of</span><br><span class="line">              movie <span class="built_in">id</span> | movie title | release date | video release date |</span><br><span class="line">              IMDb URL | unknown | Action | Adventure | Animation |</span><br><span class="line">              Children<span class="string">&#x27;s | Comedy | Crime | Documentary | Drama | Fantasy |</span></span><br><span class="line"><span class="string">              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |</span></span><br><span class="line"><span class="string">              Thriller | War | Western |</span></span><br></pre></td></tr></table></figure>
<p>作为测试，无需使用全部字段，只需挑出感兴趣的字段即可：电影id，电影名，url</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">move_splited_rdd=move_info_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取3个字段，将转为map类型，name:电影名，url：电影ur </span></span><br><span class="line">func=<span class="keyword">lambda</span> a_list:(<span class="built_in">int</span>(a_list[<span class="number">0</span>]),<span class="string">&#x27;name:%s,url:%s&#x27;</span>%(a_list[<span class="number">1</span>],a_list[<span class="number">4</span>]))</span><br><span class="line">move_map_info_rdd=move_splited_rdd.<span class="built_in">map</span>(func).collectAsMap() <span class="comment">#move_map_info_rdd 已经是字典类</span></span><br><span class="line">print(move_map_info_rdd)</span><br><span class="line"><span class="comment"># python字典类型的电影信息</span></span><br><span class="line">&#123;<span class="number">1</span>: <span class="string">&#x27;name:Toy Story (1995) url:http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)&#x27;</span>,</span><br><span class="line"> <span class="number">2</span>: <span class="string">&#x27;name:GoldenEye (1995) url:http://us.imdb.com/M/title-exact?GoldenEye%20(1995)&#x27;</span>,</span><br><span class="line"> ......</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>move_map_info_rdd的key就是电影ID，因此只需要关联model.recommendUsers(154,10)输出的<code>Rating(user=133, product=154, rating=6.346890714591231),</code> product id，即可输出完整的推荐信息如下：<br>给用户id为199的用户推荐3部电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result=model.recommendProducts(<span class="number">199</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">f&#x27;user:<span class="subst">&#123;r.user&#125;</span>,moveid:<span class="subst">&#123;r.product&#125;</span>,move_info:<span class="subst">&#123;move_map_info_rdd[r.product]&#125;</span>,rating:<span class="subst">&#123;r.rating&#125;</span>&#x27;</span>)</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user:199,moveid:854,move_info:name:Bad Taste (1987) url:http://us.imdb.com/M/title-exact?Bad%20Taste%20(1987),rating:10.774026140227157</span><br><span class="line"></span><br><span class="line">user:199,moveid:962,move_info:name:Ruby in Paradise (1993) url:http://us.imdb.com/M/title-exact?Ruby%20in%20Paradise%20(1993),rating:9.30074590770409</span><br><span class="line"></span><br><span class="line">user:199,moveid:1176,move_info:name:Welcome To Sarajevo (1997) url:http://us.imdb.com/M/title-exact?Welcome+To+Sarajevo+(1997),rating:8.813180359193545</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>将model持久化到本地后，再封装为完整的逻辑，方便重新使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(sc,<span class="string">&#x27;/opt/ml-100k/asl_model&#x27;</span>) <span class="comment"># sc为spark程序开头的spark context</span></span><br><span class="line"><span class="comment"># 若再次存储再会提示出错，所以一般是这么用：</span></span><br><span class="line"><span class="keyword">try</span>：</span><br><span class="line">    model.save(sc,path)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>model以一个目录的形式保存，而且还保存了user和product的数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ml-100k]# tree asl_model/</span><br><span class="line">asl_model/</span><br><span class="line">├── data</span><br><span class="line">│   ├── product</span><br><span class="line">│   │   ├── part-00000-bf34d65a-81e8-4124-a254-6e6044b8da2d-c000.snappy.parquet</span><br><span class="line">│   │   └── _SUCCESS</span><br><span class="line">│   └── user</span><br><span class="line">│       ├── part-00000-3953175d-e560-42a5-8de3-fcc86a4b625c-c000.snappy.parquet</span><br><span class="line">│       └── _SUCCESS</span><br><span class="line">└── metadata</span><br><span class="line">    ├── part-00000</span><br><span class="line">    └── _SUCCESS</span><br></pre></td></tr></table></figure>
<p>如何加载已训练好的本地模型？使用load方法即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load(sc,<span class="string">&#x27;/opt/ml-100k/asl_model&#x27;</span>) <span class="comment"># path为</span></span><br></pre></td></tr></table></figure>
<h5 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h5><p>将以上的处理流程封装类，便于调用。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="keyword">as</span> sc</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"><span class="keyword">import</span> os,datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MoveRecommend</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,model_path,user_path,move_path,app_name=<span class="string">&quot;move_recommend&quot;</span>,master=<span class="string">&quot;local[*]&quot;</span></span>):</span></span><br><span class="line">        self.app_name=app_name</span><br><span class="line">        self.master=master</span><br><span class="line">        self.sc=self.create_spark_context()</span><br><span class="line">        self.train_rank=<span class="number">10</span> <span class="comment"># 稀疏矩阵分解的秩</span></span><br><span class="line">        self.train_iter=<span class="number">10</span> <span class="comment"># 迭代次数</span></span><br><span class="line">        self.train_lambda=<span class="number">0.01</span> <span class="comment"># 正则化参数(惩罚因子)        </span></span><br><span class="line">        self.user_path=user_path </span><br><span class="line">        self.move_path=move_path</span><br><span class="line">        self.model_path=model_path</span><br><span class="line">        self.model=self.get_model()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_spark_context</span>(<span class="params">self</span>):</span></span><br><span class="line">        conf=SparkConf().setAppName(self.app_name).setMaster(self.master)</span><br><span class="line">        spark_context=sc.getOrCreate(conf)    </span><br><span class="line">        <span class="keyword">return</span> spark_context</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;如果给定的目录没有model，则重新训练model，如果已有model，则直接加载使用&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(self.model_path):</span><br><span class="line">            print(<span class="string">f&#x27;model not found,start traing at <span class="subst">&#123;self.get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> self.train_and_save()</span><br><span class="line">        <span class="keyword">return</span> model.load(self.sc,self.model_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_and_save</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;只用训练集，训练model并持久化到本地目录&quot;&quot;&quot;</span></span><br><span class="line">        user_rdd=self.sc.textFile(<span class="string">&quot;file://&quot;</span>+self.user_path)</span><br><span class="line">        raw_rating_rdd=user_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27;\t&#x27;</span>)[:<span class="number">3</span>]) <span class="comment"># 每行分割后为一个包含4个元素的列表，取前3项即可</span></span><br><span class="line">        rating_rdd=raw_rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]))<span class="comment"># x[0],x[1],x[2]对应用户id，电影id，评分</span></span><br><span class="line">        model=ALS.train(rating_rdd,self.train_rank,self.train_iter,self.train_lambda)</span><br><span class="line">        model.save(self.sc,self.model_path)</span><br><span class="line">        print(<span class="string">f&#x27;model training done at <span class="subst">&#123;self.get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> model </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_move_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个字典列表，每个字典存放3个电影详情字段&quot;&quot;&quot;</span>        </span><br><span class="line">        move_info_rdd=self.sc.textFile(<span class="string">&quot;file://&quot;</span>+self.move_path)</span><br><span class="line">        move_splited_rdd=move_info_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line">        <span class="comment"># 提取3个字段，将转为map类型，name:电影名，url：电影ur </span></span><br><span class="line">        func=<span class="keyword">lambda</span> a_list:(<span class="built_in">int</span>(a_list[<span class="number">0</span>]),<span class="string">&#x27;name:%s,url:%s&#x27;</span>%(a_list[<span class="number">1</span>],a_list[<span class="number">4</span>]))</span><br><span class="line">        move_map_info_rdd=move_splited_rdd.<span class="built_in">map</span>(func).collectAsMap() <span class="comment">#move_map_info_rdd 已经是字典类 </span></span><br><span class="line">        <span class="keyword">return</span> move_map_info_rdd</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend_product_by_userid</span>(<span class="params">self,user_id,num=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据给定用户id，向其推荐top N部电影&quot;&quot;&quot;</span>                </span><br><span class="line">        result= self.model.recommendProducts(user_id,num)</span><br><span class="line">        move_dict=self.get_move_dict()</span><br><span class="line">        <span class="keyword">return</span> [(r.user,r.product,move_dict[r.product],r.rating) <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend_user_by_moveid</span>(<span class="params">self,move_id,num=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据给定电影ID，推荐对该电影感兴趣的top N 个用户&quot;&quot;&quot;</span>     </span><br><span class="line">        result=self.model.recommendUsers(move_id,num)</span><br><span class="line">        move_dict=self.get_move_dict()</span><br><span class="line">        <span class="keyword">return</span> [(r.user,r.product,move_dict[r.product],r.rating) <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>调用：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m=MoveRecom(model_path=<span class="string">&#x27;/opt/ml-100k/costom_model&#x27;</span>,user_path=<span class="string">&#x27;/opt/ml-100k/u.data&#x27;</span>,move_path=<span class="string">&#x27;/opt/ml-100k/u.item&#x27;</span>)</span><br></pre></td></tr></table></figure><br>输出训练时间：<br>model not found,start traing at 26:45<br>model training done at 27:06</p>
<h5 id="项目难点说明"><a href="#项目难点说明" class="headerlink" title="项目难点说明"></a>项目难点说明</h5><p>&#8195;&#8195;上面的例子只是给出demo流程，而且数据已准备，但如果针对实际项目，则需要你处理以下两个主要难点：<br>（1） 训练数据的获取、整理和加工，并将这一流程自动化。<br>（2）模型的训练，以及根据新数据重新训练模型，以保证模型推荐效果最优，并将这一流程自动化。<br>&#8195;&#8195;至于其他工作，例如web 层面的开发，以及Apps或者说底层数据的存储，对于全栈开发者来说，并无大碍，只是需要耗费更多精力而已。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本文给出了较为入门的基于PySpark实现的推荐类的业务流程，该逻辑其实是离线的模式：训练数据已经加工好，模型训练也没有进行深度调优。事实上，如果将其作为一个生产可用项目来实施，需将大数据生态圈相关技术栈以及web 开发进行整合，此类项目的架构设计一般有下面三部分：</p>
<ul>
<li>需推荐的业务数据（包括训练集和测试集）收集、计算、存储：大数据生态圈相关技术栈实现</li>
<li>模型训练方面：离线存储PySpark计算后生成的训练模型，而且需要定时训练和更新该模型文件，以便保持最优模型。</li>
<li>以web api的方式提供推荐数据：为BI或者其他应用以get、post的方式提供推荐数据，例如post一个用户ID，返回相应的推荐条目</li>
</ul>
<p>以下简要说明两种基本架构图：<br><strong>第一种：适合数据量不大，几个节点组成的小型“大数据”服务</strong><br><img src="https://img-blog.csdnimg.cn/20200111095444386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;这种架构较为简单，数据源本身已经存储在各个业务的原有数据库中或者日志文件，开发者无需借助hadoop存储组件，自行实现数据源抽取模块，接着只需PySpark读取这些数据并训练成模型文件即可，模型文件管理可以通过定时训练更新，最后通过web API的形式为上层应用提供推荐或者匹配记录。<br>需要注意的是：构建web API方式这里用了Python栈，当然可用Java栈或者Go栈</p>
<p><strong>第二种：适合数据量大的中大型大数据服务</strong><br><img src="https://img-blog.csdnimg.cn/20200111100116354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;此类架构适合那些几十GB到几百GB级别甚至是TB级别的分布式大数据节点集群，此类场景需引入hadoop相关生态圈的技术栈，用于处理大量属鸡的存储和计算：Flume、Kafka、HBase、Hive，在计算层提供分布式的Spark组件支撑离线模型计算。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>PySpark推荐</tag>
      </tags>
  </entry>
  <entry>
    <title>构建高可用Hive HA和整合HBase开发环境（一）</title>
    <url>/2019/11/14/%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hive%20HA%E5%92%8C%E6%95%B4%E5%90%88HBase%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面的项目中，已经实现了HadoopHA、HBaseHA，本文将加入Hive数据仓库工作，并整合HBase，实现完整的大数据开发项目所具备的开发环境，为后面博客关于数据应用层开发提供支撑。</p>
<h3 id="1、Hive-Requirements"><a href="#1、Hive-Requirements" class="headerlink" title="1、Hive Requirements"></a>1、Hive Requirements</h3><p>​    按官网给出的基本环境</p>
<ul>
<li>Java 1.7：  <a href="https://issues.apache.org/jira/browse/HIVE/fixforversion/12329345/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel">Hive versions1.2</a> onward require Java 1.7 or newer. java1.7或更高版本</li>
<li>Hadoop 2.x (preferred)：推荐hadoop2.x版本</li>
</ul>
<a id="more"></a>
<p>hive安装包可在清华镜像源拉取：<code>https://mirrors.tuna.tsinghua.edu.cn/apache/hive/</code></p>
<p>目前stable版本为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apache-hive-2.3.6-bin.tar.gz 2019-08-23 02:53  221M </span><br></pre></td></tr></table></figure>
<p>最新版为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apache-hive-3.1.2-bin.tar.gz 2019-08-27 04:20  266M  </span><br></pre></td></tr></table></figure>
<p>如何定位自身Hadoop版本与hive版本的兼容呢?</p>
<p>例如本blog前面部署hadoop3.1.2，可通过在hive官网查看其对应的版本</p>
<p><code>http://hive.apache.org/downloads.html</code>，官网给出的news：</p>
<blockquote>
<p> 26 August 2019: release 3.1.2 available</p>
<p>This release works with Hadoop 3.x.y.</p>
</blockquote>
<p> hive3.1.2版本支持hadoop3.x.y版本，结合本blog内容，这里使用hive3.1.2：</p>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/">安装包下载地址</a></p>
<p>从hive官网给出的hadoop版本兼容可以看出hive2.x.y一般是兼容hadoop2.x.y</p>
<h3 id="2、Hive-环境部署"><a href="#2、Hive-环境部署" class="headerlink" title="2、Hive 环境部署"></a>2、Hive 环境部署</h3><h4 id="2-1-配置环境变量"><a href="#2-1-配置环境变量" class="headerlink" title="2.1 配置环境变量"></a>2.1 配置环境变量</h4><p>hive安装包所在路径，个人习惯将所有大数据组件放在/opt目录下，方便管理和配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# pwd</span><br><span class="line">/opt/hive-3.1.2</span><br><span class="line"></span><br><span class="line">[root@nn hive-3.1.2]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 追加到文件后面</span></span><br><span class="line">export HIVE_HOME=/opt/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@nn hive-3.1.2]# source /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看hive版本</span></span><br><span class="line">[root@nn hive-3.1.2] hive --version</span><br><span class="line">Hive 3.1.2</span><br><span class="line">Git git://HW13934/Users/gates/tmp/hive-branch-3.1/hive -r 8190d2be7b7165effa62bd21b7d60ef81fb0e4af</span><br><span class="line">Compiled by gates on ** PDT 2019</span><br><span class="line">From source with checksum 0492c08f784b188c349f6afb1d8d9847</span><br></pre></td></tr></table></figure>
<h4 id="2-2-配置hive-env-sh和hive-site-xml"><a href="#2-2-配置hive-env-sh和hive-site-xml" class="headerlink" title="2.2 配置hive-env.sh和hive-site.xml"></a>2.2 配置hive-env.sh和hive-site.xml</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# cp hive-default.xml.template  hive-site.xml</span><br><span class="line">[root@nn conf]# cp hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">[root@nn conf]# vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在文件最后修改</span></span><br><span class="line">HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export HIVE_CONF_DIR=/opt/hive-3.1.2/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/opt/hive-3.1.2/lib</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Hive-site.xm的配置项比较多，自带模板文件内容长达6900多行，仅给出重要的设置项，其他属性的设置以及描述可参考<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2">官网</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!--元数据库的mysql的配置项--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://nn:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;py_ab2018&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.readOnlyDatastore&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateTables&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateColumns&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line">    &lt;!--zookeeper的有关设置--&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">      &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hiveserver2_zk&lt;/value&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.zookeeper.publish.configs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--hiveserver2配置，可使得外部客户端使用thrift RPC协议连接远程hive--&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.client.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.client.password&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;py_ab2018&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--binary对应TCP协议，也可配成http协议--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.transport.mode&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;binary&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--thriftserver对外限制最大最小连接数--&gt;  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.min.worker.threads&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--有关日志文件--&gt;    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/scratchdir&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/resources&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.querylog.location&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/querylog&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/operation-log&lt;/value&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>注意因为hadoop做了HA配置，因此以上的配置需要在主nn节点和backup dn2节点配置上，==在第7.1章节内容将会给出hiveserver2的相关内容==。</p>
<h4 id="2-3-配置Hive的运行日志"><a href="#2-3-配置Hive的运行日志" class="headerlink" title="2.3 配置Hive的运行日志"></a>2.3 配置Hive的运行日志</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">[root@nn hive-3.1.2]# mkdir logs</span><br><span class="line"></span><br><span class="line">[root@nn conf]# cp hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line">[root@nn conf]# vi hive-log4j2.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">property.hive.log.dir = /root/hive-3.1.2/logs</span><br></pre></td></tr></table></figure>
<h4 id="2-4-加入mysql-connector"><a href="#2-4-加入mysql-connector" class="headerlink" title="2.4  加入mysql connector"></a>2.4  加入mysql connector</h4><p>hive需用通过jdbc连接mysql，该jar需自行下载，并将其拷贝至以下目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# cp mysql-connector-java-5.1.32-bin.jar /opt/hive-3.1.2/lib/</span><br></pre></td></tr></table></figure>
<h4 id="2-5-在mysql建表"><a href="#2-5-在mysql建表" class="headerlink" title="2.5 在mysql建表"></a>2.5 在mysql建表</h4><p>其实这里无需在msyql建表，因为hive-site.xml文件里面已经配置为自动创建元数据库表，hive做初始化时会自动创建。也即本节内容可以忽略。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; create database hive default character set utf8 collate utf8_general_ci</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| hive               |</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> grant all on hive.* to <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;py_ab2018&#x27;</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 容许本地访问,否则hive的schema初始化将无法访问msyql</span></span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;nn&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;localhost&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;127.0.0.1&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt;  select host,user,authentication_string from mysql.user;  +-----------+--------+-----------------------+</span><br><span class="line">| host      | user   | authentication_string |</span><br><span class="line">+-----------+--------+-----------------------+</span><br><span class="line">| localhost | root   |                       |</span><br><span class="line">| nn        | root   |                       |</span><br><span class="line">| 127.0.0.1 | root   |                       |</span><br><span class="line">| ::1       | root   |                       |</span><br><span class="line">| nn        | hive   |                       |</span><br><span class="line">| %         | hadoop |                       |</span><br><span class="line">| %         | hive   |                       |</span><br><span class="line">| localhost | hive   |                       |</span><br><span class="line">| 127.0.0.1 | hive   |                       |</span><br><span class="line">+-----------+--------+-----------------------+</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> <span class="built_in">exit</span>;(quit;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="2-6-初始化hive-schema"><a href="#2-6-初始化hive-schema" class="headerlink" title="2.6 初始化hive schema"></a>2.6 初始化hive schema</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# schematool  -initSchema -dbType mysql </span><br><span class="line">Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">[root@nn hive-3.1.2]# </span><br></pre></td></tr></table></figure>
<h4 id="2-7-在mysql上查看hive创建的元表"><a href="#2-7-在mysql上查看hive创建的元表" class="headerlink" title="2.7 在mysql上查看hive创建的元表"></a>2.7 在mysql上查看hive创建的元表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; use hive</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">MariaDB [hive]&gt; show tables;</span><br><span class="line">| AUX_TABLE                     |</span><br><span class="line">| BUCKETING_COLS                |</span><br><span class="line">| CDS                           |</span><br><span class="line">| COLUMNS_V2                    |</span><br><span class="line">| COMPACTION_QUEUE              |</span><br><span class="line">| COMPLETED_COMPACTIONS         |</span><br><span class="line">| COMPLETED_TXN_COMPONENTS      |</span><br><span class="line">| CTLGS                         |</span><br><span class="line">| DATABASE_PARAMS               |</span><br><span class="line">| DBS                           |</span><br><span class="line">| DB_PRIVS                      |</span><br><span class="line">| DELEGATION_TOKENS             |</span><br><span class="line">| FUNCS                         |</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>
<h4 id="2-8-启动hive"><a href="#2-8-启动hive" class="headerlink" title="2.8 启动hive"></a>2.8 启动hive</h4><p>启动hive之前，务必hadoop服务已经启动，若hadoop为HA结构，必须其中一个namenode节点为active节点，例如本项目中，hadoopHA为：nn和dn2都作为namenode节点。</p>
<p>除此之外，还需手动在hdfs上创建hive的工作目录：这里官方的说明如下</p>
<p>In addition, you must use below HDFS commands to create <code>/tmp</code> and <code>/user/hive/warehouse</code> (aka <code>hive.metastore.warehouse.dir</code>) and set them <code>chmod g+w</code> before you can create a table in Hive.</p>
<p>以下就是对/tmp加入group写权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir -p &#x2F;tmp&#x2F;hive</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse</span><br></pre></td></tr></table></figure>
<p>warehouse目录下放置的就是表对应的数据文件，在后面的章节会提供说明</p>
<p>启动hive，该命令是指启动hive cli，就像mysql shell<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# hive</span><br><span class="line">Hive Session ID = 627577c0-2560-4318-92af-bc2512f91d3b</span><br><span class="line"><span class="meta">hive&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>以上说明hive部署成功，jps可以看到多了一个RunJar进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# jps</span><br><span class="line">13042 QuorumPeerMain</span><br><span class="line">20163 JournalNode</span><br><span class="line">19780 NameNode</span><br><span class="line">20709 Jps</span><br><span class="line">19499 DFSZKFailoverController</span><br><span class="line">20299 RunJar</span><br><span class="line">19918 DataNode</span><br></pre></td></tr></table></figure>
<p>==启动过程可能遇到问题==：</p>
<p>1）启动hive会有一个多重绑定的提示</p>
<p>SLF4J: Class path contains multiple SLF4J bindings.<br>SLF4J: Found binding in [jar:file:/opt/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: Found binding in [jar:file:/opt/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p>
<p>原因：</p>
<p>hadoop/common/lib有个slf4j-log4j的jar包，hive的lib下也有一个slf4j-log4j</p>
<p>那么在环境变量/etc/profile都配置两者的环境，hive启动后，会找到两个slf4j-log4j，因此提示多重绑定</p>
<p>解决办法：</p>
<p>保留hadoop/common/lib有个slf4j-log4j的jar包，将hive lib目录下的slf4j-log4j重命名即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn lib]# mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak</span><br></pre></td></tr></table></figure>
<p>==注意==：<br>当这个hive的日志jar包去掉后，hive日志模式将默认使用hadoop的日志配置，启动hive cli或者在hive cli上执行任何命令时都会不断打印出日志，如果需要进程在hive cli操作数据，那么建议保留hive的log4j包。如果使用外部可视化数据库管理客户端连接hive，那么可删除之。</p>
<p>2） hive在hdfs的/tmp/hive不具有写权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rwxrwxr-x</span><br></pre></td></tr></table></figure>
<p>将用户组以及其他用户加入可读可写可执行权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -chmod -R 777 &#x2F;tmp</span><br></pre></td></tr></table></figure>
<h3 id="3、hive建表测试"><a href="#3、hive建表测试" class="headerlink" title="3、hive建表测试"></a>3、hive建表测试</h3><p>HQL语句跟SQL差别不大，若对sql非常熟悉，HQL拿来即用。相关用法参考官网：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-DDLOperations">DDL语句</a>、<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SQLOperations">HQL查询用法</a></p>
<h4 id="3-1-创建一个员工表"><a href="#3-1-创建一个员工表" class="headerlink" title="3.1 创建一个员工表"></a>3.1 创建一个员工表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create table if not exists emp(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">sexual string,</span><br><span class="line">depart_id int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by&#x27;\t&#x27;</span><br><span class="line">stored as textfile;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc emp;</span></span><br><span class="line">OK</span><br><span class="line">id                      int                                         </span><br><span class="line">name                    string                                      </span><br><span class="line">age                     int                                         </span><br><span class="line">sexual                  string                                      </span><br><span class="line">depart_id               int                                         </span><br><span class="line">Time taken: 0.263 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure>
<p>员工表的本地数据emp.txt</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1 	Aery	25	Male    1</span><br><span class="line">2 	Bery	23	Female	2</span><br><span class="line">3	Cery	26	Female	3</span><br><span class="line">4	Dery	27	Male		2</span><br></pre></td></tr></table></figure>
<h4 id="3-2-hive-cli导入测试文本数据"><a href="#3-2-hive-cli导入测试文本数据" class="headerlink" title="3.2 hive cli导入测试文本数据"></a>3.2 hive cli导入测试文本数据</h4><p>上面创建一个emp.txt文本数据，若要使用hive将其映射为一张表，需要将数据文件上传到hdfs，hive已经提供相关命令进行此类文件数据的上传操作。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/opt/hive-3.1.2/test_data/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line"></span><br><span class="line">Loading data to table default.emp</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.768 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from emp;</span></span><br><span class="line">OK</span><br><span class="line">1       Aery    25      Male    1</span><br><span class="line">2       Bery    23      Female  2</span><br><span class="line">3       Cery    26      Femalei 3</span><br><span class="line">4       Dery    27      Male    2</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from emp a <span class="built_in">where</span> a.name=<span class="string">&#x27;Dery&#x27;</span>;</span></span><br><span class="line">OK</span><br><span class="line">4       Dery    27      Male    2</span><br><span class="line">Time taken: 0.327 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></p>
<p>hive会把本地数据上传到hdfs文件系统上具体路径如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# hdfs dfs -ls /user/hive/warehouse/emp</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root root         73 ** /user/hive/warehouse/emp/emp.txt</span><br></pre></td></tr></table></figure>
<p>从上面可知，hive建的表默认放在hdfs的warehouse目录下，而且上传的用户数据文件放在相应的表名字目录下。</p>
<h4 id="3-3-加载hdfs上的数据"><a href="#3-3-加载hdfs上的数据" class="headerlink" title="3.3  加载hdfs上的数据"></a>3.3  加载hdfs上的数据</h4><p>除了可以直接在hive cli里加载本地数据，也可先把本地数据上传到hdfs上，再通过hive加载</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn test_data]# hdfs dfs -put emp.txt /tmp</span><br><span class="line">[root@nn test_data]# hdfs dfs -ls /tmp</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   3 root supergroup         73 ** /tmp/emp.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先清空之前的数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> truncate table emp;</span></span><br><span class="line">OK</span><br><span class="line">Time taken: 0.957 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> hive导入hdfs的数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">&#x27;/tmp/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">&#x27;/tmp/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line">Loading data to table default.emp</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.593 seconds</span><br><span class="line"></span><br><span class="line">hive导入本地文件所需的实际为：1.768 s，是hdfs导入的3倍。</span><br></pre></td></tr></table></figure>
<p>==todo==<br>hive 按分区上传，上传的数据会指定在相应的分区上<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">hive按分区删除数据：</span><br><span class="line">alter table table_name drop partition (partition_name=&#x27;分区名&#x27;)</span><br></pre></td></tr></table></figure></p>
<h3 id="4、为何使用Hive？"><a href="#4、为何使用Hive？" class="headerlink" title="4、为何使用Hive？"></a>4、为何使用Hive？</h3><p>&#8195;&#8195;前面的内容为hive环境构建及其测试，那么在大数据开发项目中，为何要引入Hive组件？</p>
<h4 id="4-1-无Hive组件的大数据处理"><a href="#4-1-无Hive组件的大数据处理" class="headerlink" title="4.1 无Hive组件的大数据处理"></a>4.1 无Hive组件的大数据处理</h4><p>&#8195;&#8195;从本人博客前面几篇关于大数据组件部署和技术架构解析的blog可以了解到，若没有Hive这样的组件，<br>&#8195;&#8195;当需要从hdfs的原始数据做高级数据分析时，首先肯定需要使用java写MapReduce程序，如果再加入Spark分布式内存计算引擎，那么还需使用Scala语言写spark程序（或者使用python写pyspark）。事实上，MapReduce的程序写起来比较繁琐（注意：不是难），占据大量工作和时间。对于大部分数据开发人员（含数据分析），其实更关心的是把这些海量数据“统一处理”后，最终的呈现的数据是否有价值或者提供商业决策。若无Hive这样的组件，整个项目组将耗费大量的人力去开发更低层MapReduce程序，无论业务逻辑简单与否（虽然极其复杂的业务数据需要可能还是得写MP程序才能完成）。</p>
<h4 id="4-2-Hive组件在大数据分析与处理上的优势"><a href="#4-2-Hive组件在大数据分析与处理上的优势" class="headerlink" title="4.2 Hive组件在大数据分析与处理上的优势"></a>4.2 Hive组件在大数据分析与处理上的优势</h4><p>&#8195;&#8195;在大数据处理和分析中，能否有个更高层更抽象的语言层来描述算法和数据处理流程，就像传统数据库的SQL语句。Apache项目大神早已考虑到：传统数据库的数据分析与处理，每个人都在用SQL即可完成各自分析任务，这种方式在大数据hadoop生态必须给予引入。于是就有了Pig和Hive。Pig是接近脚本方式去描述MapReduce，Hive则用的是SQL，它们把脚本和SQL语言翻译成MapReduce程序，然后再丢给底层的MapReduce或者spark计算引擎去计算。也就是说，大数据开发人员只需要用更直观易懂、大家都熟悉的SQL语言去写大数据job即可完成绝大部分MapReduce任务，而且项目组的非计算机背景工作人员也可直接通过写SQL完成相应的大数据分析任务，简直不要太爽！</p>
<p>正因为Hive如此易用和SQL的通用性，Hive逐渐成长成了大数据仓库的核心组件，甚至很多公司的流水线作业集完全是用SQL描述，因为易写易改，一看就懂，容易维护。</p>
<h4 id="4-3-Hive在hadoop项目中的作用"><a href="#4-3-Hive在hadoop项目中的作用" class="headerlink" title="4.3 Hive在hadoop项目中的作用"></a>4.3 Hive在hadoop项目中的作用</h4><ul>
<li><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张关系型数据库的表，并提供类似传统数据库的SQL查询功能</p>
<p>这里如何理解？以本文第3章节内容说明：</p>
<p>这里说的结构化的数据文件，例如emp.txt数据文件，里面的数据是结构化的，每行的字段值用tab键空格隔开，用换行符’\n’进行换行，该数据文件直接存在hdfs上映射为一张关系型数据库的表：因为是结构化数据，一行记录分4列，有即每行都有4个字段，当然可以把该数据文件emp.txt看成是一张数据库表。</p>
</li>
</ul>
<ul>
<li><p>Hive的查询效率取决于使用第一代的MapReduce计算框架还是内存Spark/Tez框架</p>
<p>这句表述如何理解？</p>
</li>
</ul>
<p>==&#8195;&#8195;4.2 章节提到，数据应用开发或者数据分析人员开始用Hive分析数据之后，虽然写SQL即可实现MP任务，但Hive在MapReduce处理任务的速度实在太慢，这是底层默认采用MapReduce计算架构。Spark/Tez作为新一代的内存计算框架既然比MP计算效率更高，当然可以引入到Hive里面，于是就有了Hive on  Spark/Hive on Tez，到此，基本完成一个数据仓库的架构了，有了Hive on  Spark/Hive on Tez，基本解决了中低速数据处理的需求，这里的中低速是指（批数据分析）：例如查询某个栏目截止到昨天的访问量，时效性滞后比较长。==</p>
<p>&#8195;&#8195;而高速数据处理的需求（流数据分析）：例如要查询截止到1小时前，某个栏目的访问量，时效性要求高，近乎实时效果。</p>
<ul>
<li>Hive只适合用来做批量数据统计分析</li>
</ul>
<h3 id="5、Hive与HBase的关系"><a href="#5、Hive与HBase的关系" class="headerlink" title="5、Hive与HBase的关系"></a>5、Hive与HBase的关系</h3><p>&#8195;&#8195;在前面的blog，给出了非常详细的HBase高可用的部署测试的描述，那么在本文中，HBase跟Hive是怎么样结合使用呢？或者他们之间有什么关系吗？</p>
<p>&#8195;&#8195;首先：Hive与HBase是没有联系的，也就是说，在大数据项目中，有Hive+Spark/MapReduce+HDFS+结构化数据，也可以独立完成大数据分析任务，同样，有HBase+HDFS+数据，也可以独立完成大数据分析任务。因为Hbase和Hive在大数据架构中处在不同位置，Hbase主要解决实时高效查询的需求，尤其是Key-Value形式的查询；而Hive主要解决数据处理和计算问题，例如联合查询、统计、汇总等。这两个组件可以独立使用，也可以配合一起使用。</p>
<h4 id="5-1-两者之间的区别"><a href="#5-1-两者之间的区别" class="headerlink" title="5.1 两者之间的区别"></a>5.1 两者之间的区别</h4><ul>
<li><p>Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的随机实时查询，如日志明细、交易清单、轨迹行为等。</p>
</li>
<li><p>Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。</p>
</li>
<li><p>通过元数据来描述Hdfs上的结构化文本数据，通俗点来说，就是定义一张表来描述HDFS上的结构化文本，包括各列数据名称，数据类型是什么等，方便我们处理数据，当前很多SQL ON Hadoop的计算引擎均用的是hive的元数据，如Spark SQL、Impala等；</p>
</li>
<li>基于第一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据；<br>也可参考以下两者的各自优点对比图：<br><img src="https://img-blog.csdnimg.cn/2019110915233826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<h4 id="5-2-两者配合使用时的大数据处理流程"><a href="#5-2-两者配合使用时的大数据处理流程" class="headerlink" title="5.2 两者配合使用时的大数据处理流程"></a>5.2 两者配合使用时的大数据处理流程</h4><p>在大数据架构中，Hive和HBase是协作关系，处理流程一般如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191109153101866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>&#8195;&#8195;1）通过ETL工具将数据源抽取到HDFS存储，这里的数据源包括mysql等关系型数据库的数据、ftp、api接口、txt、excel、日志文件等，这里说的抽取有两种意思：一种为脚本式的自动化抽取，例如写个定时任务把ftp的数据定时导入到HDFS中，另外一种抽取则是使用Apache组件Flume，能够实时抽取日志记录到kafka消息组件中，再由消费端（例如存入hbase或者mysql等）消费kafka的日志消息，这部分内容也会在本blog给出。</p>
<p>&#8195;&#8195;2）通过Hive清洗、处理原始数据；</p>
<p>&#8195;&#8195;3）HIve清洗处理后的数据，若面向海量数据随机查询场景，例如key-value，则可存入Hbase；若其他查询场景则可导入到mysql等其他数据库</p>
<p>&#8195;&#8195;4）大数据BI分析、应用的数据接口API开发，可从HBase获得查询数据。</p>
<h4 id="5-3-如果Hbase不需要Hive组件，如何实现易用的查询？"><a href="#5-3-如果Hbase不需要Hive组件，如何实现易用的查询？" class="headerlink" title="5.3 如果Hbase不需要Hive组件，如何实现易用的查询？"></a>5.3 如果Hbase不需要Hive组件，如何实现易用的查询？</h4><p>&#8195;&#8195;在文章<a href="https://blog.csdn.net/pysense/article/details/102635656">基于HadoopHA服务部署HBaseHA分布式服务（详细版）</a>的第10章节内容，提到操作HBase 表的示例，例如要查询company表的R2行记录，首先启动hbase shell，使用以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):&gt; get &#39;company&#39;,&#39;R1&#39;,&#39;staff_info:age&#39;</span><br><span class="line"></span><br><span class="line">COLUMN                  CELL                                                          </span><br><span class="line"> staff_info:age         timestamp&#x3D;**, value&#x3D;23  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;可以看到，这种查询方式适合开发人员或者hbase管理员，而对于已经非常熟悉SQL查询的分析人员来说，无疑非常不友好。Hive正好能提供一种叫“外部表”的机制实现以SQL的形式对HBase的数据进行查询操作，内容在以下章节给出。</p>
<h3 id="6、为HBase引入Hive组件"><a href="#6、为HBase引入Hive组件" class="headerlink" title="6、为HBase引入Hive组件"></a>6、为HBase引入Hive组件</h3><p>&#8195;&#8195;前面提到，引入Hive就是为了能够使用SQL语句轻松完成对于HBase上的数据进行查询任务。<br>==&#8195;&#8195;Hive连接HBase的原理：==<br>&#8195;&#8195;让hive加载到连接hbase的jar包，通过hbase提供的java api即可实现Hive对Hbase的操作，此时可以吧Hive看成是HBase的客户端，类似navicat客户至于mysql，只不过navicat提供UI操作界面，hive是通过cli shell操作，当然我们也可以使用Hive的UI操作工具来实现UI操作（后面会给出基于DBeaver来实现）</p>
<h4 id="6-1-hive-env-sh"><a href="#6-1-hive-env-sh" class="headerlink" title="6.1  hive-env.sh"></a>6.1  hive-env.sh</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件最后添加</span></span><br><span class="line">export HBASE_HOME=/opt/hbase-2.1.7</span><br></pre></td></tr></table></figure>
<h4 id="6-2-在hive-site-xml添加zookeeper集群"><a href="#6-2-在hive-site-xml添加zookeeper集群" class="headerlink" title="6.2 在hive-site.xml添加zookeeper集群"></a>6.2 在hive-site.xml添加zookeeper集群</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">&lt;!--zookeeper的有关设置--&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hiveserver2_zk&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;    </span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.zookeeper.publish.configs&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>以上两个配置实现了Hive连接至Hbase</p>
<h4 id="6-3-测试hive操作hbase"><a href="#6-3-测试hive操作hbase" class="headerlink" title="6.3 测试hive操作hbase"></a>6.3 测试hive操作hbase</h4><p>&#8195;&#8195;首先hbase有测试数据，之前创建的company table，里面有两个列簇，这里不再赘述。<br>&#8195;&#8195;在hive创建外部表，用于映射Hbase的列簇，这里以staff_info列簇作为测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> CREATE EXTERNAL TABLE staff_info(</span></span><br><span class="line">rowkey string,</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">sexual string</span><br><span class="line">) </span><br><span class="line">STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; </span><br><span class="line">WITH SERDEPROPERTIES </span><br><span class="line">(&quot;hbase.columns.mapping&quot;=&quot;:key,staff_info:name,staff_info:age,staff_info:sex&quot;) </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;company&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>外部表创建语法解释：<br>==创建一个外部表，表名为staff_info，字段有4个，（rowkey,name,age,sexual），其中rowkey为对于hbase上的rowkey，该字段不是数据字段，name、age、sexual为数据字段。处理类org.apache.hadoop.hive.hbase.HBaseStorageHandler，hbase到hive的映射关系：:key,列簇:列名1，列簇:列名2…==<br>指定映射HBase的table name</p>
<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">** INFO  [16e10346-1e6d-4bb5-b89b-bd12f3614ec7 main] zookeeper.RecoverableZooKeeper: Process identifier&#x3D;hconnection-0x448892f1 connecting to ZooKeeper ensemble&#x3D;nn:2181,dn1:2181,dn2:2181</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.151 seconds</span><br></pre></td></tr></table></figure>
<p>在hive查询相关hbase的staff_info数据<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from staff_info;</span><br><span class="line">OK</span><br><span class="line">R1      Bery    23      Female</span><br><span class="line">R2      Dery    27      Male</span><br><span class="line">Time taken: 3.562 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">hive&gt; select * from staff_info a where a.name&#x3D;&#39;Bery&#39;;</span><br><span class="line">OK</span><br><span class="line">R1      Bery    23      Female</span><br><span class="line">Time taken: 1.376 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></p>
<p>以上完成Hive和HBase的开发环境整合配置。</p>
<h3 id="7、使用SQL开发工具连接hive进行高级SQL开发"><a href="#7、使用SQL开发工具连接hive进行高级SQL开发" class="headerlink" title="7、使用SQL开发工具连接hive进行高级SQL开发"></a>7、使用SQL开发工具连接hive进行高级SQL开发</h3><p>&#8195;&#8195;在前面章节内容可以看到，hive的操作直接基于hive服务器上的hive cli上进行，使用hive交互命令式写sql效率会很低，调试也不方便，因此需要外部SQL IDE工具提高开发效率。本文采用DBeaver，也是本人长期使用的数据库管理客户端工具，重点它是开源的，在Mac上用起来流畅、UI有一定设计感！）。</p>
<p>关于DBeaver的介绍（<a href="https://dbeaver.io/">官网下载</a>）：</p>
<blockquote>
<p>DBeaver 是一个开源、跨平台、基于java语言编写的的通用数据库管理工具和 SQL 客户端，支持 MySQL, PostgreSQL, Oracle, Hive、Spark、elasticsearch等以及其他兼容 JDBC 的数据库(DBeaver可以支持的数据库太多了)</p>
<p>DBeaver 提供一个图形界面用来查看<a href="https://baike.baidu.com/item/数据库结构/5507713">数据库结构</a>、执行SQL查询和脚本，浏览和导出数据，处理<a href="https://baike.baidu.com/item/BLOB/543419">BLOB</a>/CLOB 数据，修改数据库结构等。<br><img src="https://img-blog.csdnimg.cn/20191109154205202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到，DBeaver支持各自类型数据库以及hadoop相关的组件，之后会有专门文章用DBeaver开发spark数据分析项目。</p>
</blockquote>
<p>DBeaver连接hive需要做以下几个配置，否则无法成功连接</p>
<h4 id="7-1-配置hive-site-xml和core-site-xml"><a href="#7-1-配置hive-site-xml和core-site-xml" class="headerlink" title="7.1  配置hive-site.xml和core-site.xml"></a>7.1  配置hive-site.xml和core-site.xml</h4><p>hive服务端启用相应的thrift TCP端口，暴露给客户端连接使用。<br>在2.2章节内容，hive-site.xml已经配置了hive server2服务，端口号按默认的10000，监听host为全网地址0.0.0.0，nn和dn2都需要配置hive server2。此外，还需要hadoop的配置文件core-site.xml放通拥有hdfs文件系统的用户，在本blog里，hadoop的用户为root上，需加入以下property<br>==core-site.xml配置如下==<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">  &lt;!--放通客户端以root用户访问hdfs--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><br>如果hadoop文件使用者不是’root‘用户，例如‘foo-bar’用户那么对应的name值为<br><code>&lt;name&gt;hadoop.proxyuser.foo-bar.groups&lt;/name&gt;</code>，<br>以上配置需要在nn和dn2同时配置，因为这两个节点做了hadoop HA。</p>
<p>若不配置“放通客户端以root用户访问hdfs”，使用DBeaver或者jdbc api连接hive server2会提示以下出错信息：</p>
<p>==连接错误提示==<br>Required field ‘serverProtocolVersion’ is unset! Struct:TOpenSessionResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*org.apache.hive.service.cli.HiveSQLException:Failed to open new session: java.lang.RuntimeException: ==org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate root:14:13 ==</p>
<h4 id="7-2-在nn主节点上启动hiveserver2服务"><a href="#7-2-在nn主节点上启动hiveserver2服务" class="headerlink" title="7.2 在nn主节点上启动hiveserver2服务"></a>7.2 在nn主节点上启动hiveserver2服务</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以前台进程方式打开</span></span><br><span class="line">[root@nn conf]# hiveserver2</span><br><span class="line">Hive Session ID = 1c92d507-7725-4e57-a7fe-03a9ae0cdf13</span><br></pre></td></tr></table></figure>
<p>使用jps -ml查看所有大数据组件服务的情况，RunJar表示hiveserver2服务<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# jps -ml</span><br><span class="line">16340 org.apache.hadoop.util.RunJar /opt/hive-3.1.2/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.aux.jars ****</span><br><span class="line">14085 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">14710 org.apache.hadoop.hbase.master.HMaster start</span><br><span class="line">5815 org.apache.hadoop.hdfs.tools.DFSZKFailoverController</span><br><span class="line">13273 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">16666 sun.tools.jps.Jps -ml</span><br><span class="line">5451 org.apache.zookeeper.server.quorum.QuorumPeerMain /opt/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">13547 org.apache.hadoop.hdfs.qjournal.server.JournalNode</span><br><span class="line">14876 org.apache.hadoop.hbase.regionserver.HRegionServer start</span><br><span class="line">13135 org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">13951 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br></pre></td></tr></table></figure><br>也可查看是否有10000端口<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ss -nltp|grep 10000</span><br><span class="line">LISTEN     0      50          :::10000                   :::*                  </span><br><span class="line"> users:((&quot;java&quot;,pid&#x3D;16340,fd&#x3D;506))</span><br></pre></td></tr></table></figure><br>至此，hiveserver2已经可以对外提供hive的连接服务。</p>
<h4 id="7-3-配置DBeaver连接hive"><a href="#7-3-配置DBeaver连接hive" class="headerlink" title="7.3 配置DBeaver连接hive"></a>7.3 配置DBeaver连接hive</h4><p>创建新的hive连接<br><img src="https://img-blog.csdnimg.cn/20191109165053150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>编辑驱动设置</code>里面，选择下载驱动，这里DBeaver会自动去拉取相应的jar驱动包<br><img src="https://img-blog.csdnimg.cn/20191109165332355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">驱动为：<code>hive-jdbc-uber-2.6.5.0-292.jar</code> (Uber开发的驱动？)</p>
<p>测试是否可连，以下提示远程hive服务器的版本为hive3.1.2<br><img src="https://img-blog.csdnimg.cn/20191109165734163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">下图可以看到DBeaver已经可以查看hive之前创建的emp表，以及hive的外部表——hbase的staff_info表<br><img src="https://img-blog.csdnimg.cn/20191109170010278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在DBeaver编辑器上对hive上的emp表进行简单的查询：<br><img src="https://img-blog.csdnimg.cn/20191109171320511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">至此，hive的SQL可视化开发环境已经部署完成，配合DBeaver出色的Tab自动补全，写HQL效率有效提升。</p>
<h4 id="7-4-hiveserver2的webUI"><a href="#7-4-hiveserver2的webUI" class="headerlink" title="7.4 hiveserver2的webUI"></a>7.4 hiveserver2的webUI</h4><p>&#8195;&#8195;在上一节内容，通过命令<code>hiveserver2</code>可启动远程连接服务，其实该命令还启动另外一个进程：hiveserver2自己的webUI服务进程，该web页面可看到每个客户端在hive服务器上执行过的查询语句、会话，包括IP、用户名、当前执行的操作（查询）数量、链接总时长、空闲时长等指标，是管理客户端连接和查询的后台页面。<br>在hiveserver2服务器上也即nn节点上查看10002端号：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ss -nltp|grep 10002</span><br><span class="line">LISTEN     0      50          :::10002                   :::*                   users:((&quot;java&quot;,pid=16340,fd=511))</span><br></pre></td></tr></table></figure><br>web 页面入口：<a href="http://nn:10002/">http://nn:10002/</a><br>当前连接的客户端会话<br><img src="https://img-blog.csdnimg.cn/20191109173709403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">已经完成的查询语句，这里可以看到HQL使用底层计算框架为MapReduce</p>
<p><img src="https://img-blog.csdnimg.cn/20191109173859568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">至此，已经完成使用外部SQL客户端工具DBeaver连接hive的任务，那么接下来：在Hbase导入大数据文件，部署高可用hiveserver2服务。</p>
<h3 id="8-使用beeline连接hiveserver2"><a href="#8-使用beeline连接hiveserver2" class="headerlink" title="8 使用beeline连接hiveserver2"></a>8 使用beeline连接hiveserver2</h3><p>&#8195;&#8195;在以上章节都提到两种方式连接到hiveserver2，此外，还有hive自带的一个客户端工具beeline，也可以连接到hive，按hive的官方规划，beeline将取代之前版本的hive cli。具体为何取代hive cli，参考官网说明：</p>
<blockquote>
<p>HiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline.<br>HiveCLI is now deprecated in favor of Beeline, as it lacks the<br>multi-user, security, and other capabilities of HiveServer2.  To run<br>HiveServer2 and Beeline from shell:</p>
</blockquote>
<p>连接用法hiveserver2的用法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">beeline -u jdbc:hive2:nn:10000 -n root -p ****</span><br></pre></td></tr></table></figure><br>可以看出因为beeline在使用jdbc接口连接时要求带入hive-site.xml配置账户和密码，因此官网说提供了 security功能。<br>具体使用方式这里不再</p>
<h3 id="8、部署高可用的Hive服务"><a href="#8、部署高可用的Hive服务" class="headerlink" title="8、部署高可用的Hive服务"></a>8、部署高可用的Hive服务</h3><p> 以上仅在hdfs、hbase的主节点nn配置hive单集服务，hive可以看做是hdfs对外提供的SQL客户端服务，若nn节点不可用，将导致nn节点hive服务也无法使用，因此实际生产环境，需要将hive部署为HA模式，与hdfs和hbaseHA模式一起构成完整的高可用离线分析大数据开发环境。这部分的内容在下一篇文章给出：构建高可用Hive HA和整合HBase开发环境（二）</p>
<p>​         </p>
]]></content>
      <categories>
        <category>Hive</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>Hive集群</tag>
        <tag>HBase开发环境</tag>
      </tags>
  </entry>
  <entry>
    <title>深入解析asyncio与协程</title>
    <url>/2020/01/04/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90asyncio%E4%B8%8E%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章，已经通过gevent实现高并发的协程，本文将详细讨论Python标准库异步IO——asyncio。在Python3.4中引入了协程的概念以及asyncio。asyncio底层调用yield from语法，将任务变成生成器后挂起，这种方式无法实现协程之间的自动切换，在Python3.5中正式确立引入了async和await 的语法，所有的这些工作都使得Python实现异步编程变得更容易上手。</p>
<a id="more"></a>
<h4 id="1、asyncio的基本概念"><a href="#1、asyncio的基本概念" class="headerlink" title="1、asyncio的基本概念"></a>1、asyncio的基本概念</h4><ul>
<li><p>event_loop 事件循环：每一个需要异步执行的任务都要注册到事件循环中，事件循环负责管理和调度这些任务之间的执行流程（遇到IO则自动切换协程等）。</p>
</li>
<li><p>coroutine 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。</p>
</li>
<li><p>task 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含任务的各种状态。</p>
</li>
<li><p>future： 代表将来执行或没有执行的任务的结果。它和task上没有本质的区别</p>
</li>
<li><p>async/await 关键字：在python3.5及以上，用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。</p>
</li>
</ul>
<p>&#8195;&#8195;在异步的模式里，所有代码逻辑都会运行在一个forever事件循环中（你可以把整个事件循环看成一个总控中心，它监听着当前线程创建的多个协程发发生的事件），它可以同时执行多个协程，这些协程异步地执行，直到遇到 await 关键字，事件循环将会挂起该协程，事件循环这个总控再把当前线程控制权分配给其他协程，直到其他的协程也挂起或者执行完毕，再进行下一个协程的执行。</p>
<h4 id="2、使用asyncio"><a href="#2、使用asyncio" class="headerlink" title="2、使用asyncio"></a>2、使用asyncio</h4><h5 id="2-1-使用async关键字和await定义协程"><a href="#2-1-使用async关键字和await定义协程" class="headerlink" title="2.1 使用async关键字和await定义协程"></a>2.1 使用async关键字和await定义协程</h5><p>在Python3.5之前，要实现协程方式的写法一般如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>) </span><br></pre></td></tr></table></figure><br>在Python3.5以后，全面使用async关键字和await定义协程，代码显更直观。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># async 定义了mytask为协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="comment"># 这里就像gevent的sleep方法模拟IO，而且该协程会被asyncio自动切换</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>) <span class="comment"># await 要求该行语句的IO是有返回值的例如response=request.get(url)，如果直接使用await time.sleep(2),则无法创建协程对象</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建事件循环对象，该事件循环由当前主线程拥有 </span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks=[mytask(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)] <span class="comment"># 这里mytask()是协程对象，不会离开运行。</span></span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks)) <span class="comment"># 这里实行的逻辑就像gevent.joinall(tasks)一样，表示loop一直运行直到所有的协程tasks都完成</span></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;代码中通过async关键字定义一个协程（coroutine），不过该协程不能直接运行，需将它注册到事件循环loop里面，由后者在协程内部发生IO时（asyncio.sleep(2)）时候调用协程。asyncio.get_event_loop方法可以创建一个事件循环，然后使用run_until_complete将协程注册到事件循环，并启动事件循环。<br>输出，从结果可以看出，5个协程同一时刻并发运行。<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-1 started at:17:12:37</span><br><span class="line">task-0 started at:17:12:37</span><br><span class="line">task-3 started at:17:12:37</span><br><span class="line">task-2 started at:17:12:37</span><br><span class="line">task-4 started at:17:12:37</span><br><span class="line">task-1 done at:17:12:38</span><br><span class="line">task-0 done at:17:12:38</span><br><span class="line">task-3 done at:17:12:38</span><br><span class="line">task-2 done at:17:12:38</span><br><span class="line">task-4 done at:17:12:38</span><br></pre></td></tr></table></figure><br>关于await要求该句为返回值：<br>await asyncio.sleep(2)，这里可以看看sleep返回什么<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sleep</span>(<span class="params">delay, result=<span class="literal">None</span>, *, loop=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Coroutine that completes after a given time (in seconds).&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> delay == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">yield</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> loop <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        loop = events.get_event_loop()</span><br><span class="line">    future = loop.create_future()</span><br><span class="line">    h = future._loop.call_later(delay,</span><br><span class="line">                                futures._set_result_unless_cancelled,</span><br><span class="line">                                future, result)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">yield</span> <span class="keyword">from</span> future)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        h.cancel()</span><br></pre></td></tr></table></figure><br>如果设为delay值，且loop事件循环已创建（即使代码未创建它也会自动创建），返回的是future对象(yield from future)，而这里可以挂起当前协程，直到future完成</p>
<h5 id="2-2-task对象"><a href="#2-2-task对象" class="headerlink" title="2.2 task对象"></a>2.2 task对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...同上</span><br><span class="line"><span class="comment"># async 定义了mytask为协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="comment"># 这里就像gevent的sleep方法模拟IO，而且该协程会被asyncio自动切换</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;ok&#x27;</span></span><br><span class="line"></span><br><span class="line">coro=mytask(<span class="number">1</span>)</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task=loop.create_task(coro) <span class="comment"># 将协程对象封装为task对象</span></span><br><span class="line">print(<span class="string">&#x27;before register to loop:&#x27;</span>,task)</span><br><span class="line">loop.run_until_complete(future=task)</span><br><span class="line">print(<span class="string">&#x27;after loop completed,task return the result:&#x27;</span>,task.result())</span><br></pre></td></tr></table></figure>
<p>查看打印结果：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">before register to loop: &lt;Task pending coro=&lt;mytask() running at /opt/asyn.py:9&gt;&gt;</span><br><span class="line">task-1 started at:17:39:06</span><br><span class="line">task-1 done at:17:39:07</span><br><span class="line">after loop completed,task return the result: ok</span><br></pre></td></tr></table></figure><br>将协程封装为task对象后，task在注册到事件循环之前为pending状态，1秒后，task 结束，并且通过task.result()可以获取协程结果值。<br>task对象也可用asyncio.ensure_future(coro)创建（接收coro协程或者future对象），它内部封装了loop.create_task</p>
<h5 id="2-2-future对象"><a href="#2-2-future对象" class="headerlink" title="2.2 future对象"></a>2.2 future对象</h5><p>前面定义说了future表示将来执行或没有执行的任务的结果，task是future的子类。<br>基本的方法有：<br>• cancel(): 取消future的执行，调度回调函数<br>• result(): 返回future代表的结果<br>• exception(): 返回future中的Exception<br>• add_done_callback(fn): 添加一个回调函数，当future执行的时候会调用这个回调函数。<br>• set_result(result): 将future标为运行完成，并且设置return值，该方法常用<br>使用future，可以在协程结束后自行回调函数：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line">...同上</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coru_1</span>(<span class="params">future_obj,N</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coru_1 started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    total=<span class="built_in">sum</span>(<span class="built_in">range</span>(N))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future_obj.set_result(<span class="string">&#x27;coru_1 returns:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total))</span><br><span class="line">    print(<span class="string">&#x27;coru_1 done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coru_2</span>(<span class="params">future_obj,N</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coru_2 started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    total=<span class="built_in">sum</span>(<span class="built_in">range</span>(N))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future_obj.set_result(<span class="string">&#x27;coru_2 returns:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total))</span><br><span class="line">    print(<span class="string">&#x27;coru_2 done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call_back</span>(<span class="params">future_obj</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;saved to redis at :&#x27;</span>,get_time(),future_obj,future_obj.result())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    f1=asyncio.Future()</span><br><span class="line">    f2=asyncio.Future()</span><br><span class="line">    tasks=[coru_1(f1,<span class="number">10</span>),coru_2(f2,<span class="number">20</span>)]</span><br><span class="line">    f1.add_done_callback(call_back)</span><br><span class="line">    f2.add_done_callback(call_back)</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>输出<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coru_1 started at:16:52:07</span><br><span class="line">coru_2 started at:16:52:07</span><br><span class="line">coru_1 done at:16:52:09</span><br><span class="line">coru_2 done at:16:52:09</span><br><span class="line">saved to redis at : 16:52:10 &lt;Future finished result=&#x27;coru_1 returns:45&#x27;&gt; coru_1 returns:45</span><br><span class="line">saved to redis at : 16:52:11 &lt;Future finished result=&#x27;coru_2 returns:190&#x27;&gt; coru_2 returns:190</span><br></pre></td></tr></table></figure><br>两个协程同时启动且在同一时间结束运行。之后开始回调，可以看到协程1先回调，1秒完成后，再切换到协程2回调。</p>
<h5 id="2-3-获取协程并发执行后的所有返回值"><a href="#2-3-获取协程并发执行后的所有返回值" class="headerlink" title="2.3 获取协程并发执行后的所有返回值"></a>2.3 获取协程并发执行后的所有返回值</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line">    </span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">coros=[read_file(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)] <span class="comment"># 创建多个协程</span></span><br><span class="line">tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros]<span class="comment"># 将协程封装为task对象 </span></span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"><span class="comment"># 或者loop.run_until_complete(asyncio.gether(*tasks))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重点在这里，当所有的协程结束后，可批量获取所有协程的返回结果</span></span><br><span class="line">get_all_result=[ t.result() <span class="keyword">for</span> t <span class="keyword">in</span> tasks]</span><br><span class="line">print(get_all_result)</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-0 started at:15:53:08</span><br><span class="line">task-1 started at:15:53:08</span><br><span class="line">task-2 started at:15:53:08</span><br><span class="line">task-3 started at:15:53:08</span><br><span class="line">[&#x27;task-0 done at:15:53:09&#x27;, &#x27;task-1 done at:15:53:09&#x27;, &#x27;task-2 done at:15:53:09&#x27;, &#x27;task-3 done at:15:53:09&#x27;]</span><br></pre></td></tr></table></figure><br>以上也无需使用future的回调机制获取协程返回值，直接在loop结束后，从task对象的result方法即可获得协程返回值。<br>需要注意的是：<br>用于等待所有协程完成的方法asyncio.wait和asyncio.gather，都是接受多个future或coro组成的列表，区别：asyncio.gather内边调用ensure_future方法将列表中不是task的coro封装为future对象，而wait则没有。</p>
<h5 id="2-4-asyncio-gather-vs-asyncio-wait"><a href="#2-4-asyncio-gather-vs-asyncio-wait" class="headerlink" title="2.4 asyncio.gather vs asyncio.wait"></a>2.4 asyncio.gather vs asyncio.wait</h5><p>这里再给两个例子说明这两者的区别以及应用场合：<br>asyncio.gather<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">group_id,coro_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;group&#123;&#125;-task&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(group_id,coro_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(coro_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;group&#123;&#125;-task&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(group_id,coro_id,get_time())</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建三组tasks</span></span><br><span class="line">tasks1=[asyncio.ensure_future(coro(<span class="number">1</span>,i))<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">5</span>)]</span><br><span class="line">tasks2=[asyncio.ensure_future(coro(<span class="number">2</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>,<span class="number">8</span>)]</span><br><span class="line">tasks3=[asyncio.ensure_future(coro(<span class="number">3</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">group1=asyncio.gather(*tasks1) <span class="comment"># 对第1组的协程进行分组，group1</span></span><br><span class="line">group2=asyncio.gather(*tasks2) <span class="comment"># 对第2组的协程进行分组，group2</span></span><br><span class="line">group3=asyncio.gather(*tasks3) <span class="comment"># 对第3组的协程进行分组，group3</span></span><br><span class="line"></span><br><span class="line">all_groups=asyncio.gather(group1,group2,group3) <span class="comment"># 把3个group再聚合成一个大组，也是就所有协程对象的被聚合到一个大组</span></span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">all_group_result=loop.run_until_complete(all_groups) </span><br><span class="line"><span class="keyword">for</span> index,group <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_group_result): <span class="comment"># 获取每组协程的输出</span></span><br><span class="line">    print(<span class="string">&#x27;group &#123;&#125; result:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index+<span class="number">1</span>,group))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">group1-task1 started at:35:19</span><br><span class="line">group1-task2 started at:35:19</span><br><span class="line">group1-task3 started at:35:19</span><br><span class="line">group1-task4 started at:35:19</span><br><span class="line">group2-task6 started at:35:19</span><br><span class="line">group2-task7 started at:35:19</span><br><span class="line">group3-task8 started at:35:19</span><br><span class="line">group3-task9 started at:35:19</span><br><span class="line">group 1 result:[&#x27;group1-task1 done at:35:21&#x27;, &#x27;group1-task2 done at:35:21&#x27;, &#x27;group1-task3 done at:35:21&#x27;, &#x27;group1-task4 done at:35:21&#x27;]</span><br><span class="line">group 2 result:[&#x27;group2-task6 done at:35:21&#x27;, &#x27;group2-task7 done at:35:21&#x27;]</span><br><span class="line">group 3 result:[&#x27;group3-task8 done at:35:21&#x27;, &#x27;group3-task9 done at:35:21&#x27;]</span><br></pre></td></tr></table></figure><br>从打印结果可知，每组协程都在同一时刻开始以及同一时刻结束，asyncio.gather就是用于在更高层面对task进行分组，以不同的组管理不同的协程，你可以看出gather是一个粗粒度组织协程，自动收集所有协程结束后的返回值。</p>
<p>asyncio.wait<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line">tasks=[coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">first_complete,unfinished1=loop.run_until_complete(asyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED))</span><br><span class="line"><span class="comment"># 获取首个已结束的协程返回值,注意这里firt_complete是一个set()</span></span><br><span class="line"></span><br><span class="line">first_done_task=first_complete.pop()</span><br><span class="line">print(<span class="string">&#x27;首个完成的协程返回值：&#x27;</span>,first_done_task.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished1))</span><br><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第二阶段已完成的协程返回值</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished2:</span><br><span class="line">    print(t.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第二阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished3,unfinished3=loop.run_until_complete(asyncio.wait(unfinished2))</span><br><span class="line"><span class="comment"># 获取第三阶段已完成的协程返回值</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished3:</span><br><span class="line">    print(t.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished3))</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-5 started at:23:40</span><br><span class="line">coro-1 started at:23:40</span><br><span class="line">coro-6 started at:23:40</span><br><span class="line">coro-7 started at:23:40</span><br><span class="line">coro-2 started at:23:40</span><br><span class="line">coro-8 started at:23:40</span><br><span class="line">coro-3 started at:23:40</span><br><span class="line">coro-9 started at:23:40</span><br><span class="line">coro-4 started at:23:40</span><br><span class="line">首个完成的协程返回值： coro-1 done at:23:41</span><br><span class="line">还未结束的协程数量： 8</span><br><span class="line">coro-2 done at:23:42</span><br><span class="line">coro-3 done at:23:43</span><br><span class="line">coro-4 done at:23:44</span><br><span class="line">还未结束的协程数量： 5</span><br><span class="line">coro-6 done at:23:46</span><br><span class="line">coro-7 done at:23:47</span><br><span class="line">coro-9 done at:23:49</span><br><span class="line">coro-5 done at:23:45</span><br><span class="line">coro-8 done at:23:48</span><br><span class="line">还未结束的协程数量： 0</span><br></pre></td></tr></table></figure><br>从输出结果可以很清看出<code>asyncio.wait</code>很精确的控制协程运行过程，通过<code>wait(return_when=asyncio.FIRST_COMPLETED)</code>可拿到运行完成的协程，通过<code>wait(timeout)</code>控制指定时间后放回已完成的协程。</p>
<h5 id="2-5-嵌套协程的实现（协程内调用协程）"><a href="#2-5-嵌套协程的实现（协程内调用协程）" class="headerlink" title="2.5 嵌套协程的实现（协程内调用协程）"></a>2.5 嵌套协程的实现（协程内调用协程）</h5><h6 id="一种易于理解的调用异步函数的方式"><a href="#一种易于理解的调用异步函数的方式" class="headerlink" title="一种易于理解的调用异步函数的方式"></a>一种易于理解的调用异步函数的方式</h6><p>&#8195;&#8195;在介绍嵌套协程或者闭包协程、协程内调用协程的概念前，先看看普通函数内部调用普通函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func1</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func2</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func3</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gel_all_data</span>():</span></span><br><span class="line">    result1=func1(<span class="string">&#x27;foo&#x27;</span>)</span><br><span class="line">    result2=func2(result1)</span><br><span class="line">    result3=func3(result2)</span><br><span class="line">    <span class="keyword">return</span> (result1,result2,result3)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>&#8195;&#8195;在同步的编程思维下，大家很容易理解get_all_data函数内部调用func1等三个外部函数来获取相应返回值，其实将同步改为异步的过程很简单：<br>==在每个函数前面使用关键字async向python解释器声明这是异步函数，如果需要调用外部异步函数，需使用await关键字==，将上面的同步编程改成异步编程的模式如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func1</span>(<span class="params">start_data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>) <span class="comment"># 要使用asyncio的异步sleep方法，它会让出线程控制权给其他协程，而内建的sleep为同步性质</span></span><br><span class="line">    <span class="keyword">return</span> start_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func2</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func3</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_all_data</span>():</span></span><br><span class="line">    result1=<span class="keyword">await</span> func1(<span class="string">&#x27;foo&#x27;</span>) <span class="comment"># 在异步函数内部，使用await关键字调用其他异步函数，并获取该异步函数的返回值。执行流会在此将当前线程控权让出</span></span><br><span class="line">    result2=<span class="keyword">await</span> func2(result1)<span class="comment"># 同上</span></span><br><span class="line">    result3=<span class="keyword">await</span> func3(result2) <span class="comment"># 同上</span></span><br><span class="line">    <span class="keyword">return</span>(result1,result2,result3)</span><br></pre></td></tr></table></figure><br>该异步的get_all_data其实要实现的需求为：一个协程内部调用其他协程，而且可以将返回值放置在不同的协程上，可以实现链式的协程调度，这看起来就是一个协程任务流 。<br>当熟悉了这种异步的编程模式后，可以玩一些更为进阶的例子：</p>
<h6 id="协程嵌套示例"><a href="#协程嵌套示例" class="headerlink" title="协程嵌套示例"></a>协程嵌套示例</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">inner_coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">5</span>) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    coros=[inner_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros] </span><br><span class="line">    inner_tasks,pendings=<span class="keyword">await</span> asyncio.wait(tasks) <span class="comment"># 这句实现了协程中再调用协程</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    <span class="comment"># 使用asyncio.wait(tasks)可以在外层协程里面获取嵌套协程的运行返回值</span></span><br><span class="line">    <span class="keyword">for</span> task <span class="keyword">in</span> inner_tasks:</span><br><span class="line">        print(task.result())</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">	loop.close()</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">outter_coro started at:14:52:59</span><br><span class="line">coro-0 started at:14:52:59</span><br><span class="line">coro-1 started at:14:52:59</span><br><span class="line">coro-2 started at:14:52:59</span><br><span class="line">coro-3 started at:14:52:59</span><br><span class="line"></span><br><span class="line">outter_coro done at:14:53:04</span><br><span class="line">coro-1 done at:14:53:04</span><br><span class="line">coro-3 done at:14:53:04</span><br><span class="line">coro-2 done at:14:53:04</span><br><span class="line">coro-0 done at:14:53:04</span><br></pre></td></tr></table></figure><br>可以看到外层协程和内层协程同时启动（当然外层协程函数最先执行），而且都在同一个时刻结束。<br>内层协程返回值只能在外程协程内部获取，能否在<br>loop.run_until_complete(outter_coro()) 之后，一次性获取协程返回值？ 需要改用asyncio.gather方法：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    coros=[inner_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros] </span><br><span class="line">    print(<span class="string">&#x27;outter_coro done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">all_coro_result=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> all_coro_result:</span><br><span class="line">    print(t)</span><br><span class="line">    </span><br><span class="line">loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">outter_coro started at:58:44</span><br><span class="line">outter_coro done at:58:44</span><br><span class="line">coro-0 started at:58:44</span><br><span class="line">coro-1 started at:58:44</span><br><span class="line">coro-2 started at:58:44</span><br><span class="line">coro-3 started at:58:44</span><br><span class="line">coro-0 done at:58:46</span><br><span class="line">coro-1 done at:58:46</span><br><span class="line">coro-2 done at:58:46</span><br><span class="line">coro-3 done at:58:46</span><br></pre></td></tr></table></figure><br>从外层协程outter_coro的启动时刻和结束时刻都一样可以看出，outter_coro和return await asyncio.gather(*tasks)是异步执行的，且在outter_coro结束后，loop事件循环只需管理coro-0到coro-3这4个协程。</p>
<p>从以上两种嵌套协程返回值的写法，可以看到这样逻辑：</p>
<ul>
<li>外层协程直接返回 awaitable对象给loop，loop就可以在最后获取所有协程的返回值；<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>()</span></span><br><span class="line"><span class="function">	<span class="title">return</span> <span class="title">await</span> <span class="title">asyncio</span>.<span class="title">wait</span>(<span class="params">tasks</span>) # 返回<span class="title">awaitable</span>对象给下文<span class="title">loop</span>，这里用<span class="title">asyncio</span>.<span class="title">wait</span>挂起所有协程</span></span><br><span class="line"><span class="function">	</span></span><br><span class="line">done,pending=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">	print(task.result())</span><br></pre></td></tr></table></figure></li>
<li>外层协程没有返回 awaitable对象给loop，loop无法获取所有协程的返回值，只能在外程协程里面获取所有协程返回值<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>()</span></span><br><span class="line">	done,pending=await asyncio.wait(tasks) # 没有返回awaitable对象给下文loop</span><br><span class="line">	<span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">		print(task.result())</span><br><span class="line">		</span><br><span class="line">loop.run_until_complete(outter_coro())</span><br></pre></td></tr></table></figure>
<h5 id="2-6-如何取消运行中协程"><a href="#2-6-如何取消运行中协程" class="headerlink" title="2.6 如何取消运行中协程"></a>2.6 如何取消运行中协程</h5>future（task）对象主要有以下几个状态：<br>pending、running、done、cancelled<br>创建future（task）的时候，task为pending状态:<br><code>tasks=[asyncio.ensure_future(coro) for coro in coros]</code><br>事件循环调用执行的时候且协程未结束时对应tsak为running状态，<br><code>loop.run_until_complete(outter_coro())</code><br>事件循环运行结束后，所有的task为done状态，<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">done,pending=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">	print(task.result())</span><br></pre></td></tr></table></figure>
那么最后一个cancelled状态如何实现呢？例如你想在某些协程未done之前将其cancel掉，如何处理？引用2.4章节的asyncio.wait例子说明：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line">tasks=[coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">first_complete,unfinished1=loop.run_until_complete(asyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED))</span><br><span class="line"><span class="comment"># 获取首个已结束的协程返回值,注意这里firt_complete是一个set()</span></span><br><span class="line"></span><br><span class="line">first_done_task=first_complete.pop()</span><br><span class="line">print(<span class="string">&#x27;首个完成的协程返回值：&#x27;</span>,first_done_task.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished1))</span><br><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished2:</span><br><span class="line">    print(t.result())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> unfinished2: <span class="comment"># 取消剩余未运行的task</span></span><br><span class="line">    print(<span class="string">&#x27;cancell unfinished task:&#x27;</span>,task,<span class="string">&#x27;==&gt;is canceled:&#x27;</span>,task.cancel())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
输出结果：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coro-4 started at:17:47</span><br><span class="line">coro-5 started at:17:47</span><br><span class="line">coro-1 started at:17:47</span><br><span class="line">coro-6 started at:17:47</span><br><span class="line">coro-7 started at:17:47</span><br><span class="line">coro-2 started at:17:47</span><br><span class="line">coro-8 started at:17:47</span><br><span class="line">coro-3 started at:17:47</span><br><span class="line">coro-9 started at:17:47</span><br><span class="line">首个完成的协程返回值： coro-1 done at:17:48</span><br><span class="line">还未结束的协程数量： 8</span><br><span class="line">coro-2 done at:17:49</span><br><span class="line">coro-3 done at:17:50</span><br><span class="line">coro-4 done at:17:51</span><br><span class="line">还未结束的协程数量： 5</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br></pre></td></tr></table></figure>
从输出结果可看到，8个协程task并发运行，最早结束的是coro-1，接着是coro-2、coro-3、coro-4，因为设定asyncio.wait(unfinished1,timeout=3) 3秒超时，只要超过3秒后，loop返回这些未运行的task，接着再逐个取消，可以看到5个协程被取消，True表示当前协程取消成功。</li>
</ul>
<h5 id="2-7-理解loop的相关方法"><a href="#2-7-理解loop的相关方法" class="headerlink" title="2.7  理解loop的相关方法"></a>2.7  理解loop的相关方法</h5><h6 id="loop-run-until-complate-vs-loop-run-forever"><a href="#loop-run-until-complate-vs-loop-run-forever" class="headerlink" title="loop.run_until_complate vs loop.run_forever"></a>loop.run_until_complate vs loop.run_forever</h6><p>&#8195;&#8195;<code>loop.run_until_complate</code>可以在程序的不同位置多次调用，例如在2.4 <code>asyncio.gather vs asyncio.wait</code> 提到的<code>asyncio.wait</code>用法，同一程序中能出现多个<code>loop.run_until_complate</code><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第二阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished3,unfinished3=loop.run_until_complete(asyncio.wait(unfinished2))</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished3))</span><br></pre></td></tr></table></figure><br>而对于 loop.run_forever，在同一程序中，只能有一个，因为该事件是在当前线程后台永久运行:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;start a coro&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;coro done&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro())</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.run_forever() <span class="comment"># 程序不会退出，loop一直挂在这里，等待其他future对象</span></span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro()) <span class="comment">#程序没有报错，但执行流永远不会到达这里，该句永远不会运行</span></span><br><span class="line">loop.run_forever() <span class="comment"># 程序没有报错，但执行流永远不会到达这里,该语句永远不会运行 </span></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">start a coro</span><br><span class="line">coro done</span><br><span class="line">,,,,</span><br><span class="line"># 等待程序退出</span><br></pre></td></tr></table></figure></p>
<h6 id="loop-stop"><a href="#loop-stop" class="headerlink" title="loop.stop()"></a>loop.stop()</h6><p>上面提到如果程序仅有loop.run_forever()，那么当future完成后，程序一直没有退出，若要求实现当future完成后，程序也需要正常退出，可以这样处理：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span>  datetime</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">n</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;start a coro at &#x27;</span>,get_time())</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(n)</span><br><span class="line">    print(<span class="string">&#x27;coro done at &#x27;</span>,get_time())</span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro(<span class="number">2</span>))</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.stop() <span class="comment"># 在run_forever()前，先stop</span></span><br><span class="line">print(<span class="string">&#x27;stop后，loop事件还在运行？ at &#x27;</span>,get_time())</span><br><span class="line">loop.run_forever() </span><br></pre></td></tr></table></figure><br>输出<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">stop后，loop事件还在运行？ at  43:11</span><br><span class="line">start a coro at  43:11</span><br></pre></td></tr></table></figure><br>从输出可以看到，上面的示例代码都是异步并发运行。<br><code>print(&#39;stop后，loop事件还在运行？ at &#39;,get_time())</code>语句跟future任务同时运行<br>==注意：如果把loop.stop()方法放在run_forever后面，可预见，程序不会退出==<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">loop.run_forever() </span><br><span class="line">loop.stop() <span class="comment"># 执行流永远不会到达这一句</span></span><br><span class="line">print(<span class="string">&#x27;stop后，loop事件还在运行？ at &#x27;</span>,get_time()) <span class="comment"># 执行流永远不会到达这一句</span></span><br></pre></td></tr></table></figure></p>
<h6 id="loop-call-soon-loop-call-later"><a href="#loop-call-soon-loop-call-later" class="headerlink" title="loop.call_soon/loop.call_later"></a>loop.call_soon/loop.call_later</h6><p>这两个方法用于在异步函数里面调用同步函数(普通函数)，且可以实现立刻调用或者稍后调用：<br><code>loop.call_soon(callback, *args, context=None）</code>: 立刻调用，并返回<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">name,stat=<span class="number">1</span></span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>,name,<span class="string">&#x27;keyword args:&#x27;</span>,stat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">loop</span>):</span></span><br><span class="line">    loop.call_soon(callback,<span class="string">&#x27;get first callback&#x27;</span>)</span><br><span class="line">    wrapper_func=functools.partial(callback,stat=<span class="number">2</span>)</span><br><span class="line">    loop.call_soon(wrapper_func,<span class="string">&#x27;get second call back&#x27;</span>)</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    loop.run_until_complete(run(loop))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><br>打印：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">args: get first callback keyword args: 1</span><br><span class="line">args: get second call back keyword args: 2</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;异步调用同步函数其实已经破坏了异步的并发机制，因此很少使用这些非异步的方法。<br>&#8195;&#8195;此外loop.call_soon不支持协程函数传入关键字，因此可以通过偏函数先把关键字参数”传入“callback的kwargs里面，之后在call_soon里面，就可以利用这个被”包装过的callback“再传入位置参数即可(loop.run_until_complete传入关键字参数也一样这么处理)</p>
<p><code>loop.call_later(delay, callback, *args, context=None)</code>： 再给定一个时间之后，再调用callback。context默认当前线程的上下文<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">name,stat=<span class="number">1</span></span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>,name,<span class="string">&#x27;keyword args:&#x27;</span>,stat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">loop</span>):</span></span><br><span class="line">    loop.call_later(<span class="number">2</span>,callback,<span class="string">&#x27;get first callback&#x27;</span>)</span><br><span class="line">    loop.call_soon(callback,<span class="string">&#x27;callback soon&#x27;</span>)    </span><br><span class="line">    wrapper_func=functools.partial(callback,stat=<span class="number">0</span>)</span><br><span class="line">    loop.call_later(<span class="number">1</span>,wrapper_func,<span class="string">&#x27;get second callback&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>) <span class="comment"># 这里如果不设sleep，那么call_soon执行后loop马上退出，导致2个有延时运行的callback也退出了。这里要大于等于delay时间最长的call_later</span></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    loop.run_until_complete(run(loop))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><br>打印：可以看到call_soon最先完成回调，接着才是设为1秒后运行的回调，2秒的回调<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">args: callback soon keyword args: 1</span><br><span class="line">args: get second callback keyword args: 0</span><br><span class="line">args: get first callback keyword args: 1</span><br></pre></td></tr></table></figure><br>同样，该loop.call_later不常用。</p>
<h4 id="3、asyncio进阶用法"><a href="#3、asyncio进阶用法" class="headerlink" title="3、asyncio进阶用法"></a>3、asyncio进阶用法</h4><p>&#8195;&#8195;在上面的例子中，一个主线程创建一个永久事件循环（该永久事件不会自动退出，而是run forever，除非主线程运行后没有阻塞或者手动中断程序运行），再把所有的协程注册到该永久事件循环，该方式较为基础用法的异步模式。在这一节，🔚asyncio高级用法，用线程1创建一个forever事件循环，线程2可以向事件循环中动态添加协程，而且不受主线程阻塞。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_another_loop</span>(<span class="params">loop</span>):</span></span><br><span class="line">    asyncio.set_event_loop(loop)</span><br><span class="line">    loop.run_forever()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">new_loop = asyncio.new_event_loop()</span><br><span class="line"><span class="comment"># start一个新的线程1，用于启动一个永久事件循环</span></span><br><span class="line">t = threading.Thread(target=start_another_loop,args=(new_loop,))</span><br><span class="line">t.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前线程向线程1的loop注册tasks</span></span><br><span class="line">asyncio.run_coroutine_threadsafe(read_file(<span class="number">5</span>),new_loop)</span><br><span class="line">asyncio.run_coroutine_threadsafe(read_file(<span class="number">5</span>),new_loop)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-5 started at:09:30</span><br><span class="line">coro-2 started at:09:30</span><br><span class="line">coro-2 done at:09:35</span><br><span class="line">coro-5 done at:09:35</span><br></pre></td></tr></table></figure><br>这个动态添加协程task对象有何用？如果task的参数是从redis队列实时取得，然后交由run_coroutine_threadsafe向loop注册协程，那么不就实现基于协程producer-consumer模式。</p>
<h5 id="利用redis-队列实现loop循环事件动态添加协程"><a href="#利用redis-队列实现loop循环事件动态添加协程" class="headerlink" title="利用redis 队列实现loop循环事件动态添加协程"></a>利用redis 队列实现loop循环事件动态添加协程</h5><p>&#8195;&#8195;这种方式，可以实现并发模式，producer：向redis 队列push 数据(这个数据是指协程task需要的参数，例如sleep的)，consumer：使用asyncio.run_coroutine_threadsafe(read_file(msg),new_loop)不断消费producer的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> datetime,time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCoro</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host=<span class="string">&#x27;127.0.0.1&#x27;</span>,port=<span class="number">6379</span>,key=<span class="string">&#x27;coro_queue&#x27;</span>,max_redis_conns=<span class="number">1000</span>,semaphore=<span class="number">2</span></span>):</span></span><br><span class="line">        self.r_pool=redis.ConnectionPool(host=host,port=port,max_connections=max_redis_conns)</span><br><span class="line">        self.r_conn=redis.Redis(connection_pool=self.r_pool)</span><br><span class="line">        self.r_queue_key=key</span><br><span class="line">        self.semaphore=semaphore</span><br><span class="line">        self.new_loop=asyncio.new_event_loop()</span><br><span class="line">        self.start()      </span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">self,task_id</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; 协程的worker，这里模拟IO耗时操作 &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,self.get_time()))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">        print(<span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,self.get_time()))</span><br><span class="line">        <span class="comment">#return &#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;.format(task_id,get_time()</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forever_loop</span>(<span class="params">self,loop_obj</span>):</span></span><br><span class="line">    	<span class="string">&quot;&quot;&quot;用于主线程启动一个永久事件循环，接收来自另外一个线程注册的协程对象&quot;&quot;&quot;</span></span><br><span class="line">        asyncio.set_event_loop(loop_obj)</span><br><span class="line">        loop_obj.run_forever()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_forever_loop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用一个主线程去启动一个永久事件循环&quot;&quot;&quot;</span></span><br><span class="line">        t=threading.Thread(target=self.forever_loop,args=(self.new_loop,))</span><br><span class="line">        t.start()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forever_consumer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;由另外一个子线层启动，该线程不断从redis队列获取数据，并用run_coroutine_threadsafe不断向new_loop注册task对象&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            task_id=self.r_conn.rpop(self.r_queue_key)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> task_id:</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            task_id=task_id.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            asyncio.run_coroutine_threadsafe(self.coro(<span class="built_in">int</span>(task_id)),self.new_loop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_forever_consumer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用一个子线程用于向事件循环注册协程对象 &quot;&quot;&quot;</span></span><br><span class="line">        t=threading.Thread(target=self.forever_consumer)</span><br><span class="line">        t.start()</span><br><span class="line">        t.join()<span class="comment"># 这里要阻塞当前线程，否则就无法实现不但从redis队列获取任务了。若不阻塞，主线程start()后，子线程start（）后，程序立即结束</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;在一个方法里面，同时启动两个线程，简化api&quot;&quot;&quot;</span></span><br><span class="line">        self.start_forever_loop()</span><br><span class="line">        self.forever_consumer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    coro=MyCoro() </span><br><span class="line">    coro.start()</span><br></pre></td></tr></table></figure>
<p>以上代码的逻辑：<br>创建两个线程，</p>
<ul>
<li>线程1负责启动一个forever事件循环，用于接收另外一个线程2注册的协程对象（task对象）</li>
<li>线程2负责不断从redis队列获取任务数据后，再把协程注册到线程1启动的事件循环，从而实现loop循环事件动态添加协程。<br>该例子只需要2个线程，即可实现高并发模式。</li>
</ul>
<p>测试：在redis-cli里面，先lpush一个10，再lpush1个2秒，1个4秒，最终线程2按顺序创建3个线程，都会注册到loop里面，注意对比3个协程的完成时间：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># redis-cli 在coro_queue队列添加数据</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 10</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 2</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 4</span><br><span class="line">(integer) 1</span><br><span class="line"></span><br><span class="line"># 程序输出</span><br><span class="line">coro-10 started at:56:48</span><br><span class="line">coro-2 started at:56:50</span><br><span class="line">coro-4 started at:56:51</span><br><span class="line">coro-2 done at:56:52</span><br><span class="line">coro-4 done at:56:55</span><br><span class="line">coro-10 done at:56:58</span><br></pre></td></tr></table></figure><br>从打印的时刻可以很清楚看到，3个协程并发执行，总共10秒完成。若同步模式，则需要2+4+10=16秒才能完成。</p>
<h4 id="4、asyncio最适合的使用场景"><a href="#4、asyncio最适合的使用场景" class="headerlink" title="4、asyncio最适合的使用场景"></a>4、asyncio最适合的使用场景</h4><p>&#8195;&#8195;从redis、Nginx、node.js、Tornado、Twisted这些使用IO多路复用技术的中间件或者框架，可以很明确的推出结论：异步逻辑非常适合处理有Network IO且高并发的socket连接场景，因为这些场景往往需要等待IO，例如：访问web接口数据、访问网页、访问数据库，都是client向server发起网络IO。因此本节给出asyncio的3个场景：异步爬虫，高并发的socket服务、数据库连接。<br>&#8195;&#8195;但是：asyncio的周边库似乎有不少坑，而且距离稳定生产环境有一定距离，参考<a href="https://www.zhihu.com/question/266094857/answer/304655007">知乎文章吐槽的asyncio</a>，所有很多文章介绍asyncio基本使用场合，或者自行开发的小工具，很少文章能给出一个使用asyncio实现的复杂项目。当然，协程肯定不适合CPU计算场景。<br>&#8195;&#8195;目前star较高的几个协程异步库有：<a href="https://github.com/aio-libs/aiohttp">aiohttp</a>、<a href="https://github.com/aio-libs/aioredis">aioredis</a>、<a href="https://github.com/aio-libs/aiomysql">aiomysql</a>、<a href="https://github.com/aio-libs/aiopg">aiopg</a>。aiopg:is a library for accessing a PostgreSQL database from the asyncio 。以上四个协程异步库底层通过封装asyncio实现。本节主要介绍aioredi和aiohttp，其他库可参考官方示例。</p>
<h5 id="4-1-aioredis"><a href="#4-1-aioredis" class="headerlink" title="4.1 aioredis"></a>4.1 aioredis</h5><p>&#8195;&#8195;aioredis实现的api很丰富，支持sentinel连接，运行原生redis命令的接口，但是它不支持cluster集群的连接：<code>Current release (1.3.0) of the library does not support Redis Cluster in a full manner.</code><br>单个连接示例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">key,value</span>):</span></span><br><span class="line">    redis = <span class="keyword">await</span> aioredis.create_redis(</span><br><span class="line">        <span class="string">&#x27;redis://127.0.0.1&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> redis.<span class="built_in">set</span>(key,value)</span><br><span class="line">    val = <span class="keyword">await</span> redis.get(key)</span><br><span class="line">    redis.close()</span><br><span class="line">    <span class="keyword">await</span> redis.wait_closed()</span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure><br>下面使用单例模式和协程with协议，实现基本的async redis 类：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AsynRedis</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    _instance = <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,redis_uri=<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,pool=<span class="literal">False</span>,max_conn=<span class="number">100</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span></span>):</span></span><br><span class="line">        self._redis_uri = redis_uri</span><br><span class="line">        self._encoding = encoding</span><br><span class="line">        self._pool=pool</span><br><span class="line">        self._max_conn=max_conn</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">__aenter__</span>(<span class="params">self</span>):</span> <span class="comment"># with协议入口</span></span><br><span class="line">        <span class="keyword">await</span> self.get_conn()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">__aexit__</span>(<span class="params">self, exc_type, exc, tb</span>):</span> <span class="comment">#with 协议出口</span></span><br><span class="line">        <span class="keyword">await</span> self.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_conn</span>(<span class="params">self</span>):</span> <span class="comment"># 单例模式创建redis连接，可选pool或者单连接</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._instance:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._pool:</span><br><span class="line">                self._instance = <span class="keyword">await</span> aioredis.create_redis(self._redis_uri)</span><br><span class="line">            self._instance=<span class="keyword">await</span> aioredis.create_redis_pool(self._redis_uri,maxsize=self._max_conn)</span><br><span class="line">        <span class="keyword">return</span> self._instance</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">asyn_set</span>(<span class="params">self,*args,**kwargs</span>):</span> </span><br><span class="line">        response=<span class="keyword">await</span> self._instance.<span class="built_in">set</span>(*args,**kwargs)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">asyn_get</span>(<span class="params">self,key</span>):</span></span><br><span class="line">        value=<span class="keyword">await</span> self._instance.get(key,encoding=self._encoding)</span><br><span class="line">        <span class="keyword">return</span> value </span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._instance:</span><br><span class="line">            self._instance.close()</span><br><span class="line">            <span class="keyword">await</span> self._instance.wait_closed()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_coro</span>(<span class="params">index</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsynRedis() <span class="keyword">as</span> f: <span class="comment"># 异步的aenter和aexit实现with协议</span></span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> start at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        key=<span class="string">&#x27;foo-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index)</span><br><span class="line">        result=<span class="keyword">await</span> f.asyn_set(key,get_time()) <span class="comment"># 将时刻作为value，用于观察协程并发,获取set操作返回值，True表示set成功。</span></span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,key is set? <span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    tasks=[redis_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-9 start at 57:15</span><br><span class="line">coro-4 start at 57:15</span><br><span class="line">coro-2 start at 57:15</span><br><span class="line">coro-5 start at 57:15</span><br><span class="line">coro-0 start at 57:15</span><br><span class="line">coro-8 start at 57:15</span><br><span class="line">coro-6 start at 57:15</span><br><span class="line">coro-7 start at 57:15</span><br><span class="line">coro-3 start at 57:15</span><br><span class="line">coro-1 start at 57:15</span><br><span class="line"></span><br><span class="line">coro-9 done at 57:16,key is set? True</span><br><span class="line">coro-4 done at 57:16,key is set? True</span><br><span class="line">coro-2 done at 57:16,key is set? True</span><br><span class="line">coro-5 done at 57:16,key is set? True</span><br><span class="line">coro-0 done at 57:16,key is set? True</span><br><span class="line">coro-8 done at 57:16,key is set? True</span><br><span class="line">coro-6 done at 57:16,key is set? True</span><br><span class="line">coro-7 done at 57:16,key is set? True</span><br><span class="line">coro-3 done at 57:16,key is set? True</span><br><span class="line">coro-1 done at 57:16,key is set? True</span><br></pre></td></tr></table></figure><br>可以看到10个redis协程同一时刻并发set key，并且同一时刻完成。<br>在redis-cli查看key，所有的key都value都是同一时刻，说明协程并发运行正确，而在多线程方式，则需要创建<code>10个线程</code>才可以实现<code>单线程+10协程</code>的效果，若当并发量高达1万+时，可以想象多线程将消耗大量系统资源以及线程切换，效率必然不高。<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys foo*</span><br><span class="line"> 1) &quot;foo-8&quot;</span><br><span class="line"> 2) &quot;foo-3&quot;</span><br><span class="line"> 3) &quot;foo-9&quot;</span><br><span class="line"> 4) &quot;foo-0&quot;</span><br><span class="line"> 5) &quot;foo-1&quot;</span><br><span class="line"> 6) &quot;foo-7&quot;</span><br><span class="line"> 7) &quot;foo-5&quot;</span><br><span class="line"> 8) &quot;foo-6&quot;</span><br><span class="line"> 9) &quot;foo-2&quot;</span><br><span class="line">10) &quot;foo-4&quot;</span><br><span class="line">127.0.0.1:6379&gt; get foo-8</span><br><span class="line">&quot;57:16&quot;</span><br><span class="line">127.0.0.1:6379&gt; get foo-0</span><br><span class="line">&quot;57:16&quot;</span><br></pre></td></tr></table></figure><br>这里用两个魔法方法<code>__aenter__</code>,<code>__aexit__</code>实现with协议，但要注意的是，协程的with方法只能在协程函数内部使用，这个with的上下文就是该协程的上下文。如果写在主线程外部，则提示语法出错：<br><img src="https://img-blog.csdnimg.cn/20200104125027492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这是因为协程上下文代表协程自己的栈等信息，肯定不是主线程的上下文，所以不能把<code>async with</code>写在程序的全局位置<br>注意：aioredis似乎有个bug，当MacOS系统的文件描述最大限制已设为10000，aioredis不管使用单连接或者pool方式，当并发数设为大值例如1000，部分协程完成后，剩余部分协程都会被阻塞（暂未找到原因）。</p>
<p>在项目中，如果要使用aioredis，可以用asyncio的semaphore信号量限制并发数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_coro</span>(<span class="params">semaphore,index</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> semaphore:</span><br><span class="line">        r=<span class="keyword">await</span> aioredis.create_redis(<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,db=<span class="number">0</span>)</span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> start at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">0.05</span>)</span><br><span class="line">        key=<span class="string">&#x27;bar-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index)</span><br><span class="line">        result=<span class="keyword">await</span> r.<span class="built_in">set</span>(key,get_time(),expire=<span class="number">100</span>)</span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,key is set? <span class="subst">&#123;result&#125;</span>&#x27;</span>)    </span><br><span class="line">        r.close()</span><br><span class="line">        <span class="keyword">await</span> r.wait_closed()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">max_task=<span class="number">100</span></span>):</span></span><br><span class="line">    sem=asyncio.Semaphore(<span class="number">10</span>)</span><br><span class="line">    tasks=[redis_coro(sem,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_task)]</span><br><span class="line">    <span class="keyword">await</span> asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(run())</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure></p>
<h5 id="4-1-aiohttp"><a href="#4-1-aiohttp" class="headerlink" title="4.1 aiohttp"></a>4.1 aiohttp</h5><p>&#8195;&#8195;Asynchronous HTTP client/server framework for asyncio and Python<br>aiohttp应该是除了request库外最强大的HTTP库，而且是异步实现，三个主要功能：</p>
<ul>
<li>Supports both Client and HTTP Server.</li>
<li>Supports both Server WebSockets and Client WebSockets out-of-the-box without the Callback Hell.</li>
<li>Web-server has Middlewares, Signals and pluggable routing.<br>除了基本client和server端服务，还支持websocket、web-server模块还支持可插拔的中间件，官方也给了详细的aiohttp demo代码，<a href="https://github.com/aio-libs/aiohttp-demos">链接</a></li>
</ul>
<p>以下代码逻辑：使用协程并发get html页面，并使用协程方式存储html到文件或者存到redis<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime,time</span><br><span class="line"><span class="keyword">import</span> asyncio,aiohttp,aioredis,aiofiles</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">save_to_file</span>(<span class="params">html,index</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;将html文本存放到本地目录，使用异步aiofiles实现。&quot;&quot;&quot;</span></span><br><span class="line">    file_name=<span class="string">f&#x27;/opt/test_aiohttp/html-<span class="subst">&#123;index&#125;</span>.txt&#x27;</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(file_name,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">await</span> f.write(html)</span><br><span class="line">    print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">save_to_redis</span>(<span class="params">html,index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将html文本存放到redis，采用协程模式 &quot;&quot;&quot;</span></span><br><span class="line">    r=<span class="keyword">await</span> aioredis.create_redis(<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,db=<span class="number">0</span>)</span><br><span class="line">    coro_key=<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span>&#x27;</span></span><br><span class="line">    result=<span class="keyword">await</span> r.<span class="built_in">set</span>(coro_key,html,expire=<span class="number">60</span>)</span><br><span class="line">    print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,is saved? <span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line">    r.close()</span><br><span class="line">    <span class="keyword">await</span> r.wait_closed()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_html</span>(<span class="params">semaphore,session,url,index,container=<span class="string">&#x27;file&#x27;</span></span>):</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;获取url对应的html文本，并调用存放文本的协程，这里就是前面章节提到的嵌套协层，用await调用外部协程 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> semaphore:<span class="comment"># 由外部传入的信号量，控制并发数</span></span><br><span class="line">            print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> started at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">with</span> session.get(url,timeout=<span class="number">5</span>) <span class="keyword">as</span> response:</span><br><span class="line">                    <span class="keyword">if</span> response.status==<span class="number">200</span>:</span><br><span class="line">                        <span class="keyword">if</span> container==<span class="string">&#x27;file&#x27;</span>:</span><br><span class="line">                            container_=save_to_file</span><br><span class="line">                        <span class="keyword">else</span>:container_=save_to_redis</span><br><span class="line">                        html_text=<span class="keyword">await</span> response.read()</span><br><span class="line">                        <span class="keyword">await</span> container_(html_text,index)</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">semaphore,url,max_workers=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        urls=[url]*max_workers</span><br><span class="line">        tasks=[asyncio.ensure_future(get_html(semaphore,session,each_url,index)) <span class="keyword">for</span> index,each_url <span class="keyword">in</span> <span class="built_in">enumerate</span>(urls)]</span><br><span class="line">        <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    semaphore=asyncio.Semaphore(<span class="number">5</span>)</span><br><span class="line">    loop.run_until_complete(run(semaphore,<span class="string">&#x27;http://spark.apachecn.org/#/&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 官方推荐使用Zero-sleep保证底层的一些socket连接完成关闭</span></span><br><span class="line">    loop.run_until_complete(asyncio.sleep(<span class="number">0</span>))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><br>打印结果：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-0 started at 49:26</span><br><span class="line">coro-1 started at 49:26</span><br><span class="line">coro-2 started at 49:26</span><br><span class="line">coro-3 started at 49:26</span><br><span class="line">coro-4 started at 49:26</span><br><span class="line"></span><br><span class="line">coro-0 done at 49:26</span><br><span class="line">coro-5 started at 49:26</span><br><span class="line">coro-2 done at 49:26</span><br><span class="line">coro-6 started at 49:26</span><br><span class="line">coro-4 done at 49:26</span><br><span class="line">coro-7 started at 49:26</span><br><span class="line">coro-1 done at 49:26</span><br><span class="line">coro-8 started at 49:26</span><br><span class="line">coro-3 done at 49:26</span><br><span class="line">coro-9 started at 49:26</span><br><span class="line"></span><br><span class="line">coro-6 done at 49:26</span><br><span class="line">coro-5 done at 49:27</span><br><span class="line">coro-7 done at 49:27</span><br><span class="line">coro-8 done at 49:27</span><br><span class="line">coro-9 done at 49:27</span><br></pre></td></tr></table></figure><br>这里使用asyncio.Semaphore(5)控制并发数，从输出可以看出，一开始有5个协程启动，最先5个协程运行结束后，另外5个协程同时启动。因为10个任务，分了2轮进行。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本文内容相对较多且杂，主要是asyncio协程库有较多api，这些api与同步编程的python库有较大的区别，结合asyncio实现的几个第三方协层库来看，可以看到协程在python生态位置较为小众，如果在项目（例如高性能web接口服务）中引入协程异步编程，可以考虑Tornado以及Twisted。（想想Go语言还是相当强大，协程生态成熟）</p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>协程</tag>
        <tag>asyncio</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：jdk1.8的HashMap红黑树设计原理及其源代码深入解析（不含balanceDetection方法）</title>
    <url>/2021/01/10/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9Ajdk1.8%E7%9A%84HashMap%E7%BA%A2%E9%BB%91%E6%A0%91%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E6%BA%90%E4%BB%A3%E7%A0%81%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%8D%E5%90%ABbalanceDetection%E6%96%B9%E6%B3%95%EF%BC%89/</url>
    <content><![CDATA[<p>在前面的《jdk1.8的HashMap源码分析》文章已经给出HashMap中数组+链表这一部分的内容，本篇文章将剩余的HashMap里面红黑树及其相关操作源码进行解析，内容较多，因此单独放在一篇文章进行讨论。</p>
<h4 id="一、背景知识"><a href="#一、背景知识" class="headerlink" title="一、背景知识"></a>一、背景知识</h4><p>由于红黑树的插入、删除、扩容等操作相对复杂，因此建议先熟悉基本数据结构，例如二叉树、二叉搜索树及其关于它的查找、插入、删除操作、2-3节点树等。</p>
<p>本人假定看此文章的同学已经具备基本的数据结构知识，因此，关于红黑树的背景知识，这里不再累赘。</p>
<p>冷知识：红黑树为什么叫红黑？节点为什么被标记为红色和黑色， 可以改为蓝黄树、绿蓝树等吗？</p>
<blockquote>
<p>首先红黑树第一版在1972年由<a href="https://baike.baidu.com/item/Rudolf Bayer/3014716">Rudolf Bayer</a>发明，当时的学名是平衡二叉B树（symmetric binary B-trees），还不是被称为Red Black Trees。后来平衡二叉B树在1978年被 Leo J. Guibas 和 Robert Sedgewick 修改为现在版本的红黑树，他们之所以称之为“红黑”树，因为他们在研究这种树型数据结构过程中，需要在草稿纸上作图，用的恰是红色笔和黑色笔，红黑笔非常方便给相关节点标记颜色以便可视化设计相关逻辑，因此“红黑树”的红黑来源于此。</p>
</blockquote>
<h4 id="二、红黑树性质："><a href="#二、红黑树性质：" class="headerlink" title="二、红黑树性质："></a>二、红黑树性质：</h4><p>以下5个性质结合基于序列3，7，11，15，19，23，27，31，35，构成的一棵红黑树进行理解（这里给出的是有序序列，其实即使原序列是无序的， 被重构成为红黑树后，在红黑树也会形成二叉树搜索树的有序序列）</p>
<ul>
<li>1、树上的所有节点都被标记颜色，节点可以被标记位黑色，也可以被标记为红色，</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/fabede1ed258deaa04d711f102d7b5bf.png" alt="32"></p>
<ul>
<li><p>2、root根节点必须被标记为黑色</p>
</li>
<li><p>3、所有的叶子节点都被标记为黑色，而且是NIL节点（需要注意：在JDK1.8的HashMap中，没有NIL命名的节点，也不是所谓的用null来表示NIL节点，在分析原理上可以将其当做虚构的null节点来对待，不影响结构，在平常作图中，NIL叶子节点可以忽略，在这里只是为了说明红黑树有NIL这种设计，因此在图中显式画出）</p>
</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/28b15aac87162581e219cdc209754eea.png" alt="33"></p>
<ul>
<li>4、每个被标记为红色的节点，它的两子节点一定都是黑色</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/d221fc3aeb424debba9902bae4108be4.png" alt="35"></p>
<p>第4点也可以推出这样的结论：两个红节点一定不会直接相连，也即：红色节点与红色节点不能直接连接，或者说，红色节点的父节点及其子节点都不能是红色节点</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ca3c07cdf2c39a892f1f5d7a963772ea.png" alt="36"></p>
<ul>
<li>5、任一节点到每个叶子结点的路径都包含数量相等的黑色节点，俗称：黑色平衡或者黑高，BlackHeight，如 下图的5条路径，每条路径经过黑色节点数都是2，NIL节点不作为黑色节点计数。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/e10573191d904780f357d27bb40f70d4.png" alt="37"></p>
<a id="more"></a>
<p>以上5条特点也是红黑树的构成规则</p>
<p>优势：<br>（1）自平衡。红黑树从根到叶子的最长路径不会超过最短路径的2倍，解决了二叉查找树容易不平衡的缺陷（在某些情况下会退化成一个线性结构），提高了读取性能（树越平衡，读取性能就越好）。<br>（2）虽然AVL树具有更高的读取性能（因为平衡性更好），但是当插入或删除节点时，AVL树要复杂很多，红黑树在插入或删除节点方面具有更高的效率。</p>
<p>在什么情况下需要变色，在什么情况下需要旋转？<br>在红黑二叉树中插入节点或删除节点后，如果破坏了红黑树的规则（也就是上述的特性），则需要对修改后的红黑树进行调整，使其重新符合红黑树的规则。首先是变色（往往需要多次变色，一次改变一个节点的颜色），当变色无法使得当前红黑树平衡时，就使用左旋或者右旋，旋转一次之后，然后再继续多次变色，如此反复循环，直到修改后的红黑树重新符合规则。</p>
<h4 id="三、为何要对红黑树进行变色、左旋、右旋操作？"><a href="#三、为何要对红黑树进行变色、左旋、右旋操作？" class="headerlink" title="三、为何要对红黑树进行变色、左旋、右旋操作？"></a>三、为何要对红黑树进行变色、左旋、右旋操作？</h4><p>1）首先若要生成一棵符合红黑树特点的红黑树，那么必然需要插入一定的节点（插入过程就包含了变色、左旋、右旋操作），从而构成一棵“固化平衡”的红黑树，如果已经构成这颗红黑树不再对其插入新节点或者删除节点，则无需再对其进行各种方式的调整。</p>
<p>2）对于一棵已经存在的红黑树，若要对其再插入新节点或者删除节点操作，那么这些操作可能会破坏红黑树的平衡约束，导致插入节点或者删除节点之后的“红黑树”不是一棵“平衡”的红黑树，那么怎么办？</p>
<p>这么办：根据红黑树的特点，<font color=red>需要额外设计一些补充操作来使得插入节点或删除节点之后的“不正常红黑树”变成正常的、平衡的红黑树，发明者经过研究，其实这些额外的操作无非就三种：变色、左旋、右旋</font></p>
<p>3）为何会有变色（颜色改变）操作？</p>
<p>举个特殊例子，对于序列3，7，11，15，19，23，27，31，35当插入首个节点3时。</p>
<p>如下图所示：</p>
<p>（需要明确的的一点是：红黑树的待插入节点必须先标记位红色。）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/915d8acd26548da06e143a1a1814b12c.png" alt="39"></p>
<p>这张图会让体现出：红黑树的平衡维护在视觉上好像也需要这样的变色操作。</p>
<p>4）为何会左旋操作？</p>
<p>在这里暂且不深入论证左旋操作，看下面的图简要说明：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/423ded2b57d297bcd3a967ff4cf195ed.png" alt="40"></p>
<p>可能有人会问，为何在最后需要将3节点黑色变成红色？可以保持红色吗？</p>
<p>将黑3改为红3，本质是为了这颗树看起来更加平衡，而且是黑色平衡，同时在未来的不断插入、删除节点条件下形成的红黑树也会持续保持“优良传统”的树平衡，</p>
<p>若节点3改为红色，在之后不断插入节点、不断调整红黑树结构的条件下最终得到的树将不是一棵符合红黑树特点的树，那么这个“无名树”也无法实现像红黑树的所有性能。</p>
<p>5）为何会右旋操作？</p>
<p>插入3节点，若不进行右旋，树的重心会偏向左边，看起来不平衡，通过右旋，树看起来平衡多了（其实是防止树结构变成长链表形状）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4f07194c5e2e87cd6a3f1118a26ad236.png" alt="41"></p>
<p>正是对一棵树建立“变色、左旋、右旋”的约束机制，使得该树结构符合我们期望的性能。</p>
<h4 id="四、红黑树左旋、右旋完整操作"><a href="#四、红黑树左旋、右旋完整操作" class="headerlink" title="四、红黑树左旋、右旋完整操作"></a>四、红黑树左旋、右旋完整操作</h4><p>第三节的内容则给出相对节点的结构，有助于理解红黑树是通过不断变色、左旋、右旋操作，以维持自身树的平衡，最终实现高效的查询、插入、和删除性能，因此掌握红黑树的5点特点对于红黑树所有操作才能真正理解。</p>
<p>在解释完整左旋和右旋操作前，先做以下基本约定，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/fea7d8c802188ee3cd437720c5904ce3.png" alt="43">             </p>
<p>以下约定是从l节点、r节点的视角来看其他位置节点的角色，l：left的缩写，r：right的缩写</p>
<ul>
<li>pp节点：l节点、r节点的祖父节点（Grand parent node）</li>
<li>p节点：l节点、r节点的父节点（parent node）</li>
<li>l节点：p节点的左节点（或左子节点）</li>
<li>r节点：p节点的右节点（或右子节点）</li>
<li>ll节点：l节点的左子节点 </li>
<li>lr节点：l节点右子节点 </li>
<li>rl节点：r节点左子节点 </li>
<li>rl节点：r节点右子节点 </li>
</ul>
<h5 id="4-1-理解左旋"><a href="#4-1-理解左旋" class="headerlink" title="4.1 理解左旋"></a>4.1 理解左旋</h5><p>以下的左旋图示，就像有这样拟人化操作：</p>
<p>将r节点“提起来”放在p节点所在位置，将r的左子节点rl“剪下来”。将p节点“挂在”r节点左子节点位置。</p>
<p>前面被“剪枝”的rl节点“挂到”p节点的右子节点位置——可以这样节点理解为：拿多的一侧“补给”少的一侧，以使得树两边“重量”相对接近，从而构成比上一次更加平衡的红黑树结构。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ac2c930fa74c03ecf6dc7f3f4d6bf9bb.png" alt="42"></p>
<p>为何这样的操作是“可行的”的，首先红黑树本身具有二叉搜索树的一些特征：</p>
<p>左子树上所有结点的值均小于等于它的<a href="https://baike.baidu.com/item/根结点/9795570">根结点</a>的值（若左子树不空时）</p>
<p>右子树上所有结点的值均大于等于它的<a href="https://baike.baidu.com/item/根结点/9795570">根结点</a>的值（若右子树不空时）</p>
<p>从上图也可以观察出（只考察pp、p、r、rl节点），左旋前后四个节点排序保持不变：</p>
<p>左旋前：这四个节点值的大小排序为p&lt;=rl&lt;=r&lt;=pp`</p>
<p>左旋后：这四个节点值的大小排序为p&lt;=rl&lt;=r&lt;=pp`</p>
<p>或者有可以采用投影法——从上方垂直投影到下方的方法进行考察：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c503af25e3b52b74c0c0f43f441b4e0c.png" alt="44"></p>
<p>上图清晰说明左旋操作节点值排序不变，另外一个更为重要的收益则是：左旋操作竟然可以让树的两边更加平衡</p>
<p>左旋、右旋源码做的事情无非以下三类：</p>
<p>p节点下来后要和下方rl建立关系</p>
<p>r节点上去后要和上方pp建立关系</p>
<p>再把r节点和p节点建立关系，从而实现完整的关系链</p>
<p>源码解析如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在插入节点代码片段引用了左旋、右旋操作 root = rotateLeft(root, x = xp); </span></span><br><span class="line"><span class="keyword">static</span> &lt;K,V&gt; <span class="function">TreeNode&lt;K,V&gt; <span class="title">rotateLeft</span><span class="params">(TreeNode&lt;K,V&gt; root,</span></span></span><br><span class="line"><span class="function"><span class="params">                                              TreeNode&lt;K,V&gt; p)</span> </span>&#123;</span><br><span class="line">            TreeNode&lt;K,V&gt; r, pp, rl;</span><br><span class="line">  					<span class="comment">/*</span></span><br><span class="line"><span class="comment">  					r=p.right;</span></span><br><span class="line"><span class="comment">  					if(p !=null &amp;&amp; r !=null)&#123;</span></span><br><span class="line"><span class="comment">  					    </span></span><br><span class="line"><span class="comment">  					    1) p与rl建立关系：p的右节点r的左子节点rl变成p的右节点</span></span><br><span class="line"><span class="comment">  							p.right=r.left;</span></span><br><span class="line"><span class="comment">  							rl=r.left;</span></span><br><span class="line"><span class="comment">  							if(rl!=null) </span></span><br><span class="line"><span class="comment">  									rl.parent=p;</span></span><br><span class="line"><span class="comment">  							2) r与pp建立关系： p节点的父节点pp为空的情况，说明p本来就是根节点，旋转后，r变成根节点，若p原来是pp的左节点，则r取代p后也要保持左节点位置</span></span><br><span class="line"><span class="comment">  							// rl = p.right = r.left 看不懂？ 其实是这种表达式：a=b=1,也即a=1,b=1</span></span><br><span class="line"><span class="comment">  							r.parent=p.parent</span></span><br><span class="line"><span class="comment">  							pp=p.parent</span></span><br><span class="line"><span class="comment">  							if(pp ==null) </span></span><br><span class="line"><span class="comment">                &#123;</span></span><br><span class="line"><span class="comment">                	root=r;</span></span><br><span class="line"><span class="comment">                	root.red=false;</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">               </span></span><br><span class="line"><span class="comment">                if (pp.left=p) pp.left=r</span></span><br><span class="line"><span class="comment">  							else  pp.right=r</span></span><br><span class="line"><span class="comment">  							3)r与p建立关系：p作为r的左节点，r作为p的父节点</span></span><br><span class="line"><span class="comment">  								r.left=p</span></span><br><span class="line"><span class="comment">  								p.parent=r;</span></span><br><span class="line"><span class="comment">  					</span></span><br><span class="line"><span class="comment">  					&#125;</span></span><br><span class="line"><span class="comment">  					*/</span></span><br><span class="line">  					</span><br><span class="line">            <span class="keyword">if</span> (p != <span class="keyword">null</span> &amp;&amp; (r = p.right) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> ((rl = p.right = r.left) != <span class="keyword">null</span>)</span><br><span class="line">                    rl.parent = p;</span><br><span class="line">              	<span class="comment">// 此类连续赋值变量写法一定要自行拆开多个，否则不容易理解代码逻辑</span></span><br><span class="line">                <span class="keyword">if</span> ((pp = r.parent = p.parent) == <span class="keyword">null</span>)</span><br><span class="line">                    (root = r).red = <span class="keyword">false</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (pp.left == p)</span><br><span class="line">                    pp.left = r;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    pp.right = r;</span><br><span class="line">                r.left = p;</span><br><span class="line">                p.parent = r;</span><br><span class="line">            &#125;</span><br><span class="line">  					<span class="comment">// 如果p节点为空说明现在是空树，直接返回root</span></span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">  				<span class="comment">// 返回根节点  因为根节点在旋转的过程中可能会改变 就需要返回改变后的</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="4-2-理解右旋"><a href="#4-2-理解右旋" class="headerlink" title="4.2 理解右旋"></a>4.2 理解右旋</h5><p><strong>右旋：以某个点（h）旋转，旋转点(h)左节点的右子节点变为旋转点的左节点，旋转点之前的左节点变为父节点</strong></p>
<p>右旋的工作机制其实跟左旋一样，只不过方向相反，如下图所示，文字说明以及源码分析则不再累赘。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/dce014937aa2a390228cae4925884c50.png" alt="45"></p>
<h4 id="五、插入红黑树节点"><a href="#五、插入红黑树节点" class="headerlink" title="五、插入红黑树节点"></a>五、插入红黑树节点</h4><h5 id="5-1-插入总体思路设计"><a href="#5-1-插入总体思路设计" class="headerlink" title="5.1 插入总体思路设计"></a>5.1 插入总体思路设计</h5><p>在解析插入红黑树节点及其自平衡处理前，先从put源码快速回忆HashMap插入一个元素过程：如下面注释的4种插入情况</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value,...</span></span></span><br><span class="line"><span class="function"><span class="params">				//......</span></span></span><br><span class="line"><span class="function"><span class="params">        //<span class="number">1</span>、如果key定位到空的桶位上，则直接在桶位放入该新节点           </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>)</span> &amp; hash]) </span>== <span class="keyword">null</span>)</span><br><span class="line">            tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//2、如果插入的key刚好与桶位节点（头节点）的key相同，不做插入操作，在后面更新value即可</span></span><br><span class="line">            <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">                ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                e = p;</span><br><span class="line">         <span class="comment">//3、如果插入的key与p哈希碰撞（当然key不等于p.key），且桶位节点p为红黑树节点，那么需要使用putTreeVal将key新节点插入到红黑树里面，这里是本节重点分析内容</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="comment">//4、如果桶位上p是一条冲突链，进行冲突链插入、树化等操作</span></span><br><span class="line">                  <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line">                      <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                          p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">                          <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></span><br><span class="line">                              treeifyBin(tab, hash);</span><br><span class="line">                          <span class="keyword">break</span>;</span><br><span class="line">                      &#125;</span><br><span class="line">         <span class="comment">//....</span></span><br></pre></td></tr></table></figure>
<p>对应第3种插入情况，插入的为红黑树节点，再来看看putTreeVal的内部主要3个逻辑：</p>
<p>1）待插入key节点恰好能在红黑树里面找到则返回该节点，否则进入2）步骤</p>
<p>2）待插入key节点不在红黑树里面，就需要找到合适的父节点p，再将key节点插入父节点p的左边或者右边</p>
<p>3) 红黑树新增key节点后需要对红黑树做自平衡操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);</span></span><br><span class="line">   <span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">putTreeVal</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab,<span class="keyword">int</span> h, K k, V v)</span> </span>&#123;</span><br><span class="line"><span class="comment">// ......</span></span><br><span class="line">     <span class="comment">// 1) 遍历红黑树，找与待插入key相等的节点</span></span><br><span class="line">     	<span class="keyword">boolean</span> searched = <span class="keyword">false</span>;<span class="comment">//这个searched为true时表示待插入key节点能在红黑树里找到</span></span><br><span class="line">     	TreeNode&lt;K,V&gt; root = (parent != <span class="keyword">null</span>) ? root() : <span class="keyword">this</span>;</span><br><span class="line">       <span class="keyword">for</span> (TreeNode&lt;K,V&gt; p = root;;) &#123;</span><br><span class="line">           <span class="keyword">int</span> dir, ph; K pk;</span><br><span class="line">           <span class="keyword">if</span> ((ph = p.hash) &gt; h)</span><br><span class="line">               dir = -<span class="number">1</span>;</span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> (ph &lt; h)</span><br><span class="line">               dir = <span class="number">1</span>;</span><br><span class="line">           <span class="comment">// 找与待插入key相等的节点</span></span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> ((pk = p.key) == k || (k != <span class="keyword">null</span> &amp;&amp; k.equals(pk)))</span><br><span class="line">               <span class="keyword">return</span> p;</span><br><span class="line"> <span class="comment">// .......</span></span><br><span class="line">                   <span class="keyword">if</span> (((ch = p.left) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                        (q = ch.find(h, k, kc)) != <span class="keyword">null</span>) ||</span><br><span class="line">                       ((ch = p.right) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                        (q = ch.find(h, k, kc)) != <span class="keyword">null</span>))</span><br><span class="line">                       <span class="keyword">return</span> q;</span><br><span class="line">               &#125;</span><br><span class="line">      <span class="comment">// .......</span></span><br><span class="line">      <span class="comment">// 2) 如果1)前面未找到，说明该key为新节点，需要在红黑树里面找正确的位置父节点xp，在父节点xp左边或者右边新增一个该节点，双向链表也要同时新增该节点。</span></span><br><span class="line">           TreeNode&lt;K,V&gt; xp = p;</span><br><span class="line">           <span class="keyword">if</span> ((p = (dir &lt;= <span class="number">0</span>) ? p.left : p.right) == <span class="keyword">null</span>) &#123;</span><br><span class="line">               Node&lt;K,V&gt; xpn = xp.next;</span><br><span class="line"><span class="comment">// 此操作非常容易被忽略：由于红黑树还本身也是一条双向链表，当红黑树新增节点时，双向链表也要新增对应的新节点</span></span><br><span class="line">               TreeNode&lt;K,V&gt; x = map.newTreeNode(h, k, v, xpn);</span><br><span class="line">               <span class="keyword">if</span> (dir &lt;= <span class="number">0</span>)</span><br><span class="line">                   xp.left = x; </span><br><span class="line">               <span class="keyword">else</span></span><br><span class="line">                   xp.right = x;</span><br><span class="line"><span class="comment">// .......</span></span><br><span class="line">     <span class="comment">// 3) 收尾工作：代码执行到这里，说明已经在红黑树插入了新节点， 需对红黑树做平衡操作,moveRootToFront在后面的小节给出</span></span><br><span class="line">               moveRootToFront(tab, balanceInsertion(root, x));</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>HashMap在put一个节点恰好put到红黑树里面的流程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map.put(key,value)-&gt;putval-&gt;putTreeVal-&gt;balanceInsertion-&gt;moveRootToFront</span><br></pre></td></tr></table></figure>
<p>插入节点<code>putTreeVal</code>源码并不难理解，复杂的是后面的平衡处理：<code>balanceInsertion</code></p>
<h5 id="5-2-平衡操作的设计解析"><a href="#5-2-平衡操作的设计解析" class="headerlink" title="5.2  平衡操作的设计解析"></a>5.2  平衡操作的设计解析</h5><p>在第三节提到红黑树平衡操作就是“变色、左旋、右旋”，其实在<code>balanceInsertion</code>内部实现也可以看出这些关键字：</p>
<p><code>x.red = false</code>、<code>rotateLeft</code>、<code>rotateRight</code>，当然对应下面的问题：</p>
<p>插入节点后，在什么情况下需要变色？</p>
<p>插入节点后，在什么情况下需要左旋？</p>
<p>插入节点后，在什么情况下需要右旋？</p>
<p>插入节点后，在什么情况下需要进行以上多种组合操作？</p>
<p>当然，每种平衡处理都是基于这样的前提：</p>
<p>1、对于<code>balanceInsertion(root, x)</code>入参root引用，拿到这个root节点，说明就拿到了一棵红黑树，因此对root为根结点的红黑树施加平衡调整</p>
<p>2、对于<code>balanceInsertion(root, x)</code>入参x引用，这个节点x已经在<code>balanceInsertion</code>执行前完成了位置插入，一定要记着：节点x的位置插入，不是在<code>`balanceInsertion</code>里面完成！</p>
<p>为了能将原理分析和源码分析的节点标识一一对应，这里做了如下约定：</p>
<p>示意图的节点标识来源于源码balanceInsertion里面的临时TreeNode类型的引用：<code>TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr</code></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/9b72bfae51723b12ae98034499d85d40.png" alt="47"></p>
<p>x节点：新插入的节点，在<code>balanceInsertion</code>调用前，x已经完成了插入。</p>
<p>xp节点：插入节点x的父节点</p>
<p>xpp节点：插入节点x的祖父节点</p>
<p>xppl节点（x的左变叔叔节点）：若xp节点位于xpp右边（此时xppr就是xp），那么xppl节点就是x节点的左边叔叔节点，</p>
<p>xppr节点（x的右边叔叔节点）：若xp节点位于xpp左边（此时xppl就是xp），那么xppr节点就是x节点的右边叔叔节点</p>
<h5 id="5-3-balanceInsertion图解-源码分析"><a href="#5-3-balanceInsertion图解-源码分析" class="headerlink" title="5.3 balanceInsertion图解+源码分析"></a>5.3 balanceInsertion图解+源码分析</h5><p>有了5.2的基础知识铺垫，则能很好理解按分类讨论的方式去分析每种情况的平衡操作</p>
<h6 id="5-3-1-若原root节点是null时"><a href="#5-3-1-若原root节点是null时" class="headerlink" title="5.3.1 若原root节点是null时"></a>5.3.1 若原root节点是null时</h6><p>原root节点是null时说明是空红黑树，因此插入节点x就作为root节点：<code>root=x</code>，从红黑树特点可知，<code>balanceInsertion(root, x)</code>里面会对x进行变色操作<code>x.red=false</code>,平衡调整结束，并返回x节点，同时它也是root节点</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/31bbd7b93dcdca32b0ae88f59b7caf21.png" alt="46"></p>
<p>该情况对应的源码片段（仅对应第一次循环的情况）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> &lt;K,V&gt; <span class="function">TreeNode&lt;K,V&gt; <span class="title">balanceInsertion</span><span class="params">(TreeNode&lt;K,V&gt; root,TreeNode&lt;K,V&gt; x)</span> </span>&#123;</span><br><span class="line">            x.red = <span class="keyword">true</span>; <span class="comment">// 插入节点x默认是红色节点</span></span><br><span class="line">            <span class="keyword">for</span> (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; <span class="comment">// 若root为空树，这里的for循环执行一次就退出</span></span><br><span class="line">                <span class="keyword">if</span> ((xp = x.parent) == <span class="keyword">null</span>) &#123;  <span class="comment">// 首先x节点putVal插入位置，x.parent若为null，说明x节点当前位于root节点位置，那么x就要被作为root节点看待</span></span><br><span class="line">                    x.red = <span class="keyword">false</span>; <span class="comment">// 因为x已经作为root节点，需将x节点变为黑色</span></span><br><span class="line">                    <span class="keyword">return</span> x;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//......</span></span><br></pre></td></tr></table></figure>
<h6 id="5-3-2-若原红黑树已经存在x节点时"><a href="#5-3-2-若原红黑树已经存在x节点时" class="headerlink" title="5.3.2 若原红黑树已经存在x节点时"></a>5.3.2 若原红黑树已经存在x节点时</h6><p>由于x节点已经存在，不需要插入，只需更新value或者直接返回即可，由于情况简单，无需给出图示</p>
<p>但需要注意的是：在红黑树找到相同的key节点是在<code>putTreeVal</code>实现的，而更新value是在putVal实现的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在红黑树找到相同的key是在`putTreeVal`实现的</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">putTreeVal</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab,</span></span></span><br><span class="line"><span class="function"><span class="params">                                       <span class="keyword">int</span> h, K k, V v)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (TreeNode&lt;K,V&gt; p = root;;) &#123; </span><br><span class="line">                <span class="keyword">int</span> dir, ph; K pk;</span><br><span class="line">                <span class="keyword">if</span> ((ph = p.hash) &gt; h)</span><br><span class="line">                    dir = -<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (ph &lt; h)</span><br><span class="line">                    dir = <span class="number">1</span>;</span><br><span class="line">                <span class="comment">// 1）在红黑树找到相同的key的节点</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> ((pk = p.key) == k || (k != <span class="keyword">null</span> &amp;&amp; k.equals(pk)))</span><br><span class="line">                    <span class="keyword">return</span> p;</span><br><span class="line">            		<span class="comment">//......    </span></span><br><span class="line">            &#125;</span><br><span class="line">              </span><br><span class="line"><span class="comment">//更新value是在`putVal`实现的</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,<span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">						<span class="comment">//......</span></span><br><span class="line">  					<span class="comment">//(TreeNode&lt;K,V&gt;)p的说明，在前面p是Node&lt;K,V&gt;类型的普通节点，需要将其转为TreeNode&lt;K,V&gt;类型，以进行红黑树节点相关方法的调用</span></span><br><span class="line">						e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">            <span class="comment">//......   </span></span><br><span class="line">            <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></span><br><span class="line">                V oldValue = e.value;</span><br><span class="line">                <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line">                 <span class="comment">// 2）在1)找到的相同key的红黑树节点e上，对e更新value</span></span><br><span class="line">                    e.value = value;</span><br><span class="line">            <span class="comment">//......  </span></span><br><span class="line">            &#125;      </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h6 id="5-3-3-若插入节点x的父节点xp是黑色或者父节点xp就是根节点"><a href="#5-3-3-若插入节点x的父节点xp是黑色或者父节点xp就是根节点" class="headerlink" title="5.3.3 若插入节点x的父节点xp是黑色或者父节点xp就是根节点"></a>5.3.3 若插入节点x的父节点xp是黑色或者父节点xp就是根节点</h6><p>1）因为插入节点x的父节点xp是黑色，所以可以直接插入x节点，而且x插入到xp左子节点位置或者右子节点位置，都不会影响原红黑树平衡，因此<code>balanceInsertion(root, x)</code>无需做平衡操作，直接返回root节点</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6e18ec36bf50014d068c07184a8ce632.png" alt="48"></p>
<p>2）若父节点xp就是根节点，显然xp一定黑色，所以也可以直接插入x节点，而且x插入到xp左子节点位置或者右子节点位置，都不会影响原红黑树平衡，因此<code>balanceInsertion(root, x)</code>无需做平衡操作，直接返回root节点</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/23275e61977d3bea50a9790eaa85c7a5.png" alt="49"></p>
<p>对应的源码片段：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> &lt;K,V&gt; <span class="function">TreeNode&lt;K,V&gt; <span class="title">balanceInsertion</span><span class="params">(TreeNode&lt;K,V&gt; root,TreeNode&lt;K,V&gt; x)</span> </span>&#123;</span><br><span class="line">            x.red = <span class="keyword">true</span>;</span><br><span class="line">              <span class="keyword">for</span> (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123;</span><br><span class="line">                <span class="comment">//......</span></span><br><span class="line">  							<span class="comment">//如果x的父节点xp为黑色(!xp.red)，或者xp就是根节点(xpp=null)，由于x已经在balanceInsertion调用前完成了插入因此在这里直接返回root节点就行，无需调整平衡。</span></span><br><span class="line">                <span class="comment">// 注意：对于刚接触红黑树插入操作的同学，在这里可能容易有这样的误解“父节点xp为黑色，或者xp就是根节点，故在此处执行可以安排插入x节点的操作”。然而源码在这里并没有插入节点代码，而是直接返回root节点，这容易让人混淆。</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (!xp.red || (xpp = xp.parent) == <span class="keyword">null</span>) </span><br><span class="line">                    <span class="keyword">return</span> root;</span><br><span class="line">                <span class="comment">//......</span></span><br><span class="line">              &#125;</span><br></pre></td></tr></table></figure>
<h6 id="5-3-4-插入节点x的父节点xp是红色，x的叔叔节点（xppl-xppr）是红色"><a href="#5-3-4-插入节点x的父节点xp是红色，x的叔叔节点（xppl-xppr）是红色" class="headerlink" title="5.3.4 插入节点x的父节点xp是红色，x的叔叔节点（xppl/xppr）是红色"></a>5.3.4 插入节点x的父节点xp是红色，x的叔叔节点（xppl/xppr）是红色</h6><p>由于插入节点x的父节点xp是黑色的情况已经讨论过，那么剩下需要处理父节点xp为红色的情况，当x的叔叔节（xppl/xppr）是红色，分为以下两种情况，但这两种情况都可以用同一段代码处理：</p>
<p>1）x的父节点xp和x的右边叔叔节点xppr同为红色的情况（此时xp其实也是xppl）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c8a1a0adc6378f53c0c46d28ad4f274e.png" alt="50"></p>
<p>2）x的父节点xp和x的左边叔叔节点xppl同为红色的情况（此时xp其实也是xppr）</p>
<p>此情况的图跟1）情况是对称的，这里不再给出。</p>
<p>从上图也可以看出，不管x节点位于xp节点的左边还是右边，以及xp节点位于xpp的左边还是右边，两种情况都可以用以下同一段代码处理（代码和上图紧密结合才能理解深刻）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> &lt;K,V&gt; <span class="function">TreeNode&lt;K,V&gt; <span class="title">balanceInsertion</span><span class="params">(TreeNode&lt;K,V&gt; root,TreeNode&lt;K,V&gt; x)</span> </span>&#123;</span><br><span class="line">            x.red = <span class="keyword">true</span>;</span><br><span class="line">			<span class="keyword">for</span> (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123;</span><br><span class="line">          <span class="comment">//......</span></span><br><span class="line">          <span class="comment">// 代码执行到这里，说明xp是红色，如果xp节点是xpp的左子节点（xp=xppl）</span></span><br><span class="line">					<span class="keyword">if</span> (xp == (xppl = xpp.left)) &#123;</span><br><span class="line">                    <span class="comment">// 插入节点x的右边叔叔节点非空且为红色</span></span><br><span class="line">                    <span class="keyword">if</span> ((xppr = xpp.right) != <span class="keyword">null</span> &amp;&amp; xppr.red) &#123;</span><br><span class="line">                        xppr.red = <span class="keyword">false</span>; <span class="comment">// 叔叔节点变黑</span></span><br><span class="line">                        xp.red = <span class="keyword">false</span>; <span class="comment">// 父节点xp变黑</span></span><br><span class="line">                        xpp.red = <span class="keyword">true</span>; <span class="comment">// 祖父节点xpp变红</span></span><br><span class="line">                        x = xpp; <span class="comment">//祖父节点xpp变红后，可能与xpp父节点xppp同为红色，违反红黑树平衡，因此需要通过for循环继续往上处理平衡。</span></span><br><span class="line">                    &#125;</span><br></pre></td></tr></table></figure>
<h6 id="5-3-5-插入节点x的父节点xp是红色，x的叔叔节（xppl-xppr）是黑树或者NIL节点"><a href="#5-3-5-插入节点x的父节点xp是红色，x的叔叔节（xppl-xppr）是黑树或者NIL节点" class="headerlink" title="5.3.5 插入节点x的父节点xp是红色，x的叔叔节（xppl/xppr）是黑树或者NIL节点"></a>5.3.5 插入节点x的父节点xp是红色，x的叔叔节（xppl/xppr）是黑树或者NIL节点</h6><p>由于插入节点x的父节点xp是黑色的情况已经讨论过，那么剩下需要处理父节点xp为红色的情况，当x的叔叔节点（xppl/xppr）是黑色或者NIL节点，可分为以下四种情况</p>
<p>1）“左右同红”（xp位于xpp的左边且x位于xp的右边）</p>
<p>2）“左左同红”（xp位于xpp的左边且x位于xp的左边）</p>
<p>3）“右左同红”（xp位于xpp的右边且x位于xp的左边）</p>
<p>4）“右右同红”（xp位于xpp的右边且x位于xp的右边）</p>
<font color=red>此外，插入节点x的父节点xp为红色，根据红黑树红红节点不能直接连接的特点，可以推出插入节点x的祖父节点xpp一定存在且为黑色</font>

<p>具体如下</p>
<p>1）“左右同红”（xp位于xpp的左边且x位于xp的右边）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0e8f76faf016539b015df70ce12d956f.png" alt="51"></p>
<p><code>balanceInsertion</code>对应的源码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//.......</span></span><br><span class="line">	<span class="comment">// 父节点xp在“左”</span></span><br><span class="line">         <span class="keyword">if</span> (xp == (xppl = xpp.left)) &#123;</span><br><span class="line">     					<span class="comment">//......</span></span><br><span class="line">             <span class="keyword">else</span> &#123;</span><br><span class="line">                 <span class="comment">// 在外层if给出xp是xpp.left的前提下，x是xp.right，也即“左右同红”</span></span><br><span class="line">                 <span class="keyword">if</span> (x == xp.right) &#123;</span><br><span class="line">      							<span class="comment">// 以xp作为旋转点进行左旋，会变成“左左同红”情况，如上图所示。这里返回root节点是为后面的if继续处理“左左同红”提供入口					</span></span><br><span class="line">                   <span class="comment">// 左旋后，x已变成xp的父节点，xp则变成x的左子节点								</span></span><br><span class="line">                     root = rotateLeft(root, x = xp); </span><br><span class="line">                   <span class="comment">// 由于左旋后，xp位置已经发生改变，因此需要将xp和xpp指向x新的父节点和祖父节点</span></span><br><span class="line">                     xpp = (xp = x.parent) == <span class="keyword">null</span> ? <span class="keyword">null</span> : xp.parent;</span><br><span class="line">                 &#125;</span><br><span class="line">               </span><br></pre></td></tr></table></figure>
<p>2）“左左同红”（xp位于xpp的左边且x位于xp的左边）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8cdb7771f30780b8432ee826e08545f3.png" alt="52"></p>
<p><code>balanceInsertion</code>对应的源码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> (xp == (xppl = xpp.left)) &#123;</span><br><span class="line"><span class="comment">//.......           				</span></span><br><span class="line">          <span class="keyword">else</span> &#123;</span><br><span class="line">            	<span class="comment">// 在外层if给出xp是xpp.left的前提下,x是xp.left（也即左左同红情况）</span></span><br><span class="line">              <span class="keyword">if</span> (xp != <span class="keyword">null</span>) &#123;</span><br><span class="line">                  xp.red = <span class="keyword">false</span>; <span class="comment">// 父节点xp变黑</span></span><br><span class="line">                  <span class="keyword">if</span> (xpp != <span class="keyword">null</span>) &#123; </span><br><span class="line">                      xpp.red = <span class="keyword">true</span>; <span class="comment">// 祖父节点xpp变红</span></span><br><span class="line">               <span class="comment">// 以xpp作为旋转点进行右旋，以实现本次循环中xpp、xp、x构成的三层子树黑色节点平衡。</span></span><br><span class="line">                      root = rotateRight(root, xpp); </span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>3）“右左同红”（xp位于xpp的右边且x位于xp的左边）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4d1bb2e9977277e4ebcdacc804c16113.png" alt="59"></p>
<p><code>balanceInsertion</code>对应的源码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> 					<span class="keyword">if</span> (xp == (xppl = xpp.left)) &#123;</span><br><span class="line"><span class="comment">//......</span></span><br><span class="line">          <span class="comment">// 处理&quot;“左右同红”、“左左同红”情况</span></span><br><span class="line">          &#125;						</span><br><span class="line">          <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">//运行到这里，说明xp=xpp.right</span></span><br><span class="line">              <span class="keyword">if</span> (x == xp.left) &#123; <span class="comment">// 在xp=xpp.right的前提下，x是xp.left，也即（“右左同红”）</span></span><br><span class="line">   				<span class="comment">// 以xp作为旋转点进行右旋，会变成“右右同红”情况，如上图所示。这里返回root节点是为后面的if继续处理“右右同红”提供入口					</span></span><br><span class="line">          <span class="comment">// 右旋后，x已变成xp的父节点，xp则变成x的右子节点，如上图所示			                            </span></span><br><span class="line">                	root = rotateRight(root, x = xp);</span><br><span class="line">                  xpp = (xp = x.parent) == <span class="keyword">null</span> ? <span class="keyword">null</span> : xp.parent;</span><br><span class="line">              &#125;</span><br></pre></td></tr></table></figure>
<p>4）“右右同红”（xp位于xpp的右边且x位于xp的右边）</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/48c991c7e1bc5f96b2db3be75f604474.png" alt="54"></p>
<p><code>balanceInsertion</code>对应的源码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="keyword">if</span> (xp == (xppl = xpp.left))&#123;</span><br><span class="line">        <span class="comment">//......</span></span><br><span class="line">        <span class="comment">// 处理&quot;“左右同红”、“左左同红”情况</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">	<span class="comment">//运行到这里，说明xp=xpp.right，也即xp是xpp的右子节点，如上图所示              </span></span><br><span class="line">        <span class="comment">//......                  </span></span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (x == xp.left) &#123;</span><br><span class="line"><span class="comment">// 处理&quot;“右左同红”情况</span></span><br><span class="line">                &#125;</span><br><span class="line">        <span class="comment">// 在xp=xpp.right前提下，x=xp.right，也即x节点是xp的右子节点，对应“右右同红”情况</span></span><br><span class="line">                <span class="keyword">if</span> (xp != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    xp.red = <span class="keyword">false</span>; <span class="comment">// 将xp节点变黑</span></span><br><span class="line">                    <span class="keyword">if</span> (xpp != <span class="keyword">null</span>) &#123; </span><br><span class="line">                        xpp.red = <span class="keyword">true</span>;</span><br><span class="line">   <span class="comment">//将xpp节点变红后，以xpp作为旋转点进行左旋，以实现本次循环中xpp、xp、x构成的三层子树黑色节点平衡。</span></span><br><span class="line">                        root = rotateLeft(root, xpp);</span><br><span class="line">              &#125;</span><br></pre></td></tr></table></figure>
<p>到此已完成<code>static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root,TreeNode&lt;K,V&gt; x)</code> 红黑树插入的所有情况平衡调整的源码分析，这部分内容相对核心，摸透了红黑树插入平衡的工作原理，才能有助于快速理解TreeNode内部其他方法的内部实现，具体见后面的章节</p>
<h4 id="六、关于链表树化的处理：同时理解treeifyBin、treeify、moveRootToFront方法"><a href="#六、关于链表树化的处理：同时理解treeifyBin、treeify、moveRootToFront方法" class="headerlink" title="六、关于链表树化的处理：同时理解treeifyBin、treeify、moveRootToFront方法"></a>六、关于链表树化的处理：同时理解treeifyBin、treeify、moveRootToFront方法</h4><p>在前面的一篇HasHMap源码解析的文章里，没有给出treeifyBin和treeify的源码解析，因为这两个方法重点是解决构建红黑树，因此适合放在本文且适合放在本节</p>
<p>分析思路</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map.put(key,value)--&gt;putVal--&gt;单向冲突链达到树化阈值8--&gt;treeifyBin--&gt;table达到最小树化容量阈值64--&gt;单向冲突链转为双向冲突链--&gt;使用treeify基于双向冲突链构建一棵红黑树--&gt;moveRootToFront</span><br></pre></td></tr></table></figure>
<p>这里有一个很容易理解有误的地方：</p>
<p>很多HasHMap源码文章会跟你说——“若某个桶位上的冲突链节点数量达到8个，则调用treeifyBin方法将链表转换为红黑树”，其实这样的描述很容易误导人，而且漏了一个非常重要的信息。若想真正理解这个重要的信息，需从treeifyBin方法的实现开始着手理解：treeifyBin`方法有3个作用，见如下源码说明</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeifyBin</span><span class="params">(Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> hash)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n, index; Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="comment">// 作用1：虽然putVal出现冲突链节点达到8个，但如果table数组长度小于64，则选择扩容操作，而不是进行构建红黑树</span></span><br><span class="line">    <span class="comment">// MIN_TREEIFY_CAPACITY:冲突链转为红黑树的最小数组容量，默认64</span></span><br><span class="line">        <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY)</span><br><span class="line">            resize();</span><br><span class="line">    <span class="comment">// 作用2：如果putVal出现冲突链节点达到8个且table数组长度达到64，则先将单向链转为双向链，而且双向链的节点从Node类型转为TreeNode类型      </span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((e = tab[index = (n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            TreeNode&lt;K,V&gt; hd = <span class="keyword">null</span>, tl = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">do</span> &#123;</span><br><span class="line">              	<span class="comment">// 将Node类型节点转为TreeNode类型节点</span></span><br><span class="line">                TreeNode&lt;K,V&gt; p = replacementTreeNode(e, <span class="keyword">null</span>);</span><br><span class="line">                <span class="keyword">if</span> (tl == <span class="keyword">null</span>)</span><br><span class="line">                    hd = p;</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 这里可以看到使用TreeNode的prev和next构建一条双向链表，节点顺序和单向链表一样</span></span><br><span class="line">                    p.prev = tl;</span><br><span class="line">                    tl.next = p;</span><br><span class="line">                &#125;</span><br><span class="line">                tl = p;</span><br><span class="line">            &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">            <span class="keyword">if</span> ((tab[index] = hd) != <span class="keyword">null</span>)</span><br><span class="line">                <span class="comment">// 作用3：以双向链为基础构建一棵红黑树，hd是双向链表的头节点，也是原单向链表的头节点</span></span><br><span class="line">                hd.treeify(tab);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>显然<code>hd.treeify(tab)</code>才是真正构建红黑树的核心方法，下面是简略代码流程说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeify</span><span class="params">(Node&lt;K,V&gt;[] tab)</span> </span>&#123;</span><br><span class="line">       TreeNode&lt;K,V&gt; root = <span class="keyword">null</span>;</span><br><span class="line">     <span class="comment">// 1) 外循环每次从双向链表取出一个节点</span></span><br><span class="line">       <span class="keyword">for</span> (TreeNode&lt;K,V&gt; x = <span class="keyword">this</span>, next; x != <span class="keyword">null</span>; x = next) &#123;</span><br><span class="line">		<span class="comment">// .......</span></span><br><span class="line">			<span class="comment">// 2) 将1)取出的节点插入到以root为根节点的红黑树，插入的位置需要遍历树才能找到，因此内层有for循环</span></span><br><span class="line">               <span class="keyword">for</span> (TreeNode&lt;K,V&gt; p = root;;) &#123;</span><br><span class="line"><span class="comment">// .......</span></span><br><span class="line">     <span class="comment">// 3) 插入节点后，对红黑树做平衡处理</span></span><br><span class="line">                       root = balanceInsertion(root, x);</span><br><span class="line">                 			<span class="keyword">break</span>;</span><br><span class="line">               &#125;</span><br><span class="line">     <span class="comment">// 4) 当双向链表的8个节点都构建好了红黑树之后，需要做以下操作：</span></span><br><span class="line">       moveRootToFront(tab, root);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>综合treeifyBin、treeify、moveRootToFront这三个方法的设计思想，下面以table桶位3上一条单向冲突链被树化为红黑树作为例子进行图解：</p>
<p>约定：table的长度大于等于64，按HashMap源码的hash方法和桶位的计算方法，可以推出，插入3、67、131、195、259、323、387、451这8个节点，将在桶位index=3上形成单向冲突链表</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getBinIndex</span><span class="params">(Object key,<span class="keyword">int</span> tableSize)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> hash(key) &amp; (tableSize - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span>  <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>并且单向冲突链表达到树化的条件，全过程如下图所示，务必认真读懂该图：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/6108e396d348e071561774e0b20a8319.png" alt="56"></p>
<p>从上图也可以看出，其实红黑树结构也包含了一条双向链表，TreeNode设计确实巧妙，moveRootToFront解释了为何HashMap1.8在红黑树处理中采用双向链表的本质原因！</p>
<p>注意：若没有上图或者前面的解析作为铺垫，对于moveRootToFront的Root容易产生以下误解：</p>
<p>将红黑树的根节点移到前面？这里是指移动到“什么”的前面？</p>
<p>将双向链表的根节点移动前面？双向链原根节点（头节点）不是一直都在前面了吗，为何还再次需要移动？</p>
<p>正确的理解情况参考上图：</p>
<p>插入节点后红黑树的根节点不一定是双向链表的根节点，为了保证桶位上头节点table[i]即是红黑树根节点，也是双向链表的头节点，需要在插入节点后，对双向链表执行moveRootToFront操作。</p>
<p>moveRootToFront操作在HashMap源码的以下三个地方出现：</p>
<p>1) treeify，构建完红黑树之后操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">moveRootToFront(tab, root);</span><br></pre></td></tr></table></figure>
<p>2) putTreeVal，插入一个节点后，可能触发红黑树平衡操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">moveRootToFront(tab, balanceInsertion(root, x));</span><br></pre></td></tr></table></figure>
<p>3) removeTreeNode，在红黑树上删除一个节点，可能触发红黑树平衡操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">moveRootToFront(tab, r);</span><br></pre></td></tr></table></figure>
<p>有以上“重磅设计原理”的铺垫后，下面关于treeifyBin、treeify、moveRootToFront具体源码的理解则能达到“水到渠成”效果。</p>
<p>（很多关于HashMap的红黑树源码文章会将treeifyBin、treeify、moveRootToFront三者割裂来分析，这会破坏关于红黑树全局设计原理的关联理解）</p>
<h6 id="6-1-treeifyBin"><a href="#6-1-treeifyBin" class="headerlink" title="6.1 treeifyBin"></a>6.1 treeifyBin</h6><p>putVal在单向链表尾部插入节点后，若该单向链表长度达到8，则调用treeifyBin方法：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/dc9dbd03e88dfcbaf47c23f222121aaa.png" alt="58"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,<span class="keyword">boolean</span> evict)</span>                         			<span class="comment">// ......</span></span></span><br><span class="line"><span class="function">												<span class="title">if</span> <span class="params">(binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>)</span> </span></span><br><span class="line"><span class="function">                            <span class="title">treeifyBin</span><span class="params">(tab, hash)</span></span>;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeifyBin</span><span class="params">(Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> hash)</span> </span>&#123;</span><br><span class="line">  			<span class="comment">/*入参说明：</span></span><br><span class="line"><span class="comment">  			Node&lt;K,V&gt;[] tab：HashMap的数组，已经有数据</span></span><br><span class="line"><span class="comment">  			int hash：插入到单向链表尾部节点的hash值，（新插入节点的hash值）</span></span><br><span class="line"><span class="comment">  			*/</span></span><br><span class="line">        <span class="keyword">int</span> n, index; Node&lt;K,V&gt; e;</span><br><span class="line">        <span class="keyword">if</span> (tab == <span class="keyword">null</span> || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY)</span><br><span class="line">            resize();</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((e = tab[index = (n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">          	<span class="comment">// 构建双向链表的头节点head和尾部节点tail的临时引用</span></span><br><span class="line">            TreeNode&lt;K,V&gt; hd = <span class="keyword">null</span>, tl = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">do</span> &#123;</span><br><span class="line">                <span class="comment">// 将单向链表的Node类型转为TreeNode类型</span></span><br><span class="line">                TreeNode&lt;K,V&gt; p = replacementTreeNode(e, <span class="keyword">null</span>);</span><br><span class="line">                <span class="keyword">if</span> (tl == <span class="keyword">null</span>) <span class="comment">// 第一次循环时，tl指向肯定为null</span></span><br><span class="line">                    hd = p; <span class="comment">// 将第一次遍历的节点设为双向链表头节点</span></span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                  	<span class="comment">// 新节点插入到双向链表尾部，并建立前后驱关系</span></span><br><span class="line">                    p.prev = tl;</span><br><span class="line">                    tl.next = p;</span><br><span class="line">                &#125;</span><br><span class="line">                tl = p; <span class="comment">// 将tl引用指向双向链表新插入的尾部节点</span></span><br><span class="line">            &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>); <span class="comment">// 遍历单向链表每个节点，直到e指向null，则完成将单向链表变成双向链表的调整</span></span><br><span class="line">            <span class="keyword">if</span> ((tab[index] = hd) != <span class="keyword">null</span>)  <span class="comment">// 将双向链表的头部节点放在桶位上</span></span><br><span class="line">                hd.treeify(tab); <span class="comment">// 以桶位上的双向链表为基础构建红黑树</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h6 id="6-2-treeify"><a href="#6-2-treeify" class="headerlink" title="6.2 treeify"></a>6.2 treeify</h6><p>treeify被调用的时机：<code>putVal--&gt;treeifyBin--单向链表转双向链表hd--&gt; hd.treeify(tab);</code></p>
<p>设计思想</p>
<ul>
<li><p>1） 外层for循环：每次从双向链表取出一个节点x；内层循环：遍历红黑树找到x可以插入的位置并进行平衡调整。</p>
</li>
<li><p>2）外层for循环结束：说明双向链表的所有节点都被放到红黑树位置上，完成了构建红黑树。</p>
</li>
<li><p>3)  收尾工作：插入平衡调有左旋和右旋操作，<font color=red>这些可能操作可能让引起红黑树的root节点恰好没有位于双向链表的头部</font>，执行moveRootToFront(tab, root)，使得“红黑树的root节点恰好位于双向链表的头部”</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,<span class="keyword">boolean</span> evict)</span>    </span></span><br><span class="line"><span class="function">		<span class="comment">// 桶位上的单向链表（冲突链）节点数量达到8，则调用treeifyBin</span></span></span><br><span class="line"><span class="function">		<span class="title">if</span> <span class="params">(binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>)</span>  </span></span><br><span class="line"><span class="function">                      <span class="title">treeifyBin</span><span class="params">(tab, hash)</span></span>;</span><br><span class="line">                      <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeifyBin</span><span class="params">(Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> hash)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// ......</span></span><br><span class="line">            <span class="keyword">if</span> ((tab[index] = hd) != <span class="keyword">null</span>)  <span class="comment">// 将双向链表的头部节点放在桶位上</span></span><br><span class="line">                hd.treeify(tab); <span class="comment">// 以桶位上的双向链表为基础构建红黑树</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 真正构建红黑树的核心方法</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeify</span><span class="params">(Node&lt;K,V&gt;[] tab)</span> </span>&#123;</span><br><span class="line">            TreeNode&lt;K,V&gt; root = <span class="keyword">null</span>;</span><br><span class="line">  					<span class="comment">// 外层循环：第一次循环取出的节点，由hd.treeify(tab)可知，this节点就是双向链表的头节点hd</span></span><br><span class="line">            <span class="keyword">for</span> (TreeNode&lt;K,V&gt; x = <span class="keyword">this</span>, next; x != <span class="keyword">null</span>; x = next) &#123;</span><br><span class="line">                next = (TreeNode&lt;K,V&gt;)x.next; <span class="comment">// 先保存当前处理节点的next节点，用于外层循环的下一次遍历</span></span><br><span class="line">                x.left = x.right = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123; <span class="comment">//外层循环第一次会执行此逻辑</span></span><br><span class="line">                  <span class="comment">// 首次建树，root肯定为空，直接将外层第一次取出的节点作为新建红黑树的root节点，并设为黑色</span></span><br><span class="line">                    x.parent = <span class="keyword">null</span>;</span><br><span class="line">                    x.red = <span class="keyword">false</span>;</span><br><span class="line">                    root = x;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                  <span class="comment">// 树不为空时的处理流程，对应外层循环第二次以及以后的遍历</span></span><br><span class="line">                    K k = x.key; <span class="comment">// 来自双向链表的当前处理x节点的key</span></span><br><span class="line">                    <span class="keyword">int</span> h = x.hash; <span class="comment">// 来自双向链表的当前处理x节点的hash</span></span><br><span class="line">                    Class&lt;?&gt; kc = <span class="keyword">null</span>; <span class="comment">// 比较大小的Class泛型kc变量，临时变量</span></span><br><span class="line">                  </span><br><span class="line">                    <span class="comment">// 内层循环：遍历树以找到x合适的插入位置</span></span><br><span class="line">                  	<span class="comment">/*</span></span><br><span class="line"><span class="comment">                  	假设当前遍历到树的节点p，比较待插入x节点和p节点的hash值，看谁大，</span></span><br><span class="line"><span class="comment">                  	若x小于等于p，x则插入到p节点左边位置；如果x大于p，x则插入到p节点右边位置。</span></span><br><span class="line"><span class="comment">                  	比较大小的思路：</span></span><br><span class="line"><span class="comment">                  	1）先用双方hash值比较，若hash相等，则调用用户自定义key的Comparable来比较</span></span><br><span class="line"><span class="comment">                  	2）以上方式实在无法将x和p比较大小，则使出终极大招：tieBreakOrder里面系统层面</span></span><br><span class="line"><span class="comment">                  	System.identityHashCode(a) &lt;= System.identityHashCode(b) ？</span></span><br><span class="line"><span class="comment">                  	*/</span></span><br><span class="line">                    <span class="comment">// 以下的源码不再一行一行给出，理解“比较大小的设计思想”后，即可明白。</span></span><br><span class="line">                    <span class="keyword">for</span> (TreeNode&lt;K,V&gt; p = root;;) &#123;</span><br><span class="line">                        <span class="keyword">int</span> dir, ph;</span><br><span class="line">                        K pk = p.key;</span><br><span class="line">                        <span class="keyword">if</span> ((ph = p.hash) &gt; h)</span><br><span class="line">                            dir = -<span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span> (ph &lt; h)</span><br><span class="line">                            dir = <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span> ((kc == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                                  (kc = comparableClassFor(k)) == <span class="keyword">null</span>) || <span class="comment">// 见后面小节分析</span></span><br><span class="line">                                 (dir = compareComparables(kc, k, pk)) == <span class="number">0</span>) <span class="comment">// 见后面小节分析</span></span><br><span class="line">                          <span class="comment">// tieBreak直接翻译为：平分决胜局。运行到这里，一定会分出x和p之间谁大。</span></span><br><span class="line">                            dir = tieBreakOrder(k, pk); </span><br><span class="line">                        TreeNode&lt;K,V&gt; xp = p;</span><br><span class="line">                        <span class="keyword">if</span> ((p = (dir &lt;= <span class="number">0</span>) ? p.left : p.right) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                          	<span class="comment">// p作为x的父节点，且将x放在左子节点或者右子节点</span></span><br><span class="line">                            x.parent = xp;</span><br><span class="line">                            <span class="keyword">if</span> (dir &lt;= <span class="number">0</span>)</span><br><span class="line">                                xp.left = x;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                xp.right = x;</span><br><span class="line">                            <span class="comment">// x节点插入完成后，进行红黑树平衡操作</span></span><br><span class="line">                            root = balanceInsertion(root, x);</span><br><span class="line">                            <span class="keyword">break</span>; <span class="comment">// 内层循环完成了本次节点的插入和红黑树平衡调整后，跳出循环，继续处理来自外层循环输送的双向链表下一个节点。</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">  					<span class="comment">// 运行到这里，说明双向链表的所有节点都被放到红黑树位置上，完成了构建红黑树。</span></span><br><span class="line">   					<span class="comment">// 使红黑树的root节点恰好位于双向链表的头部</span></span><br><span class="line">            moveRootToFront(tab, root);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h6 id="6-3-moveRootToFront"><a href="#6-3-moveRootToFront" class="headerlink" title="6.3 moveRootToFront"></a>6.3 moveRootToFront</h6><p>调用该方法说明双向链表里的某个节点一定是红黑树的root节点，若恰巧该红黑树root节点不是位于双向链表头部，则需将该root节点移到双向链表头部并调整其他节点（first、rp、rn）前后指向 ，如下图所示</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c0a0c62bf109100aae0af626d0ff3eab.png" alt="57"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeify</span><span class="params">(Node&lt;K,V&gt;[] tab)</span> </span>&#123;</span><br><span class="line">						<span class="comment">//......</span></span><br><span class="line">            moveRootToFront(tab, root);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &lt;K,V&gt; <span class="function"><span class="keyword">void</span> <span class="title">moveRootToFront</span><span class="params">(Node&lt;K,V&gt;[] tab, TreeNode&lt;K,V&gt; root)</span> </span>&#123;</span><br><span class="line">  				  <span class="comment">/* 入参说明：</span></span><br><span class="line"><span class="comment">  				  Node&lt;K,V&gt;[] tab：HashMap的table数组，已经有数据</span></span><br><span class="line"><span class="comment">  				  TreeNode&lt;K,V&gt; root：某个桶位上红黑树的根节点</span></span><br><span class="line"><span class="comment">  				  */</span></span><br><span class="line">            <span class="keyword">int</span> n;</span><br><span class="line">         		<span class="comment">// 若root为空或者table为空，则无需处理，若root不为空,外面传入的root节点一定是红黑树的根节点</span></span><br><span class="line">            <span class="keyword">if</span> (root != <span class="keyword">null</span> &amp;&amp; tab != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">              	</span><br><span class="line">             <span class="comment">// 利用红黑树根节点的hash值定位桶位索引，从而找出双向链表的头节点</span></span><br><span class="line">                <span class="keyword">int</span> index = (n - <span class="number">1</span>) &amp; root.hash;</span><br><span class="line">             <span class="comment">// 这里tab[index]桶位取出的是双向链表的头节点，显然该头节点的hash值和红黑树root节点hash值相同,但两者的（key，value）不一定相同</span></span><br><span class="line">                TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index]; <span class="comment">// 注意first节点代表一条双向链表</span></span><br><span class="line">             <span class="comment">// 如果当前桶位上双向链表的头节点不是红黑树root节点，需将当前桶位设为红黑树root节点</span></span><br><span class="line">                <span class="keyword">if</span> (root != first) &#123;</span><br><span class="line">                  	<span class="comment">// root为TreeNode类型，具有prev和next字段</span></span><br><span class="line">                    Node&lt;K,V&gt; rn; <span class="comment">// 红黑树root节点的next节点</span></span><br><span class="line">                    tab[index] = root; <span class="comment">// 将当前桶位设为红黑树root节点</span></span><br><span class="line">                    TreeNode&lt;K,V&gt; rp = root.prev;<span class="comment">// 红黑树root节点的prev节点</span></span><br><span class="line">                  	<span class="comment">// rn为root节点的next节点</span></span><br><span class="line">                    <span class="keyword">if</span> ((rn = root.next) != <span class="keyword">null</span>) <span class="comment">// 如果红黑树root节点不是位于原双向链表的尾部</span></span><br><span class="line">                        ((TreeNode&lt;K,V&gt;)rn).prev = rp; <span class="comment">// 将rn前驱指向红黑树root节点的prev节点</span></span><br><span class="line">                    <span class="keyword">if</span> (rp != <span class="keyword">null</span>) </span><br><span class="line">                        rp.next = rn; <span class="comment">// 将rp后驱指向红黑树root节点的next节点</span></span><br><span class="line">                  	<span class="comment">// 以上流程完成了将rp-root-rn 变成rp-rn链接</span></span><br><span class="line">                    <span class="keyword">if</span> (first != <span class="keyword">null</span>) <span class="comment">// 若原双向链表的first节点不为空</span></span><br><span class="line">                        first.prev = root;  <span class="comment">// 原双向链表的first节点的前驱指向红黑树root节点</span></span><br><span class="line">                    root.next = first; <span class="comment">//将first双向链表挂在root节点后面</span></span><br><span class="line">                    root.prev = <span class="keyword">null</span>; <span class="comment">// 红黑树root节点此时成为双向链表头节点</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="function"><span class="keyword">assert</span> <span class="title">checkInvariants</span><span class="params">(root)</span></span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h6 id="6-4-本节接6-2的treeify内容：比较x节点和p节点大小"><a href="#6-4-本节接6-2的treeify内容：比较x节点和p节点大小" class="headerlink" title="6.4 本节接6.2的treeify内容：比较x节点和p节点大小"></a>6.4 本节接6.2的treeify内容：比较x节点和p节点大小</h6><p>从双向链表取出一个x节点，若插入红黑树合适位置，就需要遍历红黑树找到一个叶子节点p，看看x与p谁大：在“比较”这件事情上，用了三个方法：comparableClassFor、compareComparables、tieBreakOrder：</p>
<p>comparableClassFor、compareComparables的使用时基于用户自定义了key对象的Comparable接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeify</span><span class="params">(Node&lt;K,V&gt;[] tab)</span> </span>&#123;                        </span><br><span class="line">												<span class="comment">//......</span></span><br><span class="line">                        <span class="keyword">int</span> dir, ph;</span><br><span class="line">                        K pk = p.key;</span><br><span class="line">                        <span class="keyword">if</span> ((ph = p.hash) &gt; h)</span><br><span class="line">                            dir = -<span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span> (ph &lt; h)</span><br><span class="line">                            dir = <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span> ((kc == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                                  <span class="comment">// 这里k换成x.key更加直观，如果key未实现comparable的比较方法</span></span><br><span class="line">                                  (kc = comparableClassFor(x.key)) == <span class="keyword">null</span>) || </span><br><span class="line">                                 <span class="comment">// 这里k换成x.key、pk换成p.key更加直观</span></span><br><span class="line">                                  (dir = compareComparables(kc, x.key, p.key)) == <span class="number">0</span>) </span><br><span class="line">                          <span class="comment">// tieBreak直接翻译为：平分决胜局。运行到这里，一定会分出x和p之间谁大。</span></span><br><span class="line">                            dir = tieBreakOrder(k, pk); </span><br><span class="line">                    		<span class="comment">//......</span></span><br></pre></td></tr></table></figure>
<p>这里<code>comparableClassFor(Object x)</code> 入参x表示key，不是指代前面提到的双向链表取出的x节点。该方法用于取出key对象的“Class实例”用于后面的比较，若用户自定义key对象实现了Comparable接口后，key实例可用其内部的<code>compareTo</code>方法进行比较</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">o1.compareTo(o2) <span class="comment">// </span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">compareTo返回值为int类型</span></span><br><span class="line"><span class="comment">若返回值为-1，说明o1小于o2</span></span><br><span class="line"><span class="comment">若返回值为0，说明o1等于o2</span></span><br><span class="line"><span class="comment">若返回值为1，说明o1大于o2</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>对于comparableClassFor的理解最好使用具体的案例，如下所示：</p>
<p>建议自行跑一遍即可理解三个方法的工作过程。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.ParameterizedType;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Type;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        P p1=<span class="keyword">new</span> P(<span class="string">&quot;foo&quot;</span>,<span class="number">10</span>);</span><br><span class="line">        P p2=<span class="keyword">new</span> P(<span class="string">&quot;bar&quot;</span>,<span class="number">20</span>);</span><br><span class="line">      </span><br><span class="line">        S s1=<span class="keyword">new</span> S(<span class="string">&quot;foo&quot;</span>,<span class="number">10</span>);</span><br><span class="line">        S s2=<span class="keyword">new</span> S(<span class="string">&quot;bar&quot;</span>,<span class="number">20</span>);</span><br><span class="line">				</span><br><span class="line">      	<span class="comment">// 可设置断点观察其内部的反射操作，kc1为P.class</span></span><br><span class="line">        Class&lt;?&gt; kc1=comparableClassFor(p1); </span><br><span class="line">        System.out.println(kc1==P.class); <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> dirP= compareComparables(kc1,p1,p2); <span class="comment">// 返回-10，说明p1小于p2</span></span><br><span class="line">        System.out.println(dirP);</span><br><span class="line">      </span><br><span class="line">      	<span class="comment">// 可设置断点观察其内部的反射操作,kc2为null，因为S类没有实现Comparable接口</span></span><br><span class="line">        Class&lt;?&gt; kc2=comparableClassFor(s1); </span><br><span class="line">        <span class="keyword">int</span> dirS= compareComparables(kc2,s1,s2); <span class="comment">// 返回0，无法比较s1和s2的大小</span></span><br><span class="line">        System.out.println(dirS);     </span><br><span class="line">      </span><br><span class="line">        <span class="keyword">int</span> finalDir= tieBreakOrder(s1,s2); <span class="comment">// 返回-1，因此s1&lt;=s2</span></span><br><span class="line">        System.out.println(System.identityHashCode(s1)); <span class="comment">// 460141958</span></span><br><span class="line">        System.out.println(System.identityHashCode(s2)); <span class="comment">// 1163157884</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">// 利用反射操作获取给定对象的Class实例kc，获取kc有何用？</span></span><br><span class="line">  	<span class="comment">// 获取到对象的kc就可以给到compareComparables使用,目的还是比较大小</span></span><br><span class="line">    <span class="comment">// dir = compareComparables(kc, x.key, p.key)) == 0</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">static</span> Class&lt;?&gt; comparableClassFor(Object x) &#123;</span><br><span class="line">        <span class="comment">// 如果x(上面的kc)实现Comparable接口，表示该类支持比较（排序）</span></span><br><span class="line">        <span class="keyword">if</span> (x <span class="keyword">instanceof</span> Comparable) &#123;</span><br><span class="line">          	<span class="comment">// 这里的Type是个高级货，参考后面的“Java的类型系统结构”</span></span><br><span class="line">            <span class="comment">// 如果是内建的String类型，由于String可直接比较，因此可直接return kc</span></span><br><span class="line">            Class&lt;?&gt; c; Type[] ts, as; Type t; ParameterizedType p;</span><br><span class="line">            <span class="keyword">if</span> ((c = x.getClass()) == String.class) <span class="comment">// bypass checks</span></span><br><span class="line">                <span class="keyword">return</span> c;</span><br><span class="line">             <span class="comment">// 实现Comparable接口的自定义P类型会进入到此逻辑</span></span><br><span class="line">            <span class="keyword">if</span> ((ts = c.getGenericInterfaces()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; ts.length; ++i) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (((t = ts[i]) <span class="keyword">instanceof</span> ParameterizedType) &amp;&amp;</span><br><span class="line">                            ((p = (ParameterizedType)t).getRawType() ==</span><br><span class="line">                                    Comparable.class) &amp;&amp;</span><br><span class="line">                            (as = p.getActualTypeArguments()) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                            as.length == <span class="number">1</span> &amp;&amp; as[<span class="number">0</span>] == c) <span class="comment">// type arg is c</span></span><br><span class="line">                        <span class="keyword">return</span> c;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>; <span class="comment">// 自定义的S类型因为没实现Comparable接口，因此kc为null</span></span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">    <span class="meta">@SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">compareComparables</span><span class="params">(Class&lt;?&gt; kc, Object k, Object x)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 若x对象为null或者两个对象的Class实例不同，两者无法比较，直接返回0</span></span><br><span class="line">       <span class="comment">// 否则说明k和x具有相同Class实例，且实现Comparable接口，可通过compareTo比较。</span></span><br><span class="line">        <span class="keyword">return</span> (x == <span class="keyword">null</span> || x.getClass() != kc ? <span class="number">0</span> :</span><br><span class="line">                ((Comparable)k).compareTo(x));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">tieBreakOrder</span><span class="params">(Object a, Object b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> d;</span><br><span class="line">        <span class="keyword">if</span> (a == <span class="keyword">null</span> || b == <span class="keyword">null</span> ||</span><br><span class="line">                (d = a.getClass().getName().</span><br><span class="line">                        compareTo(b.getClass().getName())) == <span class="number">0</span>)</span><br><span class="line">            d = (System.identityHashCode(a) &lt;= System.identityHashCode(b) ?</span><br><span class="line">                    -<span class="number">1</span> : <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> d;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// P类实现Comparable接口且覆写了compareTo方法：采用age字段比较。用该类观察comparableClassFor完整的反射操作</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">P</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">P</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">P</span> <span class="params">(String name,<span class="keyword">int</span> age)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name=name;</span><br><span class="line">        <span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(P o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.age-o.age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// S类未实现Comparable，用它测试tieBreakOrder的比较效果</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">S</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">S</span> <span class="params">(String name,<span class="keyword">int</span> age)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name=name;</span><br><span class="line">        <span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对于tieBreakOrder则很好理解：</p>
<p>1) 若a为空或者b为空，直接调用java的native方法<code>public static native int identityHashCode(Object x)</code>：System.identityHashCode(a) &lt;= System.identityHashCode(b) ?</p>
<p>2) 若a和b都不为空，比较对象a以及对象b的全路径类名，这个<code>全路径类名</code>是String类型，当然可以比较。到了这里若a、b还相同，跟1)做法一样直接调用java的native方法达到最后的比较：</p>
<p><code>d=System.identityHashCode(a) &lt;= System.identityHashCode(b) ?</code></p>
<p>Java的类型系统结构（需要结合泛型和反射知识）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Type</span></span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Type is the common superinterface for all types in the Java programming language. These include raw types, parameterized types, array types, type variables and primitive types.<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">					  ┌────┐</span><br><span class="line">                        │Type         │</span><br><span class="line">                      └────┘</span><br><span class="line">                         ▲</span><br><span class="line">                         │</span><br><span class="line">   ┌────────────┬────────┴─────────┬───────────────┐</span><br><span class="line">   │            │                  │               │</span><br><span class="line">┌─────┐┌─────────────────┐┌────────────────┐┌────────────┐</span><br><span class="line">│Class││ParameterizedType││GenericArrayType││WildcardType│</span><br><span class="line">└─────┘└─────────────────┘└────────────────┘└────────────┘</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h4 id="七、其他TreeNode方法解析"><a href="#七、其他TreeNode方法解析" class="headerlink" title="七、其他TreeNode方法解析"></a>七、其他TreeNode方法解析</h4><p>关于`TreeNode()构造方法这里不再累赘，相对简单</p>
<h6 id="7-1-root-方法"><a href="#7-1-root-方法" class="headerlink" title="7.1 root()方法"></a>7.1 <code>root()</code>方法</h6><p>该方法较简单。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**给定任意位置的TreeNode，返回该TreeNode所在红黑树的根节点root</span></span><br><span class="line"><span class="comment"> * Returns root of tree containing this node.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">root</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (TreeNode&lt;K,V&gt; r = <span class="keyword">this</span>, p;;) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((p = r.parent) == <span class="keyword">null</span>) <span class="comment">// 从当前节点this开始，不断向上溯源找父节点</span></span><br><span class="line">            <span class="keyword">return</span> r;</span><br><span class="line">        r = p;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法在<code>putTreeVal</code>源码有被使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在putVal里面：</span></span><br><span class="line"><span class="comment">// 遍历节点p调用putTreeVal方法，this表示p节点本身</span></span><br><span class="line">e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line"><span class="comment">//......</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">putTreeVal</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab,<span class="keyword">int</span> h, K k, V v)</span> </span>&#123;</span><br><span class="line">            Class&lt;?&gt; kc = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">boolean</span> searched = <span class="keyword">false</span>;</span><br><span class="line">  					<span class="comment">/*</span></span><br><span class="line"><span class="comment">  					这里的parent是指this的parent，也即上面的p节点的父节点</span></span><br><span class="line"><span class="comment">  					this=p</span></span><br><span class="line"><span class="comment">  					if(this.parent != null)&#123; // 如果p节点的父节点为为空，说明不是根结点，因此p节点调用root()方法找到根节点</span></span><br><span class="line"><span class="comment">  							root=root()</span></span><br><span class="line"><span class="comment">  					&#125;else&#123;</span></span><br><span class="line"><span class="comment">  							root=this  // // 如果p节点的父节点为为空，说明p节点就是根节点，直接返回p节点，也即this引用</span></span><br><span class="line"><span class="comment">  					&#125;</span></span><br><span class="line"><span class="comment">  					</span></span><br><span class="line"><span class="comment">  					*/</span></span><br><span class="line">            TreeNode&lt;K,V&gt; root = (parent != <span class="keyword">null</span>) ? root() : <span class="keyword">this</span>;</span><br><span class="line">  					<span class="comment">// </span></span><br></pre></td></tr></table></figure>
<h6 id="7-2-find方法"><a href="#7-2-find方法" class="headerlink" title="7.2 find方法"></a>7.2 find方法</h6><p>find方法在getTreeNode和putTreeNode都有被调用的点，这里以getTreeNode方法说明</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> hash, Object key)</span> </span>&#123;</span><br><span class="line">						<span class="comment">// ......</span></span><br><span class="line">      			<span class="comment">// (first = tab[(n - 1) &amp; hash]) != null)</span></span><br><span class="line">            <span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line">              	<span class="comment">// 如果桶位上的头节点恰好是TreeNode类型，那么需要进入红黑树遍历找出指定key的节点</span></span><br><span class="line">                <span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                    <span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);</span><br><span class="line">						<span class="comment">// ......</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Calls find for root node.这里官方注释：由红黑树的头节点来调用getTreeNode</span></span><br><span class="line"><span class="comment">// 为何由root节点调用find方法：因为找插入节点或者找树节点，必须重根节点开始遍历</span></span><br><span class="line"><span class="comment">// 其实它也支持红黑树非root节点节点调用,但内部还是会找到root节点，再从root点解开始搜索</span></span><br><span class="line"><span class="comment">// (TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key)</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">getTreeNode</span><span class="params">(<span class="keyword">int</span> h, Object k)</span> </span>&#123;</span><br><span class="line">  					<span class="comment">// 如果调用该方法的节点不是根节点，使用root()找到根节点，再从根节点开始在全树范围找指定的key</span></span><br><span class="line">            <span class="keyword">return</span> ((parent != <span class="keyword">null</span>) ? root() : <span class="keyword">this</span>).find(h, k, <span class="keyword">null</span>);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>有了前面的被调用的场合，find方法其实是在执行普通的二叉搜索树查找过程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">getNode(hash,key)--&gt;getTreeNode(hash, key)--&gt;getTreeNode--&gt;root.find</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Finds the node starting at root p with the given hash and key.</span></span><br><span class="line"><span class="comment">     * The kc argument caches comparableClassFor(key) upon first use</span></span><br><span class="line"><span class="comment">     * comparing keys.</span></span><br><span class="line"><span class="comment">     给定一个hash值和key，从红黑树里面找到这个节点p，入口当然是从root节点开始：</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// root.find(x.hash,x.key,null)</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> TreeNode&lt;K,V&gt; <span class="title">find</span><span class="params">(<span class="keyword">int</span> h, Object k, Class&lt;?&gt; kc)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">//  对于getTreeNode方法调用find来说，这里this就是红黑树的根节点，或者当前处理节点</span></span><br><span class="line">        TreeNode&lt;K,V&gt; p = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> ph, dir; K pk;</span><br><span class="line">          </span><br><span class="line">            TreeNode&lt;K,V&gt; pl = p.left, pr = p.right, q;</span><br><span class="line">			<span class="comment">// 1）小于则往p左边子树继续找，</span></span><br><span class="line">            <span class="keyword">if</span> ((ph = p.hash) &gt; h) </span><br><span class="line">                p = pl;</span><br><span class="line">          <span class="comment">// 2）大于则在往p的右边子树继续找</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (ph &lt; h)</span><br><span class="line">                p = pr;</span><br><span class="line">          <span class="comment">// 3) 若两者key相等则返回p节点,结束搜索</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((pk = p.key) == k || (k != <span class="keyword">null</span> &amp;&amp; k.equals(pk)))</span><br><span class="line">                <span class="keyword">return</span> p;</span><br><span class="line">          <span class="comment">// 若能运行到这里，说明两者hash相等，key不同，此时若p的左子树为空，只能去p右子树继续找；同理，若p的右子树为空，只能去p左子树继续找</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (pl == <span class="keyword">null</span>) </span><br><span class="line">                p = pr;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (pr == <span class="keyword">null</span>)</span><br><span class="line">                p = pl;</span><br><span class="line">         <span class="comment">// 若能运行到这里，说明两者hash相等，key不同，且p的左、右子树不空，则尝试调用用户自定义的Comparable接口并使用compareTo比较，根据返回的int值判断是进入左边还是右边子树继续找</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((kc != <span class="keyword">null</span> ||</span><br><span class="line">                      (kc = comparableClassFor(k)) != <span class="keyword">null</span>) &amp;&amp;</span><br><span class="line">                     (dir = compareComparables(kc, k, pk)) != <span class="number">0</span>)</span><br><span class="line">                p = (dir &lt; <span class="number">0</span>) ? pl : pr;</span><br><span class="line">		<span class="comment">// 若能运行到这里，说明两者hash相等，key不同，且p的左、右子树不空，且自定义key对象没有实现Comparable接口（或者实现了Comparable接口但CompareTo方法无法比出大小），导致两者无法比出大小，因此不能二分查询了，只能穷举查找：先从p右边子树递归查找，直到右边子树找完都为空，那么再从左边子树全部节点查找</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((q = pr.find(h, k, kc)) != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> q;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                p = pl;</span><br><span class="line">        &#125; <span class="keyword">while</span> (p != <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>; <span class="comment">// 找遍红黑树都没找给定key的节点，返回null</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h6 id="7-3-split方法（设计巧妙，作为重点理解的方法之一）"><a href="#7-3-split方法（设计巧妙，作为重点理解的方法之一）" class="headerlink" title="7.3 split方法（设计巧妙，作为重点理解的方法之一）"></a>7.3 split方法（设计巧妙，作为重点理解的方法之一）</h6><p>spit方法仅在一个地方使用，那就是在resize方法内部。spit方法设计也挺巧妙，值得借鉴！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize()&#123;</span><br><span class="line"> 			<span class="comment">// .......</span></span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</span><br><span class="line">                Node&lt;K,V&gt; e;</span><br><span class="line">                <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    oldTab[j] = <span class="keyword">null</span>;</span><br><span class="line">                    <span class="keyword">if</span> (e.next == <span class="keyword">null</span>)</span><br><span class="line">                        newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                      	<span class="comment">// 对红黑树进行扩容处理</span></span><br><span class="line">                        ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">      <span class="comment">// ......</span></span><br><span class="line">                  &#125;    </span><br></pre></td></tr></table></figure>
<p>split的总体设计思想：</p>
<p>对于以下内容，需要理解“高位节点链表”和“低位节点链表”的含义，参考HashMap的resize源码分析。本节不再回顾相关内容，避免累赘。</p>
<p>低位节点特征满足：<code>(e.hash &amp; oldCap) == 0</code>（使用位运算才能看出端倪）</p>
<p>高位节点特征满足：<code>(e.hash &amp; oldCap) != 0</code>（使用位运算才能看出端倪）</p>
<p>1）若<code>table[i]</code>桶位上的红黑树，它所有的TreeNode节点恰好符合“低位节点”特征，那么在resize后，会构建一条“低位节点双向链表”；此外这棵红黑树root节点在新表的位置还是<code>i</code>，即<code>newTab[i]=root</code>，而且红黑树无需调整</p>
<p>2）若<code>table[i]</code>桶位上的红黑树，它所有的TreeNode节点恰好符合“高位节点”特征，那么在resize后，会构建一条“高位节点双向链表”；此外这棵红黑树root节点在新表的位置<code>i+oldCap</code>，即<code>newTab[i+oldCap]=root</code>，而且红黑树无需调整</p>
<p>3）若<code>table[i]</code>桶位上的红黑树它所有的TreeNode节点中，既有“高位节点”又有“低位节点”，这时spit方法真正起效了，此时红黑树会被spit成一条“低位节点双向链表”和一条“高位节点双向链表”</p>
<p>低位节点双向链表的头部节点位于<code>newTab[i]</code>上，若该链表长度大于6，将基于该双向链构建一棵红黑树；若长度&lt;=6，则将该双向链表变成单向链表</p>
<p>高位节点双向链表的头部节点位于<code>newTab[i+oldCap]</code>上，若该链表长度大于6，并基于该双向链构建一棵红黑树；若长度&lt;=6，则将该双向链表变成单向链表</p>
<p>再查看其源码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">index: 桶位号，也是红黑树root节点所在的桶位位置，也是红黑树TreeNode用prev、next构成的双向链表头位置</span></span><br><span class="line"><span class="comment">bit:这里的bit是resize传入的旧表容量oldCap</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// 回顾resize调用点： ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap)</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">split</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> index, <span class="keyword">int</span> bit)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 这里的this其实红黑树的root节点</span></span><br><span class="line">  					TreeNode&lt;K,V&gt; b = <span class="keyword">this</span>;</span><br><span class="line">            <span class="comment">// “低位节点双向链表”的头、尾指针</span></span><br><span class="line">            TreeNode&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">  					<span class="comment">// “高位节点双向链表”的头、尾指针</span></span><br><span class="line">            TreeNode&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">  					<span class="comment">//记录低、高位节点双向链表长度，以判断是否需要执行untreeify-&gt;双向变单向</span></span><br><span class="line">            <span class="keyword">int</span> lc = <span class="number">0</span>, hc = <span class="number">0</span>;</span><br><span class="line">  					<span class="comment">// 由于红黑树TreeNode其实也有prev、next字段，遍历它原本的双向链表</span></span><br><span class="line">            <span class="keyword">for</span> (TreeNode&lt;K,V&gt; e = b, next; e != <span class="keyword">null</span>; e = next) &#123;</span><br><span class="line">                next = (TreeNode&lt;K,V&gt;)e.next; <span class="comment">// 保存next节点，用于驱动之后的循环</span></span><br><span class="line">                e.next = <span class="keyword">null</span>;</span><br><span class="line">                <span class="comment">// 遍历红黑树过程中，若当前节点e恰好是“低位特征节点”，则构建“低位节点双向链表”</span></span><br><span class="line">                <span class="keyword">if</span> ((e.hash &amp; bit) == <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">// 个人更推荐使用treeifyBin里面构建双向链表的写法，更为直观</span></span><br><span class="line">                    <span class="keyword">if</span> ((e.prev = loTail) == <span class="keyword">null</span>)</span><br><span class="line">                        loHead = e;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        loTail.next = e;</span><br><span class="line">                    loTail = e;</span><br><span class="line">                    ++lc;<span class="comment">//“低位节点双向链表”每加一个节点，lcc加1计数</span></span><br><span class="line">                &#125;</span><br><span class="line">               <span class="comment">// 遍历红黑树过程中，若当前节点e恰好是“高位特征节点”，则构建“高位节点双向链表”</span></span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                  	<span class="comment">// 个人更推荐使用treeifyBin里面构建双向链表的写法，更为直观</span></span><br><span class="line">                    <span class="keyword">if</span> ((e.prev = hiTail) == <span class="keyword">null</span>)</span><br><span class="line">                        hiHead = e;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        hiTail.next = e;</span><br><span class="line">                    hiTail = e;</span><br><span class="line">                    ++hc;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (loHead != <span class="keyword">null</span>) &#123;</span><br><span class="line">              	<span class="comment">// “低位节点双向链表”长度&lt;= 树退化链的阈值6，则不再保持红黑树结构，直接转为单向链表</span></span><br><span class="line">                <span class="keyword">if</span> (lc &lt;= UNTREEIFY_THRESHOLD)</span><br><span class="line">                    tab[index] = loHead.untreeify(map);</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    tab[index] = loHead;</span><br><span class="line">                  	<span class="comment">//上面已经有一条低位双向链，若高位双向链表不为空，说明原红黑树被拆开两条双向链</span></span><br><span class="line">                  	<span class="comment">// 因此基于低位双向链构建一棵新红黑树</span></span><br><span class="line">                    <span class="keyword">if</span> (hiHead != <span class="keyword">null</span>) </span><br><span class="line">                        loHead.treeify(tab);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">  					<span class="comment">// 原理同上</span></span><br><span class="line">            <span class="keyword">if</span> (hiHead != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (hc &lt;= UNTREEIFY_THRESHOLD)</span><br><span class="line">                  	<span class="comment">// 注意高位节点在新表的位置index+oldCap</span></span><br><span class="line">                    tab[index + bit] = hiHead.untreeify(map); </span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    tab[index + bit] = hiHead;</span><br><span class="line">                    <span class="keyword">if</span> (loHead != <span class="keyword">null</span>)</span><br><span class="line">                        hiHead.treeify(tab);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="八、红黑树删除的设计过程"><a href="#八、红黑树删除的设计过程" class="headerlink" title="八、红黑树删除的设计过程"></a>八、红黑树删除的设计过程</h4><p>由于红黑树删除的设计相对复杂很多，因此单独在另外一篇文章进行解析</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>Java高级主题</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度讨论jdk1.7的ConcurrentHashMap设计及其核心方法实现</title>
    <url>/2021/04/18/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%AE%A8%E8%AE%BAjdk1.7%E7%9A%84ConcurrentHashMap%E8%AE%BE%E8%AE%A1%E5%8F%8A%E5%85%B6%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>阅读本文前，需要部分JDK源码深度知识储备：Unsafe与CAS原理、jdk1.7的HashMap原理设计以及分析过其源代码</p>
<p><img src="https://img-blog.csdnimg.cn/b127d7c290df4cc1b6ae2242f91614cd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>《gitee 博客封面图》</p>
<h4 id="1、为何引入ConcurrentHashMap这种适应并发场景数据结构？"><a href="#1、为何引入ConcurrentHashMap这种适应并发场景数据结构？" class="headerlink" title="1、为何引入ConcurrentHashMap这种适应并发场景数据结构？"></a>1、为何引入ConcurrentHashMap这种适应并发场景数据结构？</h4><ul>
<li>HashTable</li>
</ul>
<p>从Java7和Java8的HashMap源码设计可以得出基本结论：两者都不是线程安全，如果非得在并发情况下使用它们，会出现一些问题，如Java7的HashMap死循环导致CPU利用率飙高、put/get不一致问题等，当然也有绝对线程安全的HashTable，但从其源码实现来看，HashTable简单粗暴地在put/get/size等方法前加一个<code>synchronized</code> 修饰：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> V <span class="title">get</span><span class="params">(Object key)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> V <span class="title">put</span><span class="params">(K key, V value)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="comment">//...</span></span></span><br></pre></td></tr></table></figure>
<p>在并发操作HashTable的线程数量不多的场景下，其性能影响不大，而且线程隔离程度最高（保证线程安全）；但在高并发场景下，HashTable的性能显得力不从心，考察以下情况：</p>
<p>例如有1000个线程对HashTable（数组长度为1024）进行读写操作，由于synchronized是对整个数组进行加锁，只要有一个线程在数组的某个桶位上进行put/get等操作，其他999个线程都会被阻塞无法做其他事情，就连读操作也会被阻塞而不能并发读。</p>
<a id="more"></a>
<ul>
<li>Collections.synchronizedMap(map)</li>
</ul>
<p>此方法也能实现线程安全：用它可以将非线程安全版本的HashMap“包装”为线程安全的HashMap</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;Integer,String&gt; map=Collections.synchronizedMap(<span class="keyword">new</span> HashMap&lt;Integer,String&gt;());</span><br><span class="line">map.put(<span class="number">1</span>,<span class="string">&quot;a&quot;</span>);</span><br><span class="line">map.put(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>);</span><br><span class="line">System.out.println(map);</span><br></pre></td></tr></table></figure>
<p>其内部实现方式，如外界不指定加锁对象，那么它会对“当前SynchronizedMap对象的引用”加锁，然后再套用HashMap的方法即可完成线程安全的设计，简单、粗暴、直接：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SynchronizedMap(Map&lt;K,V&gt; m) &#123;</span><br><span class="line">      <span class="keyword">if</span> (m==<span class="keyword">null</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">      <span class="keyword">this</span>.m = m; <span class="comment">// </span></span><br><span class="line">      mutex = <span class="keyword">this</span>;<span class="comment">// 当前SynchronizedMap对象的引用,后面的get、put会对其加锁</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    	<span class="comment">// HashTable是在方法前面加synchronized修饰</span></span><br><span class="line">      <span class="keyword">synchronized</span> (mutex) &#123;<span class="keyword">return</span> m.get(key);&#125;</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">synchronized</span> (mutex) &#123;<span class="keyword">return</span> m.put(key, value);&#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Collections.unmodifiableMap(map)</li>
</ul>
<p>当然也还有一个冷门的unmodifiableMap类支持多线程并发的读的场景，但不支持并发写，因为unmodifiableMap类是只读的map，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HashMap&lt;Integer,String&gt; map=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">map.put(<span class="number">1</span>,<span class="string">&quot;a&quot;</span>);</span><br><span class="line">map.put(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>);</span><br><span class="line">Map&lt;Integer, String&gt; map1 = Collections.unmodifiableMap(map);</span><br><span class="line">map1.put(<span class="number">1</span>,<span class="string">&quot;1&quot;</span>); <span class="comment">// 抛出UnsupportedOperationException</span></span><br></pre></td></tr></table></figure>
<p>查看unmodifiableMap源码即可了解其设计思想：对“写入操作”相关方法改造为抛出异常，从而实现将可读写的map“包装”成只读map</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>
<p>此外unmodifiableMap类业务场景受限，它不支持并发写入等复杂操作的场景。</p>
<p>以上三种map虽然都是线程安全map，但它们在高并发场景下，读写（当然unmodifiableMap不支持写入操作）性能低，因此有必要介绍能支持高并发的HashMap，也即ConcurrentHashMap简称CHM，当然本文特指jdk1.7版本的CHM。</p>
<h4 id="2、ConcurrentHashMap双层结构图及其基本术语"><a href="#2、ConcurrentHashMap双层结构图及其基本术语" class="headerlink" title="2、ConcurrentHashMap双层结构图及其基本术语"></a>2、ConcurrentHashMap双层结构图及其基本术语</h4><p><img src="https://img-blog.csdnimg.cn/b127d7c290df4cc1b6ae2242f91614cd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>从图中可以看出，ConcurrentHashMap具有两层结构：Segment数组+HashEntry数组</p>
<ul>
<li>第一层结构：是一个Segment数组，该数组每个元素都是一个Segment对象，Segment对象是什么？首先它是ConcurrentHashMap的一个内部“辅佐类”，本质是一个锁对象，继承了ReentrantLock，只不过在ReentrantLock基础上增加各种功能，Segment定义源码如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Segment</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">ReentrantLock</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  			<span class="comment">// ...</span></span><br><span class="line">        Segment(<span class="keyword">float</span> lf, <span class="keyword">int</span> threshold, HashEntry&lt;K,V&gt;[] tab) &#123;</span><br><span class="line">            <span class="keyword">this</span>.loadFactor = lf;</span><br><span class="line">            <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">            <span class="keyword">this</span>.table = tab;</span><br><span class="line">        &#125;</span><br><span class="line">  		<span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所谓的“分段锁”、“锁分段”说的就是Segment数组里每个元素（每个桶位）都是一个锁，换句话说：Segment数组的每个桶位都持有一个锁，都可以独立加锁和解锁，桶位之间的读写操作互不影响，这种直接继承ReentrantLock来封装成为新的“工具”的设计思想确实巧妙。</p>
<ul>
<li>第二层结构：从Segment类的定义可以看出，每个Segment对象又包含了一个HashEntry数组，它才是键值对或者冲突链真正存放的地方，显然这一层结构就是大家所熟悉的Java7 HashMap结构。在Segment对象内部，对HashEntry数组的并发写操作采用CAS无锁原子操作机制，并发读操作使用volatile和Unsafe机制实现。</li>
</ul>
<p>这里还是以前面第1节的案例说明：</p>
<blockquote>
<p>例如有1000个线程对ConcurrentHashMap（数组长度为1024）进行(key,value)读写操作，假设1000个线程能够均匀分别1000个不同Segment数组桶位上，由于Segment数组每个桶位都是一把锁，每个线程对自己桶位进行加锁后独立进行读写操作，完全不会阻塞其他桶位上的线程操作，因此1000个线程可以同一时刻并发执行读写，整个ConcurrentHashMap的操作效率大增。</p>
</blockquote>
<h4 id="3、重点成员变量以及构造方法"><a href="#3、重点成员变量以及构造方法" class="headerlink" title="3、重点成员变量以及构造方法"></a>3、重点成员变量以及构造方法</h4><p>从最熟悉的用法put开始：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ConcurrentHashMap&lt;Integer,String&gt; map=<span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">map.put(<span class="number">1</span>,<span class="string">&quot;a&quot;</span>);</span><br></pre></td></tr></table></figure></p>
<p>通常是以该类的重点成员变量以及最多参数那个构造方法作为分析切入口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">		<span class="comment">// 第一层结构Segment数组的默认初始容量</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_INITIAL_CAPACITY = <span class="number">16</span>;</span><br><span class="line">		<span class="comment">//第二层结构里面HashMap的负载因子，注意：这里不是Segment数组的负载因子，Segment数组没有扩容机制，初始化设定多少就是多少（原因看后面的源码分析），因此不需要LOAD_FACTOR</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> DEFAULT_LOAD_FACTOR = <span class="number">0.75f</span>;</span><br><span class="line">		<span class="comment">// 写并发度，也即允许同时操作多少个线程来操作该Segment数组，数组的一个桶位对应一个写操作并发度，读并发度不限制</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CONCURRENCY_LEVEL = <span class="number">16</span>;</span><br><span class="line">		<span class="comment">// 第二层结构里面HashMap的最大容量</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br><span class="line">    <span class="comment">// 第二层结构里面HashMap的初始默认容量</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_SEGMENT_TABLE_CAPACITY = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 第一层结构Segment数组的最大容量65536，Doug Lea自己也认为该最值设定有点保守，因为同一台服务器，cpu、内存等配置不同，同时开启的线程数量也不同，性能好的服务器，同时打开10万个线程也不成问题</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_SEGMENTS = <span class="number">1</span> &lt;&lt; <span class="number">16</span>; <span class="comment">// slightly conservative</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Number of unsynchronized retries in size and containsValue</span></span><br><span class="line"><span class="comment">     * methods before resorting to locking.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 在计算size时，加阻塞锁的最大尝试次数，后面的size方法会提到。</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> RETRIES_BEFORE_LOCK = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对应第一层结构的Segment数组，注意这里是final修饰，也再次能说明：Segment数组一旦创建，以后就不能修改和扩容了</span></span><br><span class="line">    <span class="keyword">final</span> Segment&lt;K,V&gt;[] segments;</span><br><span class="line">		<span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ConcurrentHashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity,</span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">float</span> loadFactor, <span class="keyword">int</span> concurrencyLevel)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!(loadFactor &gt; <span class="number">0</span>) || initialCapacity &lt; <span class="number">0</span> || concurrencyLevel &lt;= <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">      <span class="comment">// 如果给定的并发度例如10万，显然大于默认Segment数组的长度，只能取到65536，</span></span><br><span class="line">        <span class="keyword">if</span> (concurrencyLevel &gt; MAX_SEGMENTS)</span><br><span class="line">            concurrencyLevel = MAX_SEGMENTS;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// Find power-of-two sizes best matching arguments</span></span><br><span class="line">  <span class="comment">// 用于算出给定相关参数下的最佳Segment数组容量，2的n次幂，以下以默认构造方法的入参作为说明，因此给定initialCapacity=16作为实例说明，其他参数loadFactor=0.75，concurrencyLevel=16。</span></span><br><span class="line">        <span class="keyword">int</span> sshift = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> ssize = <span class="number">1</span>; <span class="comment">// ssize变量就是创建Segment数组的容量值</span></span><br><span class="line">        <span class="keyword">while</span> (ssize &lt; concurrencyLevel) &#123; </span><br><span class="line">          <span class="comment">// 找出大于等于并发度的2整数幂值，因为concurrencyLevel默认为16，因此经过以下简单计算，sshift=4，ssize=16</span></span><br><span class="line">            ++sshift;</span><br><span class="line">            ssize &lt;&lt;= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// segmentShift=32-4=28，用于计算key的hash值，采用这种方式能够使得key分布更加离散</span></span><br><span class="line">        <span class="keyword">this</span>.segmentShift = <span class="number">32</span> - sshift;</span><br><span class="line">      	<span class="comment">// segmentMask=16-1=15&lt;==&gt;1111,使用二进制能够快速计算定位到key对应的桶位号，这个知识点已经在jdk1.8HashMap解析的文章详细解析过。</span></span><br><span class="line">        <span class="keyword">this</span>.segmentMask = ssize - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">            initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">        <span class="keyword">int</span> c = initialCapacity / ssize; <span class="comment">// 由于initialCapacity=16， ssize=16，那么c计算为1</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 由于c*ssize=1*16恰好能满足c * ssize&gt;=initialCapacity</span></span><br><span class="line">        <span class="keyword">if</span> (c * ssize &lt; initialCapacity) </span><br><span class="line">            ++c; <span class="comment">// 此时c=1</span></span><br><span class="line">        <span class="keyword">int</span> cap = MIN_SEGMENT_TABLE_CAPACITY; <span class="comment">//默认创建HashEntry[]的容量，值为2</span></span><br><span class="line">        <span class="keyword">while</span> (cap &lt; c) <span class="comment">//  由于c=1,显然这里cap不再调整，cap还是取默认值MIN_SEGMENT_TABLE_CAPACITY=2</span></span><br><span class="line">            cap &lt;&lt;= <span class="number">1</span>; </span><br><span class="line">      </span><br><span class="line">        <span class="comment">// create segments and segments[0]</span></span><br><span class="line">      <span class="comment">// 创建第0个segment对象，用于未来其他线程创建新segment对象时以s0作为模板去“克隆创建”</span></span><br><span class="line">        Segment&lt;K,V&gt; s0 =</span><br><span class="line">            <span class="keyword">new</span> Segment&lt;K,V&gt;(loadFactor, (<span class="keyword">int</span>)(cap * loadFactor),</span><br><span class="line">                             (HashEntry&lt;K,V&gt;[])<span class="keyword">new</span> HashEntry[cap]); <span class="comment">// 从上面可知，这里cap就是2，因此位于s0位置的HashEntry数组长度为2</span></span><br><span class="line">        <span class="comment">// 创建一个容量为ssize的segments数组</span></span><br><span class="line">        Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])<span class="keyword">new</span> Segment[ssize];</span><br><span class="line">        <span class="comment">// 数组的segments[0]=s0 </span></span><br><span class="line">        UNSAFE.putOrderedObject(ss, SBASE, s0); <span class="comment">// ordered write of segments[0]</span></span><br><span class="line">        <span class="keyword">this</span>.segments = ss; <span class="comment">// segments数组内部变量名</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>该构造方法有以下几个重要细节：</p>
<ul>
<li><p>1、Segment数组是在new ConcurrentHashMap&lt;&gt;()时创建的，默认容量为16，同时会在Segment数组的第0个桶位上创建一个Segment对象，赋给变量s0，而Segment数组其他1~15的位置则为null，也即惰性创建，在以后有新的key put进来时再到相应的下标创建新的Segment。此外，在Segment数组第0号下标也创建好一个默认容量为2的HashEntry数组。</p>
</li>
<li><p>2、这里使用<code>UNSAFE.putOrderedObject(ss, SBASE, s0)</code> ，为何不使用 putObjectVolatile()方法？</p>
</li>
</ul>
<blockquote>
<p>首先putOrderedObject()是有序、延迟版本的 putObjectVolatile方法，用它写入值不会立即被其他线程获取到，这样做的好处是写入性能比 putObjectVolatile() 高，</p>
</blockquote>
<ul>
<li>3、如何理解“这样做的好处是写入性能比 putObjectVolatile() 高？</li>
</ul>
<blockquote>
<p>首先，假设在new构造方法阶段和put阶段，采用putObjectVolatile()，那么s0会马上写入主存，由于new阶段之后紧接着put操作，而put方法里有相关操作也涉及到立即写入主存的需求，如果这些写入操作都直接使用putObjectVolatile，那么相当于主存与cpu缓存之间来回跑了几次，其实完全可以这么设计：“从new到put方法的finally之前，积攒多个写入操作后再一次性写入主存”</p>
<p>Doug Lea就是这么干的：在new构造方法开始使用putOrderedObject，让s0变量写入主存的时间延迟到<code>put</code>方法的结尾前，也即在finally的unclock执行前，就能实现“积攒多个写入操作后一次性写入主存”，因此写入性能有提高的，而且保证的数据一致性。</p>
<p>注意：JMM规定，对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）</p>
<p>参考：<a href="https://www.zhihu.com/question/60888757">https://www.zhihu.com/question/60888757</a></p>
</blockquote>
<h4 id="4-put方法解析"><a href="#4-put方法解析" class="headerlink" title="4 put方法解析"></a>4 put方法解析</h4><p>基于ConcurrentHashMap两层结构可知，要put一个key进去，需要两次hash定位，第一次定位在Segment数组对应的Segment上，第二次定位：在第一次定位的基础上，确定key位于HashEntry数组的哪个桶位上，显然，第二次定位才是key真正要put的位置，由于是多线程并发put，因此代码设计相对复杂：</p>
<p>第一次定位</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    Segment&lt;K,V&gt; s;</span><br><span class="line">    <span class="comment">// 这里可以看出ConcurrentHashMap不允许放入null</span></span><br><span class="line">    <span class="keyword">if</span> (value == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">int</span> hash = hash(key);</span><br><span class="line">  	<span class="comment">// 对比HashMap的index=hashCode&amp;table.length-1的桶位计算即可知道，j就是确定key所在的Segment数据下标位置</span></span><br><span class="line">  	<span class="comment">// 第一次定位，也即找出key所在的Segment数组的下标位置，这里的hash右移了segmentShift位，目的是让key的高位特征参与桶位计算，使得hash进一步分散</span></span><br><span class="line">    <span class="keyword">int</span> j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask;</span><br><span class="line">  	</span><br><span class="line">  	<span class="comment">// 投机写法：若当前还未有其他线程写入，那么可以使用getObject快速获取segment实例</span></span><br><span class="line">  	<span class="comment">// 如果key对应的Segment对象恰好为null，说明还未创建Segment对象，因此需要在ensureSegment方法内部创建key对应的Segment对象，具体逻辑参考ensureSegment方法的解析</span></span><br><span class="line">    <span class="keyword">if</span> ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject          <span class="comment">// nonvolatile; recheck in ensureSegment</span></span><br><span class="line">         (segments, (j &lt;&lt; SSHIFT) + SBASE)) == <span class="keyword">null</span>) </span><br><span class="line">        s = ensureSegment(j);</span><br><span class="line">  	<span class="comment">// 真正将键值对put入HashEntry数组</span></span><br><span class="line">    <span class="keyword">return</span> s.put(key, hash, value, <span class="keyword">false</span>);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二次定位 <code>s.put(key, hash, value, false)</code>  </p>
<p>该put方法是Segment对象自己的内部方法，其内部设计也很巧妙，这里给出其总体设计思想：</p>
<p>1、此put方法是真正将key放入HashEntry数组的逻辑，那么当前线程必须要获得锁才能保证独占put</p>
<p>2、使用非租塞方式自旋方式获取锁，但不会让当前线程白白浪费自旋，因此在自旋期间给当前线程安排了任务（对应下面的scanAndLockForPut方法）</p>
<p>3、任务是：麻烦你（当前线程）在自旋的过程中，顺便帮我检查key是否存在链表中，如果不存在，顺便帮我提前创建key对应的Entry节点，以便我获取锁后可以马上使用。（对应下面的HashEntry<K,V> node = tryLock() ? null :scanAndLockForPut(key, hash, value);）</p>
<p>4、拿到锁后，即可在HashEntry数组上找到合适位置并插入新key</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 对应上面的 s.put(key, hash, value, false)调用       </span></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">put</span><span class="params">(K key, <span class="keyword">int</span> hash, V value, <span class="keyword">boolean</span> onlyIfAbsent)</span> </span>&#123;</span><br><span class="line"><span class="comment">//由于segment对象自己就是一个ReentrantLock，因此当然有tryLock()加锁方法，所以在这里先尝试第一次加锁</span></span><br><span class="line"><span class="comment">// 若第一次加锁成功，则node没机会提前被创建，因此对应为null。若第一次加锁失败（说明有其他线程竞争并抢占成功），当前线程会在scanAndLockForPut里面尝试加锁且顺便提前为key创建好对应的node节点。 </span></span><br><span class="line">            HashEntry&lt;K,V&gt; node = tryLock() ? <span class="keyword">null</span> :scanAndLockForPut(key, hash, value);</span><br><span class="line">  <span class="comment">// ===========================加锁成功（进入临界区）===========================</span></span><br><span class="line">            V oldValue;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              	<span class="comment">// 这一句代码非常能体现作者对并发性能的理解是有多细腻！参考后面解释</span></span><br><span class="line">                HashEntry&lt;K,V&gt;[] tab = table;</span><br><span class="line">                <span class="keyword">int</span> index = (tab.length - <span class="number">1</span>) &amp; hash;</span><br><span class="line">              	<span class="comment">// 使用Volatile语义获取key对应的桶位头节点，也即冲突链的头结点，用于下面遍历</span></span><br><span class="line">                HashEntry&lt;K,V&gt; first = entryAt(tab, index);</span><br><span class="line">              	<span class="comment">// 以下内容其实在HashMap解析中已经讲过，逻辑不难</span></span><br><span class="line">             <span class="comment">// 从冲突链的头节点开始遍历，在合适位置插入key，若key已经在链表中存在，则更新其value</span></span><br><span class="line">                <span class="keyword">for</span> (HashEntry&lt;K,V&gt; e = first;;) &#123;</span><br><span class="line">                  	<span class="comment">//冲突链的头节点不为空，则进入链表遍历，若在链中找到对应key，则结束遍历。</span></span><br><span class="line">                    <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        K k;</span><br><span class="line">                        <span class="keyword">if</span> ((k = e.key) == key ||</span><br><span class="line">                            (e.hash == hash &amp;&amp; key.equals(k))) &#123;</span><br><span class="line">                            oldValue = e.value;</span><br><span class="line">                            <span class="keyword">if</span> (!onlyIfAbsent) &#123;</span><br><span class="line">                                e.value = value;</span><br><span class="line">                              <span class="comment">// 更新value后，ConcurrentMap结构修改次数加1</span></span><br><span class="line">                                ++modCount;</span><br><span class="line">                            &#125;</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                      	<span class="comment">// 用于保持链表的遍历</span></span><br><span class="line">                        e = e.next;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// 程序运行到该分支，说明key所在的桶位头节点是null</span></span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                 <span class="comment">// 若上面的自旋加锁期间顺便把key对应的node节点创建好，那么就直接使用头插法将此node插入桶位上</span></span><br><span class="line">                        <span class="keyword">if</span> (node != <span class="keyword">null</span>)</span><br><span class="line">                            node.setNext(first);</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                <span class="comment">// 若上面 的tryLock() ? 没有机会提前将key对应的node节点创建好，在这里也可以为该key创建节点，显然是用头插法</span></span><br><span class="line">                            node = <span class="keyword">new</span> HashEntry&lt;K,V&gt;(hash, key, value, first);</span><br><span class="line">                <span class="comment">// HashEntry容量统计加1</span></span><br><span class="line">                        <span class="keyword">int</span> c = count + <span class="number">1</span>;</span><br><span class="line">                 <span class="comment">// 添加新节点后，若HashEntry数组容量达到扩容阈值，则进行扩容rehash</span></span><br><span class="line">                        <span class="keyword">if</span> (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY)</span><br><span class="line">                            rehash(node);</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  注意，这里是在HashEntry数组的index桶位创建key对应的node节点，也是用了延迟写入主存的策略（putOrderedObject）：</span></span><br><span class="line"><span class="comment">	setEntryAt(tab,i,e) </span></span><br><span class="line"><span class="comment">			&#123;</span></span><br><span class="line"><span class="comment">        UNSAFE.putOrderedObject(tab, ((long)i &lt;&lt; TSHIFT) + TBASE, e);</span></span><br><span class="line"><span class="comment">   		 &#125;</span></span><br><span class="line"><span class="comment">	*/</span>        </span><br><span class="line">                        setEntryAt(tab, index, node);</span><br><span class="line">                        ++modCount;</span><br><span class="line">                        count = c;</span><br><span class="line">                        oldValue = <span class="keyword">null</span>;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">            put方法在代码开始位置就获得了锁，那么key写入后，自然需要释放锁，注意这里需联系前面构造方法里面的</span></span><br><span class="line"><span class="comment">            UNSAFE.putOrderedObject(ss, SBASE, s0)使用延迟写入主存的原因</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">                unlock();<span class="comment">// 执行unlock之前，jvm能把之前每个调用putOrderedObject的写入操作在此刻全部一起写入到主存</span></span><br><span class="line">  <span class="comment">// ===========================成功释放锁（离开临界区）===========================</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>在put方法的前面，Dung Lea安排了<code>HashEntry&lt;K,V&gt;[] tab = table</code>，将原本volatile修饰的table变量转为普通变量，</p>
<p>这是因为：在这一句之前，当前线程已经通过tryLock获取了独占锁，因此只会有一个线程对table进行写操作，既然如此，那就不必再使用volatile语义的table变量，将table赋值给put方法中的一个局部变量，从而使得能够减少volatile带来的不必要性能损耗。</p>
<h5 id="ensureSegment方法解析"><a href="#ensureSegment方法解析" class="headerlink" title="ensureSegment方法解析"></a>ensureSegment方法解析</h5><p>该方法的解析主要是来自前面put方面里面的<code>s = ensureSegment(j)，主要设计思想：</code></p>
<p>1、先投机性去主存Segment数组j下标取segment对象，若该位置对应的Segment对象不为空，则直接返回它，</p>
<p>2、若在1步骤发现j位置下Segment为null，那么当前线程准备把它创建了，创建方式当然基于自旋+CAS</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the segment for the given index, creating it and</span></span><br><span class="line"><span class="comment"> * recording in segment table (via CAS) if not already present.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> k the index</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the segment</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Segment&lt;K,V&gt; <span class="title">ensureSegment</span><span class="params">(<span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// 构造方法里面的初始化的Segment数组</span></span><br><span class="line">    <span class="keyword">final</span> Segment&lt;K,V&gt;[] ss = <span class="keyword">this</span>.segments;</span><br><span class="line">  	<span class="comment">// key定位到j下标对应的相对地址，注意j下标和u的不同</span></span><br><span class="line">  	<span class="comment">// int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask;</span></span><br><span class="line">  	<span class="comment">// s = ensureSegment(j);</span></span><br><span class="line">    <span class="keyword">long</span> u = (k &lt;&lt; SSHIFT) + SBASE; <span class="comment">// raw offset</span></span><br><span class="line">    Segment&lt;K,V&gt; seg;</span><br><span class="line">  	<span class="comment">//到这这一步,可能出现多线程并发创建Segment，因此使用Volatile语义判断Segment数组j下标是否已经创建Segment实例以及该Segment对应的hashEntry数组</span></span><br><span class="line">    <span class="keyword">if</span> ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == <span class="keyword">null</span>) &#123;</span><br><span class="line">      	<span class="comment">// 若当前j下标元素为空，用Segment数组的第0号桶位上的segment实例作为模板为桶位j创建segment实例</span></span><br><span class="line">        Segment&lt;K,V&gt; proto = ss[<span class="number">0</span>]; <span class="comment">// use segment 0 as prototype</span></span><br><span class="line">        <span class="keyword">int</span> cap = proto.table.length;</span><br><span class="line">        <span class="keyword">float</span> lf = proto.loadFactor;</span><br><span class="line">        <span class="keyword">int</span> threshold = (<span class="keyword">int</span>)(cap * lf);</span><br><span class="line">        HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])<span class="keyword">new</span> HashEntry[cap]; </span><br><span class="line">   <span class="comment">// 再次检查当前j下标是否有其他线程创建了segment实例，若没有创建，就使用自旋+CAS方式来创建，这时一定原子操作，而且仅有一个线程会创建segment成功</span></span><br><span class="line">        <span class="keyword">if</span> ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u))</span><br><span class="line">            == <span class="keyword">null</span>) &#123; <span class="comment">// recheck</span></span><br><span class="line">            Segment&lt;K,V&gt; s = <span class="keyword">new</span> Segment&lt;K,V&gt;(lf, threshold, tab);</span><br><span class="line">          	<span class="comment">// 自旋检查j下标位置是否有其他线程创建了segment实例，若自旋中恰好发现还未创建segment实例，则使用CAS原子操作创建</span></span><br><span class="line">            <span class="keyword">while</span> ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u))</span><br><span class="line">                   == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (UNSAFE.compareAndSwapObject(ss, u, <span class="keyword">null</span>, seg = s))</span><br><span class="line">                  	<span class="comment">// 若当前线程创建segment实例成果，则退出自旋</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> seg;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>作者这样考虑ensureSegment的设计：考察多个线程一起调用ensureSegment情况，其实只要其中一个线程抢先在j位置创Segment对象，那么其他线程只需要判断该位置不为空时就不用重复在j位置创建segment实例，直接 return seg即可。</p>
<p>再使用并发思维理解以下代码片段：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> Segment&lt;K,V&gt; s = <span class="keyword">new</span> Segment&lt;K,V&gt;(lf, threshold, tab);</span><br><span class="line"><span class="comment">// 自旋检查j下标位置是否有其他线程创建了segment实例</span></span><br><span class="line"> <span class="keyword">while</span> ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u))</span><br><span class="line">        == <span class="keyword">null</span>) &#123;</span><br><span class="line">     <span class="keyword">if</span> (UNSAFE.compareAndSwapObject(ss, u, <span class="keyword">null</span>, seg = s))</span><br><span class="line">       	<span class="comment">// 若当前线程能成功创建segment实例，则退出自旋</span></span><br><span class="line">         <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>考察A、B两个线程同时执行到上面compareAndSwapObject，由于原子性操作，因此可保证只有A线程（假设A线程先抢到）去操作CAS且返回true，B线程去<code>UNSAFE.compareAndSwapObject(ss, u, null, seg = s)</code> 就会返回false，此时的u位置不再是null而是A线程已写入的segment实例，线程B在while循环发现seg已不为空，所以B线程不会再重复创建segment对象。</p>
<h5 id="scanAndLockForPut解析1"><a href="#scanAndLockForPut解析1" class="headerlink" title="scanAndLockForPut解析1"></a>scanAndLockForPut解析1</h5><p>put方法里面，因为要写入key，线程对当前segment写入操作前，当然需要先获取锁</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HashEntry&lt;K,V&gt; node = tryLock() ? <span class="keyword">null</span> :scanAndLockForPut(key, hash, value);</span><br><span class="line">V oldValue;</span><br></pre></td></tr></table></figure>
<p>为了能够通俗理解<code>scanAndLockForPut</code>逻辑设计，这先给出ReentrantLock阻塞锁和非阻塞锁的对比，因此这一节称为<code>scanAndLockForPut解析1</code>，真正的源码解析安排下一节：<code>scanAndLockForPut解析2</code></p>
<ul>
<li>阻塞锁</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> ReentrantLock Lock= <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Lock.lock();</span><br><span class="line">        System.out.println(<span class="string">&quot;线程A在运行中&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5</span>*<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        Lock.unlock();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line">    Thread.sleep(<span class="number">1</span>);<span class="comment">// 让线程A先运行</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 线程A在工作时，线程B被阻塞了，啥事都做不了</span></span><br><span class="line">    <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            Lock.lock();</span><br><span class="line">            System.out.println(<span class="string">&quot;线程B在运行中&quot;</span>);</span><br><span class="line">            Lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从上面的运行结果来看：线程A在工作时，线程B被阻塞了，而且被阻塞了5秒。在5秒里，线程B啥事都做不了，线程B明明也可以干活，但却被阻塞，这种并发设计未能充分利用cpu。</p>
<ul>
<li>非租塞锁</li>
</ul>
<p>如果使用非阻塞锁，那么线程A在工作时，线程B不会被阻塞，线程B可以自由做其他事情，没有白白自旋</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">final</span> ReentrantLock Lock= <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            Lock.lock();</span><br><span class="line">            System.out.println(<span class="string">&quot;线程A在运行中&quot;</span>);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">5</span>*<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            Lock.unlock();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).start();</span><br><span class="line">        Thread.sleep(<span class="number">1</span>);<span class="comment">// 让线程A先运行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 线程A在工作时，使用tryLock非阻塞锁，这样线程B不会被线程A阻塞，线程A工作的同时，线程B同时也可以去干点别的事情</span></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">while</span> (!Lock.tryLock())&#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;线程B在工作中，对链表扫描是否存在key节点&quot;</span>);</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">                Lock.unlock();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">线程A在运行中</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br><span class="line">线程B在工作中，对链表扫描是否存在key节点</span><br></pre></td></tr></table></figure>
<p>使用trylock非阻塞锁后，在这5秒时间内线程B没有白白浪费CPU资源，它顺便完成了查询key节点的工作，对比scanAndLockForPut的<code>while (!tryLock())</code>，即可理解其设计思想：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> HashEntry&lt;K,V&gt; <span class="title">scanAndLockForPut</span><span class="params">(K key, <span class="keyword">int</span> hash, V value)</span> </span>&#123;</span><br><span class="line">    HashEntry&lt;K,V&gt; first = entryForHash(<span class="keyword">this</span>, hash);</span><br><span class="line">    HashEntry&lt;K,V&gt; e = first;</span><br><span class="line">    HashEntry&lt;K,V&gt; node = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">int</span> retries = -<span class="number">1</span>; <span class="comment">// negative while locating node</span></span><br><span class="line">    <span class="keyword">while</span> (!tryLock()) &#123; <span class="comment">// 若当前线程抢锁失败后不会白白浪费自旋cpu资源，而是顺便完成链表的遍历以及新key节点的创建</span></span><br><span class="line"> <span class="comment">// ......         </span></span><br><span class="line">         node = <span class="keyword">new</span> HashEntry&lt;K,V&gt;(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">  			<span class="comment">// ...... </span></span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此外，在<code>scanAndLockForPut</code> 内部使用非阻塞锁，也不会阻塞读线程在桶位的并发读，一石二鸟！</p>
<h5 id="scanAndLockForPut解析2"><a href="#scanAndLockForPut解析2" class="headerlink" title="scanAndLockForPut解析2"></a>scanAndLockForPut解析2</h5><p>完整逻辑分析</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">	<span class="function"><span class="keyword">private</span> HashEntry&lt;K,V&gt; <span class="title">scanAndLockForPut</span><span class="params">(K key, <span class="keyword">int</span> hash, V value)</span> </span>&#123;</span><br><span class="line">      		<span class="comment">// 获取key定位在HashEntry数组的桶位头节点</span></span><br><span class="line">          HashEntry&lt;K,V&gt; first = entryForHash(<span class="keyword">this</span>, hash);</span><br><span class="line">          HashEntry&lt;K,V&gt; e = first;</span><br><span class="line">          <span class="comment">// 该节点就是线程自旋抢锁过程中“顺便”把key节点提前创建好</span></span><br><span class="line">          HashEntry&lt;K,V&gt; node = <span class="keyword">null</span>;</span><br><span class="line">      		<span class="comment">// 记录自旋次数</span></span><br><span class="line">          <span class="keyword">int</span> retries = -<span class="number">1</span>; <span class="comment">// negative while locating node</span></span><br><span class="line">          <span class="comment">// 自旋尝试抢锁（加锁）</span></span><br><span class="line">          <span class="keyword">while</span> (!tryLock()) &#123;</span><br><span class="line">              <span class="comment">// 以下的执行流都是基于“未拿到锁的条件下”进行的</span></span><br><span class="line">              HashEntry&lt;K,V&gt; f; <span class="comment">// to recheck first below</span></span><br><span class="line">         		 <span class="comment">// 分支1：把能做的事情先做好了：创建了一个新的node节点，但还没获取到锁，只能继续循环</span></span><br><span class="line">              <span class="keyword">if</span> (retries &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                	<span class="comment">// 若当前key定位到的桶位为空</span></span><br><span class="line">                  <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                      <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="comment">// speculatively create node</span></span><br><span class="line">                          node = <span class="keyword">new</span> HashEntry&lt;K,V&gt;(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">                   <span class="comment">// 将retries置为0：是为了下一次循环跳到分支2或者分支3处理逻辑 </span></span><br><span class="line">                      retries = <span class="number">0</span>;</span><br><span class="line">                  &#125;</span><br><span class="line"><span class="comment">//如果给定的key能在链表中找到，就不需要new一个node节点，继续while去获取锁</span></span><br><span class="line">                  <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(e.key))</span><br><span class="line">                      retries = <span class="number">0</span>;</span><br><span class="line">                  <span class="keyword">else</span> </span><br><span class="line">                  <span class="comment">//   如果给定的key不是头节点key，说明要继续遍历该链表</span></span><br><span class="line">                      e = e.next;</span><br><span class="line">              &#125; <span class="comment">// 分支2：如果自旋获取锁的尝试大于64次，此时调用独占锁（阻塞锁），保证当前线程一定能获取到锁</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (++retries &gt; MAX_SCAN_RETRIES) &#123;</span><br><span class="line">                  lock();</span><br><span class="line">                  <span class="keyword">break</span>;</span><br><span class="line">              &#125;<span class="comment">// 分支3：注意，此分支还未加锁，因此多个线程可能并发执行以下逻辑，如果获取锁的尝试次数未超默认值，在1~64次重试过程中，每达到偶数次：(retries &amp; 1) == 0，当前线程就去检查头节点是否发生了改变，如果头结点发生改变，说明有其他线程已经抢先将某个新节点放入链表头位置，因此当前线程只能重新实施while (!tryLock())。</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> ((retries &amp; <span class="number">1</span>) == <span class="number">0</span> &amp;&amp;</span><br><span class="line">                       (f = entryForHash(<span class="keyword">this</span>, hash)) != first) &#123;</span><br><span class="line">                  e = first = f; <span class="comment">// re-traverse if entry changed</span></span><br><span class="line">                	<span class="comment">// 重置尝试次数</span></span><br><span class="line">                  retries = -<span class="number">1</span>;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">return</span> node;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>这里有一个细节需要注意：代码最后部分“关于重试次数为偶数时就去判断当前桶位的头结点有无变化”，作者设计是基于这么思考：其实无需遍历64次，因为“安排当前线程在自旋过程中顺便去创建node节点”这个任务可要也可不要，如果在头节点恰好在奇数次例如（第3次）发生改变，那么在偶数次（第4次及其以后）就会错过判断，怎么办？ 从代码逻辑可以看出，retries次数达到64后，就会直接加阻塞锁成功，然后退出自旋，那么node节点将会在scanAndLockForPut外部被创建。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(!tryLock())&#123;</span><br><span class="line">  		<span class="comment">//</span></span><br><span class="line">	   <span class="keyword">else</span> <span class="keyword">if</span> (++retries &gt; MAX_SCAN_RETRIES) &#123;</span><br><span class="line">           lock();</span><br><span class="line">           <span class="keyword">break</span>;</span><br><span class="line">     <span class="comment">//  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Doug Lea设计的scanAndLockForPut确实精妙，拿捏得死死的！（ 让线程没有”空跑、摸鱼”的余地）</p>
<h4 id="5、size方法"><a href="#5、size方法" class="headerlink" title="5、size方法"></a>5、size方法</h4><p>先理解主要设计设计思想：</p>
<p>1、从ConcurrentHashMap的两层结构可知：将每个segment段的HashEntry数组节点数进行累加即可得到总数，在多线程场景下，可以这样设计：先锁住所有segment段，累加后再解锁所有segment段。但这样设计有一个问题：直接加锁后，会阻塞其他线程对当前Segment段的并发写，有一丁点性能损耗。</p>
<p>2、基于1的优化：先“投机性赌一把”，赌遍历时没有其他线程来更新该map，那么当前线程可以先不加锁尝试3次遍历累计所有segment的更改次数（modCounts），如果相邻两次更改次数一样，说明这两次统计过程中没有其他线程来更新该map，赌成功了，此时size的计算结果相对很准确。（后面会解释为何是相对很准确，而不是100%准确）</p>
<p>如果相邻两次更改次数不一样，说明这两次统计过程中有其他线程来（put/remove/clear）该map，赌失败了，下一次尝试统计就需要对所有segment段加锁再统计size</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// 先尝试几次不加锁情况下的统计，如果统计失败，说明有持续的并发线程来更改HashEntry数组， resort to直译是使用(武力)，在这里则表示使用”暴力“的阻塞锁</span></span><br><span class="line">       <span class="comment">// 作者英文注释，简单明了解释了size方法设计思想:Try a few times to get accurate count. On failure due to continuous async changes in table, resort to locking.</span></span><br><span class="line">       <span class="keyword">final</span> Segment&lt;K,V&gt;[] segments = <span class="keyword">this</span>.segments;</span><br><span class="line">       <span class="keyword">int</span> size;</span><br><span class="line">     	<span class="comment">// 统计size是否有溢出32位最大值</span></span><br><span class="line">       <span class="keyword">boolean</span> overflow; <span class="comment">// true if size overflows 32 bits</span></span><br><span class="line">     	<span class="comment">// sum 和last 用于比较相邻两次的map修改数</span></span><br><span class="line">       <span class="keyword">long</span> sum;         <span class="comment">// sum of modCounts</span></span><br><span class="line">       <span class="keyword">long</span> last = <span class="number">0L</span>;   <span class="comment">// previous sum</span></span><br><span class="line">       <span class="keyword">int</span> retries = -<span class="number">1</span>; <span class="comment">// first iteration isn&#x27;t retry</span></span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">             	<span class="comment">// 如果无锁情况下，统计尝试次数达到3次（从-1到1），则对所有segment段都加锁</span></span><br><span class="line">               <span class="keyword">if</span> (retries++ == RETRIES_BEFORE_LOCK) &#123;</span><br><span class="line">                   <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; segments.length; ++j)</span><br><span class="line">                       ensureSegment(j).lock(); <span class="comment">// force creation</span></span><br><span class="line">               &#125;</span><br><span class="line">               sum = <span class="number">0L</span>;</span><br><span class="line">               size = <span class="number">0</span>;</span><br><span class="line">               overflow = <span class="keyword">false</span>;</span><br><span class="line">             	<span class="comment">// 遍历所有segment并累计modCount，以及累计所有segment的HashEntry的节个数c</span></span><br><span class="line">               <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; segments.length; ++j) &#123;</span><br><span class="line">                   Segment&lt;K,V&gt; seg = segmentAt(segments, j);</span><br><span class="line">                   <span class="keyword">if</span> (seg != <span class="keyword">null</span>) &#123;</span><br><span class="line">                       sum += seg.modCount;</span><br><span class="line">                       <span class="keyword">int</span> c = seg.count;</span><br><span class="line">ß                     	<span class="comment">// 判断累计的节个数c是否有溢出</span></span><br><span class="line">                       <span class="keyword">if</span> (c &lt; <span class="number">0</span> || (size += c) &lt; <span class="number">0</span>)</span><br><span class="line">                           overflow = <span class="keyword">true</span>;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">             	<span class="comment">// 相邻两次modCount总修改数相等，说明当前没有其他线程来（put/remove/clear）map结构，可以结束统计</span></span><br><span class="line">               <span class="keyword">if</span> (sum == last)</span><br><span class="line">                   <span class="keyword">break</span>;</span><br><span class="line">               last = sum;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">         	<span class="comment">// 前面加锁统计完size后，这里就需要释放锁</span></span><br><span class="line">           <span class="keyword">if</span> (retries &gt; RETRIES_BEFORE_LOCK) &#123;</span><br><span class="line">               <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; segments.length; ++j)</span><br><span class="line">                   segmentAt(segments, j).unlock();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">     	<span class="comment">// size溢出则返回Integer.MAX_VALUE，否则返回实际统计的size值</span></span><br><span class="line">       <span class="keyword">return</span> overflow ? Integer.MAX_VALUE : size;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从finally的逻辑可以看出，size结果是相对很准确，而不是100%准确，因为它在逐个释放锁的过程中，有可能其他线程正在对“已释放锁的Segment”进行写操作（put/remove）。</p>
<h4 id="6、rehash方法"><a href="#6、rehash方法" class="headerlink" title="6、rehash方法"></a>6、rehash方法</h4><p>rehash方法是Segment内部类方法，因此所谓的ConcurrentHashMap扩容，也是只是委托了对应的Segment段里面的HashEntry数组扩容，而不是Segment数组扩容！需要留意的是，rehash在put方法内部调用，执行rehash方法时，当前线程已经获得阻塞锁，因此可以独立扩容处理。</p>
<p>此外，java7的ConcurrentHashMap的原表节点转移到新表的设计跟java7的HashMap有点不一样</p>
<p>java7的HashMap扩容设计相对简单：针对冲突链的处理，将低位节点和高位节点区分开，分别放在新表的i位置、oldTab.length+i位置</p>
<p>而java7的ConcurrentHashMap的扩展设计：针对冲突链的处理，将“在新数组桶位相同的连续子链”和其他剩余节点区分开，然后两者放到新表对应位置。</p>
<p><font color=red> 新数组桶位相同的连续子链是指 ：子链尾节点必须是该父链尾节点，此子链才能定义为”连续子类“，所以19-&gt;35-&gt;51不是rehash定义的“连续子链”</font>， 参考后面的图示。</p>
<p>这里找子链的方法其实也不难，在一些leecode算法题例如在长字符串找一个“字符连续相同的子串”（abaaaab=&gt;aaaa）、在链表里找一个子链等。</p>
<p>以下准备了两张图解用于解释其扩容过程，基本约定：</p>
<p>1、hash计算约定：假设使用简单hash方法，也即hash（数字）=数字本身，桶位计算方式：j=hash&amp;oldTab.length-1</p>
<p>2、原数组长度为8，扩容阈值=8*0.75=6，链表已经达到6个节点，因此该桶位链表需要扩容，新表容量为16</p>
<p>3、在原数组桶位i=3位置上，形成冲突链，给定两种假设：</p>
<p>A、桶位含有“在新数组桶位相同的连续子链”的链表：3-&gt;11-&gt;19-&gt;35-&gt;51-&gt;67-&gt;null，如下图所示</p>
<p>（图中也清楚展示了“低位节点”和“高位节点”的情况，关于什么是低位和高位节点，只要有看过本博客关于HashMap的深度文章解析，这些是基本常识，这里不再累赘）<br><img src="https://img-blog.csdnimg.cn/3238fd00eb9a4704a77076200e1f9df6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="“桶位有连续子链”对应的扩容过程图解："><a href="#“桶位有连续子链”对应的扩容过程图解：" class="headerlink" title="“桶位有连续子链”对应的扩容过程图解："></a>“桶位有连续子链”对应的扩容过程图解：</h5><p><img src="https://img-blog.csdnimg.cn/198b28346f38434b879c3bfc4c67b771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>B、桶位不含有“子链”的链表：3-&gt;11-&gt;19-&gt;35-&gt;51-&gt;27-&gt;null，如下图所示</p>
<p>图中也清楚展示了“低位节点”和“高位节点”的情况</p>
<p><img src="https://img-blog.csdnimg.cn/afab5722a7e149bd89688b03bb7a10cf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="“桶位无连续子链”对应的扩容过程下面图解："><a href="#“桶位无连续子链”对应的扩容过程下面图解：" class="headerlink" title="“桶位无连续子链”对应的扩容过程下面图解："></a>“桶位无连续子链”对应的扩容过程下面图解：</h5><p><img src="https://img-blog.csdnimg.cn/e0247aaaadae4453a0e2da4cb18c117f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="rehash源码详细解析"><a href="#rehash源码详细解析" class="headerlink" title="rehash源码详细解析"></a>rehash源码详细解析</h5><p>基于以上四张图，再理解rehash源码设计思路则会简单很多。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rehash</span><span class="params">(HashEntry&lt;K,V&gt; node)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 以下的原数组和新数组分别指代同一个segment段下，原HashEntry数组和新HashEntry数组</span></span><br><span class="line">            HashEntry&lt;K,V&gt;[] oldTable = table;</span><br><span class="line">            <span class="keyword">int</span> oldCapacity = oldTable.length;</span><br><span class="line">          	<span class="comment">// 新数组容量=2倍原数组容量</span></span><br><span class="line">            <span class="keyword">int</span> newCapacity = oldCapacity &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 新数组的扩容阈值</span></span><br><span class="line">            threshold = (<span class="keyword">int</span>)(newCapacity * loadFactor);</span><br><span class="line">          	</span><br><span class="line">          	<span class="comment">// new一个新数组    </span></span><br><span class="line">            HashEntry&lt;K,V&gt;[] newTable =(HashEntry&lt;K,V&gt;[]) <span class="keyword">new</span> HashEntry[newCapacity];</span><br><span class="line">            </span><br><span class="line"> <span class="comment">// 使用位方式计算节点在新数组的桶位，在jdk1.8的HashMap源码解析中，已经对这部分知识讲得非常清楚，这里不再赘述</span></span><br><span class="line">            <span class="keyword">int</span> sizeMask = newCapacity - <span class="number">1</span>;</span><br><span class="line">          	<span class="comment">// 遍历原数组每个桶位</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; oldCapacity ; i++) &#123;</span><br><span class="line">                HashEntry&lt;K,V&gt; e = oldTable[i];</span><br><span class="line">             		<span class="comment">// 若原数组i桶位头节点不为空，且next节点为null，说明该桶位仅有一个头节点，计算出该节点在新数组的idx位置后，头插法：newTable[idx] = e</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    HashEntry&lt;K,V&gt; next = e.next;</span><br><span class="line">                    <span class="keyword">int</span> idx = e.hash &amp; sizeMask;</span><br><span class="line">                    <span class="keyword">if</span> (next == <span class="keyword">null</span>)   <span class="comment">//  Single node on list</span></span><br><span class="line">                        newTable[idx] = e;</span><br><span class="line"><span class="comment">//  若原数组i桶位头节点不为空，且next节点不为空，说明该桶位是一条冲突链，它的扩容处理比较复杂，可结合上面的图解内容进行综合分析</span></span><br><span class="line">                    <span class="keyword">else</span> &#123; <span class="comment">// Reuse consecutive sequence at same slot</span></span><br><span class="line">                        HashEntry&lt;K,V&gt; lastRun = e;</span><br><span class="line">                        <span class="keyword">int</span> lastIdx = idx;</span><br><span class="line">                        <span class="keyword">for</span> (HashEntry&lt;K,V&gt; last = next;</span><br><span class="line">                             last != <span class="keyword">null</span>;</span><br><span class="line">                             last = last.next) &#123;</span><br><span class="line">                          	<span class="comment">// 计算当前遍历节点在新数组桶位号</span></span><br><span class="line">                            <span class="keyword">int</span> k = last.hash &amp; sizeMask;</span><br><span class="line">                          	</span><br><span class="line"><span class="comment">// 如果前一个节点在新数组桶位号和后一个节点在新数组桶位号相同，说明找到“连续的、新表桶位相同的子链”(对应图解里面图4的情况)，将lastRun指向该子链头节点，对应图中节点11。</span></span><br><span class="line">                            <span class="keyword">if</span> (k != lastIdx) &#123;</span><br><span class="line">                                lastIdx = k;</span><br><span class="line">                                lastRun = last;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"><span class="comment">//                       </span></span><br><span class="line">                        newTable[lastIdx] = lastRun;<span class="comment">// lastRun指向的子链整体迁移</span></span><br><span class="line">                        <span class="comment">// Clone remaining nodes</span></span><br><span class="line">                        <span class="keyword">for</span> (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123;<span class="comment">// 迁移其他非lastRun节点</span></span><br><span class="line">                            V v = p.value;</span><br><span class="line">                            <span class="keyword">int</span> h = p.hash;</span><br><span class="line">                            <span class="keyword">int</span> k = h &amp; sizeMask;</span><br><span class="line">                            HashEntry&lt;K,V&gt; n = newTable[k];</span><br><span class="line">                            newTable[k] = <span class="keyword">new</span> HashEntry&lt;K,V&gt;(h, p.key, v, n); <span class="comment">// 注意这里是用旧桶位节点的key和value来创建新节点，因此原桶位节点不会有任何改动，也说明rehash的同时也支持并发读</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          </span><br><span class="line">          <span class="comment">// 完成扩容后，对rehash(node)里面的新节点node放入到新newTable里面</span></span><br><span class="line">            <span class="keyword">int</span> nodeIndex = node.hash &amp; sizeMask; <span class="comment">// add the new node</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        这里用putOrderedObject添加新节点到新数组里面，也是将其写入主存的操作延迟到外部方法put的finally的unlock位置，目的还是为减少频繁的“线程工作内存&lt;--&gt;主存”之间“来回跑”，等”凑齐“多个延迟写入的操作，然后在unlock前一并写入主存，提高put性能，作者对put性能提高的设计确定足够细腻！！！</span></span><br><span class="line"><span class="comment">        final void setNext(HashEntry&lt;K,V&gt; n) &#123;</span></span><br><span class="line"><span class="comment">            UNSAFE.putOrderedObject(this, nextOffset, n);</span></span><br><span class="line"><span class="comment">        &#125;  </span></span><br><span class="line"><span class="comment">        */</span>  </span><br><span class="line">            node.setNext(newTable[nodeIndex]);</span><br><span class="line">            newTable[nodeIndex] = node;</span><br><span class="line">            table = newTable; <span class="comment">// 将table引用指向newTable，原table指向的对象将会被GC</span></span><br><span class="line">           </span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="7、其他方法"><a href="#7、其他方法" class="headerlink" title="7、其他方法"></a>7、其他方法</h4><p>这里需要强调的是，前面第4到6节核心方法的设计及其源码都能理解的话，那么关于jdk1.7的ConcurrentHashMap其他方法的理解则变得相对简单很多</p>
<h5 id="get方法："><a href="#get方法：" class="headerlink" title="get方法："></a>get方法：</h5><p>两次定位，第一次定位到key对应segment位置，第二次定位到对应的HashEntry数组位置</p>
<p>两次定位都使用UNSAFE.getObjectVolatile方法实现，使用无锁实现高效率并发读，这也证明了读操作是并发的，不受写操作、独占锁影响，而且基于 happen-before</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Segment&lt;K,V&gt; s; <span class="comment">// manually integrate access methods to reduce overhead</span></span><br><span class="line">    HashEntry&lt;K,V&gt;[] tab;</span><br><span class="line">    <span class="keyword">int</span> h = hash(key);</span><br><span class="line">    <span class="keyword">long</span> u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE;</span><br><span class="line">  	<span class="comment">// key对应的segment和HashEntry数组都在的情况下</span></span><br><span class="line">    <span class="keyword">if</span> ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        (tab = s.table) != <span class="keyword">null</span>) &#123;</span><br><span class="line">      	<span class="comment">// 定位HashEntry数组的对应桶位，然后再桶位上查找是否key这个节点</span></span><br><span class="line">        <span class="keyword">for</span> (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile</span><br><span class="line">                 (tab, ((<span class="keyword">long</span>)(((tab.length - <span class="number">1</span>) &amp; h)) &lt;&lt; TSHIFT) + TBASE);</span><br><span class="line">             e != <span class="keyword">null</span>; e = e.next) &#123;</span><br><span class="line">            K k;</span><br><span class="line">          	<span class="comment">// 找到则返回值</span></span><br><span class="line">            <span class="keyword">if</span> ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k)))</span><br><span class="line">                <span class="keyword">return</span> e.value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">// 找不到key，返回null</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="remove方法"><a href="#remove方法" class="headerlink" title="remove方法"></a>remove方法</h5><p>首先remove方法设计相对简单，它是委托key所在的Segment段实施的，因此不会影响其他Segment段的并发读写操作。</p>
<p>可以看出，remove属于写操作，跟put方法一样，首先需要获取独占锁。</p>
<p>remove两次hash定位和get方法一样，第一次先定位（路由、查找）到对应的Segment段，第二次定位HashEntry数组的桶位，remove方法内部内部封装了Segment的remove方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// map.remove(&quot;foo&quot;)  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> hash = hash(key);</span><br><span class="line">       Segment&lt;K,V&gt; s = segmentForHash(hash);</span><br><span class="line">       <span class="keyword">return</span> s == <span class="keyword">null</span> ? <span class="keyword">null</span> : s.remove(key, hash, <span class="keyword">null</span>);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>Segment段内部remove方法设计：</p>
<p>设计说明</p>
<p>A、remove头节点情况：例如需要移除3-&gt;7-&gt;11-&gt;15-&gt;null的头节点3</p>
<p>e=3,next=7-&gt;11-&gt;15-&gt;null，</p>
<p>因此只需将节点7插入原桶位头节点即可：<code>setEntryAt(tab, index, next)</code>，而且使用<code>UNSAFE.putOrderedObject</code>方式，不急着写入到主存，等到finally的unlock前才一并写入主存。</p>
<p>B、remove非头结点情况：例如需要移除3-&gt;7-&gt;11-&gt;15-&gt;null的节点7</p>
<p>pred=3,e=7,next=11-&gt;15-&gt;null</p>
<p>只需将节点3的next指向节点11即可：pred.setNext(next)，也是使用UNSAFE.putOrderedObject方法，不急着写入到主存，等到finally的unlock前才一并写入主存</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">remove</span><span class="params">(Object key, <span class="keyword">int</span> hash, Object value)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// 先获取独占锁，跟scanAndLockForPut方法类似，但更简单，少了创建Node节点的任务。</span></span><br><span class="line">    <span class="keyword">if</span> (!tryLock())</span><br><span class="line">        scanAndLock(key, hash);</span><br><span class="line">    <span class="comment">//====tryLock加锁成功，进入临界区====    </span></span><br><span class="line">    V oldValue = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        HashEntry&lt;K,V&gt;[] tab = table;</span><br><span class="line">        <span class="keyword">int</span> index = (tab.length - <span class="number">1</span>) &amp; hash;</span><br><span class="line">     		 <span class="comment">// 定位key所在的桶位</span></span><br><span class="line">        HashEntry&lt;K,V&gt; e = entryAt(tab, index);</span><br><span class="line">        HashEntry&lt;K,V&gt; pred = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// 遍历链表，找出链表中与给定key相同的节点</span></span><br><span class="line">        <span class="keyword">while</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">            K k;</span><br><span class="line">            HashEntry&lt;K,V&gt; next = e.next;</span><br><span class="line">            <span class="keyword">if</span> ((k = e.key) == key ||</span><br><span class="line">                (e.hash == hash &amp;&amp; key.equals(k))) &#123;</span><br><span class="line">                V v = e.value;</span><br><span class="line">                <span class="keyword">if</span> (value == <span class="keyword">null</span> || value == v || value.equals(v)) &#123;</span><br><span class="line">                  	<span class="comment">// 对应A情况</span></span><br><span class="line">                    <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">                        setEntryAt(tab, index, next);</span><br><span class="line">                    <span class="comment">// 对应B情况</span></span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        pred.setNext(next);</span><br><span class="line">                  	<span class="comment">// 删除一个节点，map结构变化次数加1，节点数量当然需要减1</span></span><br><span class="line">                    ++modCount;</span><br><span class="line">                    --count;</span><br><span class="line">                    oldValue = v;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            pred = e;</span><br><span class="line">            e = next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    	<span class="comment">//====释放锁，离开临界区====    </span></span><br><span class="line">       <span class="comment">// 释放独占锁， 前面所有使用UNSAFE.putOrderedObject的逻辑，将在这里一并写入到主存中</span></span><br><span class="line">        unlock();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> oldValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>replace方法类似，此处不再累赘。</p>
<h5 id="isEmpty方法"><a href="#isEmpty方法" class="headerlink" title="isEmpty方法"></a>isEmpty方法</h5><p>第一次遍历所有Segment，对每个Segment的modCount进行累加：sum += seg.modCount，而且在遍历过程中，只要出现Segment对应的HashEntry数组长度不为0时，即可直接返回结果，不需再遍历剩余的Segment。</p>
<p>第二次遍历所有Segment，如果Segment里面的HashEntry数组长度为0，则进行操作：sum -= seg.modCount</p>
<p>设计思路：</p>
<ul>
<li><p>要证明ConcurrentHashMap为空，那么以上两次操作后，sum变量必须为0，表示两次统计modCount没有发生变化且为0次ModCount。</p>
</li>
<li><p>要证明ConcurrentHashMap不为空，那么在第二次遍历所有Segment，只要出现任意一个Segment的的HashEntry数据长度不为0，即可证明当前ConcurrentHashMap不为空</p>
</li>
</ul>
<p>以上就是isEmpty的设计思想，当然遍历数组Segment过程中访问Segment段还是会使用<code>UNSAFE.getObjectVolatile(ss, u)</code>方式来取。</p>
<p>为何不用map.size()==0来判断是否为空？</p>
<p>对比size方法计算流程和isEmpty计算流程可以得出答案，isEmpty代码简洁且效率会快一些，因为isEmpty在遍历过程出现一个Segment的HashEntry数组长度不为0即可返回结果，而size方法要计算所有Segment且需要计算两次（运气不好时还需要加锁计算）才能得出结论。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isEmpty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> sum = <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">final</span> Segment&lt;K,V&gt;[] segments = <span class="keyword">this</span>.segments;</span><br><span class="line">   <span class="comment">// 对应第一次遍历</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; segments.length; ++j) &#123;</span><br><span class="line">        Segment&lt;K,V&gt; seg = segmentAt(segments, j);</span><br><span class="line">        <span class="keyword">if</span> (seg != <span class="keyword">null</span>) &#123;</span><br><span class="line">          	<span class="comment">// 只要有其中一个Segment的HashEntry数组长度不为0，即可返回结果：不空</span></span><br><span class="line">            <span class="keyword">if</span> (seg.count != <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            sum += seg.modCount;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// 对应第二次遍历的情况</span></span><br><span class="line">    <span class="keyword">if</span> (sum != <span class="number">0L</span>) &#123; <span class="comment">// recheck unless no modifications</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; segments.length; ++j) &#123;</span><br><span class="line">            Segment&lt;K,V&gt; seg = segmentAt(segments, j);</span><br><span class="line">            <span class="keyword">if</span> (seg != <span class="keyword">null</span>) &#123;</span><br><span class="line">              <span class="comment">// 只要有其中一个Segment的HashEntry数组长度不为0，即可返回结果：不空</span></span><br><span class="line">                <span class="keyword">if</span> (seg.count != <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                sum -= seg.modCount;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      </span><br><span class="line">     <span class="keyword">if</span> (sum != <span class="number">0L</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="8、关键总结"><a href="#8、关键总结" class="headerlink" title="8、关键总结"></a>8、关键总结</h4><p><code>HashEntry</code>的成员变量<code>value</code>和<code>next</code>是被关键字<code>volatile</code>修饰的，也就是说所有线程都可以及时检查到其他线程对这两个变量的改变，因而可以在不加锁的情况下读取到这两个引用的最新值</p>
<h5 id="8-1-ConcurrentHashMap与HashMap不同点对比"><a href="#8-1-ConcurrentHashMap与HashMap不同点对比" class="headerlink" title="8.1 ConcurrentHashMap与HashMap不同点对比"></a>8.1 ConcurrentHashMap与HashMap不同点对比</h5><ul>
<li><p>最简单区别当然线程安全：ConcurrentHashMap写操作：put/remove/replace都是需要加锁（scanAndLockForPut、scanAndLock）</p>
</li>
<li><p>HashMap允许Key和Value为null，而ConcurrentHashMap不允许key、value为空，参考8.5解释</p>
</li>
<li><p>HashMap不允许通过Iterator遍历的同时通过HashMap修改（强一致性要求），而ConcurrentHashMap允许该行为（本质原因CHM是弱一致性），并且该更新对后续的遍历可见，参考8.6</p>
</li>
</ul>
<h5 id="8-2-ConcurrentHashMap的并发度问题"><a href="#8-2-ConcurrentHashMap的并发度问题" class="headerlink" title="8.2 ConcurrentHashMap的并发度问题"></a>8.2 ConcurrentHashMap的并发度问题</h5><p> ConcurrentHashMap的并发度concurrencyLevel在new 构造方法就已经固定（默认并发度16个线程），例如一开始给定Segment数组是128，那么并发度最多128线程同时写操作，但对于读操作，则不限制，可以是128也可以10000等不同数量线程并发读。</p>
<p>此提问可衍生另外一个知识点：</p>
<p>如果ConrruentHashMap需要扩容，通过第6节的rehash方法可知，它是委托key所在的Segment段去扩容该段里面的HashEntry数组，而不是对Segment数组本身扩容，对于这个问题，如果不了解ConrruentHashMap，应该也会惯性思维认为：“扩容时，Segment数组也会被扩容”这样的错误理解。</p>
<p>由于Segment数组初始化就限制了并发度，因此需要你提前根据业务场景设定号并发度值，这也算是jdk1.7 ConrruentHashMap需要优化的地方。</p>
<h5 id="8-3-ConcurrentHashMap分段锁的巧妙设计思想是值得借鉴"><a href="#8-3-ConcurrentHashMap分段锁的巧妙设计思想是值得借鉴" class="headerlink" title="8.3 ConcurrentHashMap分段锁的巧妙设计思想是值得借鉴"></a>8.3 ConcurrentHashMap分段锁的巧妙设计思想是值得借鉴</h5><p>尤其在一些计高并发计数场景，例如jdk1.8的LongAdder、jdk1.8 ConcurrentHashMap里面的fullAddCount方法，但jdk1.8的思路更优，采用分段无锁方式，比Segment分段锁lock更高性能。</p>
<h5 id="8-4-ConcurrentHashMap有可能退化成SynchronizedMap"><a href="#8-4-ConcurrentHashMap有可能退化成SynchronizedMap" class="headerlink" title="8.4 ConcurrentHashMap有可能退化成SynchronizedMap"></a>8.4 ConcurrentHashMap有可能退化成SynchronizedMap</h5><p>假设有些业务的key不够合理，绝大部分的key都hash到同一个segment段，那么容易导致多个线程仅在这个Segment段进行写操作，退化成SynchronizedMap，这个段就是全局锁，这也是jdk1.7 ConrruentHashMap需要优化的地方。</p>
<h5 id="8-5-关于ConcurrentHashMap的key和value都不能为空的讨论"><a href="#8-5-关于ConcurrentHashMap的key和value都不能为空的讨论" class="headerlink" title="8.5 关于ConcurrentHashMap的key和value都不能为空的讨论"></a>8.5 关于ConcurrentHashMap的key和value都不能为空的讨论</h5><ul>
<li>ConcurrentHashMap的value不能为空的原因</li>
</ul>
<p>考察HashMap：在单线程操作的HashMap场景下，value可以放入null值，当使用get方法返回的值是null时，这个“null”存在二义性：要么key对应的value为null，要么key不在map里面，那么怎么唯一区分呢？很简单，只需结合containsKey方法就可以唯一确定取出的null是属于哪种情况，演示逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 单线程能顺序执行以下所有逻辑</span></span><br><span class="line"><span class="comment">// 如果map包含该key，那么get返回的null是属于“key存在且对应的value为null”的情况</span></span><br><span class="line"><span class="comment">// 否则抛出key不存在的提示，这时就可以知道get返回的null是属于“key存不在map里面”的情况</span></span><br><span class="line"><span class="keyword">if</span> (m.containsKey(k)) &#123;</span><br><span class="line">   <span class="keyword">return</span> m.get(k);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   <span class="keyword">throw</span> <span class="keyword">new</span> KeyNotPresentException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>基于以上的知识铺垫，下面我们再通过反证法来论证ConcurrentHashMap的value不能真实原因</p>
<p>在并发场景下 ，若将ConcurrentHashMap的value可以设为null，当使用get方法返回的值是null时，存在二义性：要么key对应的value为null，要么key不在map里面，考察使用containsKey方法来区分，参考图解</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (m.containsKey(k)) &#123;</span><br><span class="line">   <span class="keyword">return</span> m.get(k);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   <span class="keyword">throw</span> <span class="keyword">new</span> KeyNotPresentException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/749bdcdee5ce43819369aec363bf8703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>这张图很好解释了ConcurrentHashMap的value不能设为空的原因，并发条件下，执行m.containsKey(k)和m.get(k)之间会有其他线程“捣乱流程”：</p>
<blockquote>
<p>线程A 执行m.containsKey(k)后返回true，线程A此时认为key存在map中，正准执行m.get前，线程B提前删除该key，接着线程A使用m.get得到的是null，然后线程A很自信得出“key是存在，只是value对应为null”的结论，而实际上呢，该key已经被线程B删除，实际情况为key不存在map中，ConcurrentHashMap这就是无法解决的二义性。</p>
</blockquote>
<ul>
<li>至于ConcurrentHashMap的key不能为空的依据：</li>
</ul>
<p>1、纯粹是基于Java关于null是否符合“程序优雅设计与否”的经验知识，其实Doug Lea认为map中允许键值为null是一种不合理的设计，HashMap虽然可以判断二义性，但是Doug Lea仍然觉得这样设计是不合理的。在java项目中，如果key为null通常意味着有些地方已经有出错的苗头，所以早点抛异常比允许null key更合适。</p>
<p>2、允许key为null另外一个不够优雅地方就是不方便遍历哈希表，尤其对于ConcurrentMap。</p>
<p>3、基于以上背景，Doug Lea在源码hash计算方法设计上就不支持key为null的处理，若为null，那么使用k.hashCode()抛出NPE，可以参考hash(key)方法。</p>
<h5 id="8-5-Fast-fail产生原因"><a href="#8-5-Fast-fail产生原因" class="headerlink" title="8.5 Fast-fail产生原因"></a>8.5 Fast-fail产生原因</h5><p><a href="http://www.jasongj.com/java/concurrenthashmap/">http://www.jasongj.com/java/concurrenthashmap/</a></p>
<p>在使用迭代器的过程中如果HashMap被修改，那么<code>ConcurrentModificationException</code>将被抛出，也即Fast-fail策略。</p>
<p>当HashMap的iterator()方法被调用时，会构造并返回一个新的EntryIterator对象，并将EntryIterator的expectedModCount设置为HashMap的modCount（该变量记录了HashMap被修改的次数）。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HashIterator() &#123;</span><br><span class="line">  expectedModCount = modCount;</span><br><span class="line">  <span class="keyword">if</span> (size &gt; <span class="number">0</span>) &#123; <span class="comment">// advance to first entry</span></span><br><span class="line">  Entry[] t = table;</span><br><span class="line">  <span class="keyword">while</span> (index &lt; t.length &amp;&amp; (next = t[index++]) == <span class="keyword">null</span>)</span><br><span class="line">    ;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在通过该Iterator的next方法访问下一个Entry时，它会先检查自己的expectedModCount与HashMap的modCount是否相等，如果不相等，说明HashMap被修改，直接抛出<code>ConcurrentModificationException</code>。该Iterator的remove方法也会做类似的检查。该异常的抛出意在提醒用户及早意识到线程安全问题。</p>
<h5 id="8-6-ConcurrentHashMap高并发读为什么可以无锁？"><a href="#8-6-ConcurrentHashMap高并发读为什么可以无锁？" class="headerlink" title="8.6 ConcurrentHashMap高并发读为什么可以无锁？"></a>8.6 ConcurrentHashMap高并发读为什么可以无锁？</h5><p>首先：JMM实现了对volatile的保证：对volatile域的写入操作happens-before于每一个后续对同一个域的读写操作。</p>
<p>HashEntry定义中使用volatile修饰value、next字段，恰恰能享受到以上JMM所提及volatile两点收益：</p>
<p>1、volatile语义可以保证写操作在读操作之前（写操作happens-before于读操作），也即保证了写操作对后续的读操作都是可见的</p>
<p>2、其次在写入操作的时候使用`UNSAFE.putOrderedObjectE写入主存的时机延迟到put方法的unlock前，保证了数据的一致性。</p>
<p>有了以上两个机制后，那么使用 UNSAFE.getObjectVolatile即可支持并发无锁读</p>
<h5 id="8-8-为何HashEntry节点类型设置为final"><a href="#8-8-为何HashEntry节点类型设置为final" class="headerlink" title="8.8 为何HashEntry节点类型设置为final"></a>8.8 为何HashEntry节点类型设置为final</h5><p>首先ConcurrentHashMap的HashEntry为final类型（一旦创建就成为不可变类），而HashMap的Entry节点是非final类型，ConcurrentHashMap为何这么设计？</p>
<p>要回答这一问题，其实需要理解final有什么用？jvm会对final定义的变量做怎样的处理？<a href="https://www.infoq.cn/article/java-memory-model-6/">具体解释可参考此文章</a></p>
<p>简单来说：</p>
<p>1、对于使用final修饰的对象，java编译器会保证在读final域之前一定被写入过，即保证写在读之前发生，避免多线程并发条件下出现读后写，读到非预期的值。</p>
<p>ok，按这样的思路去解释HashEntry类：java编译器会保证在读HashEntry节点域里面的属性如key、value之前，一定会保证key、value先被写入后再来读，避免并发情况下出现“读后写，读到非预期的值”，也即保证线程安全。</p>
<p>2、对比volatile，final不需要额外的线程本地内存和主存之间的同步开销。</p>
<h5 id="8-7-HashMap在第一次put时才完成初始化，那么ConcurrentHashMap在什么时机开始初始化？"><a href="#8-7-HashMap在第一次put时才完成初始化，那么ConcurrentHashMap在什么时机开始初始化？" class="headerlink" title="8.7 HashMap在第一次put时才完成初始化，那么ConcurrentHashMap在什么时机开始初始化？"></a>8.7 HashMap在第一次put时才完成初始化，那么ConcurrentHashMap在什么时机开始初始化？</h5><p>ConcurrentHashMap的初始化在new 构造方法时就已经完成部分初始化，完成Segment数组创建、第0号位的Segment对象创建以及在其里面的HashEntry容量2的创建，因此，它的初始化可不是在第一次put才初始化</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度讨论高并发跳表数据结构ConcurrentSkipListMap的源代码实现（上）</title>
    <url>/2022/01/07/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%AE%A8%E8%AE%BA%E9%AB%98%E5%B9%B6%E5%8F%91%E8%B7%B3%E8%A1%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84ConcurrentSkipListMap%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    <content><![CDATA[<p>在单线程场景下，HashMap适用于key为无序的键值对存放场景，而TreeMap适用于key为有序的键值对存放场景。</p>
<p>在高并发场景下，ConcurrentHashMap适用于key为无序的键值对存场景，但对于高并发且要求key有序的场景下，TreeMap非线程安全显然无法满足此场景， 在Concurrent包里面只有跳表：ConcurrentSkipListMap可以满足”基于乐观锁高性能的并发读写、key有序”的需求，而且其设计不会像ConcurrentHashMap这么复杂，但确有着恰当的应用场景，例如对于时序流式数据的存放（最近比较热门的物联网大数据引擎TDengine），可以将乱序的记录以时间戳作为key插入到跳表中，跳表内部处理插入时会比较key的hash值大小以找到节点合适的插入位置，那么在读取时跳表返回的记录就是有序了。</p>
<p>jdk1.8的ConcurrentSkipListMap在本文简写为CSM，Dung Lea在源代码开头的注释详细介绍了CSM总体设计思路并给出字符型展示的CSM结构图，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* Head nodes          Index nodes</span><br><span class="line">* +-+    right        +-+                      +-+</span><br><span class="line">* |<span class="number">2</span>|----------------&gt;| |---------------------&gt;| |-&gt;<span class="keyword">null</span></span><br><span class="line">* +-+                 +-+                      +-+</span><br><span class="line">*  | down              |                        |</span><br><span class="line">*  v                   v                        v</span><br><span class="line">* +-+            +-+  +-+       +-+            +-+       +-+</span><br><span class="line">* |<span class="number">1</span>|-----------&gt;| |-&gt;| |------&gt;| |-----------&gt;| |------&gt;| |-&gt;<span class="keyword">null</span></span><br><span class="line">* +-+            +-+  +-+       +-+            +-+       +-+</span><br><span class="line">*  v              |    |         |              |         |</span><br><span class="line">* Nodes  next     v    v         v              v         v</span><br><span class="line">* +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+</span><br><span class="line">* | |-&gt;|A|-&gt;|B|-&gt;|C|-&gt;|D|-&gt;|E|-&gt;|F|-&gt;|G|-&gt;|H|-&gt;|I|-&gt;|J|-&gt;|K|-&gt;<span class="keyword">null</span></span><br><span class="line">* +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+  +-+</span><br></pre></td></tr></table></figure>
<p>CSM源代码解析文章说明：由于CSM解析内容较多，因此全文分为“深度讨论高并发跳表数据结构ConcurrentSkipListMap的源代码实现（上）”和“深度讨论高并发跳表数据结构ConcurrentSkipListMap的源代码实现（下）”两篇文章<br>上篇关注的重点：CSM数据结构设计原理、doGet、doPut核心方法解析<br>下篇关注的中断：doRemove核心方法解析、总结</p>
<p><img src="https://img-blog.csdnimg.cn/0eb498c68cbb4bf1999927e150db78b1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>《gitee 博客文章封面》</p>
<a id="more"></a>
<h4 id="CSM的基本用法"><a href="#CSM的基本用法" class="headerlink" title="CSM的基本用法"></a>CSM的基本用法</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Objects;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentSkipListMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SkipListDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// ConcurrentHashMap&lt;String,Integer&gt; sm= new ConcurrentHashMap&lt;&gt;();     </span></span><br><span class="line">        ConcurrentSkipListMap&lt;String,Integer&gt; sm=<span class="keyword">new</span> ConcurrentSkipListMap&lt;&gt;();</span><br><span class="line">        List&lt;Thread&gt; list=<span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="number">10</span> ; i++) &#123;</span><br><span class="line">            Thread t=<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    sm.put(Thread.currentThread().getName(),(<span class="keyword">int</span>)Thread.currentThread().getId());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            list.add(t);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Thread thread : list) &#123;</span><br><span class="line">           thread.start();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (Thread thread : list) &#123;</span><br><span class="line">           thread.join();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(sm);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用CHM输出的结果为无序结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;Thread-3&#x3D;12, Thread-4&#x3D;13, Thread-5&#x3D;14, Thread-6&#x3D;15, Thread-7&#x3D;16, Thread-8&#x3D;17, Thread-9&#x3D;18, Thread-0&#x3D;9, Thread-1&#x3D;10, Thread-2&#x3D;11&#125;</span><br></pre></td></tr></table></figure>
<p>使用CSM输出的结果为有序结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;Thread-0&#x3D;9, Thread-1&#x3D;10, Thread-2&#x3D;11, Thread-3&#x3D;12, Thread-4&#x3D;13, Thread-5&#x3D;14, Thread-6&#x3D;15, Thread-7&#x3D;16, Thread-8&#x3D;17, Thread-9&#x3D;18&#125;</span><br></pre></td></tr></table></figure>
<h4 id="CSM-数据结构图"><a href="#CSM-数据结构图" class="headerlink" title="CSM 数据结构图"></a>CSM 数据结构图</h4><p><img src="https://img-blog.csdnimg.cn/0933286e19b245f29e6f9067fedf6ae2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>这个跳表结构有四层，其中base-level层是完整的数据节点链表，在base-level层上面的三层都是索引节点构成的索引链，从图中可以直观看到：上层的索引链表是下层索引链表的“快车道”</p>
<p>从这种图也可以总结出跳表（这里是泛指跳表结构不是指Java的CSM）的基本特征：</p>
<ul>
<li>由最底层的完整数据节点链表层+上面的多层索引层组成。</li>
<li>每一层都是一个有序的链表</li>
<li>如果一个key出现在第n层中，则它在最底层以及1到第n-1层也都会出现</li>
</ul>
<p>而对于jdk1.8实现的功能完整的ConcurrentSikpListMap，可按上面的结构图进行简单说明各个元素的构成（最底层到最顶层）：</p>
<ul>
<li><p>Node节点：存放数据的节点，只在最底层的数据层链表出现，在插入操作时，可能会被随机算法选中上升为“index索引节点”</p>
</li>
<li><p>base-level：指代数据层链表的位置，此层存放的是完整数据节点链表，该层的所有节点都是Node类型，不含有Index类型节点！</p>
</li>
<li><p>BASE_HEADER：这个节点不是数据层链表的第一个数据节点，它是辅助节点，可以看做是数据层链表的“索引节点”，但它是Node类型：<code>new Node&lt;K,V&gt;(null, BASE_HEADER, null)</code></p>
</li>
<li><p>Index节点：位于索引层的节点，采用随机算法把它插入在数据层上方的索引层链表上。</p>
</li>
<li>HeadIndex节点：作为辅助节点，非数据节点，是当前索引层链表的头节点，它也会指向下一层index节点索引链表的头节点。Dung Lea称它是dummy node</li>
<li>level：索引节点所在层序号，从level=1开始计数，最高不超过31（含31层），文章后面给出了计算解释。</li>
<li>right指针：HeadIndex、Index节点专有字段，可以使得遍历链表的方向向右移动</li>
<li>down指针：HeadIndex、Index节点专有字段，可以使得遍历链表的方向向下移动</li>
<li>head节点：所有线程的读写操作都是这个头节点开始作为遍历入口，就像“迷宫的入口点，然后根据向右边还是向下移动，直到走到适合索引节点位置”，它指向最顶层索引层的HeadIndex节点。</li>
<li>node指针：此指针最特殊！！ 因为每个数据节点，它垂直上方的所有索引节点的node指针都指向数据节点本身，<code>new Index&lt;K,V&gt;(node=新插入的数据节点,down=下一层索引节点, right=当前索引节点的后继索引节点)</code>，注意到上图结构图中，level=1层的索引节点与base-level数据层的数据节点之间不是通过down指针连接的，而是通过node指针连接的，这点务必在后面源码分析反复想起，否则理解错了CSM结构图，那么就无法正确解析源代码设计。</li>
</ul>
<p>还有marker标记型节点，它在删除节点的时机会被使用。</p>
<h4 id="相关节点的定义："><a href="#相关节点的定义：" class="headerlink" title="相关节点的定义："></a>相关节点的定义：</h4><h5 id="数据节点："><a href="#数据节点：" class="headerlink" title="数据节点："></a>数据节点：</h5><p>Node类除了构造器，还定义了一些基本读写方法，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** node是数据节点，内部存放了key和value，node节点能构成有序链表</span></span><br><span class="line"><span class="comment"> * Nodes hold keys and values, and are singly linked in sorted</span></span><br><span class="line"><span class="comment"> * order, possibly with some intervening marker nodes. The list is</span></span><br><span class="line"><span class="comment"> * headed by a dummy node accessible as head.node. The value field</span></span><br><span class="line"><span class="comment"> * is declared only as Object because it takes special non-V</span></span><br><span class="line"><span class="comment"> * values for marker and header nodes.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> K key; <span class="comment">// 注意final修饰</span></span><br><span class="line">    <span class="keyword">volatile</span> Object value;  <span class="comment">// 注意volatile修饰，因为cas会操作它</span></span><br><span class="line">    <span class="keyword">volatile</span> Node&lt;K,V&gt; next;<span class="comment">// 注意volatile修饰，因为cas会操作它</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 用于创建正常的数据节点，可以看到它没有right字段和down字段，因为它就是位于底层的数据链表中，是最熟悉链表普通节点</span></span><br><span class="line"><span class="comment">     * Creates a new regular node.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    Node(K key, Object value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a new marker node. A marker is distinguished by</span></span><br><span class="line"><span class="comment">     * having its value field point to itself.  Marker nodes also</span></span><br><span class="line"><span class="comment">     * have null keys, a fact that is exploited in a few places,</span></span><br><span class="line"><span class="comment">     * but this doesn&#x27;t distinguish markers from the base-level</span></span><br><span class="line"><span class="comment">     * header node (head.node), which also has a null key.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 使用Node类型创建marker节点（在删除操作会被使用），和数据节点的区别：无key，且value指向自己。</span></span><br><span class="line">    Node(Node&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.key = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">this</span>.value = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 使用cas更改当前节点value的值</span></span><br><span class="line"><span class="comment">     * compareAndSet value field</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">casValue</span><span class="params">(Object cmp, Object val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> UNSAFE.compareAndSwapObject(<span class="keyword">this</span>, valueOffset, cmp, val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 使用cas更改当前节点的next指向</span></span><br><span class="line"><span class="comment">     * compareAndSet next field</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">casNext</span><span class="params">(Node&lt;K,V&gt; cmp, Node&lt;K,V&gt; val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> UNSAFE.compareAndSwapObject(<span class="keyword">this</span>, nextOffset, cmp, val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns true if this node is a marker. This method isn&#x27;t</span></span><br><span class="line"><span class="comment">     * actually called in any current code checking for markers</span></span><br><span class="line"><span class="comment">     * because callers will have already read value field and need</span></span><br><span class="line"><span class="comment">     * to use that read (not another done here) and so directly</span></span><br><span class="line"><span class="comment">     * test if value points to node.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> true if this node is a marker node</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">//因为HeadIndex的key也null，因此使用value==this才能唯一区别出这是marker节点</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isMarker</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value == <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 用于判断node节点是否就是base-level的HeadIndex节点</span></span><br><span class="line"><span class="comment">     * Returns true if this node is the header of base-level list.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> true if this node is header node</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isBaseHeader</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value == BASE_HEADER;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 使用cas将一个marker节点放在当前节点的后面,其中f节点是当前节点的后继节点</span></span><br><span class="line"><span class="comment">    将：</span></span><br><span class="line"><span class="comment">    node-&gt;f-&gt;...</span></span><br><span class="line"><span class="comment">    变成：</span></span><br><span class="line"><span class="comment">    node-&gt;marker-&gt;f-&gt;...</span></span><br><span class="line"><span class="comment">     * Tries to append a deletion marker to this node.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> f the assumed current successor of this node</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> true if successful</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">appendMarker</span><span class="params">(Node&lt;K,V&gt; f)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> casNext(f, <span class="keyword">new</span> Node&lt;K,V&gt;(f));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 当前节点通过添加marker节点来帮助删除操作或者用于将当前节点脱离前驱节点。</span></span><br><span class="line"><span class="comment">     * Helps out a deletion by appending marker or unlinking from</span></span><br><span class="line"><span class="comment">     * predecessor. This is called during traversals when value</span></span><br><span class="line"><span class="comment">     * field seen to be null.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> b predecessor  </span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> f successor</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 如果在这里不能理解其含义，可在后面的删除操作中关于(b,n,f)三个节点调整解析中得到回答。</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">helpDelete</span><span class="params">(Node&lt;K,V&gt; b, Node&lt;K,V&gt; f)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Rechecking links and then doing only one of the</span></span><br><span class="line"><span class="comment">         * help-out stages per call tends to minimize CAS</span></span><br><span class="line"><span class="comment">         * interference among helping threads.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">if</span> (f == next &amp;&amp; <span class="keyword">this</span> == b.next) &#123;</span><br><span class="line">            <span class="keyword">if</span> (f == <span class="keyword">null</span> || f.value != f) <span class="comment">// not already marked</span></span><br><span class="line">                casNext(f, <span class="keyword">new</span> Node&lt;K,V&gt;(f));</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                b.casNext(<span class="keyword">this</span>, f.next);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 返回数据节点key对应的value，如果当前节点value字段指向自己说明是一个mark节点，则返回null，如果当前节点value字段BASE_HEADER，说明是底层的HeadIndex节点也是返回null</span></span><br><span class="line"><span class="comment">     * Returns value if this node contains a valid key-value pair,</span></span><br><span class="line"><span class="comment">     * else null.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> this node&#x27;s value if it isn&#x27;t a marker or header or</span></span><br><span class="line"><span class="comment">     * is deleted, else null</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">V <span class="title">getValidValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Object v = value;</span><br><span class="line">        <span class="keyword">if</span> (v == <span class="keyword">this</span> || v == BASE_HEADER)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">        <span class="keyword">return</span> vv;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="索引节点Index"><a href="#索引节点Index" class="headerlink" title="索引节点Index"></a>索引节点Index</h5><p>Index节点位于“快车道——索引层链表”</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/* ---------------- Indexing -------------- */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Index nodes represent the levels of the skip list.  Note that</span></span><br><span class="line"><span class="comment">   * even though both Nodes and Indexes have forward-pointing</span></span><br><span class="line"><span class="comment">   * fields, they have different types and are handled in different</span></span><br><span class="line"><span class="comment">   * ways, that can&#x27;t nicely be captured by placing field in a</span></span><br><span class="line"><span class="comment">   * shared abstract class.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 可以看到Index索引节点的下一个节点指针不是next字段而是right字段，但他们作用都是指向后继节点，取用不同的名称是为了区别Node的next字段、方便理解和阅读。</span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Index</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/*  相关指针的局部指向示意图：</span></span><br><span class="line"><span class="comment">          this → right → indexNode  → indexNode → null  </span></span><br><span class="line"><span class="comment">          ↓</span></span><br><span class="line"><span class="comment">          down (Index&lt;K,V&gt;类型)</span></span><br><span class="line"><span class="comment">          ↓</span></span><br><span class="line"><span class="comment">          node (最底层的数据节点)</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">      <span class="keyword">final</span> Node&lt;K,V&gt; node;  <span class="comment">//索引节点在垂直方向上指向的最底层的对应数据节点，Node类型</span></span><br><span class="line">      <span class="keyword">final</span> Index&lt;K,V&gt; down; <span class="comment">//注意 down指针是Index索引节点类型，不是数据节点Node类型</span></span><br><span class="line">      <span class="keyword">volatile</span> Index&lt;K,V&gt; right;<span class="comment">//注意 right指针是Index索引节点类型，不是数据节点类型</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * Creates index node with given values.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">    	<span class="comment">// Index位于索引层的链表，那么它的down字段指向下层的Index节点（或者最底层的node数据节点），right字段指向后继索引节点</span></span><br><span class="line">      Index(Node&lt;K,V&gt; node, Index&lt;K,V&gt; down, Index&lt;K,V&gt; right) &#123;</span><br><span class="line">          <span class="keyword">this</span>.node = node;</span><br><span class="line">          <span class="keyword">this</span>.down = down;</span><br><span class="line">          <span class="keyword">this</span>.right = right;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/** 使用cas设置Index节点的right节点：在插入操作和删除操作使用</span></span><br><span class="line"><span class="comment">       * compareAndSet right field</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">casRight</span><span class="params">(Index&lt;K,V&gt; cmp, Index&lt;K,V&gt; val)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">return</span> UNSAFE.compareAndSwapObject(<span class="keyword">this</span>, rightOffset, cmp, val);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * Returns true if the node this indexes has been deleted.</span></span><br><span class="line"><span class="comment">       * <span class="doctag">@return</span> true if indexed node is known to be deleted</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">indexesDeletedNode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          <span class="keyword">return</span> node.value == <span class="keyword">null</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">      参考后文</span></span><br><span class="line"><span class="comment">       * Tries to CAS newSucc as successor.  To minimize races with</span></span><br><span class="line"><span class="comment">       * unlink that may lose this index node, if the node being</span></span><br><span class="line"><span class="comment">       * indexed is known to be deleted, it doesn&#x27;t try to link in.</span></span><br><span class="line"><span class="comment">       * <span class="doctag">@param</span> succ the expected current successor</span></span><br><span class="line"><span class="comment">       * <span class="doctag">@param</span> newSucc the new successor</span></span><br><span class="line"><span class="comment">       * <span class="doctag">@return</span> true if successful</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">    </span><br><span class="line">      <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">link</span><span class="params">(Index&lt;K,V&gt; succ, Index&lt;K,V&gt; newSucc)</span> </span>&#123;</span><br><span class="line">          Node&lt;K,V&gt; n = node;</span><br><span class="line">          newSucc.right = succ;</span><br><span class="line">          <span class="keyword">return</span> n.value != <span class="keyword">null</span> &amp;&amp; casRight(succ, newSucc);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">      参考后文</span></span><br><span class="line"><span class="comment">       * Tries to CAS right field to skip over apparent successor</span></span><br><span class="line"><span class="comment">       * succ.  Fails (forcing a retraversal by caller) if this node</span></span><br><span class="line"><span class="comment">       * is known to be deleted.</span></span><br><span class="line"><span class="comment">       * <span class="doctag">@param</span> succ the expected current successor</span></span><br><span class="line"><span class="comment">       * <span class="doctag">@return</span> true if successful</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">unlink</span><span class="params">(Index&lt;K,V&gt; succ)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">return</span> node.value != <span class="keyword">null</span> &amp;&amp; casRight(succ, succ.right);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Unsafe mechanics，Index类内部的Unsafe实例创建</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> sun.misc.Unsafe UNSAFE;</span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> rightOffset;</span><br><span class="line">      <span class="keyword">static</span> &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">              UNSAFE = sun.misc.Unsafe.getUnsafe();</span><br><span class="line">              Class&lt;?&gt; k = Index.class;</span><br><span class="line">              rightOffset = UNSAFE.objectFieldOffset</span><br><span class="line">                  (k.getDeclaredField(<span class="string">&quot;right&quot;</span>));</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> Error(e);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="HeadIndex节点"><a href="#HeadIndex节点" class="headerlink" title="HeadIndex节点"></a>HeadIndex节点</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* ---------------- Head nodes -------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**  从结构图也可以看出：该层HeadIndex的right指针指向的就是该层链表的头节点，down指针指向下一层的HeadIndex节点，关键的level属性：表示该层的序号，最小值是1，也即第一层</span></span><br><span class="line"><span class="comment"> * Nodes heading each level keep track of their level.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HeadIndex</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Index</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> level;</span><br><span class="line">    HeadIndex(Node&lt;K,V&gt; node, Index&lt;K,V&gt; down, Index&lt;K,V&gt; right, <span class="keyword">int</span> level) &#123;</span><br><span class="line">        <span class="keyword">super</span>(node, down, right);</span><br><span class="line">        <span class="keyword">this</span>.level = level;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="get方法解析"><a href="#get方法解析" class="headerlink" title="get方法解析"></a>get方法解析</h4><p>有了以上基本数据结构说明后，理解查找节点的过程来进一步解析CSM核心设计。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">return</span> doGet(key); <span class="comment">// get方法内部封装的私有的doGet方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>doGet总体思路：使用comparator比较器，先找到key对于的前驱节点b，此刻起，有(b,n,f)三个节点关系（b:当前节点n的前驱节点，f:当前节点n的后继节点），这三个指针非常重要，要想读到准确n节点的value，显然在第一次读到(b,n,f)时，还需要再次检查b节点、n节点有无被其他线程修改或者删除，确保前后两次读一致，才可以正常地把key和n节点进行比较，若key==n.key，则可查询到key对应的值。</p>
<p>b节点：给定key的前驱节点，一般有b.key&lt;key；n节点：要与key作为比较，f节点：n节点后驱节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> V <span class="title">doGet</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line">  <span class="comment">/* 外层循环的outer非常有用：若当前读线程在内部循环找到(b,n,f)且准备读取数据节点n，有其他写线程（如删除操作）抢先一步将正常的数据n节点标记为marker或者更改了n节点，那么就需要要求当前读线程重新跳回外层循环再次定位(b,n,f)这个三个特殊节点，或者说：如果在内层循环，当前线程前后读取的(b,n,f)三个节点中的b节点、n节点有出现不一致读，则需要再次遍历，直到前后读取的(b,n,f)中的b、n都未改动时，返回的value才是key对应的value。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">    outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">      	<span class="comment">// ① 在内循环开始时，找到给定key的前驱节点b，以及b节点的next节点：n节点，b节点key一定是小于给定的key，否则b节点就不是给定key的前驱节点</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">            Object v; <span class="keyword">int</span> c; <span class="comment">// c是comparator的比较结果，0，小于0，大于0</span></span><br><span class="line">          	<span class="comment">//② 来到数据层链表的尾部位置如： b → null，因此n指向null，说明key未能在数据层链表找到，可直接返回。</span></span><br><span class="line">          	<span class="comment">// 这里也对应下面③b=n;n=f后移操作，也是作为外层循环首要考虑的程序出口条件。</span></span><br><span class="line">            <span class="keyword">if</span> (n == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">break</span> outer; <span class="comment">// 注意：break outer是指结束结束外层循环来到return null语句</span></span><br><span class="line">          	<span class="comment">// n节点的后继节点为f节点</span></span><br><span class="line">            Node&lt;K,V&gt; f = n.next;</span><br><span class="line">          	<span class="comment">// 当前时刻n节点已经不是b节点的后继节点，说明n节点被其他线程改过(前、后不一致性读)，那么当前线程回到内层for循环① 位置继续下一轮重试。</span></span><br><span class="line">            <span class="keyword">if</span> (n != b.next)                <span class="comment">// inconsistent read</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">          	<span class="comment">// 若线程执行到这里发现n节点value变为null，说明n节点被标记为“删除”，当前线程进入helpDelete帮助删除n节点，然后回到内层for循环①位置继续下一轮重试。</span></span><br><span class="line">            <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123;    <span class="comment">// n is deleted</span></span><br><span class="line">                n.helpDelete(b, f);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// 若线程执行到这里发现b节点value变为null，此时说明前驱节点b被其他线程删除，或者n节点被其他线程标记为marker节点，当前线程只能回到内层for循环①位置继续下一轮重试。</span></span><br><span class="line">            <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n)  <span class="comment">// b is deleted</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">          	<span class="comment">// 代码执行到这里说明b节点和n节点前后两次读取一致，那么可以开始比较key和n.key,若相等，n节点就是要找的key对应的节点，返回其value即可</span></span><br><span class="line">            <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">                <span class="keyword">return</span> vv;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// b节点已经是给定key的前驱节点，有：b.key&lt;key,若c&lt;0,说明key&lt;n.key,因为b节点和n节点之间已经没有节点，既然key对应的不是b节点也不是n节点，说明key节点不在跳表里面，那么当前读线程只能跳出外层循环并结束读取操作，返回null。</span></span><br><span class="line">            <span class="keyword">if</span> (c &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span> outer;</span><br><span class="line">          	<span class="comment">//③ 代码执行到这里，说明要查找的key大于n节点的key，所以要继续向当前链表右边方向遍历，b和n指针同时向右边移动一个节点位置的指向，可以想象：当b来到链表尾部最后一个节点时，此时n=b.next=null 回到for循环就会进入②位置的分支：满足if (n == null)，然后break outer退出外层循环后return null</span></span><br><span class="line">            b = n;</span><br><span class="line">            n = f;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>doGet方法的设计整体相对清晰，最关键的一个点是如何找到key的前驱节点b？通过findPredecessor实现。</p>
<h5 id="findPredecessor方法实现："><a href="#findPredecessor方法实现：" class="headerlink" title="findPredecessor方法实现："></a>findPredecessor方法实现：</h5><p>此方法有两个功能：</p>
<p>功能 1：找到给定key对应的前驱节点node</p>
<p>功能2：also unlinks indexes to deleted nodes found along the way，此功能具体分析放在后文的doRemove方法，在本小节中只需关注功能1即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/* ---------------- Traversal -------------- */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns a base-level node with key strictly less than given key,</span></span><br><span class="line"><span class="comment">     * or the base-level header if there is no such node.  Also</span></span><br><span class="line"><span class="comment">     * unlinks indexes to deleted nodes found along the way.  Callers</span></span><br><span class="line"><span class="comment">     * rely on this side-effect of clearing indices to deleted nodes.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key the key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a predecessor of key</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node&lt;K,V&gt; <span class="title">findPredecessor</span><span class="params">(Object key, Comparator&lt;? <span class="keyword">super</span> K&gt; cmp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(); <span class="comment">// don&#x27;t postpone errors</span></span><br><span class="line">      	<span class="comment">// 两层循环，若当前线程在遍历索引节点过程中，当前遍历节点被修改，说明读不一致，那么当前线程必须回到外层循环重新遍历</span></span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="comment">/* </span></span><br><span class="line"><span class="comment">            注意q、r、d的指向方向，如下所示</span></span><br><span class="line"><span class="comment">            q → r → indexNode  → indexNode → null  </span></span><br><span class="line"><span class="comment">            ↓</span></span><br><span class="line"><span class="comment">            d</span></span><br><span class="line"><span class="comment">            先将q指向head节点，那么r和d自然就出来了，如下所示</span></span><br><span class="line"><span class="comment">            q(head) → r (q.right) → indexNode  → indexNode → null  </span></span><br><span class="line"><span class="comment">            ↓</span></span><br><span class="line"><span class="comment">            d(q.down)</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">          	<span class="comment">// 这里很关键：q、r、d三个指针都是Index索引节点类型，说明findPredecessor方法只会在索引层去找key的前驱节点，而不会去到最底层的数据节点链表去找。</span></span><br><span class="line">            <span class="keyword">for</span> (Index&lt;K,V&gt; q = head, r = q.right, d;;) &#123;</span><br><span class="line">              	<span class="comment">//①分支：从上面的q、r位置关系可知，q、r都在同一层索引层移动，如果r节点不为空，说明可以继续向右遍历索引链表，若r来到索引链表尾部null位置，那么执行流跳过①分支来到④分支</span></span><br><span class="line">                <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                  	<span class="comment">//取出索引节点指向的数据节点node，此节点的key将和给定key比较</span></span><br><span class="line">                    Node&lt;K,V&gt; n = r.node;</span><br><span class="line">                    K k = n.key;</span><br><span class="line">                   <span class="comment">/*② 此分支建议深度理解了doRemove方法后，再回来理解它则简单很多。</span></span><br><span class="line"><span class="comment">                  在remove方法中有其他写线程使用n.casValue(v, null)将目标删除节点n的value字段设为null，因此只要读线程进入此分支，说明r索引节点指向的数据节点node已经被其他写线程标记为删除状态，此时要将q节点和r节点断开链接，也即将q → r → r.right 变成q → r.right</span></span><br><span class="line"><span class="comment">                   */</span></span><br><span class="line">                    <span class="keyword">if</span> (n.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (!q.unlink(r))</span><br><span class="line">                            <span class="keyword">break</span>;           <span class="comment">// 若unlink内部的cas失败，则继续重试</span></span><br><span class="line">                      	<span class="comment">// 如果上面unlink成功，旧r节点断开q节点，此时重新读取q的right节点，然后继续下一轮循环。</span></span><br><span class="line">                        r = q.right;         <span class="comment">// reread r</span></span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">//③分支：给定的key比r.key大，说明只好向右继续查找：q、r指针同时右边移动一个节点</span></span><br><span class="line">                    <span class="keyword">if</span> (cpr(cmp, key, k) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        q = r;</span><br><span class="line">                        r = r.right;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">/*④分支：findPredecessor结束点：找到了key的前驱节点。注意这里q已经是第一层的目标索引节点，q.down指向null，q.node指向数据节点node，所以不要误解为q.down指向node，而node不为空，参考后面图文解释！</span></span><br><span class="line"><span class="comment">            q (索引层leve=1的索引节点) </span></span><br><span class="line"><span class="comment">            ↓</span></span><br><span class="line"><span class="comment">            d (最底层数据节点)，显然此时d是Node节点类型，</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">                <span class="keyword">if</span> ((d = q.down) == <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">return</span> q.node;</span><br><span class="line">              	<span class="comment">//⑤分支：遍历到当前索引层链表尾部r=null也没找到key的前驱节点，那么只能垂直在向下移动，在下一层索引层的链表头部继续重复以上逻辑。务必记住：此“向下移动一层”的操作只在索引层操作，不会“下移到数据链表层”！</span></span><br><span class="line">                q = d;</span><br><span class="line">                r = d.right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>q.unlink(r))</code>的定义：</p>
<p>unlink方法是Index索引节点内部方法，设计目的是：当Index.node指向的数据节点已经被标记删除，那么此Index节点也需要被删除，假设r是当前处理索引节点：</p>
<p>将q → r → r.right 变成q → r.right，对应到unlink的相关变量则为：</p>
<p>将q → succ → succ.right 变成q → succ.right</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * Tries to CAS right field to skip over apparent successor</span></span><br><span class="line"><span class="comment"> 若调用此方法时，q恰好被标记为删除状态，那么cas会失败，要求unlink调用者再次循环重试，这就是findPredecessor方法里面的②分支设计为重试的原因。 </span></span><br><span class="line"><span class="comment"> * succ.  Fails (forcing a retraversal by caller) if this node</span></span><br><span class="line"><span class="comment"> * is known to be deleted.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> succ the expected current successor</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if successful</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">unlink</span><span class="params">(Index&lt;K,V&gt; succ)</span> </span>&#123;</span><br><span class="line">  	 <span class="comment">// 这里node.value是指q.node.value，不是r.node.value！要求q指向的数据节点node还没被删除才能进行cas</span></span><br><span class="line">    <span class="keyword">return</span> node.value != <span class="keyword">null</span> &amp;&amp; casRight(succ, succ.right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>findPredecessor方法最关键的一个设计点——返回逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*④分支：findPredecessor结束点：找到了key的前驱节点</span></span><br><span class="line"><span class="comment">            q (索引层leve=1的索引节点) </span></span><br><span class="line"><span class="comment">            ↓</span></span><br><span class="line"><span class="comment">            d (最底层数据节点)，显然此时d是Node节点类型，</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">                <span class="keyword">if</span> ((d = q.down) == <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">return</span> q.node;</span><br></pre></td></tr></table></figure>
<p>以上代码很容易让人产生错误理解（如果提前理解了doPut方法中idx插入索引节点的设计，则能正确理解④分支的设计逻辑），具体说明人下面两张图所示</p>
<p>正确的理解：<br><img src="https://img-blog.csdnimg.cn/65f7be0bca5246dfa5f518aa5a4898a2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>错误的理解：</p>
<p><img src="https://img-blog.csdnimg.cn/7f2a17ac64e2460386eb38952646841e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>虽然在前面CSM的数据结构图中 level=1的索引层节点画的图是直接指向下方数据层节点，但必须要清楚在真正的CSM设计中，是不存在这个down指向关系，你可以理解是第一层索引节点通过q.node指针实现指向最底层的数据节点。</p>
<h5 id="图示doGet方法查找图"><a href="#图示doGet方法查找图" class="headerlink" title="图示doGet方法查找图"></a>图示doGet方法查找图</h5><p>分为两步理解：<br><img src="https://img-blog.csdnimg.cn/0eb498c68cbb4bf1999927e150db78b1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>
<p>doGet方法和findPredecessor的执行流程能够让你理解了CSM，但要想深入掌握它，还得从put方法和remove方法中找到答案</p>
<h4 id="put方法解析"><a href="#put方法解析" class="headerlink" title="put方法解析"></a>put方法解析</h4><p>Main insertion method. Adds element if not present, or replaces value if present and onlyIfAbsent is false.</p>
<p>如果key不在csm中可插入key，onlyIfAbsent默认为false，表示如果该key已经在csm中，则替换旧value。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (value == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">return</span> doPut(key, value, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>doPut的主要设计思路：</p>
<p>1、找到合适位置插入key节点</p>
<ul>
<li><p>1.1 找到key在数据层链表中前驱节点b（这个逻辑在doGet已经解析过），并给出(b,n,f)这三个指针的指向</p>
</li>
<li><p>1.2 将新插入的节点z用cas设置到b的next中，也即将节点z插入到 b → n，也即变成  b → z → n </p>
</li>
</ul>
<p>2、通过随机算法来决定是否要在新插入的节点z上（垂直方向上）构建索引节点（若层数n，就构建n个索引节点），</p>
<p>3、将这些索引节点在各自的索引层链接好左右关系指向。</p>
<p>建议先用单线程的角度去理解doPut主体逻辑，也即假设b节点、n节点不会被其他写线程删除或者更改，这样可以快速理解doPut设计逻辑。</p>
<h5 id="doPut方法第一部分："><a href="#doPut方法第一部分：" class="headerlink" title="doPut方法第一部分："></a>doPut方法第一部分：</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> V <span class="title">doPut</span><span class="params">(K key, V value, <span class="keyword">boolean</span> onlyIfAbsent)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; z;             <span class="comment">// added node</span></span><br><span class="line">    <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line">    outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">      	<span class="comment">// 注意：以下内层循环所有逻辑（除了findPredecessor）只发生在最底层的数据链表层中进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">          	<span class="comment">// 1.1 若不是在数据链表尾部，则可以继续遍历，否则也即来到链表尾部就跳到1.2分支</span></span><br><span class="line">            <span class="keyword">if</span> (n != <span class="keyword">null</span>) &#123;</span><br><span class="line">                Object v; <span class="keyword">int</span> c;</span><br><span class="line">              	<span class="comment">// f暂存n节点之后子链，用于后续遍历</span></span><br><span class="line">                Node&lt;K,V&gt; f = n.next;</span><br><span class="line">              	<span class="comment">//1.1.1 单线程情况下，n还是b的next节点，若高并发写情况下，n节点可能被其他线程更改，那么当前线程第一次读取的b.next和现在读取的b.next显然不一致，也即“inconsistent read”</span></span><br><span class="line">                <span class="keyword">if</span> (n != b.next)               <span class="comment">// inconsistent read</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">              	<span class="comment">//1.1.2 n的值为null，说明被其他线程标记位删除状态，只能先去helpDelete，然后再回到内层循环重试。</span></span><br><span class="line">                <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123;   <span class="comment">// n is deleted</span></span><br><span class="line">                    n.helpDelete(b, f);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">//1.1.3 前驱节点b被删除或者n节点value是指向自己（说明n是一个mark节点），前驱节点都被其他线程删除，只能退出本轮回到内层for循环继续重试。</span></span><br><span class="line">                <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n) <span class="comment">// b is deleted</span></span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">              	<span class="comment">//1.1.4 给定的key比n节点还大，那么只能继续向右比较</span></span><br><span class="line">                <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                  	<span class="comment">// 更新b、n指针向后移动一步</span></span><br><span class="line">                    b = n;</span><br><span class="line">                    n = f;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">//1.1.5 如果key==n.key,说明已找到该key。</span></span><br><span class="line">                <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (onlyIfAbsent || n.casValue(v, value)) &#123;</span><br><span class="line">                        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span> V vv = (V)v;</span><br><span class="line">                        <span class="keyword">return</span> vv;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>; <span class="comment">// restart if lost race to replace value</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// else c &lt; 0; fall through</span></span><br><span class="line">            &#125;</span><br><span class="line">				<span class="comment">/* 1.2</span></span><br><span class="line"><span class="comment">          	第一种情况：根据1.1 n不为空，那么在 b → n 之间插入  b → z → n </span></span><br><span class="line"><span class="comment">          	第二种情况：如果1.1 的n节点为空说明来到链表尾部，也即b前驱节点后面就是null，那么b → null 变成b → z → null</span></span><br><span class="line"><span class="comment">           	*/</span>   </span><br><span class="line">            z = <span class="keyword">new</span> Node&lt;K,V&gt;(key, value, n);  <span class="comment">//创建key这个插入节点，然后有 z → n </span></span><br><span class="line">          	<span class="comment">// 1.3 </span></span><br><span class="line">            <span class="keyword">if</span> (!b.casNext(n, z)) <span class="comment">// cas: 在n节点前、后时刻读一致性时，z节点才能成功CAS插入b前驱节点后面</span></span><br><span class="line">                <span class="keyword">break</span>;         <span class="comment">// restart if lost race to append to b若cas失败，只能回到内循环重试</span></span><br><span class="line">            <span class="keyword">break</span> outer;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h5 id="doput方法第二部分："><a href="#doput方法第二部分：" class="headerlink" title="doput方法第二部分："></a>doput方法第二部分：</h5><p>在第一部分逻辑插入号z节点后，那么接下里就得考虑z节点垂直方向是否需要创建索引层节点。</p>
<p>如何决定是否需要？ 当然是根据随机数算法满足test条件来决定是否需要执行。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">     <span class="comment">/* 这里随机数的生成考虑很细致：ThreadLocalRandom.nextInt也可以实现线程自己本地随机数，而nextSecondarySeed能做到生成和“当前线程业务代码调用的ThreadLocalRandom.nextInt”不冲突的随机数。</span></span><br><span class="line"><span class="comment">     * To produce random values without interference across threads,</span></span><br><span class="line"><span class="comment">     * we use within-JDK thread local random support (via the</span></span><br><span class="line"><span class="comment">     * &quot;secondary seed&quot;, to avoid interference with user-level</span></span><br><span class="line"><span class="comment">     * ThreadLocalRandom.)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">		<span class="keyword">int</span> rnd = ThreadLocalRandom.nextSecondarySeed(); </span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">0x80000001是16进制数：1000 0000 0000 0000 0000 0000 0000 0001</span></span><br><span class="line"><span class="comment">考察rnd &amp; 0x80000001 == 0成立时rnd的取值特征：</span></span><br><span class="line"><span class="comment">0nnn nnnn nnnn nnnn nnnn nnnn nnnn nnn0</span></span><br><span class="line"><span class="comment">其中，n表示0或者1，其实就是表达rnd只要是偶数就可使得测试条件通过，所以概率为50%。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        <span class="keyword">if</span> ((rnd &amp; <span class="number">0x80000001</span>) == <span class="number">0</span>) &#123; <span class="comment">// test highest and lowest bits</span></span><br><span class="line">            <span class="keyword">int</span> level = <span class="number">1</span>, max;</span><br><span class="line">            <span class="comment">/* 1、根据rnd的比特位值为1的特征先计算一个“level值”，</span></span><br><span class="line"><span class="comment">            例如0nnn nnnn nnnn nnnn nnnn nnnn nnnn nnn0，30个n全部为1，那么</span></span><br><span class="line"><span class="comment">            level=1+30=31，计算出31层，是否就是真的要在跳表上加31层索引节点呢？ 显然没那么简单，具体看第2点逻辑				</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (((rnd &gt;&gt;&gt;= <span class="number">1</span>) &amp; <span class="number">1</span>) != <span class="number">0</span>)</span><br><span class="line">                ++level;</span><br><span class="line">          	<span class="comment">// 这一句非常关键：第一层的索引节点Index，它的down指向idx=null，而不是指向其下方的node数据节点。 </span></span><br><span class="line">            Index&lt;K,V&gt; idx = <span class="keyword">null</span>;</span><br><span class="line">            HeadIndex&lt;K,V&gt; h = head; <span class="comment">// head指向的是最顶层的HeadIndex节点，不是最底层的数据链表头节点</span></span><br><span class="line">          <span class="comment">/*</span></span><br><span class="line"><span class="comment">          2、如果计算的level未超过现有索引层数，那么好处理：在z节点上方建立level个索引节点</span></span><br><span class="line"><span class="comment">          例如，假设当前h.level=3，也即有三层索引层，rnd=2,那么level计算结果为2，因此根据以下逻辑，</span></span><br><span class="line"><span class="comment">          在z节点垂直上方创建2个index节点,且每个index的right还是null，换句话说，这些索引节点还未与其所在层的前后索引节点建立指向关系，如下：</span></span><br><span class="line"><span class="comment">          index2</span></span><br><span class="line"><span class="comment">          ↓</span></span><br><span class="line"><span class="comment">          index1</span></span><br><span class="line"><span class="comment">          |</span></span><br><span class="line"><span class="comment">          z</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">            <span class="keyword">if</span> (level &lt;= (max = h.level)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= level; ++i)</span><br><span class="line">                    idx = <span class="keyword">new</span> Index&lt;K,V&gt;(z, idx, <span class="keyword">null</span>); </span><br><span class="line">            &#125;</span><br><span class="line">          </span><br><span class="line">          <span class="comment">/*</span></span><br><span class="line"><span class="comment">          3、如果level&gt;现有层数，例如level计算值为7，现有层数为3，那么也只能给它加一层，而不是加（7-3）层，</span></span><br><span class="line"><span class="comment">          以下max按3作为案例说明</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            <span class="keyword">else</span> &#123; <span class="comment">// try to grow by one level </span></span><br><span class="line">              	<span class="comment">// level=3+1=4</span></span><br><span class="line">                level = max + <span class="number">1</span>; <span class="comment">// hold in array and later pick the one to use</span></span><br><span class="line">              	<span class="comment">// 存放要添加的索引节点数组，level为4，那么idxs容量为5</span></span><br><span class="line">                <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span>Index&lt;K,V&gt;[] idxs =</span><br><span class="line">                    (Index&lt;K,V&gt;[])<span class="keyword">new</span> Index&lt;?,?&gt;[level+<span class="number">1</span>]; </span><br><span class="line">              	<span class="comment">// 注意：这里从idxs的下标1开始放入idx索引节点，idxs[0]不参与实际逻辑。</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= level; ++i)</span><br><span class="line">                  	<span class="comment">/* 跟上面2部分类似，但它这里每次创建好的节点放到idxs数组中，方便后面新创建HeadIndex使用：</span></span><br><span class="line"><span class="comment">                  	 		newh = new HeadIndex&lt;K,V&gt;(oldbase, newh, idxs[j], j);</span></span><br><span class="line"><span class="comment">                  	*/</span></span><br><span class="line">                    idxs[i] = idx = <span class="keyword">new</span> Index&lt;K,V&gt;(z, idx, <span class="keyword">null</span>);</span><br><span class="line">              	<span class="comment">// 3.1 </span></span><br><span class="line">                <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                    h = head;</span><br><span class="line">                  	<span class="comment">// 根据3的假设，原来由3层索引层</span></span><br><span class="line">                    <span class="keyword">int</span> oldLevel = h.level;</span><br><span class="line">                  	<span class="comment">// 假设当前时刻仅有当前线程添加索引层，那么4&lt;=3，不需要重试。</span></span><br><span class="line">                    <span class="keyword">if</span> (level &lt;= oldLevel) <span class="comment">// lost race to add level:有其他插入线程抢先一步升高了索引层，当前线程只能下一轮重试,因此在3.1使用了for循环</span></span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                  	<span class="comment">// 3.2 </span></span><br><span class="line">                    HeadIndex&lt;K,V&gt; newh = h;</span><br><span class="line">                    Node&lt;K,V&gt; oldbase = h.node;</span><br><span class="line">                  	<span class="comment">// 创建新添加的索引层的头节点，新增多少层，就添加多少个头节点，</span></span><br><span class="line">                  	<span class="comment">// 由于oldLevel=3，level=4,因此以下计算后只在第4层添加一个HeadIndex，并且该HeadIndex的四个字段都有了指向关系。</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> j = oldLevel+<span class="number">1</span>; j &lt;= level; ++j)</span><br><span class="line">                        newh = <span class="keyword">new</span> HeadIndex&lt;K,V&gt;(oldbase, newh, idxs[j], j); <span class="comment">// 这就是为何前面需要创建idxs数组的原因。</span></span><br><span class="line">                  	<span class="comment">// 3.3 将跳表头节点Head指向新添加的HeadIndex</span></span><br><span class="line">                    <span class="keyword">if</span> (casHead(h, newh)) &#123;</span><br><span class="line">                        h = newh;</span><br><span class="line">                      	<span class="comment">// 这里level = oldLevel=3，也即将idx指向第3层新增索引节点，为何不是第4层新增的索引节点呢？因为在3.2中，在第4层新增的HeadIndex节点已经将right指针指向对应新增的索引节点。因此之后要处理的索引节点是从旧level层数开始处理down、right指向关系</span></span><br><span class="line">                        idx = idxs[level = oldLevel];</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第2部分的逻辑对应下图：</p>
<p>根据key计算的level未超过现有索引层数3时的逻辑图<br><img src="https://img-blog.csdnimg.cn/0bbafd8d861a46d08af5e813c9a13562.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>第3部分的逻辑对应下图：</p>
<p>根据key计算的level超过现有索引层数3时的逻辑图，不管是计算level=4大于level=3还是其他比3值更大，也只能增加一层索引层：level = max + 1<br><img src="https://img-blog.csdnimg.cn/aa0ae26faa314d75bec56d81b08fd0ac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>在新增加的level=4的索引层，需要添加一个新的HeadIndex节点，根据以下逻辑可知：新增的HeadIndex节点已经将right指针指向对应新增的索引节点，如图位置1所示</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = oldLevel+<span class="number">1</span>; j &lt;= level; ++j)</span><br><span class="line">    newh = <span class="keyword">new</span> HeadIndex&lt;K,V&gt;(old base, newh, idxs[j], j);</span><br></pre></td></tr></table></figure>
<h5 id="doput方法第三部分："><a href="#doput方法第三部分：" class="headerlink" title="doput方法第三部分："></a>doput方法第三部分：</h5><p>在doput方法第二部分完成新插入数据节点z对应的多层索引节点创建，但索引节点idx还未与前后已有节点建立指向关系，因此接下里代码逻辑中负责完成此事。</p>
<p>为了能具体化分析以下逻辑，不妨假设插入z节点后，计算出的level=4高于当前跳表oldLevel=3，根据上面3.3的逻辑可知，此时h执行第4层的headIndex，根据<code>idx = idxs[level = oldLevel]</code> ，有idx指针指向第3层的待处理索引节点idx，level=oldLevel=3，根据这些变量代入以下逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">				<span class="comment">// find insertion points and splice in.</span></span><br><span class="line">				<span class="comment">// splice in 是指拼接的意思，也即将idx加入所在层的链表关系中</span></span><br><span class="line">				<span class="comment">//insertionLevel = level =3，</span></span><br><span class="line">        splice: <span class="keyword">for</span> (<span class="keyword">int</span> insertionLevel = level;;) &#123;</span><br><span class="line">          	<span class="comment">// 关于insertionLevel的理解，结合上图可知，insertionLevel=3，说明第3层idx加入此层索引层链表中，也称为待插入层。</span></span><br><span class="line">            <span class="keyword">int</span> j = h.level; <span class="comment">// 这个j表征执行流走向第几层（也称为当前处理层），h.level的值为4</span></span><br><span class="line">          	<span class="comment">// 类似doGet方法，从顶层的左上方head位置开始遍历而且只在索引层去检索，目的是把每个需要insertionLevel层的idx节点t链入q → r之间，变成：q → t → r 。注意：t的方向是从高层idx向低层的idx移动指向，idx：新增加的索引阶段</span></span><br><span class="line">            <span class="keyword">for</span> (Index&lt;K,V&gt; q = h, r = q.right, t = idx;;) &#123;</span><br><span class="line">              	<span class="comment">//3.1 两种情况可以说明idx链入工作完成：条件1：说明q来到当前索引层的链表末尾位置，q会指向null 2、当t指向null时，垂直方向多个idx节点全部链入对应索引层。</span></span><br><span class="line">                <span class="keyword">if</span> (q == <span class="keyword">null</span> || t == <span class="keyword">null</span>)</span><br><span class="line">                    <span class="keyword">break</span> splice;</span><br><span class="line">              	<span class="comment">//3.2 对于q → r → r.right...情况，r不是null时，也即q不是尾节点时</span></span><br><span class="line">                <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                  	<span class="comment">// q → r → r.right，也即熟悉的（b,n,f）子链</span></span><br><span class="line">                    Node&lt;K,V&gt; n = r.node;</span><br><span class="line">                    <span class="comment">// compare before deletion check avoids needing recheck</span></span><br><span class="line">                    <span class="keyword">int</span> c = cpr(cmp, key, n.key); </span><br><span class="line">                  	<span class="comment">// 如果当前索引节点r指向的数据节点n被标记删除，说明索引节点r也需要删除，此时要将q节点和r节点断开链接，也即将q → r → r.right 变成q → r.right。前面findPredecessor方法内部也用到了该unlink方法，此逻辑被称为side-effect！</span></span><br><span class="line">                    <span class="keyword">if</span> (n.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (!q.unlink(r))<span class="comment">// q、r断开失败则需要重试</span></span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                      	<span class="comment">// 取出q新的右节点继续遍历重试</span></span><br><span class="line">                        r = q.right;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// r.key&lt;key,只能在同一层继续向右找到key插入的位置。 </span></span><br><span class="line">                    <span class="keyword">if</span> (c &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        q = r;</span><br><span class="line">                        r = r.right;</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">             <span class="comment">/*3.3 如果当前处理层指针恰好移动到位于需要对新增索引节点建立前后指向关系的待插入层</span></span><br><span class="line"><span class="comment">              那么将待处理的索引节点 t=idx 链入q → r 之间，变成：q → t → r </span></span><br><span class="line"><span class="comment">              */</span></span><br><span class="line">                <span class="keyword">if</span> (j == insertionLevel) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!q.link(r, t))</span><br><span class="line">                        <span class="keyword">break</span>; <span class="comment">// restart t=idx 链入失败只能回到循环重试。</span></span><br><span class="line">                    <span class="comment">// 索引节点idx指向的新插入数据节点z已被其他写线程标为删除状态，那么当前线程就不需要给z节点建立垂直方向的索引，相反当前线程利用findNode去帮助把z节点删除，同时findNode里面的findPredecessor方法也会在find过程中沿途清理z节点上方的idxs索引节点，这就是findNode的“side-effect” 思想：去删除数据节点z的同时顺便沿途把与z对应新建的索引节点idxs都删除了</span></span><br><span class="line">                    <span class="keyword">if</span> (t.node.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        findNode(key);</span><br><span class="line">                        <span class="keyword">break</span> splice;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// 如果q.link(r, t)成功，且新插入的数据节点z未被删除，那么当前待插入层insertionLevel已完成，如果insertionLevel自减1等于0，说明insertionLevel指向最底层的数据层，显然数据层不需要处理，说明所有idxs都在各自所在的索引层链表里面，可结束。</span></span><br><span class="line">                    <span class="keyword">if</span> (--insertionLevel == <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">break</span> splice;</span><br><span class="line">                &#125; <span class="comment">// </span></span><br><span class="line">           </span><br><span class="line">              	</span><br><span class="line">         <span class="comment">// 这里的理解相对有点绕：若当前待插入层已经处理完，则t指向下一个层的待处理索引节点idx。什么场景不需要执行 t = t.down呢？当遍历层指针j在一开始就指向新增的最高层时，t此时是指向（最高层-1）的idx，因此只有当遍历层指针j来到当前待插入层insertionLevel且t已经链入该层的q → r之间，说明当前insertionLevel层已处理完，那么t可以移动下一层的idx</span></span><br><span class="line">                <span class="keyword">if</span> (--j &gt;= insertionLevel &amp;&amp; j &lt; level)</span><br><span class="line">                    t = t.down;</span><br><span class="line">         <span class="comment">// 1、当前遍历层指针j指向最高层时 2、当前遍历层指针j指向当前待插入层insertionLevel且insertionLevel层的t节点已经链入该层的q → r之间。这两个条件都会使得q，r下移一层</span></span><br><span class="line">                q = q.down;</span><br><span class="line">                r = q.right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于<code>q.link(r, t)</code> 的说明</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// if (!q.link(r, t))</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">link</span><span class="params">(Index&lt;K,V&gt; succ, Index&lt;K,V&gt; newSucc)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 这里的node节点就是索引节点q.node指向的数据节点,succ就是r，newSucc就是t（新增的idx节点）</span></span><br><span class="line">        Node&lt;K,V&gt; n = node; </span><br><span class="line">      	<span class="comment">// t.right=r </span></span><br><span class="line">        newSucc.right = succ;</span><br><span class="line">      	<span class="comment">/* link能成功的两个条件同时成立：</span></span><br><span class="line"><span class="comment">      		1、要求q指向的node数据节点一定不能处于被标记为&quot;删除状态&quot;</span></span><br><span class="line"><span class="comment">      		2、q的后继节点r索引节点未被其他写线程标记为&quot;删除状态&quot;</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">return</span> n.value != <span class="keyword">null</span> &amp;&amp; casRight(succ, newSucc);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="findNode的作用"><a href="#findNode的作用" class="headerlink" title="findNode的作用"></a>findNode的作用</h5><p>此方法在doPut的“第三部分逻辑：新建的索引链表如何插入到该层链表中”起到关键作用，此外它还被doRemove、computeIfPresent、merge、replace等方法在内部调用，因此需要对其设计逻辑进行解析。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// findNode的源代码注释其实写得很清楚，需要看完一下源码再解析</span></span><br><span class="line"><span class="comment">// 以下逻辑一定要带着这样的前提：新插入的z节点被标记删除后，需要对其上方的新建索引节点都一起删除</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node&lt;K,V&gt; <span class="title">findNode</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (key == <span class="keyword">null</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(); <span class="comment">// don&#x27;t postpone errors</span></span><br><span class="line">      Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;</span><br><span class="line"> </span><br><span class="line">      outer: <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">        	<span class="comment">//1、找到key在数据层链表的前驱节点b，从b开始向右遍历。这里最关键：findPredecessor，除了找到z节点的前驱节点b，它的额外收益还把沿途找到的z对应的索引节点都删除了</span></span><br><span class="line">          <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) &#123;</span><br><span class="line">              Object v; <span class="keyword">int</span> c;</span><br><span class="line">            	<span class="comment">//2、 来到链表末尾则可结束</span></span><br><span class="line">              <span class="keyword">if</span> (n == <span class="keyword">null</span>)</span><br><span class="line">                  <span class="keyword">break</span> outer;</span><br><span class="line">            	<span class="comment">// 又是经典的（b,n,f）三节点指针</span></span><br><span class="line">              Node&lt;K,V&gt; f = n.next;</span><br><span class="line">            	<span class="comment">//3、 n已不是b的后继节点，重试</span></span><br><span class="line">              <span class="keyword">if</span> (n != b.next)                <span class="comment">// inconsistent read</span></span><br><span class="line">                  <span class="keyword">break</span>;</span><br><span class="line">            	<span class="comment">//4、 n被标记为删除状态，则使用helpDelete真正删除n节点，然后重试</span></span><br><span class="line">              <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) &#123;    <span class="comment">// n is deleted</span></span><br><span class="line">                  n.helpDelete(b, f);</span><br><span class="line">                  <span class="keyword">break</span>;</span><br><span class="line">              &#125;</span><br><span class="line">            	<span class="comment">//5、 b被标记为删除状态，或者b被标记位删除且b.next是一个marker节点，也只能重试</span></span><br><span class="line">              <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n)  <span class="comment">// b is deleted</span></span><br><span class="line">                  <span class="keyword">break</span>;</span><br><span class="line">            	<span class="comment">//6、 执行到这里：说明n节点就是z节点本身，显然它在已经在4步骤被删除掉，因此可直接返回该n节点，需要注意：返回的n节点只有key不为空，其value是null的。</span></span><br><span class="line">              <span class="keyword">if</span> ((c = cpr(cmp, key, n.key)) == <span class="number">0</span>)</span><br><span class="line">                  <span class="keyword">return</span> n;</span><br><span class="line">            	<span class="comment">// 说明key原本就不在链表上，可以直接返回。</span></span><br><span class="line">              <span class="keyword">if</span> (c &lt; <span class="number">0</span>)</span><br><span class="line">                  <span class="keyword">break</span> outer;</span><br><span class="line">            <span class="comment">// b、n同时后移一步</span></span><br><span class="line">              b = n;</span><br><span class="line">              n = f;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>“新插入的z节点被标记删除后，需要对其上方的新建索引节点都一起删除”基于这个原因用findNode方法去完成，那么可以这样提问：能否直接调用<code>findPredecessor</code> 删除逻辑呢？如下所示</p>
<p>findPredecessor是在检索的沿途中会删除那些被标记“删除状态的”索引节点，它只在索引层范围内进行，不会下沉到数据层链表中操作。</p>
<p>而findNode方法可以进入到数据层链表中，把已经标记为删除状态的数据节点node删除，这个功能是findPredecessor不具备的，同时findNode内部调用findPredecessor方法，完成了额外的索引节点删除功能。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> (t.node.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">  	<span class="comment">// findNode(key)</span></span><br><span class="line">    findPredecessor(key, cmp);</span><br><span class="line">    <span class="keyword">break</span> splice;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下是官方关于findNode方法设计说明及其实现功能：新插入的z节点被标记删除后，需要对其上方的新建索引节点都一起删除</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">返回一个null（如果给定的key不在数据链表里面）或者一个持有key的node（node的value当然是null）,清除沿途遍历到的所有已标记“node.value&#x3D;null”的数据层节点（注意这里的nodes是数据层链表中的节点，不是指清除索引层节点）</span><br><span class="line">Returns node holding key or null if no such, clearing out any deleted nodes seen along the way.</span><br><span class="line">从key的前驱节点b开始遍历base-level的数据层链表（如有必要则需要重复遍历），向右移动过程中遇到标记位删除的节点就处理它</span><br><span class="line">Repeatedly traverses at base-level looking for key starting at predecessor returned from findPredecessor, processing base-level deletions as encountered. </span><br><span class="line">调用者可以使用findNode方法获得额外收益即沿途过程中能清除被标记为删除状态的数据节点。</span><br><span class="line">Some callers rely on this side-effect of clearing deleted nodes.</span><br><span class="line"></span><br><span class="line">出现以下三种情况之一，都会使得执行流再次回到b节点重新遍历</span><br><span class="line">Restarts occur, at traversal step centered on node n,</span><br><span class="line">（1）考察(b,n,f)三个节点，若n节点已发生改变，不再是b.next执行的原n节点，因此本轮遍历无法处理这种删除操作（cannot unlink），需重试</span><br><span class="line">if: (1) After reading n&#39;s next field, n is no longer assumed predecessor b&#39;s current successor, which means that we don&#39;t have a consistent 3-node snapshot and so cannot unlink any subsequent deleted nodes encountered. </span><br><span class="line"></span><br><span class="line">（2）考察(b,n,f)三个节点，若n.value&#x3D;null，说明n被标记为删除状态，这种情况下，先去n.helpDelete(b, f)删除n节点本身，然后重新遍历</span><br><span class="line">(2) n&#39;s value field is null, indicating n is deleted, in which case we help out an ongoing structural deletion before retrying. Even though there are cases where such unlinking doesn&#39;t require restart, they aren&#39;t sorted out here because doing so would not usually outweigh cost of restarting. </span><br><span class="line"></span><br><span class="line">（3）考察(b,n,f)三个节点，若n是marker节点（说明b被标记）或者b.value&#x3D;null,说明findPredecessor方法返回的是一个被标记删除的数据节点，此时无法正确获得predecessor节点就不能继续执行unlink操作。只能重新回到for循环继续调用findPredecessor，找到新的(b,n,f)</span><br><span class="line">(3) n is a marker or n&#39;s predecessor&#39;s value field is null, indicating (among other possibilities) that findPredecessor returned a deleted node. We can&#39;t unlink the node because we don&#39;t know its predecessor, so rely on another call to findPredecessor to notice and return some earlier predecessor, which it will do. </span><br><span class="line">第3种check只会在循环的开始才会严格执行检查，但每次迭代都会执行此检查，以帮助调用方避免和其他线程发生竞争，如果调用方cas失败，只能retry。</span><br><span class="line">This check is only strictly needed at beginning of loop, (and the b.value check isn&#39;t strictly needed at all) but is done each iteration to help avoid contention with other threads by callers that will fail to be able to change links, and so will retry anyway. </span><br><span class="line"></span><br><span class="line">doPut, doRemove, and findNear方法中loop设计都会有以上三种情况的检查逻辑。findFirst、findLast也有类似的检查逻辑</span><br><span class="line">The traversal loops in doPut, doRemove, and findNear all include the same three kinds of checks. And specialized versions appear in findFirst, and findLast and their variants. They can&#39;t easily share code because each uses the reads of fields held in locals occurring in the orders they were performed.</span><br><span class="line">Params:</span><br><span class="line">key – the key</span><br><span class="line">Returns:</span><br><span class="line">node holding key, or null if no suc</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深入解析ThreadLocal的数据结构设计原理及其源代码实现</title>
    <url>/2021/12/05/Java%E9%AB%98%E7%BA%A7%E4%B8%BB%E9%A2%98%EF%BC%9A%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90ThreadLocal%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>ThreadLocal可以实现完全基于无锁且也不是基于CAS的线程隔离需求，让每个线程可以有自己的本地实例，但如果对ThreadLocal底层设计不了解，那么对甚至无法正确ThreadLocal及其可能出现的内存泄露问题。可以说ThreadLocal的源代码设计也一种非常优秀的可支持“高并发”的实现。</p>
<p><img src="https://img-blog.csdnimg.cn/875e7788579d4c779bb5f3cff8ce278f.png" alt="在这里插入图片描述"></p>
<p>《gitee 博客文章封面》</p>
<a id="more"></a>
<h3 id="基本用法："><a href="#基本用法：" class="headerlink" title="基本用法："></a>基本用法：</h3><h4 id="demo1"><a href="#demo1" class="headerlink" title="demo1"></a>demo1</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerA</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">            ThreadLocal&lt;String&gt; var1=<span class="keyword">new</span> ThreadLocal&lt;&gt;();</span><br><span class="line">            ThreadLocal&lt;String&gt; var2=<span class="keyword">new</span> ThreadLocal&lt;&gt;();</span><br><span class="line">            <span class="comment">// 普通用法</span></span><br><span class="line">            var1.set(<span class="string">&quot;foo A&quot;</span>);</span><br><span class="line">            var2.set(<span class="string">&quot;bar A&quot;</span>);</span><br><span class="line">            System.out.println(var1.get()); <span class="comment">// 输出foo A</span></span><br><span class="line">            System.out.println(var2.get()); <span class="comment">// 输出bar A</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerB</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">            ThreadLocal&lt;String&gt; var1=<span class="keyword">new</span> ThreadLocal&lt;&gt;();</span><br><span class="line">            ThreadLocal&lt;String&gt; var2=<span class="keyword">new</span> ThreadLocal&lt;&gt;();</span><br><span class="line">            <span class="comment">// 普通用法</span></span><br><span class="line">            var1.set(<span class="string">&quot;foo B&quot;</span>);</span><br><span class="line">            var2.set(<span class="string">&quot;bar B&quot;</span>);</span><br><span class="line">            System.out.println(var1.get()); <span class="comment">// 输出foo B</span></span><br><span class="line">            System.out.println(var2.get()); <span class="comment">// 输出bar B</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> ProducerA().start();</span><br><span class="line">        <span class="keyword">new</span> ProducerB().start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里创建了两个线程，每个线程内部有自己的ThreadLocal变量，线程之间ThreadLocal变量内部的set和get互相独立，互不影响，无需使用锁即可实现了线程安全操作。线程内部的变量使用set方法给定初始值、get方法取值，可以猜测其内部有类似HashMap这样的设计，但是否照搬HashMap数据结构设计呢？ 其实不然。</p>
<p>在这里，ProducerA内部其实是创建了一个称为“ThreadLocalMap”的Map结构用于存放ThreadLocal变量和它的value，ProducerB内部也创建了一个ThreadLocalMap，也即每个线程绑定一个自己内部ThreadLocalMap，这里提到的ThreadLocalMap就是提供了set、get方法的底层Map数据结构，所谓的ThreadLocal数据结构分析其实就是特指其内部的ThreadLocalMap的数据结构分析。</p>
<h4 id="demo2"><a href="#demo2" class="headerlink" title="demo2"></a>demo2</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HoldCount</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> tid=<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ThreadLocal&lt;HoldCount&gt; rh=ThreadLocal.withInitial(HoldCount::<span class="keyword">new</span>); <span class="comment">// 设定rh这个ThreadLocal变量的初始值</span></span><br><span class="line">        rh.set(<span class="keyword">new</span> HoldCount());  <span class="comment">// 将计数器放在rh中缓存</span></span><br><span class="line">        HoldCount h= rh.get();</span><br><span class="line">        System.out.println(h.count); <span class="comment">// 这里输出的rh初始值，也即HoldCount的count属性初始值:0。</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            h.count++;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(h.count);</span><br><span class="line"></span><br><span class="line">        HoldCount newh=rh.get();<span class="comment">// 更新缓存计数器后，再从ThreadLocal重新读取</span></span><br><span class="line">        System.out.println(newh.count); <span class="comment">// 可以读取新的计数值</span></span><br><span class="line">        <span class="comment">// 在rh这个ThreadLocal里面的Map结构中移除HoldCount实例对象</span></span><br><span class="line">        rh.remove();</span><br><span class="line">        System.out.println(rh.get()); <span class="comment">// 此时rh里面Map已经不存在HoldCount对象，因此这里返回NUll</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在demo2中，给出了使用ThreadLocal后需要及时删除其实例对象的情况，这部分原因将在文章后面给出深入分析。</p>
<h3 id="ThreadLocal内部数据结构简析"><a href="#ThreadLocal内部数据结构简析" class="headerlink" title="ThreadLocal内部数据结构简析"></a>ThreadLocal内部数据结构简析</h3><p>可以看到set方法是由内部ThreadLocalMap实现的set方法，既然是个“Map”，那么当然可以猜测是否跟HashMap的数据结构：数据+链表+红黑树类似呢？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(T value)</span> </span>&#123;</span><br><span class="line">    Thread t = Thread.currentThread();</span><br><span class="line">    ThreadLocalMap map = getMap(t);</span><br><span class="line">    <span class="keyword">if</span> (map != <span class="keyword">null</span>)</span><br><span class="line">        map.set(<span class="keyword">this</span>, value);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        createMap(t, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实ThreadLocalMap的数据结构没有HashMap数据结构复杂，ThreadLocalMap底层仅有一个table数组，这里，也许你会好奇：HashMap为了解决hash冲突，在数组的桶位上加入一条单向链表，冲突的entry自然会放入到此链表中（或者红黑树），那么问题来了，ThreadLocalMap底层仅有一个table数组，它是如何解决hash冲突？以下正式其设计原理之一，这里的数组给出最简单的情况，不包括“stale entry（无效entry）”的情况，以便让读者快速理解ThreadLocalMap设计原理：<br><img src="https://img-blog.csdnimg.cn/692e077da2654f0eb1ec67ff4f54a4cf.png" alt="在这里插入图片描述"><br>可以看到图中所说的“解决冲突的方式：从i=3开始向后遍历出首个空slot，也即i=5，将keyC放入此空slot即可”的逻辑被称为“线性探测法”，所谓的“线性”就是o(n)复杂度的遍历操作，所谓的“探测”就是不断向后“探测、寻找”，直到找到首个空slot位置。</p>
<p>以上内容为ThreadLocalMap的放入new Entry的简单情况，如果有理解HashMap源代码设计的读者应该可以猜到其他重要设计：例如，当数组容量不够时，如何扩容，也即rehash（注意ThreadLocalMap里面的resize和rehash不是同一个逻辑），再例如ThreadLocalMap里面已经存在的entry，如果它的key已经变成无效（stale），那么如何该清理，或者说在set和get的线性探测过程中遇到有stale entry时，该如何清理？这些问题将在后面逐个深入探讨。</p>
<h3 id="ThreadLocalMap的基本成员变量"><a href="#ThreadLocalMap的基本成员变量" class="headerlink" title="ThreadLocalMap的基本成员变量"></a>ThreadLocalMap的基本成员变量</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalMap</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The entries in this hash map extend WeakReference, using</span></span><br><span class="line"><span class="comment">     * its main ref field as the key (which is always a</span></span><br><span class="line"><span class="comment">     * ThreadLocal object).  Note that null keys (i.e. entry.get()</span></span><br><span class="line"><span class="comment">     * == null) mean that the key is no longer referenced, so the</span></span><br><span class="line"><span class="comment">     * entry can be expunged from table.  Such entries are referred to</span></span><br><span class="line"><span class="comment">     * as &quot;stale entries&quot; in the code that follows.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// ThreadLocalMap底层数组存放的WeakReference类型的entry，使用弱引用类型是为了能够高效GC，避免内存泄露，文章后面给出此设计的讨论</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Entry</span> <span class="keyword">extends</span> <span class="title">WeakReference</span>&lt;<span class="title">ThreadLocal</span>&lt;?&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="comment">/** The value associated with this ThreadLocal. */</span></span><br><span class="line">        Object value;</span><br><span class="line">		<span class="comment">/* entry的key就是ThreadLocal对象，例如一个线程内部有10个ThreadLocal变量，那么此线程内部的ThreadLocalMap将存放这10个entry，这里的value就是ThreadLocal变量的“值”。</span></span><br><span class="line"><span class="comment">    例如demo1中：</span></span><br><span class="line"><span class="comment">        ThreadLocal&lt;String&gt; var1=new ThreadLocal&lt;&gt;();</span></span><br><span class="line"><span class="comment">        var1.set(&quot;foo A&quot;)</span></span><br><span class="line"><span class="comment">        那么entry的key就是这个名称为var1的ThreadLocal对象，value就是字符串“foo A”</span></span><br><span class="line"><span class="comment">        */</span>  </span><br><span class="line">        Entry(ThreadLocal&lt;?&gt; k, Object v) &#123;</span><br><span class="line">            <span class="keyword">super</span>(k);</span><br><span class="line">            value = v;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The initial capacity -- MUST be a power of two.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 数组的初始容量16</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> INITIAL_CAPACITY = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The table, resized as necessary.</span></span><br><span class="line"><span class="comment">     * table.length MUST always be a power of two.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//ThreadLocalMap的底层数组，这里也采用2的次方，原因在HashMap的源代码讨论已经给出深入的解析，这里不再累赘。</span></span><br><span class="line">    <span class="keyword">private</span> Entry[] table;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The number of entries in the table.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 数据含有entry的个数，注意即使entry的key处于stale状态，它也算一个entry</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The next size value at which to resize.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> threshold; <span class="comment">// Default to 0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Set the resize threshold to maintain at worst a 2/3 load factor.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 注意区别于HashMap的扩容阈值是len*3/4</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setThreshold</span><span class="params">(<span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">        threshold = len * <span class="number">2</span> / <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Increment i modulo len.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 返回数组当前位置i的下一个位置i+1,如果下一个位置超过数组长度，那么下一个位置又从下标0开始，这种方式实现了所谓的“环形数组”，在后面get、set方法中或者stale entry清空机制的处理中可以看到它的用处</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">nextIndex</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ((i + <span class="number">1</span> &lt; len) ? i + <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Decrement i modulo len.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="comment">// 逻辑同上，方向相反，返回当前位置i的前一个位置i+1，如果来到数组头部，那么前一个位置即回到数组末尾</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">prevIndex</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ((i - <span class="number">1</span> &gt;= <span class="number">0</span>) ? i - <span class="number">1</span> : len - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="set方法完全解析"><a href="#set方法完全解析" class="headerlink" title="set方法完全解析"></a>set方法完全解析</h3><h4 id="set方法本身"><a href="#set方法本身" class="headerlink" title="set方法本身"></a>set方法本身</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Set the value associated with key.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> key the thread local object</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> value the value to be set</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(ThreadLocal&lt;?&gt; key, Object value)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// We don&#x27;t use a fast path as with get() because it is at</span></span><br><span class="line">      <span class="comment">// least as common to use set() to create new entries as</span></span><br><span class="line">      <span class="comment">// it is to replace existing ones, in which case, a fast</span></span><br><span class="line">      <span class="comment">// path would fail more often than not.</span></span><br><span class="line">      </span><br><span class="line">      Entry[] tab = table;</span><br><span class="line">      <span class="keyword">int</span> len = tab.length;</span><br><span class="line">      <span class="comment">// 1.计算给定key对应的桶位，此hash算法能够最大程度将key平均分布到数组对应的桶位上，具体算法参考文后说明</span></span><br><span class="line">      <span class="keyword">int</span> i = key.threadLocalHashCode &amp; (len-<span class="number">1</span>);</span><br><span class="line">      <span class="comment">/* 2.线性探测发的实现</span></span><br><span class="line"><span class="comment">      从定位到i桶位开始遍历，直到遇到一个entry确实是null的空桶位，如果此遍历过程中遇到stale entry那么将其替换即可完成set操作。</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">      <span class="keyword">for</span> (Entry e = tab[i];</span><br><span class="line">           e != <span class="keyword">null</span>;</span><br><span class="line">           e = tab[i = nextIndex(i, len)]) &#123;</span><br><span class="line">          ThreadLocal&lt;?&gt; k = e.get();</span><br><span class="line">		<span class="comment">// 3.如果此桶位的key恰好是给定给定的key，那么更新此桶位的value后可直接返回。</span></span><br><span class="line">          <span class="keyword">if</span> (k == key) &#123;</span><br><span class="line">              e.value = value;</span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">		<span class="comment">// 4. 当前桶位的entry的key为null（注意这个key是弱引用类型，说明此entry已经被GC），使用replaceStaleEntry放入新entry</span></span><br><span class="line">          <span class="keyword">if</span> (k == <span class="keyword">null</span>) &#123;</span><br><span class="line">              replaceStaleEntry(key, value, i);</span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"><span class="comment">// 5.在2的线性探测过程中，遇到entry为空的即可来到这流程,直接放入新entry，并且数组的entry数量加1，在这里应该可以猜到，当向数组添加一个新entry后接下来就要判断是否需要扩容</span></span><br><span class="line">      tab[i] = <span class="keyword">new</span> Entry(key, value);</span><br><span class="line">      <span class="keyword">int</span> sz = ++size;</span><br><span class="line">    	<span class="comment">// 6.满足扩容的条件：cleanSomeSlots返回False且entry数量达到扩容阈值</span></span><br><span class="line">      <span class="keyword">if</span> (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold)</span><br><span class="line">          rehash();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关于线性探测法的说明</p>
<p>1.为何会有“环形数组或者环形遍历”的设计:<code>nextIndex</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (Entry e = tab[i];</span><br><span class="line">     e != <span class="keyword">null</span>;</span><br><span class="line">     e = tab[i = nextIndex(i, len)]) </span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/9b76747291534dd4b553699e0ea6b551.png" alt="在这里插入图片描述"></p>
<p>图1所示，假设目前有一个keyX定位到的桶位是i=13，但此桶位已存在entry，只能继续向后探测，来到数组尾部的桶位也不为空，此时经过<code>e = tab[i = nextIndex(i, len)]</code>的计算后，线性探测再次回到数组的头部位置重新遍历，如图2所示，当遍历到i=6时，发现此桶位为空，即可跳出循环接着在此位置放置新的entry，这就是“环形数组或者环形遍历”的底层设计逻辑。</p>
<h4 id="更加离散的hash计算"><a href="#更加离散的hash计算" class="headerlink" title="更加离散的hash计算"></a>更加离散的hash计算</h4><p>ThreadLocalMap内部的hash计算方式没有采用类似HashMap的计算方式,而是自行设计了一套</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocal</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * ThreadLocals rely on per-thread linear-probe hash maps attached</span></span><br><span class="line"><span class="comment">     * to each thread (Thread.threadLocals and</span></span><br><span class="line"><span class="comment">     * inheritableThreadLocals).  The ThreadLocal objects act as keys,</span></span><br><span class="line"><span class="comment">     * searched via threadLocalHashCode.  This is a custom hash code</span></span><br><span class="line"><span class="comment">     * (useful only within ThreadLocalMaps) that eliminates collisions</span></span><br><span class="line"><span class="comment">     * in the common case where consecutively constructed ThreadLocals</span></span><br><span class="line"><span class="comment">     * are used by the same threads, while remaining well-behaved in</span></span><br><span class="line"><span class="comment">     * less common cases.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threadLocalHashCode = nextHashCode();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The next hash code to be given out. Updated atomically. Starts at</span></span><br><span class="line"><span class="comment">     * zero.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 巧妙利用了原子累加器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> AtomicInteger nextHashCode =</span><br><span class="line">        <span class="keyword">new</span> AtomicInteger();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The difference between successively generated hash codes - turns</span></span><br><span class="line"><span class="comment">     * implicit sequential thread-local IDs into near-optimally spread</span></span><br><span class="line"><span class="comment">     * multiplicative hash values for power-of-two-sized tables.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// hash值递增步长，每次对新的i计算hash值前先加上此数，计算结果能更加离散,这个值对应的十进制数为1640531527，这个值就是带符号的32位int的最大值的黄金分割值取正</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> HASH_INCREMENT = <span class="number">0x61c88647</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns the next hash code.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">nextHashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每次对新的i计算hash值前先加上此数，计算结果能更加离散</span></span><br><span class="line">        <span class="keyword">return</span> nextHashCode.getAndAdd(HASH_INCREMENT);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>模拟ThreadLocalMap的hash计算方式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalHashCode</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span>  <span class="keyword">int</span> HASH_INCREMENT = <span class="number">0x61c88647</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> tableLength=<span class="number">16</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="number">32</span> ; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> h=i*HASH_INCREMENT+HASH_INCREMENT;</span><br><span class="line">            <span class="keyword">int</span> index=h &amp; (tableLength-<span class="number">1</span>);</span><br><span class="line">            System.out.println(i+<span class="string">&quot;定位的桶位是:&quot;</span>+index);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以观察不同桶位计算出的hash值确实足够离散：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0定位的桶位是:7</span><br><span class="line">1定位的桶位是:14</span><br><span class="line">2定位的桶位是:5</span><br><span class="line">3定位的桶位是:12</span><br><span class="line">4定位的桶位是:3</span><br><span class="line">5定位的桶位是:10</span><br><span class="line">6定位的桶位是:1</span><br><span class="line">7定位的桶位是:8</span><br><span class="line">8定位的桶位是:15</span><br><span class="line">9定位的桶位是:6</span><br><span class="line">10定位的桶位是:13</span><br><span class="line">11定位的桶位是:4</span><br><span class="line">12定位的桶位是:11</span><br><span class="line">13定位的桶位是:2</span><br><span class="line">14定位的桶位是:9</span><br><span class="line">15定位的桶位是:0</span><br><span class="line">16定位的桶位是:7</span><br><span class="line">17定位的桶位是:14</span><br><span class="line">18定位的桶位是:5</span><br><span class="line">19定位的桶位是:12</span><br><span class="line">20定位的桶位是:3</span><br><span class="line">21定位的桶位是:10</span><br><span class="line">22定位的桶位是:1</span><br><span class="line">23定位的桶位是:8</span><br><span class="line">24定位的桶位是:15</span><br><span class="line">25定位的桶位是:6</span><br><span class="line">26定位的桶位是:13</span><br><span class="line">27定位的桶位是:4</span><br><span class="line">28定位的桶位是:11</span><br><span class="line">29定位的桶位是:2</span><br><span class="line">30定位的桶位是:9</span><br><span class="line">31定位的桶位是:0</span><br></pre></td></tr></table></figure>
<h4 id="replaceStaleEntry方法解析（核心内容）"><a href="#replaceStaleEntry方法解析（核心内容）" class="headerlink" title="replaceStaleEntry方法解析（核心内容）"></a>replaceStaleEntry方法解析（核心内容）</h4><p>在set方法中的第4点：替换失效的entry</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 4. 当前桶位的entry的key为null（注意这个key是弱引用类型，说明此entry已经被GC），使用replaceStaleEntry放入新entry</span></span><br><span class="line">        <span class="keyword">if</span> (k == <span class="keyword">null</span>) &#123;</span><br><span class="line">            replaceStaleEntry(key, value, i);</span><br><span class="line">            <span class="keyword">return</span>;</span><br></pre></td></tr></table></figure>
<p>其源码设计包括两个重要的核心功能：替换对应位置失效的entry和具有顺带功能（As a side effect）的清理其他失效entry，其中清理entry的逻辑设计最为复杂。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">replaceStaleEntry</span><span class="params">(ThreadLocal&lt;?&gt; key, Object value,</span></span></span><br><span class="line"><span class="function"><span class="params">                               <span class="keyword">int</span> staleSlot)</span> </span>&#123;</span><br><span class="line">    Entry[] tab = table;</span><br><span class="line">    <span class="keyword">int</span> len = tab.length;</span><br><span class="line">    Entry e;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Back up to check for prior stale entry in current run.</span></span><br><span class="line">    <span class="comment">// We clean out whole runs at a time to avoid continual</span></span><br><span class="line">    <span class="comment">// incremental rehashing due to garbage collector freeing</span></span><br><span class="line">    <span class="comment">// up refs in bunches (i.e., whenever the collector runs).</span></span><br><span class="line">    <span class="keyword">int</span> slotToExpunge = staleSlot;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    1、staleSlot是对于给定key用线性探测法“前向遍历”找到的首次出现的stale entry对应的下标</span></span><br><span class="line"><span class="comment">    为何第1步骤的前向遍历没有安排类似第2步骤的“替换操作等逻辑呢”，因为“ThreadLocal本身用的是开发地址法，冲突的key都被放置在后面空的slot，就算来到table末尾再从头遍历，它也是遵循“向后放置发生冲突的key””</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = prevIndex(staleSlot, len);</span><br><span class="line">         (e = tab[i]) != <span class="keyword">null</span>;</span><br><span class="line">         i = prevIndex(i, len))</span><br><span class="line">        <span class="keyword">if</span> (e.get() == <span class="keyword">null</span>)</span><br><span class="line">            slotToExpunge = i;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Find either the key or trailing null slot of run, whichever</span></span><br><span class="line">    <span class="comment">// occurs first</span></span><br><span class="line">   <span class="comment">// 2. 从set方法传入的staleSlot下标开始向后遍历</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = nextIndex(staleSlot, len);</span><br><span class="line">         (e = tab[i]) != <span class="keyword">null</span>;</span><br><span class="line">         i = nextIndex(i, len)) &#123;</span><br><span class="line">        ThreadLocal&lt;?&gt; k = e.get();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If we find key, then we need to swap it</span></span><br><span class="line">        <span class="comment">// with the stale entry to maintain hash table order.</span></span><br><span class="line">        <span class="comment">// The newly stale slot, or any other stale slot</span></span><br><span class="line">        <span class="comment">// encountered above it, can then be sent to expungeStaleEntry</span></span><br><span class="line">        <span class="comment">// to remove or rehash all of the other entries in run.</span></span><br><span class="line">     <span class="comment">/*</span></span><br><span class="line"><span class="comment">     以下逻辑非常关键：</span></span><br><span class="line"><span class="comment">     3.1 如果从stale slot开始的“后向遍历”的第i下标又出现了key冲突，说明给定的key“本应放在stale slot 下标位置，但是因为冲突，被迫挪到比stale slot 更靠后的位置i”，既然现在stale slot已失效，那么就可以将给定key放回本应该更靠近hash定位的下标位置staleSlot。这里采用交换两者位置即可实现此逻辑。这就是”to swap it with the stale entry to maintain hash table order”所要表达的逻辑。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">        <span class="keyword">if</span> (k == key) &#123;</span><br><span class="line">            e.value = value;  </span><br><span class="line">            tab[i] = tab[staleSlot];</span><br><span class="line">            tab[staleSlot] = e;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Start expunge at preceding stale entry if it exists</span></span><br><span class="line">           <span class="comment">// 3.2 如果slotToExpunge还是staleSlot，说明第1步骤的“前向探测”没有stale entry，那么就将清理起始下标改到i，因为i下标位置存放的是在3.1交换过来的stale entry：tab[i] = tab[staleSlot]</span></span><br><span class="line">            <span class="keyword">if</span> (slotToExpunge == staleSlot)</span><br><span class="line">                slotToExpunge = i;</span><br><span class="line">        <span class="comment">/* 3.3 到此，我们知道，截止到i下标的stale entry情况ß：</span></span><br><span class="line"><span class="comment">         [某个空slot,staleSlot)：从staleSlot的前向位置都没有stale entry</span></span><br><span class="line"><span class="comment">         staleSlot：将i位置的有效entry交换过来tab[staleSlot] = e</span></span><br><span class="line"><span class="comment">         [staleSlot+1,i-1]：后向遍历没有出现stale entry</span></span><br><span class="line"><span class="comment">         i：存放的是从staleSlot交换过来的stale entry</span></span><br><span class="line"><span class="comment">         因此slotToExpunge肯定是从i下标开始做清理工作。</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">            cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If we didn&#x27;t find stale entry on backward scan, the</span></span><br><span class="line">        <span class="comment">// first stale entry seen while scanning for key is the</span></span><br><span class="line">        <span class="comment">// first still present in the run.</span></span><br><span class="line">      	<span class="comment">// 3.4 后向遍历在第i下标发现了一个stale entry且在前向遍历没有出现stale entry，那么清理开始下标当然要重置为i，那么staleSlot位置还存放着stale entry且没有也没有像3.1这样的“swap it”的设计，那么staleSlot自己是如何处理呢。它会在接下里的第4步骤中被处理掉！ 那么在这个步骤发现第i号下标新的stale entry又是如何处理呢？ 它会在第5个步骤被清理掉</span></span><br><span class="line">        <span class="keyword">if</span> (k == <span class="keyword">null</span> &amp;&amp; slotToExpunge == staleSlot)</span><br><span class="line">            slotToExpunge = i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If key not found, put new entry in stale slot</span></span><br><span class="line">   <span class="comment">/* 4. 在整个run都没有找到对应的key且也没有发现stale entry（除了staleSlot本身整个），那么好办，直接将staleSlot这个在set方法一开始就发现的stale entry的位置替换为新 entry即可，这就是为何方法名字命名为replaceStaleEntry。</span></span><br><span class="line"><span class="comment">   </span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    tab[staleSlot].value = <span class="keyword">null</span>;</span><br><span class="line">    tab[staleSlot] = <span class="keyword">new</span> Entry(key, value);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If there are any other stale entries in run, expunge them</span></span><br><span class="line">  <span class="comment">// 第5步骤：接第3.4步骤出现的情况。</span></span><br><span class="line">    <span class="keyword">if</span> (slotToExpunge != staleSlot)</span><br><span class="line">        cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>关于“A run”的理解（理解run及其内部清理标记逻辑才能透彻理解set背后的原理）</p>
<p>As a side effect, this method expunges all stale entries in the “run” containing the stale entry(A run is a sequence of entries between two null slots.)<br><img src="https://img-blog.csdnimg.cn/ac10ec60579e410f910401b2ab956b87.png" alt="在这里插入图片描述"><br>如上图所示：</p>
<p>一个”run”就是在两个空slot之间的slots，例如上图，i=0和i=12之间就是一个“run”。 </p>
<p>1.为何这“run”是以两个空slot作为边界呢？</p>
<p>这是因为replaceStaleEntry的第1步骤使用prevIndex前向探测，直到遇到null slot则结束循环，而第2步骤使用nextIndex后向探测，直到遇到null slot则结束循环，因此可以得出一前一后都是null slot作为边界。</p>
<p>2.结合replaceStaleEntry的源代码分析的第1点：显然经过prevIndex的“前向探测”探测到了首个i=1的stale entry，因此slotToExpunge指向i=1表示此下标是接下expungeStaleEntry清理的起始下标。</p>
<p>3.根据1可知，如果slotToExpunge下标和staleSlot下标相等，说明“前向探测”根本没发现stale entry，也即slotToExpunge指向没动过。</p>
<p>4.根据第3.4步骤可知，<code>k == null &amp;&amp; slotToExpunge == staleSlot</code>，说明除了在set方法第一次发现的staleSlot，还在replaceStaleEntry的后向探测中的第i位置又发现了一个stale entry，因此起始清理下标要重置为slotToExpunge=i</p>
<p><img src="https://img-blog.csdnimg.cn/875e7788579d4c779bb5f3cff8ce278f.png" alt="在这里插入图片描述"></p>
<p>关于replaceStaleEntry内部最关键的“替换算法”，也即对应第第3.1步骤到第3.3步骤如上图1和图2所示：</p>
<p>不妨假设i=8就是“给定的key”hash定位时发生的冲突下标，假设i=11的key1等于“给定的key”，也即对应第3.1步骤，</p>
<p>此时实施3.1步骤的“swap”逻辑：</p>
<p>将staleSlot=9的stale entry交换到key1的i=11位置，原i=11位置entry交换到staleSlot=9位置并且key不变但value被更新为“给定key对应的value”。</p>
<p>经过这么处理，“给定的key”所在桶位显然更靠近原本属于它的8号桶位，而不是像之前“被迫挪到”11号桶位，这就是源代码注释说提到的“we need to swap it with the stale entry to maintain hash table order”</p>
<p>以上的算法设计可以抽象为以下类比逻辑：</p>
<blockquote>
<p>A本应坐在1号位，但发现来晚了，1号位置有人坐了，2、3、4也有人坐了，A被迫坐在5号位，某个时刻“新来的B”发现2号位已经变成staleSlot且1号位还有人在坐，那么此时B可以将A交换到2号位且把2号位的stale entry交换到5号位，那么此刻位于2号位的A显然更靠近“本属于自己的1号座位</p>
</blockquote>
<h4 id="关于staleSlot的清理逻辑设计"><a href="#关于staleSlot的清理逻辑设计" class="headerlink" title="关于staleSlot的清理逻辑设计"></a>关于staleSlot的清理逻辑设计</h4><p>在4.2 中replaceStaleEntry为我们展示了精密的如何找出“起始清理下标”的算法设计，从</p>
<p><code>cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);</code>可知，清理逻辑被设计为两部分：<br>第一部分：<code>expungeStaleEntry(slotToExpunge)</code>，此方法返回一个i下标。第一部分的清理可称为“线性地清理”——“linear expunge”。注意此过程还包括rehash过程！！</p>
<p>第二部分：<code>cleanSomeSlots(i, len)</code>，第二部分的清理可以称为“Heuristically expunge”，这里并不打算翻译为“启发性清理”，因为此处不建议使用中文硬翻译。（若要翻译，则可以翻译为“试探性地清理”）</p>
<p>以下是关于“线性地清理-expungeStaleEntry”的解析：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Expunge a stale entry by rehashing any possibly colliding entries</span></span><br><span class="line"><span class="comment"> * lying between staleSlot and the next null slot.  This also expunges</span></span><br><span class="line"><span class="comment"> * any other stale entries encountered before the trailing null.  See</span></span><br><span class="line"><span class="comment"> * Knuth, Section 6.4</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> staleSlot index of slot known to have null key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the index of the next null slot after staleSlot</span></span><br><span class="line"><span class="comment"> * (all between staleSlot and this slot will have been checked</span></span><br><span class="line"><span class="comment"> * for expunging).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 1.这里入参的staleSlot，就是replaceStaleEntry探测到的slotToExpunge下标</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">expungeStaleEntry</span><span class="params">(<span class="keyword">int</span> staleSlot)</span> </span>&#123;</span><br><span class="line">    Entry[] tab = table;</span><br><span class="line">    <span class="keyword">int</span> len = tab.length;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// expunge entry at staleSlot</span></span><br><span class="line">    <span class="comment">// 2.首先清空当前slotToExpunge下标的stale entry</span></span><br><span class="line">    tab[staleSlot].value = <span class="keyword">null</span>;</span><br><span class="line">    tab[staleSlot] = <span class="keyword">null</span>;</span><br><span class="line">    size--;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Rehash until we encounter null</span></span><br><span class="line">  <span class="comment">// 3. 在slotToExpunge+1到恰好遇到null slot之间进行逐个探测清理</span></span><br><span class="line">    Entry e;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = nextIndex(staleSlot, len);</span><br><span class="line">         (e = tab[i]) != <span class="keyword">null</span>;</span><br><span class="line">         i = nextIndex(i, len)) &#123;</span><br><span class="line">        ThreadLocal&lt;?&gt; k = e.get();</span><br><span class="line">        <span class="comment">// 3.1 又出现stale entry可直接清空</span></span><br><span class="line">        <span class="keyword">if</span> (k == <span class="keyword">null</span>) &#123;</span><br><span class="line">            e.value = <span class="keyword">null</span>;</span><br><span class="line">            tab[i] = <span class="keyword">null</span>;</span><br><span class="line">            size--;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 3.2 再次计算当前位置i放置的entry对应的hash值，如果hash值和当前i桶位一致，说明没有冲突，此entry恰好就是位于“本属于自己的桶位上”，如果hash值和当前i不一致，说明“此entry因为冲突被迫放到了第i位置，而第i位置不是此entry的直接定位”，可以将位于i桶位的“entry”放在“属于自己的h桶位”，这样就保证了entry能最大程度靠近或者就位于“本属于自己的桶位”范围以内，目的是为了提供线性探测查询效率！</span></span><br><span class="line">         </span><br><span class="line">            <span class="keyword">int</span> h = k.threadLocalHashCode &amp; (len - <span class="number">1</span>); </span><br><span class="line">          	<span class="comment">// 从向后探测的开放地址法可知，h值更小，i值更大，正是因为原h位置有冲突，e才被放置到更靠后的第i位置</span></span><br><span class="line">            <span class="keyword">if</span> (h != i) &#123;</span><br><span class="line">              <span class="comment">// 因为发生冲突被迫放置在i位置的entry,后面会被放到它的直接定位h桶位，因此i位置可以置为null</span></span><br><span class="line">                tab[i] = <span class="keyword">null</span>; </span><br><span class="line"></span><br><span class="line">                <span class="comment">// Unlike Knuth 6.4 Algorithm R, we must scan until</span></span><br><span class="line">                <span class="comment">// null because multiple entries could have been stale.</span></span><br><span class="line">               <span class="comment">/*</span></span><br><span class="line"><span class="comment">   虽然h桶位就是此entry的直接定位，但是考虑到h桶位可能被放置了其他entry，因此需要加入“向后探测”的逻辑，直到发现下一个位置为null slot。</span></span><br><span class="line"><span class="comment">   tab[h] = e  的写法就实现了“因为发生冲突被迫放置在i位置的entry，现在能够最接近地放到本属于自己直接定位的h桶位*/</span></span><br><span class="line">                <span class="keyword">while</span> (tab[h] != <span class="keyword">null</span>)</span><br><span class="line">                    h = nextIndex(h, len);</span><br><span class="line">                tab[h] = e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> i; </span><br><span class="line"> <span class="comment">/*</span></span><br><span class="line"><span class="comment"> 显然此i就是第3步骤for循环里面从slotToExpunge向后探测到下一个null slot下标，此下标会被cleanSomeSlots方法中利用起来。</span></span><br><span class="line"><span class="comment"> for (i = nextIndex(staleSlot, len);</span></span><br><span class="line"><span class="comment">         (e = tab[i]) != null;</span></span><br><span class="line"><span class="comment">         i = nextIndex(i, len))</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>expungeStaleEntry难点其实在第3.2步骤中遇到的情况，需要做个简单的rehash，保证entry更加靠近“本属于自己的直接定位h桶位”，过程解析参考以下算法流程：<br><img src="https://img-blog.csdnimg.cn/adb19cd01cd640d7bd8f5fc77a5daf8d.png" alt="在这里插入图片描述"><br>当然，如果在图2的中i=8不是null slot，那么从h位置开始<code>while (tab[h] != null)</code>探测，也会探测到i=10位置是个null slot，结果就是<code>table[h=i=10]=e</code>，e还是位于第i位置上。</p>
<h4 id="cleanSomeSlots-i-len-方法"><a href="#cleanSomeSlots-i-len-方法" class="headerlink" title="cleanSomeSlots(i, len)方法"></a>cleanSomeSlots(i, len)方法</h4><p>注意，要清楚<code>cleanSomeSlots(i, len)</code> i和len含义，否则无法理解cleanSomeSlots目的，这里的<code>i</code>就是<code>expungeStaleEntry</code>返回的一个空slot，len是table长度。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">cleanSomeSlots</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> removed = <span class="keyword">false</span>;</span><br><span class="line">    Entry[] tab = table;</span><br><span class="line">    <span class="keyword">int</span> len = tab.length;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="comment">// 1.</span></span><br><span class="line">        i = nextIndex(i, len);</span><br><span class="line">        Entry e = tab[i];</span><br><span class="line">        <span class="comment">//2.</span></span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span> &amp;&amp; e.get() == <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="comment">// 3.</span></span><br><span class="line">            n = len;</span><br><span class="line">            removed = <span class="keyword">true</span>;</span><br><span class="line">            <span class="comment">//4 .</span></span><br><span class="line">            i = expungeStaleEntry(i);</span><br><span class="line">        &#125;</span><br><span class="line">       <span class="comment">// 5.控制探测的次数</span></span><br><span class="line">    &#125; <span class="keyword">while</span> ( (n &gt;&gt;&gt;= <span class="number">1</span>) != <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> removed;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1.首先，考虑最简单的情况，如果第i和len之前都不存在stale entry，那么就相当于在1和len范围内折半探测，时间复杂度为log2(n)</p>
<p>2.其次，考虑到探测在i和len过程中，出现了stale entry，此时会将n重置为len长度，继续while，再一轮log2(n)次遍历</p>
<p>这就是所谓的“Heuristically scan”，因为是log2(n)，即使出现如2情况，此试探性的探测动作也是可以很快完成。</p>
<h4 id="set方法内部的rehash"><a href="#set方法内部的rehash" class="headerlink" title="set方法内部的rehash"></a>set方法内部的rehash</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      tab[i] = <span class="keyword">new</span> Entry(key, value);</span><br><span class="line">      <span class="keyword">int</span> sz = ++size;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">1. 显然如果cleanSomeSlots返回true，表明在table中清理了不少于1个的stale entry，恰好可以腾出不少于1个空slot，显然不需要table扩容。</span></span><br><span class="line"><span class="comment">2. 当!cleanSomeSlots(i, sz) 表示没有遇到stale entry且table的entry数量已经达到了阈值，可以进入扩容逻辑</span></span><br><span class="line"><span class="comment">*/</span> </span><br><span class="line">      <span class="keyword">if</span> (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold)</span><br><span class="line">          rehash();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>rehash内部设计：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Re-pack and/or re-size the table. First scan the entire</span></span><br><span class="line"><span class="comment">   * table removing stale entries. If this doesn&#x27;t sufficiently</span></span><br><span class="line"><span class="comment">   * shrink the size of the table, double the table size.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rehash</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">//1.扩容前，先从头到尾线性清理一下stale entry，运气好的话，清理的stale entry后恰好有足够多的null slot，这样省去真正的扩容操作，效率更高。</span></span><br><span class="line">      expungeStaleEntries();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Use lower threshold for doubling to avoid hysteresis</span></span><br><span class="line">      <span class="comment">// 2.此时的size是在1步骤清理完stale entry后的实际entry个数，只有当此时的size达到了0.75threshold才会去扩容，</span></span><br><span class="line">      <span class="keyword">if</span> (size &gt;= threshold - threshold / <span class="number">4</span>)</span><br><span class="line">          resize();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Double the capacity of the table.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">resize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      Entry[] oldTab = table;</span><br><span class="line">      <span class="keyword">int</span> oldLen = oldTab.length;</span><br><span class="line">      <span class="comment">// 这里看出是两倍扩容</span></span><br><span class="line">      <span class="keyword">int</span> newLen = oldLen * <span class="number">2</span>;</span><br><span class="line">      Entry[] newTab = <span class="keyword">new</span> Entry[newLen];</span><br><span class="line">      <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 1.从旧表开始逐个遍历</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldLen; ++j) &#123;</span><br><span class="line">          Entry e = oldTab[j];</span><br><span class="line">          <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">              ThreadLocal&lt;?&gt; k = e.get();</span><br><span class="line">              <span class="comment">// 2.旧表当前j位置出现stale entry，那么直接将entry的value强引用设为null，Help the GC</span></span><br><span class="line">              <span class="keyword">if</span> (k == <span class="keyword">null</span>) &#123;</span><br><span class="line">                  e.value = <span class="keyword">null</span>; <span class="comment">// Help the GC</span></span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="comment">// 3. 旧表当前遍历位置j是正常的entry，那么用新表newLen计算它在新表的桶位号  </span></span><br><span class="line">                  <span class="keyword">int</span> h = k.threadLocalHashCode &amp; (newLen - <span class="number">1</span>);</span><br><span class="line">                  <span class="comment">// 4. 开放地址法在新表中为“当前旧表遍历位置下entry”找到对应的null slot新表h位置</span></span><br><span class="line">                  <span class="keyword">while</span> (newTab[h] != <span class="keyword">null</span>)</span><br><span class="line">                      h = nextIndex(h, newLen); <span class="comment">// 注意这里是在新表计算</span></span><br><span class="line">                  newTab[h] = e; </span><br><span class="line">                  count++; </span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      setThreshold(newLen);</span><br><span class="line">      size = count;</span><br><span class="line">      table = newTab; <span class="comment">// table 指向新表引用</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>rehash的逻辑相对简单。</p>
<h3 id="get方法解析"><a href="#get方法解析" class="headerlink" title="get方法解析"></a>get方法解析</h3><p>有了set方法完全解析流程后，对于get方法则很好理解</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the value in the current thread&#x27;s copy of this</span></span><br><span class="line"><span class="comment"> * thread-local variable.  If the variable has no value for the</span></span><br><span class="line"><span class="comment"> * current thread, it is first initialized to the value returned</span></span><br><span class="line"><span class="comment"> * by an invocation of the &#123;<span class="doctag">@link</span> #initialValue&#125; method.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the current thread&#x27;s value of this thread-local</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> T <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Thread t = Thread.currentThread();</span><br><span class="line">    ThreadLocalMap map = getMap(t);</span><br><span class="line">     <span class="comment">// 1、ThreadLocalMap已经存在时</span></span><br><span class="line">    <span class="keyword">if</span> (map != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// getEntry才是正在在底层table去查找给定key对应的entry</span></span><br><span class="line">        ThreadLocalMap.Entry e = map.getEntry(<span class="keyword">this</span>);</span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">            T result = (T)e.value;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 如果线程t的ThreadLocal内部ThreadLocalMap还未初始化，直接返回ThreadLocal初始化时设定的初始值</span></span><br><span class="line">    <span class="keyword">return</span> setInitialValue();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> getMap的逻辑</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Get the map associated with a ThreadLocal. Overridden in</span></span><br><span class="line"><span class="comment">  * InheritableThreadLocal.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span>  t the current thread</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@return</span> the map</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function">ThreadLocalMap <span class="title">getMap</span><span class="params">(Thread t)</span> </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> t.threadLocals;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Create the map associated with a ThreadLocal. Overridden in</span></span><br><span class="line"><span class="comment">  * InheritableThreadLocal.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> t the current thread</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> firstValue value for the initial entry of the map</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// 这里可以看出，原来ThreadLocal并不是独立存在，而是它里面的ThreadLocalMap绑定当前线程的成员变量threadLocals，因此ThreadLocalMap的生命周期和线程同在</span></span><br><span class="line"> <span class="function"><span class="keyword">void</span> <span class="title">createMap</span><span class="params">(Thread t, T firstValue)</span> </span>&#123;</span><br><span class="line">     t.threadLocals = <span class="keyword">new</span> ThreadLocalMap(<span class="keyword">this</span>, firstValue);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * Construct a new map initially containing (firstKey, firstValue).</span></span><br><span class="line"><span class="comment">      * ThreadLocalMaps are constructed lazily, so we only create</span></span><br><span class="line"><span class="comment">      * one when we have at least one entry to put in it.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"> ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123;</span><br><span class="line">         table = <span class="keyword">new</span> Entry[INITIAL_CAPACITY];</span><br><span class="line">         <span class="keyword">int</span> i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - <span class="number">1</span>);</span><br><span class="line">         table[i] = <span class="keyword">new</span> Entry(firstKey, firstValue);</span><br><span class="line">         size = <span class="number">1</span>;</span><br><span class="line">         setThreshold(INITIAL_CAPACITY);</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>
<p>getEntry整体设计：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Get the entry associated with key.  This method</span></span><br><span class="line"><span class="comment"> * itself handles only the fast path: a direct hit of existing</span></span><br><span class="line"><span class="comment"> * key. It otherwise relays to getEntryAfterMiss.  This is</span></span><br><span class="line"><span class="comment"> * designed to maximize performance for direct hits, in part</span></span><br><span class="line"><span class="comment"> * by making this method readily inlinable.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>  key the thread local object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the entry associated with key, or null if no such</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Entry <span class="title">getEntry</span><span class="params">(ThreadLocal&lt;?&gt; key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = key.threadLocalHashCode &amp; (table.length - <span class="number">1</span>);</span><br><span class="line">    Entry e = table[i];</span><br><span class="line">    <span class="comment">// 1. 对应上面源代码注释提到“Only the fast path：a direct hit of existing key” 逻辑，也即给定key对应的i桶位的entry恰好存放的就是key的entry。</span></span><br><span class="line">    <span class="keyword">if</span> (e != <span class="keyword">null</span> &amp;&amp; e.get() == key)</span><br><span class="line">        <span class="keyword">return</span> e;</span><br><span class="line">   <span class="comment">// 2. 说明这个key在之前是发生冲突了，放置到比i更靠后的位置，需要采用“后向探测”去检索。</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> getEntryAfterMiss(key, i, e);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Version of getEntry method for use when key is not found in</span></span><br><span class="line"><span class="comment"> * its direct hash slot.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>  key the thread local object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>  i the table index for key&#x27;s hash code</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span>  e the entry at table[i]</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the entry associated with key, or null if no such</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Entry <span class="title">getEntryAfterMiss</span><span class="params">(ThreadLocal&lt;?&gt; key, <span class="keyword">int</span> i, Entry e)</span> </span>&#123;</span><br><span class="line">    Entry[] tab = table;</span><br><span class="line">    <span class="keyword">int</span> len = tab.length;</span><br><span class="line">    <span class="comment">// 从给定的entry开始后向遍历探测</span></span><br><span class="line">    <span class="keyword">while</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">        ThreadLocal&lt;?&gt; k = e.get();</span><br><span class="line">        <span class="comment">// 1.找到，则返回</span></span><br><span class="line">        <span class="keyword">if</span> (k == key)</span><br><span class="line">            <span class="keyword">return</span> e;</span><br><span class="line">        <span class="comment">// 2.遇到stale entry，调用expungeStaleEntry清理它，此时i位置就是slotToExpunge起始清理的下标</span></span><br><span class="line">        <span class="keyword">if</span> (k == <span class="keyword">null</span>)</span><br><span class="line">            expungeStaleEntry(i);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        <span class="comment">// 3.继续向后探测下一个entry  </span></span><br><span class="line">            i = nextIndex(i, len);</span><br><span class="line">        e = tab[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="remove方法"><a href="#remove方法" class="headerlink" title="remove方法"></a>remove方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Remove the entry for key.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">(ThreadLocal&lt;?&gt; key)</span> </span>&#123;</span><br><span class="line">    Entry[] tab = table;</span><br><span class="line">    <span class="keyword">int</span> len = tab.length;</span><br><span class="line">    <span class="keyword">int</span> i = key.threadLocalHashCode &amp; (len-<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (Entry e = tab[i];</span><br><span class="line">         e != <span class="keyword">null</span>;</span><br><span class="line">         e = tab[i = nextIndex(i, len)]) &#123;</span><br><span class="line">        <span class="keyword">if</span> (e.get() == key) &#123;</span><br><span class="line"><span class="comment">// 目的是主动让entry的父类成员变量置null，那么entry自然就不会存在任何引用了，直接从正常的entry变成stale entry</span></span><br><span class="line">            e.clear(); </span><br><span class="line"><span class="comment">//            </span></span><br><span class="line">            expungeStaleEntry(i);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> e.clear()关键逻辑：此方法来自<code>import java.lang.ref.Reference；</code>可以看到<code>e.clear()</code>目的是主动让entry的父类成员变量置null，那么entry自然就不会存在任何引用了，直接从正常的entry变成stale entry</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Clears this reference object.  Invoking this method will not cause this</span></span><br><span class="line"><span class="comment"> * object to be enqueued.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt; This method is invoked only by Java code; when the garbage collector</span></span><br><span class="line"><span class="comment"> * clears references it does so directly, without invoking this method.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.referent = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="ThreadLocal弱引用和内存泄露问题"><a href="#ThreadLocal弱引用和内存泄露问题" class="headerlink" title="ThreadLocal弱引用和内存泄露问题"></a>ThreadLocal弱引用和内存泄露问题</h3><p>在前面的所有内容中，我们都知道在线性探测中用<code>if (k == null)</code> 去判断当前桶位的entry是否为变为一个stale entry，放入一个正常的entry的为何会在某个时刻变成“失效的entry”？这是因为entry的key被设计为<code>WeakReference ,</code>这是ThreadLocalMap关键设计之一。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">注意以下源代码的解析</span></span><br><span class="line"><span class="comment">   To help deal with</span></span><br><span class="line"><span class="comment"> * very large and long-lived usages, the hash table entries use</span></span><br><span class="line"><span class="comment"> * WeakReferences for keys.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalMap</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The entries in this hash map extend WeakReference, using</span></span><br><span class="line"><span class="comment">     * its main ref field as the key (which is always a</span></span><br><span class="line"><span class="comment">     * ThreadLocal object).  Note that null keys (i.e. entry.get()</span></span><br><span class="line"><span class="comment">     * == null) mean that the key is no longer referenced, so the</span></span><br><span class="line"><span class="comment">     * entry can be expunged from table.  Such entries are referred to</span></span><br><span class="line"><span class="comment">     * as &quot;stale entries&quot; in the code that follows.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Entry</span> <span class="keyword">extends</span> <span class="title">WeakReference</span>&lt;<span class="title">ThreadLocal</span>&lt;?&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="comment">/** The value associated with this ThreadLocal. */</span></span><br><span class="line">        Object value;</span><br><span class="line"></span><br><span class="line">        Entry(ThreadLocal&lt;?&gt; k, Object v) &#123;</span><br><span class="line">            <span class="keyword">super</span>(k); <span class="comment">// key 是弱引用类型</span></span><br><span class="line">            value = v; <span class="comment">// value 是强引用类型  </span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>所谓的java弱引用：一旦有gc，那么WeakReference类型的对象就会被回收，用以下demo说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WeakReferenceDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Entry</span></span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String key;</span><br><span class="line">        <span class="keyword">private</span> String value;</span><br><span class="line">        Entry(String key,String value) &#123;</span><br><span class="line">            <span class="keyword">this</span>.key = key;</span><br><span class="line">            <span class="keyword">this</span>.value=value;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Entry&#123;&quot;</span> +</span><br><span class="line">                    <span class="string">&quot;key=&#x27;&quot;</span> + key + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                    <span class="string">&quot;, value=&#x27;&quot;</span> + value + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                    <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Entry entry=<span class="keyword">new</span> Entry(<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>); <span class="comment">// entry显然是一个强引用</span></span><br><span class="line">        WeakReference&lt;Entry&gt; entryWeakReference=<span class="keyword">new</span> WeakReference&lt;&gt;(entry); <span class="comment">// entryWeakReference是一个弱引用</span></span><br><span class="line"></span><br><span class="line">        System.gc();<span class="comment">// gc1</span></span><br><span class="line">        System.out.println(<span class="string">&quot;after gc1,entry:&quot;</span>+entry); <span class="comment">// Entry&#123;key=&#x27;foo&#x27;, value=&#x27;bar&#x27;&#125;</span></span><br><span class="line">        System.out.println(<span class="string">&quot;after gc1,entryWeakReference:&quot;</span>+entryWeakReference.get()); <span class="comment">// Entry&#123;key=&#x27;foo&#x27;, value=&#x27;bar&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        entry=<span class="keyword">null</span>; <span class="comment">// 此时entry强引用被置为null，那么会被gc回收</span></span><br><span class="line">        System.gc();<span class="comment">// gc2</span></span><br><span class="line">        System.out.println(<span class="string">&quot;after gc2,entryWeakReference:&quot;</span>+entryWeakReference.get()); <span class="comment">// null</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">after gc1,entry:Entry&#123;key&#x3D;&#39;foo&#39;, value&#x3D;&#39;bar&#39;&#125;</span><br><span class="line">after gc1,entryWeakReference:Entry&#123;key&#x3D;&#39;foo&#39;, value&#x3D;&#39;bar&#39;&#125;</span><br><span class="line">after gc2,entryWeakReference:null</span><br></pre></td></tr></table></figure>
<p>注意到ThreadLocalMap中的Entry，key类型是<code>WeakReference&lt;ThreadLocal&lt;?&gt;&gt;</code> 弱引用的，因此一旦此key没有指向强引用，那么key显然会变为null，那么gc时作为key的ThreadLocal对象在jvm堆中就会被回收，对应的Entry就是一个<code>stale entry</code>，注意，如果Entry的value此时还不是null也即还是处于强引用类型状态，这会引出另外一个问题：ThreadLocal内存泄露问题，或者说：</p>
<p>为何ThreadLocal会有内存泄露问题？</p>
<p>其实比较好理解，首先ThreadLocal内部ThreadLocalMap存放的Entry对象和当前线程的生命周期一致，只要线程不结束，且Entry的value也即给ThreadLocal对象设置的value没有被删除（强引用还在），那么这个Entry就不会被回收，假设一个线程内部的ThreadLocalMap里面有很多这样的Entry，那么就会面临内存泄露的风险，</p>
<p>考虑线程池的情况，例如有线程使用ThreadlLocal对象，此线程位于线程池中会一直保持运行，对于它的ThreadlLocal对象内部的ThreadLocalMap来说，如果map中Entry的value没有被外界使用完后及时删除，就导致此Entry一直得不到回收，容易发生内存泄露。</p>
<p>Entry的key采用弱引用类型，value为何不采用同样的弱引用类型设计呢？</p>
<p>首先，key是线程本地变量ThreadLocal，它本身可以被回收，但是其变量的值value本身是在其他地方被使用着，例如value放着的是一个Session对象或者事务管理中的Connection对象，如果value被设计为弱引用类型，那么在也业务层面被使用“线程本地变量的value”——Session对象或者Connection对象就会随机被回收，导致业务层出错，显然无法接受这种情况。所以value保持强引用的设计才是符合实际情况的。</p>
<h4 id="如何避免内存泄露"><a href="#如何避免内存泄露" class="headerlink" title="如何避免内存泄露"></a>如何避免内存泄露</h4><p>代码中满足一定<code>ThreadLocal.get()、ThreadLocal.set()</code>逻辑设计的情况下，主动调用<code>ThreadLocalMap.remove</code> 来移除Entry对象的引用关系，这种高级且科学用法，其实在<code>ReentrantReadWriteLock</code>的源代码设计有所体现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      定义一个给每个线程自己的内部读锁计数器</span></span><br><span class="line"><span class="comment">      * A counter for per-thread read hold counts.</span></span><br><span class="line"><span class="comment">      * Maintained as a ThreadLocal; cached in cachedHoldCounter</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">     <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HoldCounter</span> </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">         <span class="comment">// Use id, not reference, to avoid garbage retention</span></span><br><span class="line">         <span class="keyword">final</span> <span class="keyword">long</span> tid = getThreadId(Thread.currentThread());</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * ThreadLocal subclass. Easiest to explicitly define for sake</span></span><br><span class="line"><span class="comment">      * of deserialization mechanics.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">	<span class="comment">// 如何实现每个线程独立记录的读锁计数器？ 使用ThreadLocal即可保证线程隔离的计数，互不影响。</span></span><br><span class="line">     <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalHoldCounter</span></span></span><br><span class="line"><span class="class">         <span class="keyword">extends</span> <span class="title">ThreadLocal</span>&lt;<span class="title">HoldCounter</span>&gt; </span>&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> HoldCounter <span class="title">initialValue</span><span class="params">()</span> </span>&#123;  <span class="comment">// 线程自己持有的读锁计数器初始值0</span></span><br><span class="line">             <span class="keyword">return</span> <span class="keyword">new</span> HoldCounter();</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;  </span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * The number of reentrant read locks held by current thread.</span></span><br><span class="line"><span class="comment">      * Initialized only in constructor and readObject.</span></span><br><span class="line"><span class="comment">      * Removed whenever a thread&#x27;s read hold count drops to 0.</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">     <span class="keyword">private</span> <span class="keyword">transient</span> ThreadLocalHoldCounter readHolds;</span><br><span class="line"></span><br><span class="line">     Sync() &#123;</span><br><span class="line">         readHolds = <span class="keyword">new</span> ThreadLocalHoldCounter();</span><br><span class="line">         setState(getState()); <span class="comment">// ensures visibility of readHolds</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment">// 线程释放自己持有的读锁</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryReleaseShared</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>&#123;</span><br><span class="line">         Thread current = Thread.currentThread();</span><br><span class="line">         <span class="comment">// 第一个持有读锁的线程恰好是当前线程，</span></span><br><span class="line">         <span class="keyword">if</span> (firstReader == current) &#123;</span><br><span class="line">             <span class="comment">// assert firstReaderHoldCount &gt; 0;</span></span><br><span class="line">             <span class="comment">// “第一个持有读锁的线程” 也准备释放读锁，firstReader不再指向任何读线程</span></span><br><span class="line">             <span class="keyword">if</span> (firstReaderHoldCount == <span class="number">1</span>)</span><br><span class="line">                 firstReader = <span class="keyword">null</span>;</span><br><span class="line">           	<span class="comment">// 否则“第一个持有读锁的线程”重入锁次数减1</span></span><br><span class="line">             <span class="keyword">else</span></span><br><span class="line">                 firstReaderHoldCount--;</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">             HoldCounter rh = cachedHoldCounter;</span><br><span class="line">             <span class="comment">// 如果线程自己缓存的读锁计数器对象为空，或者线程自己的读锁计数器缓存的线程不是当前线程</span></span><br><span class="line">             <span class="keyword">if</span> (rh == <span class="keyword">null</span> || rh.tid != getThreadId(current))</span><br><span class="line">                <span class="comment">// 当前线程持有读锁的计数器readHolds</span></span><br><span class="line">                 rh = readHolds.get();</span><br><span class="line">             <span class="keyword">int</span> count = rh.count;</span><br><span class="line">             <span class="comment">// 当前线程持有读锁的计小于等于1，说明在本次读锁退出后，当前线程不再持有任何读锁，也即不再使用“计数器ThreadLocalHoldCounter”，因此用在它身上的ThreadLocal&lt;HoldCounter&gt;对象需要马上移除，避免ThreadLocal发生内存泄露。</span></span><br><span class="line">             <span class="keyword">if</span> (count &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">                 readHolds.remove();</span><br><span class="line">                 <span class="keyword">if</span> (count &lt;= <span class="number">0</span>)</span><br><span class="line">                     <span class="keyword">throw</span> unmatchedUnlockException();</span><br><span class="line">             &#125;</span><br><span class="line">            <span class="comment">// 线程自己持有读锁自减1</span></span><br><span class="line">             --rh.count;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">           	<span class="comment">// 总的读锁锁-1</span></span><br><span class="line">             <span class="keyword">int</span> c = getState();</span><br><span class="line">             <span class="keyword">int</span> nextc = c - SHARED_UNIT;</span><br><span class="line">             <span class="keyword">if</span> (compareAndSetState(c, nextc))</span><br><span class="line">                 <span class="comment">// Releasing the read lock has no effect on readers,</span></span><br><span class="line">                 <span class="comment">// but it may allow waiting writers to proceed if</span></span><br><span class="line">                 <span class="comment">// both read and write locks are now free.</span></span><br><span class="line">                 <span class="keyword">return</span> nextc == <span class="number">0</span>;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>
<p>从这里<code>ReentrantReadWriteLock</code>的关于ThreadLocal的使用中，再次理解了源代码注释提到的“可以使得状态和线程关联起来”，这里的“状态”就是读线锁中的每个线程持有读锁的数量，显然它和该线程绑定了起来，因此体现ThreadLocal变量使用的完美场景。</p>
<h4 id="ThreadLocal在类中常见用法"><a href="#ThreadLocal在类中常见用法" class="headerlink" title="ThreadLocal在类中常见用法"></a>ThreadLocal在类中常见用法</h4><p>在ThreadLocal源代码文件的注释开头有提到以下说明：</p>
<blockquote>
<p>This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID).</p>
</blockquote>
<p>尤其这句<code>ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID).</code></p>
<p>ThreadLocal实例通常是位于类中的私有静态字段，目的是为了实现把“状态”与线程（例如，用户ID或事务ID）绑定起来。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadId</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Atomic integer containing the next thread ID to be assigned</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicInteger nextId = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Thread local variable containing each thread&#x27;s ID</span></span><br><span class="line">  	 <span class="comment">// 放在类的静态字段位置，这样类的其他方法可以直接使用“此线程局部变量” </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ThreadLocal&lt;Integer&gt; threadId =</span><br><span class="line">        <span class="keyword">new</span> ThreadLocal&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span> </span><br><span class="line">      			 <span class="function"><span class="keyword">protected</span> Integer <span class="title">initialValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> nextId.getAndIncrement();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Returns the current thread&#x27;s unique ID, assigning it if necessary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> threadId.get();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外一个例子是在《Thinkinkg in Java 4》提供的demo</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Accessor</span> <span class="keyword">implements</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span>  <span class="keyword">final</span>  <span class="keyword">int</span> id;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Accessor</span><span class="params">(<span class="keyword">int</span> idn)</span></span>&#123;id=idn;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (!Thread.currentThread().isInterrupted())&#123;</span><br><span class="line">            ThreadLocalHolder.increment();</span><br><span class="line">            System.out.println(<span class="keyword">this</span>);</span><br><span class="line">            Thread.yield();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;#&quot;</span> +id+<span class="string">&quot;:&quot;</span>+ThreadLocalHolder.get();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalHolder</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 作为类ThreadLocalHolder的静态变量，并指定初始值的生成方式</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ThreadLocal&lt;Integer&gt; value=<span class="keyword">new</span> ThreadLocal&lt;Integer&gt;()&#123;</span><br><span class="line">        <span class="keyword">private</span> Random rand=<span class="keyword">new</span> Random(<span class="number">10</span>);</span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> Integer <span class="title">initialValue</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="keyword">return</span> rand.nextInt(<span class="number">10000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">increment</span><span class="params">()</span></span>&#123;</span><br><span class="line">        value.set(value.get()+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">get</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> value.get();&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建5个线程，每个线程都有ThreadLocalHolder的一个副本，且独立计数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        ExecutorService exec= Executors.newCachedThreadPool();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++)&#123;</span><br><span class="line">            exec.execute(<span class="keyword">new</span> Accessor(i));</span><br><span class="line">        &#125;</span><br><span class="line">        TimeUnit.SECONDS.sleep(<span class="number">3</span>);</span><br><span class="line">        exec.shutdownNow();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：可以看到5个线程实现自己内部的独立自增计数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#0:7114</span><br><span class="line">#1:2381</span><br><span class="line">#2:7294</span><br><span class="line">#3:3291</span><br><span class="line">#4:5247</span><br><span class="line">#0:7115</span><br><span class="line">#1:2382</span><br><span class="line">#2:7295</span><br><span class="line">#3:3292</span><br><span class="line">#4:5248</span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<p>这里也可以引出另外一个问题，ThreadLocal变量放在类中使用时，一般作为类的静态字段使用，为何？</p>
<p>其实很好理解，类的静态变量确保在类的多次实例化后仍然保持在内存中仅有一份副本，或者说为了避免重复创建thread specific object（与线程相关的变量），例如ThreadLocal变量管理了一个Session对象，那么当然希望在同一个线程中，此Session对象仅有一份实例，如果存在多份，那么就无法实现所谓“在同一session完成相关业务”的设计，导致逻辑出错。</p>
<h3 id="为何ThreadLocal没有直接采用ConcurrentHashMap这样的Map数据结构？"><a href="#为何ThreadLocal没有直接采用ConcurrentHashMap这样的Map数据结构？" class="headerlink" title="为何ThreadLocal没有直接采用ConcurrentHashMap这样的Map数据结构？"></a>为何ThreadLocal没有直接采用ConcurrentHashMap这样的Map数据结构？</h3><ol>
<li><p>首先，如果ThreadLocal使用ConcurrentHashMap来达到key-value管理目的，那么是无法实现“线程本地变量即：每个线程持有自己的本地实例”这样的需求，因此对于<code>Josh Bloch and Doug Lea</code>来说，需要给ThreadLocal设计全新一套的数据结构及其一些算法细节，以打造出可以支持和实现“线程本地变量且不需要基于任何锁的支持即可实现线程隔离”功能的数据结构，这显然是非常创新的工作，虽然ConcurrentHashMap的源代码设计已经堪称十分优秀。</p>
</li>
<li><p>其次，既然不采用ConcurrentHashMap这样内部复杂设计的Map结构，那么就要设计出非常高效、简约的数据结构，因此设计了底层只有一个数组table的ThreadLocalMap，不再有什么单链表、红黑树等结构，采用开放寻址法解决hash冲突，同时，只基于数组实现相关逻辑的代码会变得更加直观、简单，例如在扩容、清理stale entry方面，仅需基于数组的前后线性遍历即可。</p>
</li>
<li><p>ThreadLocal底层只基于一个数组table，结合设计特定的hash魔数，可以使得Entry的hash在数组中分散很均匀，大大降低了冲突概率，提高查询效率。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）</title>
    <url>/2020/01/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%BC%82%E6%AD%A5IO%E7%9A%84%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91%E2%80%94%E2%80%94IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%EF%BC%88select%E3%80%81poll%E3%80%81epoll%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面两篇文章<a href="https://blog.csdn.net/pysense/article/details/103721630">《gevent与协程》</a>和<a href="https://blog.csdn.net/pysense/article/details/103745410">《asyncio与协程》</a>，讨论了有关协程异步编程方面的内容，从代码层面和基本的demo可以大致理解协程的工作方式。如果要深入理解为何单线程基于事件的驱动可以在“低能耗”的条件下达到高性能的IO服务，则要研究Linux底层实现原理——IO多路复用，而理解IO多路复用的前提是对文件描述符有较为深入的理解，因此本文把文件描述符和IO多路复用放在同一篇文章里，形成全局的体系化认知，这就是本文讨论的内容。</p>
<a id="more"></a>
<h4 id="1、理解文件描述符"><a href="#1、理解文件描述符" class="headerlink" title="1、理解文件描述符"></a>1、理解文件描述符</h4><h5 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h5><p>&#8195;&#8195;在Linux中，一切皆文件，而理解文件描述符才能理解“一切皆文件”的真实含义，IO多路复用的select、poll和epoll机制正是通过操作文件描述符集合来处理IO事件。<br>含义，这里引用百度的介绍：</p>
<blockquote>
<p>&#8195;&#8195;文件描述符是一个索引号，是一个非负整数，它指向普通的文件或者I/O设备，它是连接用户空间和内核空间纽带。在linux系统上<a href="https://baike.baidu.com/item/内核/108410">内核</a>（kernel）利用文件描述符（file descriptor）来访问文件。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要使用文件描述符来指定待读写的文件。（在Windows系统上，文件描述符被称作文件句柄）</p>
</blockquote>
<p>当你看完本篇内容后，再回它这段解释，总结得真到位！在后面会给出为何文件描述符是一个非负整数，而不是其他更为复杂数据结构呢（例如hash map、list、链表等）？</p>
<h5 id="1-2-打开一个文件"><a href="#1-2-打开一个文件" class="headerlink" title="1.2 打开一个文件"></a>1.2 打开一个文件</h5><p>&#8195;&#8195;当某个进程打开一个已有文件或创建一个新文件时，内核向该进程返回一个文件描述符（一个非负整数）。<br>(在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于<a href="https://baike.baidu.com/item/UNIX">UNIX</a>、<a href="https://baike.baidu.com/item/Linux">Linux</a>这样的操作系统。)<br>这里以打开的iPython shell进程调用os.open为例，OS是Centos7.5<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> os</span><br><span class="line">In [<span class="number">6</span>]: fd = os.<span class="built_in">open</span>( <span class="string">&quot;/opt/test.txt&quot;</span>, os.O_RDWR|os.O_CREAT) <span class="comment"># os.O_RDWR读写模式打开，os.O_CREAT若文件不存在则创建               </span></span><br><span class="line">In [<span class="number">7</span>]: fd                                                                             </span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">17</span> <span class="comment"># 这个17就是file descriptor</span></span><br></pre></td></tr></table></figure><br>在Python里面，os.open方法返回文件描述符是更为底层API，而open方法是返回python文件对象，是更贴近用户的API。</p>
<p>在linux系统上查看以上iPython进程打开的所有文件描述符示例：（这里就是一个文件描述表的大致形式，每一个文件描述符指向一个文件或者设备）<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ll /proc/11622/fd #11622为ipython的shell进程</span><br><span class="line">total 0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 0 -&gt; /dev/pts/0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 1 -&gt; /dev/pts/0</span><br><span class="line">lr-x------ 1 root root 64 **** 16:43 10 -&gt; pipe:[41268]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 11 -&gt; pipe:[41268]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 12 -&gt; anon_inode:[eventpoll]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 13 -&gt; socket:[41269]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 14 -&gt; socket:[41270]</span><br><span class="line">lr-x------ 1 root root 64 **** 16:43 15 -&gt; pipe:[41271]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 16 -&gt; pipe:[41271]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 17 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 18 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 19 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 2 -&gt; /dev/pts/0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 20 -&gt; anon_inode:[eventpoll]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 3 -&gt; /dev/null</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 4 -&gt; /root/.ipython/profile_default/history.sqlite</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 5 -&gt; /root/.ipython/profile_default/history.sqlite</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 6 -&gt; anon_inode:[eventpoll]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 7 -&gt; socket:[41266]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 8 -&gt; socket:[41267]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 9 -&gt; anon_inode:[eventpoll]</span><br></pre></td></tr></table></figure></p>
<p>因为在ipython里面，<code>fd = os.open( &quot;/opt/test.txt&quot;, os.O_RDWR)</code> 运行3次，也就文件<code>/opt/test.txt</code>打开3次，所以返回个文件描述符:17、18、19（从这里说明，同一进程可以同一时刻打开同一文件多次）</p>
<p>11622进程号指向当前iPython shell，查看它打开的文件描述符18，指向被打开文件：<code>/opt/test.txt</code>：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ll /proc/11622/fd/18 </span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 /proc/11622/fd/18 -&gt; /opt/test.txt</span><br></pre></td></tr></table></figure><br>关闭文件描述符就关闭了所打开的文件<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">In [14]: os.close(19)                                                                  </span><br><span class="line">In [15]: os.close(18)                                                                  </span><br><span class="line">In [16]: os.close(17)</span><br></pre></td></tr></table></figure></p>
<h5 id="1-3-对文件描述符进行读写"><a href="#1-3-对文件描述符进行读写" class="headerlink" title="1.3 对文件描述符进行读写"></a>1.3 对文件描述符进行读写</h5><p>读：通过给定文件描述符读文件内容<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># os.read()方法的docstring</span></span><br><span class="line"><span class="string">os.read()</span></span><br><span class="line"><span class="string">Signature: os.read(fd, length, /)</span></span><br><span class="line"><span class="string">Docstring: Read from a file descriptor.  Returns a bytes object.</span></span><br><span class="line"><span class="string">Type:      builtin_function_or_method</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd=os.<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,os.O_RDWR|os.O_CREAT)</span><br><span class="line">data=os.read(fd,<span class="number">64</span>) <span class="comment">#指定读文件前64byte内容 </span></span><br><span class="line">print(data) <span class="comment"># b&#x27;foo\nbar\n\n&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>写：通过给定文件描述符将数据写入到文件<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># os.read()方法的docstring</span></span><br><span class="line"><span class="string">Signature: os.write(fd, data, /)</span></span><br><span class="line"><span class="string">Docstring: Write a bytes object to a file descriptor.</span></span><br><span class="line"><span class="string">Type:      builtin_function_or_method</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd=os.<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,os.O_RDWR|os.O_CREAT)</span><br><span class="line">byte_nums=os.write(fd,<span class="string">b&#x27;save data by file descriptor directly \n&#x27;</span>) <span class="comment"># 注意要写入byte类型的数据</span></span><br><span class="line">print(byte_nums) <span class="comment"># 返回写入byte字符串长度（字符个数）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>了解基本调用底层的os读写文件描述符的方法，也可以封装出一个类似内建open方法的定制myopen类。</p>
<h5 id="1-4-通过管道打开文件描述符"><a href="#1-4-通过管道打开文件描述符" class="headerlink" title="1.4 通过管道打开文件描述符"></a>1.4 通过管道打开文件描述符</h5><p>也可以通过管道pipe方法（创建一个无名管道）同时打开一个读文件描述符以及一个写文件描述符。（有关管道的定义和理解本文不再累赘，可参考其他博文。）<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd_read,fd_write=os.pipe()</span><br><span class="line">print(<span class="string">&#x27;fd_read:&#x27;</span>,fd_read,<span class="string">&#x27;fd_write:&#x27;</span>,fd_write) <span class="comment">#系统返回两个整数3、4， fd_read: 3 fd_write: 4</span></span><br><span class="line">os.write(fd_write,<span class="string">b&#x27;foo&#x27;</span>) <span class="comment"># 向管道的写端写入数据</span></span><br><span class="line">os.read(fd_read,<span class="number">64</span>) <span class="comment"># 从管道的读端读取数据</span></span><br></pre></td></tr></table></figure><br>创建管道时总是返回相邻的两个整数，因为stderr为2，故之后创建的文件描述符只能从3开始，示意图如下：<br><img src="https://img-blog.csdnimg.cn/20200105162510770.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如果尝试向管道另外一端的fd_write描述符读取数据，就会报错，所以对于管道，读数据只能在读文件描述符上读操作，写入数据只能在写文件描述符操作。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">os.read(fd_write,64)</span><br><span class="line">OSError: [Errno 9] Bad file descriptor</span><br></pre></td></tr></table></figure>
<p>如果已经把fd_read读取完好后，此时管道为空，若再读取该管道，进程会被阻塞，因为写管道端没有数据写入，这是管道的性质之一——数据一旦被读走，便不在管道中存在，若此时还继续向读端反复读取，则进程会被阻塞。</p>
<p>注意写入管道的字符个数是有限制的，当超过管道容量时，写入操作被阻塞，可以通过以下方法精确策略出<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pipe_capacity</span>(<span class="params">size</span>):</span></span><br><span class="line">    fd_read,fd_write=os.pipe()</span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    print(<span class="string">&quot;start to count&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,size+<span class="number">1</span>):</span><br><span class="line">        os.write(fd_write,<span class="string">b&#x27;a&#x27;</span>)</span><br><span class="line">        total=i</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">&quot;end to count,total bytes:&quot;</span>,total)</span><br><span class="line"></span><br><span class="line">get_pipe_capacity(<span class="number">64</span>*<span class="number">1024</span>)</span><br><span class="line">输出：</span><br><span class="line">start to count</span><br><span class="line">end to count,total <span class="built_in">bytes</span>: <span class="number">65536</span></span><br></pre></td></tr></table></figure></p>
<p>往管道写入<code>64*1024</code> 大小的byte时，管道写端未发生阻塞，当把写入的byte数改为：写入<code>64*1024+1</code>时，写入操作被阻塞了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_pipe_capacity(<span class="number">64</span>*<span class="number">1024</span>+<span class="number">1</span>)</span><br><span class="line">输出:</span><br><span class="line">start to count <span class="comment"># 执行流被阻塞，无后续输出。</span></span><br></pre></td></tr></table></figure>
<p>通过该方法可以精确测量出pipe默认容量为64KB。<br>看到这部内容，是否有人联想到在使用subprocess执行某些cmd命令后，一直卡在读取输出上？<br>常见用法：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p= subprocess.Popen(your_cmd, shell= <span class="literal">True</span>, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) <span class="comment"># 问题出现在：标准输出使用了管道，而管道有容量限制，当命令返回的数据大小超过管道64KB时，执行流卡在这里</span></span><br><span class="line">bytes_result, err = p.communicate(timeout=<span class="number">1</span>)</span><br><span class="line">str_result = <span class="built_in">str</span>(bytes_result, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> p.returncode != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> str_result:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><br>问题出现在：subprocess.Popen标准输出使用了管道，而管道有容量限制，当你的your_cmd返回的数据大小超过管道64KB时（stdout获取返回数据，用了管道存放），执行流卡在subprocess.Popen这里，其实进程阻塞了。<br>既然知道管道有容量限制，那么可以将stdout定向到本地文件系统，那么输出的数据就存放到容量更大的文件，建议使用临时文件作为重定向输出，如下所示：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stdout_by_tempfile</span>():</span></span><br><span class="line">    <span class="comment"># SpooledTemporaryFile也是一个普通文件对象，当然支持with协议（它的源码实现了__enter__和__exit__方法）</span></span><br><span class="line">    <span class="keyword">with</span> tempfile.SpooledTemporaryFile(buffering=<span class="number">1</span>*<span class="number">1024</span>) <span class="keyword">as</span> tf:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个临时文件对象，注意这个bufferfing不是限制只能存储1024字节的数据，而是输出内容超过1024字节后，自动将输出的数据缓存到临时文件里&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            fd = tf.fileno() <span class="comment"># 返回文件描述符，这个文件描述符不再指向管道，而是指向某个临时文件</span></span><br><span class="line">            p = subprocess.Popen(your_cmd,stdin=fd,stdout=fd,stderr=fd,shell=<span class="literal">True</span>) <span class="comment"># 将stdout输出的内容定向到文件描述符指向的文件</span></span><br><span class="line">            p.communicate(timeout=<span class="number">5</span>) <span class="comment"># 指定输出超时时间</span></span><br><span class="line">            tf.seek(<span class="number">0</span>) <span class="comment"># 将文件对象指针放置起始位置，以便读取从头到尾的完整的已存数据</span></span><br><span class="line">            output_data = tf.readlines() <span class="comment"># 一次读取临时文件的所有数据（这是读取的是byte类型），也可用迭代器（如果数据上几百M）这里tf就是普通文件对象，因为也有readlines、readline、write、tell等常见文件操作方法</span></span><br><span class="line">            save_to_db(output_data) <span class="comment"># 将your_cmd输出的数据存到db或者其他地方</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><br>经过对文件描述符的讨论，现在你可以轻松“操作进程的stdin或者stdout”</p>
<h5 id="1-5-常见的文件描述符0、1、2"><a href="#1-5-常见的文件描述符0、1、2" class="headerlink" title="1.5 常见的文件描述符0、1、2"></a>1.5 常见的文件描述符0、1、2</h5><p>&#8195;&#8195;在Linux系统上，每个进程都有属于自己的stdin、stdout、stderr。标准输入（standard input）的文件描述符是 0，标准输出（standard output）是 1，标准错误（standard error）是 2。尽管这种习惯并非<a href="https://baike.baidu.com/item/Unix">Unix</a>内核的特性，但是因为一些 shell 和很多应用程序都使用这种习惯，因此，如果内核不遵循这种习惯的话，很多应用程序将不能使用。</p>
<p>&#8195;&#8195;在上面的iPython例子中，ll /proc/11622/fd 其实是列出属于11622进程的文件描述符表，可以看到，每个进程拥有的fd数值从0到linux限制的最大值，其中每个进程自己的0、1、2就是用于当前进程的标准输入、标准输出和标准错误。</p>
<blockquote>
<p>关于文件描述符表简单介绍：操作系统内核为每个进程在u_block结构中维护文件描述符表，所有属于该进程的文件描述符都在该表中建立索引:数值—&gt;某个文件。你可以把整个文件描述表看成是一个C语言的数组，数组的元素指向文件引用，数组的下表就是它的文件描述符</p>
</blockquote>
<p>下面，已iPython的一个shell进程为例，如何将stdin、stdout、stderr的0、1、2替换为其他文件描述符：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 也可以用sys模块，例如：sys.stdout.fileno() </span></span><br><span class="line">In [<span class="number">4</span>]: stdin_fd=os.sys.stdin.fileno() <span class="comment"># 当前iPython shell进程的标准输入</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: stdin_fd</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: stdout_fd=os.sys.stdout.fileno() <span class="comment">#当前iPython shell进程的标准输出</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: stdout_fd</span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: stderr_fd=os.sys.stderr.fileno() <span class="comment">#当前iPython shell进程的标准错误</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: stderr_fd</span><br><span class="line">Out[<span class="number">9</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>os.sys.stdin等是什么呢？其实这些对象跟open(file_name,mode) 打开文件返回的文件对象是一样的，例如下面：os.sys.stdin是以utf-8编码的只读模式的文件对象，os.sys.stdout以及os.sys.stderr是以utf-8编码的写模式的文件对象，既然是文件对象，那么读对象就支持read、readline等方法，写对象则支持write等方法<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">21</span>]: os.sys.stdin</span><br><span class="line">Out[<span class="number">21</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stdin&gt;&#x27;</span> mode=<span class="string">&#x27;r&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: os.sys.stdout</span><br><span class="line">Out[<span class="number">22</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stdout&gt;&#x27;</span> mode=<span class="string">&#x27;w&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: os.sys.stderr</span><br><span class="line">Out[<span class="number">23</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stderr&gt;&#x27;</span> mode=<span class="string">&#x27;w&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br></pre></td></tr></table></figure><br>将文件描述符2：stdout替换为其他打开某个文件的文件描述符：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">2</span>]: f=<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">In [<span class="number">3</span>]: f.fileno()</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">11</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: os.sys.stdout=f</span><br><span class="line">In [<span class="number">5</span>]: os.sys.stdout.fileno() <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br><span class="line">In [<span class="number">7</span>]: print(<span class="string">&#x27;stdout redirect to file&#x27;</span>) <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br><span class="line">In [<span class="number">8</span>]: os.sys.stdout <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br></pre></td></tr></table></figure><br>查看/opt/test.txt文件内容<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line"> % cat test.txt</span><br><span class="line"></span><br><span class="line">11</span><br><span class="line"></span><br><span class="line">stdout redirect to file</span><br><span class="line"></span><br><span class="line">&lt;_io.TextIOWrapper name=&#x27;/opt/test.txt&#x27; mode=&#x27;w&#x27; encoding=&#x27;UTF-8&#x27;&gt;</span><br></pre></td></tr></table></figure><br>可以看到当前进程os.sys.stdout标准输出不再是2，而是11，指向某个已打开的文件。<br>当拿到一个已知的文件描述符后（一个非负整数），那么可以调用os.write(fd,bstr)方法向fd指向的文件写入数据，例如向文件描述符为11写入b’foo’字符串<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">11</span>]: os.write(<span class="number">11</span>,<span class="string">b&#x27;foo\n&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>查看文件/opt/test.txt内容：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">% cat test.txt</span><br><span class="line"></span><br><span class="line">11</span><br><span class="line"></span><br><span class="line">stdout redirect to file</span><br><span class="line"></span><br><span class="line">&lt;_io.TextIOWrapper name=&#x27;/opt/test.txt&#x27; mode=&#x27;w&#x27; encoding=&#x27;UTF-8&#x27;&gt;</span><br><span class="line">foo</span><br><span class="line">3</span><br></pre></td></tr></table></figure><br>==这一小节内容是想表述这么一个逻辑：如果要进程要对文件写入数据、或者读取数据（这不就是IO吗），底层必须通过文件描述符来实现，这就为讨论IO多路复用提供很好的知识背景，因为IO多路复用就是涉及到client向server写入数据，或者从server读取数据的需求。==</p>
<h5 id="1-6-进程打开文件描述符的个数"><a href="#1-6-进程打开文件描述符的个数" class="headerlink" title="1.6 进程打开文件描述符的个数"></a>1.6 进程打开文件描述符的个数</h5><p>&#8195;&#8195;文件描述符的有效范围是 0 到 OPEN_MAX。centos7.5默认每个进程最多可以打开 1024个文件（0 -1023）。对于 FreeBSD 、Mac OS X  和 Solaris 来说，每个进程最多可以打开文件的多少取决于<a href="https://baike.baidu.com/item/系统内存">系统内存</a>的大小，int 的大小，以及系统管理员设定的限制。Linux 2.4.22 强制规定最多不能超过 1,048,576 。</p>
<p>调整文件描述符打开数量的限制：</p>
<p>管理用户可以在etc/security/limits.conf配置文件中设置他们的文件描述符极限，如下例所示。<br><code>softnofile 10240</code><br><code>hardnofile 20480</code><br>系统级文件描述符极限还可以通过将以下三行添加到/etc/rc.d/rc.local启动脚本中来设置：<br><figure class="highlight text"><table><tr><td class="code"><pre><span class="line">\#Increasesystem-widefiledescriptorlimit.</span><br><span class="line">echo4096&gt;/proc/sys/fs/file-max</span><br><span class="line">echo16384&gt;/proc/sys/fs/inode-max</span><br></pre></td></tr></table></figure><br>在一些基于IO事件实现的高性能中间件例如redis、nginx、gevent等，在其官方的调优教程，一般会建议将系统打开文件描述符的数量设为大值，以便发挥并发性能。</p>
<h5 id="1-7-文件描述符底层原理"><a href="#1-7-文件描述符底层原理" class="headerlink" title="1.7 文件描述符底层原理"></a>1.7 文件描述符底层原理</h5><p>&#8195;&#8195;之所以将文件描述符的底层原理放在本节最后讨论，是考虑到，当前面的内容你已经理解后，那么再讨论背后原理，将更容易理解。<br>&#8195;&#8195;总结1.2~1.6的内容：</p>
<ul>
<li>进程只有拿到文件描述符才能向它指向的物理文件写入数据或者读取数据，然后再把这些数据用socket方式（通过网卡）远程传输给client。</li>
<li>文件描述符就是操作系统为了高效管理已打开文件所创建的一个索引。给os.wirte传入fd，进程非常迅速通过fd找到已打开的文件，进程高效率了，作为操作系统当然也更高效管理这些进程。</li>
</ul>
<p>&#8195;&#8195;那么不禁会提问：为什么进程只有拿到文件描述符才能向它指向的物理文件写入数据或者读取数据？本节内容回答此问题，相关图或者表述参考这些文章：<a href="https://blog.csdn.net/qq_28114615/article/details/94590598">《Linux中文件描述符的理解(文件描述符、文件表项、i-node)》</a>（推荐这篇文章，作者从源码的角度解析fd的理解）、<a href="https://blog.csdn.net/wan13141/article/details/89433379">《Linux文件描述符到底是什么？》</a></p>
<p>&#8195;&#8195;基本知识背景：理解数组、指针、结构体以及内存，c语言的结构体像Python的类，都是为了封装属性和方法，形成一个“具备多个功能”的object。<br><strong>原理</strong><br>&#8195;&#8195;一个 Linux 进程启动后，它在内核中每一个打开的文件都需要由3种数据结构支撑运行：</p>
<ul>
<li><p>每个进程对应一张打开文件描述符表，属于进程级的数据结构，进程通过调用系统IO方法（传入文件描述符）访问文件数据（用户态切到内核态）；</p>
</li>
<li><p>内核维持一张打开文件表，文件表由多个文件表项组成，属于系统级数据结构，该文件表创建者和管理由内核负责，每个进程可共享；</p>
</li>
<li><p>每个打开的文件对应一个i-node数据结构，系统通过i-node可以取到位于磁盘的数据（用于返回给用户态，内核态切回用户态），存在于内核中。<br>（机智的小伙伴应该联想到这个技术点：为何用户程序读取文件数据，会出现用户态到内核态切换，然后再由内核态转到用户态？上面3个表可以回答这个问题）</p>
</li>
</ul>
<p>三者的关系图如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20200105184452579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>文件描述符表</strong><br>&#8195;&#8195;在Linux中，对于每一个进程，都会分配一个PCB（进程控制块——Processing Control Block），在C代码实现上，这个数据结构名为<code>task_struct</code>，它里面有一个成员变量<code>*files</code>(属于files_struct类型)，<code>files_struct</code>的指针又指向一个<code>指针数组fd_array</code>，数组每一个元素都是一个指向<code>file类型的指针</code>，该进程打开的每个文件都属于file类型。从这里得出：<br>所谓文件描述符，就是fd_array[NR_OPEN_DEFAULT]这个指针数组的索引号，这也回答了为何文件描述符为非负整数。</p>
<p><code>task_struct类型--&gt;*files指针(files_struct类型)--&gt;fd_array(文件描述符表)</code><br>==task_struct类型的定义(省略部分代码)==：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> &#123;</span></span><br><span class="line">	......</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">files_struct</span> *<span class="title">files</span>;</span> </span><br><span class="line">	......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>==files_struct类型的定义(省略部分代码)==：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">files_struct</span> &#123;</span></span><br><span class="line">	......</span><br><span class="line">	<span class="keyword">int</span> next_fd; #进程新打开一个文件对应的文件描述符</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">file</span> __<span class="title">rcu</span> * <span class="title">fd_array</span>[<span class="title">NR_OPEN_DEFAULT</span>];</span> <span class="comment">//进程级打开文件描述符表</span></span><br><span class="line">	......	</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br><strong>系统级文件表</strong><br>&#8195;&#8195;每一个打开的文件都对应于一个file结构体（c语言上用结构体，而其他高级语言例如python或者java则称为<code>file类型</code>或者<code>file对象</code>），在该结构体中，f_flags描述了文件标志，f_pos描述了文件的偏移位置，而在<code>f_path中有含有一个指向一个inode结点的指针</code>，因此f_path非常关键，它直接指向物理文件存储的inode节点。<br>文件表指向逻辑大致如下：<br><code>file类型 --&gt; f_path变量（path类型 --&gt; *dentry指针（dentry类型）--&gt; d_inode指针（inode类型）</code></p>
<p>==file类型的定义(省略部分代码)==：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> &#123;</span></span><br><span class="line">	......	</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">path</span>		<span class="title">f_path</span>;</span>     <span class="comment">//属于path类型，包括目录项以及i-node</span></span><br><span class="line">	<span class="keyword">atomic_long_t</span>		f_count;  <span class="comment">//文件打开次数</span></span><br><span class="line">	<span class="keyword">fmode_t</span>			f_mode;   <span class="comment">//文件打开时的mode，对应于open函数的mode参数</span></span><br><span class="line">	......		</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>==path类型的定义(省略部分代码)==：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">path</span> &#123;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">vfsmount</span> *<span class="title">mnt</span>;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">dentry</span> *<span class="title">dentry</span>;</span><span class="comment">//目录项</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>==dentry类型的定义(省略部分代码)==：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">dentry</span> &#123;</span></span><br><span class="line">	......	</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">inode</span> *<span class="title">d_inode</span>;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">super_block</span> *<span class="title">d_sb</span>;</span>	<span class="comment">/* The root of the dentry tree *</span></span><br><span class="line"><span class="comment"> 	......	</span></span><br><span class="line"><span class="comment">&#125;;</span></span><br></pre></td></tr></table></figure><br>从以上“file类型嵌套链”可知：进程打开一个文件后，系统给它返回一个文件描述符fd，进程通过fd调用系统io方法，系统（内核）通过f_path再到dentry指针找到物理文件的inode，从而找到相应的数据块。</p>
<p><strong>系统级的文件i-node表</strong><br>&#8195;&#8195;继续上面内容，内核找到i-node节点后，就能获取文件数据块在磁盘上的位置以及文件大小等文件的元数据信息，使得进程能够根据已打开文件对应的文件描述符一路定位到磁盘上相应文件的位置，从而进行文件读写。<br>==inode类型的定义(省略部分代码)==：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">inode</span> &#123;</span></span><br><span class="line">    .......</span><br><span class="line">	<span class="keyword">umode_t</span>			i_mode;     <span class="comment">//权限</span></span><br><span class="line">	<span class="keyword">uid_t</span>			i_uid;      <span class="comment">//用户id</span></span><br><span class="line">	<span class="keyword">gid_t</span>			i_gid;      <span class="comment">//组id</span></span><br><span class="line">    .......</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">long</span>		i_ino;   <span class="comment">//inode节点号</span></span><br><span class="line">	<span class="keyword">loff_t</span>			i_size;   <span class="comment">//文件大小</span></span><br><span class="line">	.......</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>		<span class="title">i_atime</span>;</span>  <span class="comment">//最后一次访问(access)的时间</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>		<span class="title">i_mtime</span>;</span>  <span class="comment">//最后一次修改(modify)的时间</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>		<span class="title">i_ctime</span>;</span>  <span class="comment">//最后一次改变(change)的时间</span></span><br><span class="line">    .......	</span><br><span class="line">	<span class="keyword">blkcnt_t</span>		i_blocks;    <span class="comment">//块数</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">address_space</span>	*<span class="title">i_mapping</span>;</span>   <span class="comment">//块地址映射</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>上面有Linux文件常见的基本属性，例如访问时间、修改时间、权限、属主等，系统级的文件表里每一个文件表项都会指向i-node，这个i-node对应磁盘中的一个物理文件。</p>
<h5 id="本节小结"><a href="#本节小结" class="headerlink" title="本节小结"></a>本节小结</h5><p>&#8195;&#8195;到处，有关文件描述符的底层原理介绍完毕，本节内容也只是抛砖引玉，读者可自行去检索Linux文件系统原理或者VFS虚拟文件系统原理等底层文件系统知识(从这里联想到不得不佩服Apache开发hdfs文件系统的大咖团队，他们对Linux文件系统的底层实现应该且必须是绝对掌握的)。如果你能理解以上全部内容，那么在第2部分的IO多路复用中提到的大部分概念，将不再晦涩难懂。</p>
<h4 id="2、-IO多路复用原理"><a href="#2、-IO多路复用原理" class="headerlink" title="2、 IO多路复用原理"></a>2、 IO多路复用原理</h4><p>&#8195;&#8195;IO：input和output，一般指数据的写入、数据的读取。IO主要分为两类：硬盘 IO和网络IO，本内容主要针对网络 IO。复用的含义？复用当然理解为重复使用某个<code>事物</code>，而在本文，这个<code>事物</code>是一个线程，因此，IO多路复用，是指并发的socket连接复用一个IO线程(换句话说：只需要一个线程，即可为多个client同时提供socket连接请求)。在第1章节中，如果用户程序要将数据写入或者读取数据，那么它在底层必须通过文件描述符才能达到相应操作结果，因此IO多路复用与文件描述符密切相连，这就是为何在第一章节里给出了大量有关文件描述符知识的原因。</p>
<h5 id="2-1-IO触发用户空间与内核空间之间的切换"><a href="#2-1-IO触发用户空间与内核空间之间的切换" class="headerlink" title="2.1 IO触发用户空间与内核空间之间的切换"></a>2.1 IO触发用户空间与内核空间之间的切换</h5><p>在本博客前面有关大数据项目的文章里，其中<a href="https://blog.csdn.net/pysense/article/details/103301847">《深入理解kafka》</a>提到kafka通过通过sendfile（零拷贝机制）提高消费者端的吞吐量，其中就提到用户空间与内核空间之间的切换，结合第1章节内容简要介绍：<br><img src="https://img-blog.csdnimg.cn/2020010622220268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p>
<ul>
<li>用户程序通过系统调用获得网络和文件的数据</li>
<li>内核负责网络和文件数据的读写 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file_path=<span class="string">&#x27;/opt/test.txt&#x27;</span> <span class="comment"># 上下文为用户空间</span></span><br><span class="line">fd=os.<span class="built_in">open</span>(file_path,os.O_RDWR|os.O_CREAT) <span class="comment"># 用户空间切换到内核空间</span></span><br><span class="line">data=os.read(fd,<span class="number">64</span>) <span class="comment">#指定读文件前64byte内容  # 上下文从用户空间切换到内核空间，数据准备好后，上下文再从内核空间再切换到用户空间</span></span><br><span class="line">print(data) <span class="comment"># 上下文为用户空间</span></span><br></pre></td></tr></table></figure>
对于os.read的过程，用文件描述符的背景也可以理解：read底层用fd读取文件数据的流程：进程级文件描述符，到系统级文件表，再到系统级i-node表。从进程级到系统级，这里从代码层面展示用户空间到内核的空间的上下文切换<br>所以只要有网络IO或者磁盘IO，必然会发生用户空间到内核空间的上下文切换。</li>
</ul>
<p>图的原理参考<br><a href="https://www.cnblogs.com/yanguhung/p/10145755.html">https://www.cnblogs.com/yanguhung/p/10145755.html</a></p>
<h5 id="2-2-IO模型的介绍"><a href="#2-2-IO模型的介绍" class="headerlink" title="2.2 IO模型的介绍"></a>2.2 IO模型的介绍</h5><p><strong>IO模型基本分类</strong>：<br>（1）Blocking I/O（同步阻塞IO）：最常见也最传统IO模型，即代码语句按顺序执行若某一条语句执行需等待那么后面的代码会被阻塞，例如常见顺序步骤：读取文件、等待内核返回数据、拿到数据、处理输出<br>（2）同步非阻塞IO（Non-blocking IO）：默认创建的socket为阻塞型，将socket设置为NONBLOCK，业务流程则变为同步非阻塞IO<br>（3）IO多路复用（IO Multiplexing ）：即经典的Reactor设计模式，有时也称为异步阻塞IO，Java中的Selector和Linux中的epoll都是这种模型。<br>（4）异步IO（Asynchronous IO）：即经典的Proactor设计模式，也称为异步非阻塞IO<br>==这里也给出个人在知乎看到一篇关于IO模型更为形象的回答：<a href="https://www.zhihu.com/question/32163005">链接</a>==，通过购买火车票的场景来介绍5种IO模型（本章节未提到的信号驱动的IO模型）</p>
<p><a href="https://mp.weixin.qq.com/s/E3PYOSCuO4O6JB2FpHyZCg">https://mp.weixin.qq.com/s/E3PYOSCuO4O6JB2FpHyZCg</a></p>
<p><strong>同步和异步</strong><br>&#8195;&#8195;同步是指用户线程发起IO请求后需要等待或者轮询内核IO操作完成后才能继续执行；例如内核读文件需要耗时10秒，那么用户线程发起读取文件IO后，等待内核从磁盘拷贝到内存10秒，接着用户线程才能进行下一步对文件内容进行其他操作，按顺序执行。<br>&#8195;&#8195;而异步是指用户线程发起IO请求后仍继续执行，当内核IO操作完成后会通知用户线程，或者调用用户线程注册的回调函数。</p>
<p><strong>阻塞和非阻塞</strong><br>&#8195;&#8195;阻塞是指内核空间IO操作需要为把数据返回到用户空间；而非阻塞是指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成。</p>
<p>以下为四个模型的内容，图和部分文参考此篇<a href="https://www.cse.huji.ac.il/course/2004/com1/Exercises/Ex4/I.O.models.pdf">《英文原文文章》</a></p>
<h6 id="同步阻塞IO"><a href="#同步阻塞IO" class="headerlink" title="同步阻塞IO"></a>同步阻塞IO</h6><p>&#8195;&#8195;同步阻塞IO模型是最简单的IO模型，如图1所示：<img src="https://img-blog.csdnimg.cn/20200111121353731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;用户线程通过系统调用recvfrom方法向内核发起IO读文件操作（application switch to kernel），后面的代码被阻塞，用于线程处于等待当中，当内核已经从磁盘拿到数据并加载到内核空间，然后将数据拷贝到用户空间（kernel switch to application），用户线程再进行最后的data process数据处理。</p>
<p>同步阻塞IO模型的伪代码描述为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">read(socket, buffer) # 执行流被阻塞，直到buffer有数据可以读或者内核抛给用户程序一个error信号，程序才会往下执行。</span><br><span class="line">process(buffer) </span><br></pre></td></tr></table></figure><br>缺点分析：<br>&#8195;&#8195;用在多线程高并发场景（例如10万并发），服务端与客户端一对一连接，对于server端来说，将大量消耗内存和CPU资源（用户态到内核态的上下文切换），并发能力受限。</p>
<h6 id="同步非阻塞IO"><a href="#同步非阻塞IO" class="headerlink" title="同步非阻塞IO"></a>同步非阻塞IO</h6><p>&#8195;&#8195;同步非阻塞IO是在同步阻塞IO的基础上，将socket设置为NONBLOCK。这样做用户线程可以在发起IO请求后可以立即返回，原理图如下：<br><img src="https://img-blog.csdnimg.cn/20200111123715373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;在该图中，用户线程前面3次不断发起调用recvfrom，内核还未准备好数据，因此只能返回error of EWOULDBLOCK，直到最后一次调用recvfrom时，内核已经将数据拷贝到用户buffer端，此次可读取到数据，接下来就是process the data。</p>
<p>同步非阻塞IO模型的伪代码描述为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while true:</span><br><span class="line">        try:</span><br><span class="line">        	streaming_data&#x3D;read(buffer)</span><br><span class="line">    	 	do_someting(streaming_data)</span><br><span class="line">    		do_foo(streaming_data)	    </span><br><span class="line">    		do_bar(streaming_data)     </span><br><span class="line">	    except error of EWOULDBLOCK:</span><br><span class="line">	         print(&#39;kernel not ready for data yet,going to next loop&#39;)</span><br><span class="line">	         pass</span><br><span class="line">    	sleep(0.1)</span><br></pre></td></tr></table></figure>
<p>该模式有两个明显的缺点：</p>
<p>&#8195;&#8195;第一点：即client需要循环system call，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请求的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求。如果有10万个客户端连接，那么将消耗大量的serverCPU资源和占用带宽。</p>
<p>&#8195;&#8195;第二点：虽然设定了一个间隔时间去轮询，但也会发生一定响应延迟，因为每间隔一小段时间去轮询一次read操作，而任务可能在两次轮询之间的任意时间就已经完成，这会导致整体数据吞吐量的降低。</p>
<p>&#8195;&#8195;（以上的流程就像你在Starbucks店点了一杯cappuccino ，付款后，咖啡师正在制作中，而你却每隔0.1秒从座位走到点餐台问咖啡师OK了没，以至于你根本无法腾出时间享受<code>用一台MacBook Pro优雅的coding的下午茶美好时光</code>。当然如果仅有你1个人以这种方式去询问，咖啡师应该还可以接受（假设“客户是上帝这个真理“在Starbucks能够严格实施）。假设有10万个客户，都以这方式去轮询咖啡师，想象下画面…）</p>
<h6 id="IO多路复用模式"><a href="#IO多路复用模式" class="headerlink" title="IO多路复用模式"></a>IO多路复用模式</h6><p>&#8195;&#8195;前面两种模式缺点明显，那么 IO多路复用模式就是为了解决以上两种情况，IO多路复用是指内核一旦发现进程指定的一个或者多个IO事件准备读取，它就通知该进程，原理图如下：<br><img src="https://img-blog.csdnimg.cn/20200111144223563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;前面两种IO模型用户线程直接调用recvfrom来等待内核返回数据，而IO复用则通过调用select（还有poll或者epoll）系统方法，此时用户线程会阻塞在select语句处，等待内核copy数据到用户态，用户再收到内核返回可读的socket文件描述符，伪代码如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">while true:</span><br><span class="line">	all_fds=select()# 执行流在此处阻塞，当之前注册的socket文件描述符集合有其中的fd发生IO事件，内核会放回所有fds（注意：select不会返回具体发生IO事件的fd，需要用户线程自行查找）</span><br><span class="line">    for each_fd in all_fds:</span><br><span class="line">        if can_read(fd):  # 遍历内核返回每个socket文件描述符对象来判断到底是哪个流产生的IO事件。</span><br><span class="line">        	process_data(fd) # 找到了发生IO事件的文件描述符fd</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>此IO模型优点：<br>&#8195;&#8195;用户线程终于可以实现一个线程内同时发起和处理多个socket的IO请求，用户线程注册多个socket，（对于内核就是文件描述符集合），然后不断地调用select读取被激活的socket 文件描述符。（在这里，select看起就像是用户态和内核态之间的一个代理）<br>缺点在下文会谈到。</p>
<h6 id="IO多路复用适用场景："><a href="#IO多路复用适用场景：" class="headerlink" title="IO多路复用适用场景："></a>IO多路复用适用场景：</h6><p>&#8195;&#8195;从Redis、Nginx等这些强大的用于高并发网络访问的中间件可知，IO多路复用目前使用最突出的场景就是：socket连接，也即web服务，一般指高性能网络服务。<br>&#8195;&#8195;与多进程和多线程技术的简单粗暴的业务实现不同，I/O多路复用技术的最大优势是系统开销小，系统不必创建多进程或者多线程，也不必维护这些进程/线程的复杂上下文以及内存管理，从而大大减小了系统的开销，极大提升响应时间。</p>
<h4 id="3、深入理解select、poll"><a href="#3、深入理解select、poll" class="headerlink" title="3、深入理解select、poll"></a>3、深入理解select、poll</h4><p>&#8195;&#8195;上面第2节内容提到了IO多路复用的基本工作原理，目前linux支持I/O多路复用的系统调用常见有 select，poll，epoll（linux2.4内核前主要是select和poll，epoll方法则是从Linux 2.6内核引入），它们都是实现这么一个逻辑：一个进程可以监听多个文件描述符（10k-100k不等，看服务器性能），一旦某个文件描述符就绪（一般是读就绪或者写就绪），内核返回这些可读写的文件描述符给到用户线程，从而让用户线程进行相应的读写操作，这一过程支持并发请求。<br>下面就linux实现IO多路复用三种方式进行详细讨论：</p>
<h5 id="理解select函数"><a href="#理解select函数" class="headerlink" title="理解select函数"></a>理解select函数</h5><p>&#8195;&#8195;select：在一段时间内，监听用户线程感兴趣的文件描述符上面的可读、可写和异常等事件，在这里通过简单介绍其C接口的用法即可理解select功能，API：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/select.h&gt;</span></span></span><br><span class="line"><span class="keyword">int</span> select（<span class="keyword">int</span> nfds, fd_set * readfds, fd_set * writefds, fd_set * exceptfds, <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> * <span class="title">timeout</span>);</span></span><br></pre></td></tr></table></figure><br>函数参数解释，<a href="https://my.oschina.net/ijaychen/blog/184647">参考文章</a><br><code>nfds</code>：<br>&#8195;&#8195;非负整数的变量，表示当前线程打开的所有件文件描述符集的总数，nfds=maxfdp+1，计算方法就是当前线程打开的最大文件描述符+1</p>
<p><code>*readfds</code>:<br>&#8195;&#8195;fd_set集合类型的指针变量，表示当前线程接收到内核返回的可读事件文件描述符集合（有数据到了这个状态称之为读事件），如果这个集合中有一个文件可读，内核给select返回一个大于0的值，表示有文件可读，如果没有可读的文件，则根据timeout参数再判断是否超时，若内核阻塞当前线程的时长超出timeout，select返回0，若发生错误返回负值。传入NULL值，表示不关心任何文件的读变化</p>
<p><code>*writefd</code>:<br>&#8195;&#8195;当前有多少个写事件（关心输出缓冲区是否已满）<br>最后一个结构体表示每个几秒钟醒来做其他事件，用来设置select等待时间</p>
<p><code>*exceptfds</code>：<br>&#8195;&#8195;监视文件描述符集合中的有抛出异常的fd</p>
<p><code>timeout</code>：<br>&#8195;&#8195;select()的超时结束时间，它可以使select处于三种状态：<br>（1）若将NULL以形参传入，select置于阻塞状态，当前线程一直等到内核监视文件描述符集合中某个文件描述符发生变化为止；<br>（2）若将时间值设为0秒0毫秒，表示非阻塞，不管文件描述符是否有变化，都立刻返回继续执行，文件无变化返回0，有变化返回一个正值；<br>（3）timeout的值大于0，等待时长，即select在timeout时间内阻塞，超时后返回-1，否则在超时后不管怎样一定返回。</p>
<p><code>select函数返回值</code>：<br>&#8195;&#8195;执行成功则返回绪的文件描述符的总数。如果在超过时间内没有任何文件描述符准备就绪，将返回0；失败则返回-1并设置errno；若在select等待事件内程序接收到信号，则select立即返回-1，并设置errno为EINTER。<br>（从这里可以得出：写C的同学尤其是Unix 网络开发方向，对什么select、poll、epoll早已轻车熟路）</p>
<p><strong>select的优点</strong><br>&#8195;&#8195;select目前几乎在所有的平台上支持，其良好跨平台支持。</p>
<p> <strong>select的缺点</strong></p>
<p>（1）打开的文件描述符有最大值限制<br>&#8195;&#8195;默认1024，当然可自行设为较大值，例如10万，取决于服务器性内存和cpu配置。 </p>
<p>（2）对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。<br>&#8195;&#8195;当一个线程发起socket请求数较大时例如100，用户线程每次select()都会触发server端的内核遍历所有文件描述符，如果有1万个client发起这种IO请求，server的内核要遍历1万<em>100=100万的文件描述符。可想而知这种时间复杂度为o(n)是非常低效率的。<br>（3）第2点说了，当并发量大时，服务端提供server socket连接的进程需要维护一个用来存放大量fd的数据结构（参考1.7章节的内容：`task_struct类型—&gt;</em>files指针(files_struct类型)—&gt;fd_array(文件描述符表)`），会导致用户态和内核态之间在传递该数据结构时复制占用内存开销大。<br><a href="https://www.itnotebooks.com/?p=1106">https://www.itnotebooks.com/?p=1106</a></p>
<h5 id="理解poll函数"><a href="#理解poll函数" class="headerlink" title="理解poll函数"></a>理解poll函数</h5><p>本节内容部分参考<a href="https://blog.csdn.net/skypeng57/article/details/82743681">《poll函数解析》</a>，oll函数的定义：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">poll</span><span class="params">(struct pollfd *fds, <span class="keyword">nfds_t</span> nfds, <span class="keyword">int</span> timeout)</span></span>;</span><br></pre></td></tr></table></figure><br>pollfd类型定义：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span>&#123;</span></span><br><span class="line">　　<span class="keyword">int</span> fd;              <span class="comment">//文件描述符：socket或者其他输入设备的对应fd</span></span><br><span class="line">　　<span class="keyword">short</span> events;    <span class="comment">//用户向内核注册感兴趣的事件（读事件、写事件、异常事件）</span></span><br><span class="line">　　<span class="keyword">short</span> revents;   <span class="comment">//内核返回给用户注册的就绪事件</span></span><br><span class="line">　　&#125;;</span><br></pre></td></tr></table></figure><br>events有以下三大类：<br><img src="https://img-blog.csdnimg.cn/2020011122281639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">例如fd=10，events=POLLRDNORM<br>revents：返回用户在调用poll注册的感兴趣且已就绪的事件</p>
<p>参数说明：<br>pollfd类型的<code>*fds</code>变量：传入socket的文件描述符，用户线程通过fds[i].events注册感兴趣事件(可读、可写、异常)，<br><code>nfds</code>:<br>跟select的nfds参数相同<br><code>timeout</code>:<br>INFTIM:永远等待<br>0:立即返回，不阻塞<br>大于0:等待给定时长</p>
<p><code>函数返回值</code>：<br>成功时，poll() 返回结构体中 revents事件不为 0 的文件描述符个数；<br>如果在超时前没有任何事件发生，poll()返回 0；</p>
<p><code>工作流程</code><br>（1）pollfd初始化，传入socket的文件描述符，设置感兴趣事件event，以及内核revent。设置时间限制（用户线程通过fds[i].events传入感兴趣事件，内核通过修改fds[i].revents向用户线程返回已经就绪的事件）<br>（2）用户线程调用poll，并阻塞于此处<br>（3）内核返回就绪事件，并处理该事件</p>
<p>==select与poll本质差别不大，只是poll没有最大文件描述符的限制，因为它是基于链表来存储的==</p>
<p><code>poll缺点</code>：<br>（1）大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是否有意义。<br>（2）poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd</p>
<p>这里引用了这篇文章<a href="https://www.cnblogs.com/zhuwbox/p/4222382.html">《linux 下 poll 编程》</a>代码来说明poll流程，程序逻辑并不难理解，能够让poll返回就绪的事件，是内核驱动通过中断信号来判断事件是否发生：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line">......</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OPEN_MAX 1024</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd, sockfd, i, maxi;</span><br><span class="line">    <span class="keyword">char</span> buf[MAXLINE];</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> <span class="title">client</span>[<span class="title">OPEN_MAX</span>];</span><span class="comment">//存放客户端发来的所有socket对应的文件描述符，限定最大可用文件描述符1024</span></span><br><span class="line">    ......</span><br><span class="line">    client[<span class="number">0</span>].fd = listenfd;<span class="comment">// 传入socket对应的文件描述符</span></span><br><span class="line">    client[<span class="number">0</span>].events = POLLRDNORM;<span class="comment">//关心监听套机字的读事件</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(;;)</span><br><span class="line">    &#123;</span><br><span class="line">        nready = poll(client, maxi + <span class="number">1</span>, <span class="number">-1</span>); <span class="meta">#server 进程调用poll，用户线程在此处阻塞，直到内核返回就绪的POLLRDNORM事件的文件描述符集合</span></span><br><span class="line">        <span class="keyword">if</span>(client[<span class="number">0</span>].revents &amp; POLLRDNORM) # 如果收到内核返回client注册的事件</span><br><span class="line">        &#123;</span><br><span class="line">            connfd = accept(listenfd, (SA *) &amp;cliaddr, &amp;clilen); # 获取每个client socket连接对应的文件描述符</span><br><span class="line">            <span class="keyword">if</span>(connfd &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; OPEN_MAX; ++i)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(client[i].fd &lt; <span class="number">0</span>)</span><br><span class="line">                    client[i].fd = connfd; # 将客户端的请求的fd加到polled这个列表</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(i == OPEN_MAX)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;too many clients&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            client[i].events = POLLRDNORM;# 为客户端的fd注册可读事件</span><br><span class="line">            <span class="keyword">if</span>(i &gt; maxi)</span><br><span class="line">            &#123;</span><br><span class="line">                maxi = i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(--nready &lt;=<span class="number">0</span> )</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; OPEN_MAX; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>((sockfd = client[i].fd) &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(client[i].revents &amp; POLLRDNORM | POLLERR) <span class="meta"># server通过轮询所有的文件描述符，如果revents有读事件或者异常事件</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>((n = read(sockfd, buf, MAXLINE)) &lt; <span class="number">0</span>)# 读取数据</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(errno == ECONNRESET)</span><br><span class="line">                    &#123;</span><br><span class="line">                        close(sockfd); # 若该就绪的文件描述符返回的异常事件，则重置</span><br><span class="line">                        client[i].fd = <span class="number">-1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;read error!\n&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(n == <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    close(sockfd);</span><br><span class="line">                    client[i].fd = <span class="number">-1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    write(sockfd, buf,  n);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(--nready &lt;= <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;从上面看，不管select和poll都需要在返回后，都需要通过遍历文件描述符来获取已经就绪的socket（<code>for(i = 1; i &lt; OPEN_MAX; ++i)</code>==&gt;<code>if(client[i].revents &amp; POLLRDNORM | POLLERR)</code>）。事实上，高并发连接中例如10k个连接，在同一时刻可能只有小部分的socket fd处于就绪状态，但server端进程却为此不断的遍历，当注册的描述符数量的增长，其效率也会线性下降。<br>该图为select、poll和Epoll性能对比（还有一个更高性能的Kqueue）<br><img src="https://img-blog.csdnimg.cn/20200111174200198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到，随着socket连接数量增大（对应文件描述符数量也增加），select、poll处理响应更慢，epoll响应速度几乎不受文件描述符数量的影响。</p>
<h4 id="4、深入理解epoll"><a href="#4、深入理解epoll" class="headerlink" title="4、深入理解epoll"></a>4、深入理解epoll</h4><p>&#8195;&#8195;考虑到epoll为本文核心内容，而且知识相对更有深度，因此将其单独作为1个章节讨论。部分内容参考的以下文章（吸收多篇文章内容后，你会赞叹epoll设计的精巧）：<br><a href="https://blog.csdn.net/daaikuaichuan/article/details/83862311">《epoll原理详解及epoll反应堆模型》</a><br><a href="https://www.itnotebooks.com/?p=1106">《Linux下的I/O复用与epoll详解》</a><br><a href="https://blog.csdn.net/mango_song/article/details/42643971">《我读过最好的Epoll模型讲解》</a><br><a href="https://mp.weixin.qq.com/s/FRg_lSHDiZofzTZApU6z9Q">《 彻底搞懂epoll高效运行的原理 》</a></p>
<h5 id="4-1-epoll的c语言接口详解："><a href="#4-1-epoll的c语言接口详解：" class="headerlink" title="4.1 epoll的c语言接口详解："></a>4.1 epoll的c语言接口详解：</h5><p>这里介绍epoll的IO模型完成创建过程：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_create</span><span class="params">(<span class="keyword">int</span> size)</span></span>;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>功能说明：创建一个epoll实例，参数<code>size</code>用来指定该epoll对象可以管理的socket文件描述符的个数。在Linux 2.6.8以后的版本中，参数 size 已被忽略，但是必须大于0。</p>
</li>
<li><p>函数返回值：一个代表新创建的epoll实例的文件描述符epoll_fd，这个描述符由server端持有，用于<code>统管所有client请求的socket连接对应的文件描述符</code></p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_ctl</span><span class="params">(<span class="keyword">int</span> epfd, <span class="keyword">int</span> op, <span class="keyword">int</span> fd, struct epoll_event *event)</span></span>;  </span><br></pre></td></tr></table></figure>
<ul>
<li>epfd：epoll_create创建的epoll对象，以文件描述符形式epoll_fd返回，对于一个server进程，它对于只有一个epoll_fd。</li>
<li>op:监听socket时告诉内核要监听针对这个socket（其实是socket对应的文件描述符fd）什么类型的事件，主要有以下三种要监听的事件类型需要注册到epoll_fd上：<br>— EPOLL_CTL_ADD：注册新的fd到epfd中；<br>— EPOLL_CTL_MOD：修改已经注册的fd的监听事件；<br>— EPOLL_CTL_DEL：从epfd中删除一个fd；</li>
</ul>
<ul>
<li>fd：epfd需要监听的fd，主要指server为client的socket请求创建对应文件描述符，来一个socket连接就对应新建一个文件描述符，</li>
<li><p>epoll_event：告诉内核需要对所注册文件描述符的什么类型事件进行监听，和poll函数支持的事件（读、写、异常）类型基本相同，不同是epoll还可以监听两个额外的事件：EPOLLET和EPOLLONESHOT（水平触发和边缘触发），这是epoll高性能的关键，文章后面会深入讨论。epoll_event结构如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> &#123;</span></span><br><span class="line">  <span class="keyword">__uint32_t</span> events;  <span class="comment">/* Epoll的事件类型 */</span></span><br><span class="line">  <span class="keyword">epoll_data_t</span> data;  <span class="comment">/* User data variable */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>其中events就包括以下事件：</p>
</li>
<li><p>EPOLLIN ： 监听的文件描述符可读（包括client主动向server断开socket连接的事件）</p>
</li>
<li>EPOLLOUT：  监听的文件描述符可写；</li>
<li>EPOLLPRI： 监听的文件描述符有紧急的数据可读；</li>
<li>EPOLLERR： 监听的文件描述符有异常事件；</li>
<li>EPOLLRDHUP： 监听的文件描述对应的socket连接被挂断；这里挂断像不像打电话给对方，对方挂断你的电话的意思？没错，这里是指socket连接的client断开了TCP连接（TCP半开），此时epoll监听对应的socket文件描述符会触发一个EPOLLRDHUP事件。==（这里也需要给出一个知识：使用 2.6.17 之后版本内核，对端连接断开触发的 epoll 事件会包含 EPOLLIN | EPOLLRDHUP，即 0x2001。有了这个事件，对端断开连接的异常就可以在TCP层进行处理，无需到应用层处理，提高断开响应速度。）==<br>EPOLLET：将EPOLL设为边缘触发模式Edge Triggered，一般用法如下：<br><code>ev.events = EPOLLIN|EPOLLET; //监听读状态同时设置ET模式</code><br>如果要设为水平触发Level Triggered，只需：<br><code>ev.events = EPOLLIN //默认就是水平触发模式</code></li>
<li><p>EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket文件描述符，需要再次把这个socket加入到EPOLL队列里。</p>
</li>
<li><p>关于epoll_event类型的data用法如下：<br>定义一个变量ev，类型为struct epoll_event<br>ev.data.fd = 10;//将要监听的文件描述符绑定到ev.data.fd<br>ev.events = EPOLLIN|EPOLLET; //监听读状态同时设置ET模式</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_wait</span> <span class="params">( <span class="keyword">int</span> epfd, struct epoll_event* events, <span class="keyword">int</span> maxevents, <span class="keyword">int</span> timeout )</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>函数返回值：成功时返回有IO（或者异常）事件就绪的文件描述符的数量，如果在timeout时间内没有描述符准备好则返回0。出错时，epoll_wait()返回-1并且把errno设置为对应的值</p>
<ul>
<li>events：内核检测监听描述符发生了事件，内核将这些描述符的所有就绪事件以events数组返回给用户，。</li>
<li>maxevents：指定最多监听多少个事件类型</li>
</ul>
</li>
<li>timeout：指定epoll的超时时间，单位是毫秒。当timeout为-1是，epoll_wait调用将永远阻塞，直到某个时间发生。当timeout为0时，epoll_wait调用将立即返回。</li>
</ul>
<p>==为更好理解该epoll_wait，这里给个简单epoll工作流程说明：==</p>
<ul>
<li>1、假设socket server进程持有的epoll_fd为3，即<code>epoll_fd=epoll_create(size=1024);</code></li>
<li><p>2、假设现有2个client向server发起socket连接，server给它分配的文件描述符是4和5，并且注册的事件为：EPOLLIN可读事件，注册过程如下：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">ev.data.fd = 4</span><br><span class="line">ev.events = EPOLLIN</span><br><span class="line">epoll_ctl(epfd=4, op=EPOLL_CTL_ADD, fd=4, &amp;ev);  # 这里不是C语言的写法，只是为了方便说明原理，将关键字参数也列出来，用类似python的参数语法</span><br><span class="line"></span><br><span class="line">ev.data.fd = 5</span><br><span class="line">ev.events = EPOLLIN</span><br><span class="line">epoll_ctl(epfd=4, op=EPOLL_CTL_ADD, fd=5, &amp;ev);  </span><br></pre></td></tr></table></figure>
</li>
<li><p>3、假设现在文件描述符5可读事件触发（例如内核已经完成将data.log拷贝到用户空间）</p>
</li>
<li>4、调用epoll_wait(epfd=3, events,maxevents=10, timeout=-1)，返回1，表示当前内核告诉server进程有1个文件描述符发生了事件，这里events存放的是就绪文件描述符及其事件：<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">ready_fds=epoll_wait(epfd=3, events,maxevents=10, timeout=-1)</span><br><span class="line">for i in range(ready_fds）：</span><br><span class="line">	ev=events[i] //从内核返回的事件数组中取出epoll_event类型</span><br><span class="line">	print(ev.data.fd) //这里返回的就绪文件描述符是5，对应client的第二个socket连接</span><br><span class="line">	print(ev.evevts）//这里返回EPOLLIN可读事件</span><br><span class="line">	os.read(ev.data.fd)//读取文件描述符5指向的数据</span><br></pre></td></tr></table></figure>
==请重点关注第4点，这里可以初步回答epoll适合的场景以及为何epoll比select和poll更高效的原因：（这里说了是<code>初步</code>，第4.2和4.3节将给出更有深度的内容）<br>&#8195;&#8195;epoll适合的场景：适用于连接数量大且长连接，但活动连接较少的情况。如何解释？在上面的例子中，我们假设了2个client socket连接，现在，我们假设10万个socket连接，而当中”活跃“（就绪）的文件描述符只有100个，调用epoll_wait返回100，接着在for循环里面将非常快速处理完可读事件。==<br>&#8195;&#8195;也就是说，使用epoll的IO效率，不会随着socket连接数（文件描述符connect_fd数量）的增加而降低。因为内核只返回少量活跃就绪的fd才会被回调处理；<br>&#8195;&#8195;换句话说：epoll几乎跟外部连接总数无关，它只管“就绪”的连接，这就是Epoll的效率就会远远高于select和poll的原因。<br>&#8195;&#8195;现在大家应该可以理解第3节最后给出的Libevet Benchmark图：100个active条件下，连接的文件描述符从2500到15000，可以看到epoll高性能几乎保持稳定，因为它只需处理这固定的100个就绪的fds，而select和poll，要处理的是从2500个到15000个，因此处理时长也是线性增长，效率越来越低。</li>
</ul>
<p>==简单总结epoll3个方法即可完成高效且并发数高的文件描述符监听的基本流程==</p>
<ul>
<li>A、server端持有可统管所有socket文件描述符的唯一epoll_fd=epoll_create()</li>
<li>B 、epoll_ctl(epoll_fd，添加或者删除所有待监听socket文件描述符以及它们的事件类型)</li>
<li>C、返回有事件发生的就绪文件描述符 =epoll_wait(epoll_fd)</li>
</ul>
<h5 id="4-2-epoll用红黑树管理所有监听文件描述符"><a href="#4-2-epoll用红黑树管理所有监听文件描述符" class="headerlink" title="4.2 epoll用红黑树管理所有监听文件描述符"></a>4.2 epoll用红黑树管理所有监听文件描述符</h5><h6 id="用python设计一个伪epoll模型？"><a href="#用python设计一个伪epoll模型？" class="headerlink" title="用python设计一个伪epoll模型？"></a>用python设计一个伪epoll模型？</h6><p>&#8195;&#8195;在4.1介绍epoll的相关函数中，大家应该有这么一个<code>开发者直觉</code>：<br>每次有新的文件描述符，则将其注册到一个”由内核管理的数据结构“，而且还是向该数据结构添加、删除文件描述符感兴趣的事件。ok，既然提到这个数据结构可以<code>添加、删除</code>，这个直觉告诉我们，内核是否用一个类似python列表（或者链表）的方式来管理所有监听文件描述符呢（其中列表中每项用）？不妨假设epoll就是用python列表管理fds，来讨论”原版epoll“的设计：<br>1) epoll_create(size)是内核创建一个python列表<br>epoll_list= epoll_create(size=100) //全局list变量，用户进程持有，可监听100个外部文件描述符。<br>2) 假设现在有2个新的外部socket连接请求server，3个tcp的socket连接对应文件描述符为4、5、6，注册事件为EPOLLIN可读<br>注册过程如下<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoll_list= epoll_create(size=<span class="number">100</span>) </span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">	connect_fd=socket_obj.accept().fd</span><br><span class="line">	epoll_ctl(epoll_list, op=<span class="string">&#x27;EPOLL_CTL_ADD&#x27;</span>,&#123;<span class="string">&#x27;fd&#x27;</span>:connect_fd,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;)</span><br><span class="line">	</span><br></pre></td></tr></table></figure><br>op都是添加，说明是向epoll_list进行append操作</p>
<p>3) 由于用户空间持有epoll_list，若内核要处理epoll，需将epoll_list拷贝到内核空间，对于内核，它看到的epoll_list如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoll_list=[ </span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">6</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p>4) 假设现在文件描述符4、5可读，最终调用回调函数处理业务：<br><code>ready_fds_list=epoll_wait(epoll_list,events,maxevents=10, timeout=-1)</code># epoll_list又从内核空间拷贝到用户空间，当前仅有2个文件描述符，性能还ok，而且假设返回的是就绪文件描述符列表:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ready_fds_list=[ </span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">]</span><br><span class="line"><span class="keyword">for</span>  each_fd <span class="keyword">in</span> ready_fds_list:</span><br><span class="line">	callback(each_fd)</span><br></pre></td></tr></table></figure><br>经过上面4个步骤，我们貌似造出了一个可易于理解的、python版本的<code>epoll</code>。下面介绍<code>原版epoll设计</code></p>
<h6 id="真epoll设计"><a href="#真epoll设计" class="headerlink" title="真epoll设计"></a>真epoll设计</h6><p><strong>第一点</strong>：上面简陋python版本epoll，用列表来管理所有监听的文件描述符<br><code>epoll_list= epoll_create(size=100)</code><br>==真实设计==：<code>epoll_fd= epoll_create(size=100)</code>，Linux用一个特殊文件描述符，并创建了eventpoll实例，eventpoll里面指向了一个<code>红黑树</code>，这棵树就是用于存放所有监听的文件描述符及其事件。</p>
<p><strong>第二点</strong>：上面简陋python版本epoll，用列表来管理所有监听的文件描述符，每次有新的socket连接，注册fd以及获取活动fd都会发生用户态到内核态、内核态到用户态切换：拷贝的对象——epoll_list。假设当前服务器有10万个socket连接请求，那么将发生10万次用户态到内核态切换，以及10万次内核态到用户态的切换，显然效率极低。<br>==真实设计==：第一点说了，用epoll_fd指向一颗存放在内核空间的红黑树，如何避免用户态和内核态频繁切换？Linux用mmap()函数解决。mmap将用户空间的一块地址和内核空间的一块地址同时映射到同一块物理内存地址，使得这块物理内存对用户进程和内核均可访问，砍掉用户态和内核态切换的环节（注意区别zero copy）。也就是说内核拿到epoll_fd后，可直接操作epoll_fd指向的红黑树上存放的所有被监听的文件描述符，妙了！<br>==这里说的epoll_fd是如何实现在底层指向一个红黑树呢?==<br>用户进程调用：epoll_fd=epoll_create(size)时，用户进程mmap映射的一块物理地址上就创建一个eventpoll结构体对象，该对象包含一个红黑树的根节点，从而实现epoll_fd由此至终都指向这颗红黑树（内核也可直接访问）：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventpoll</span>&#123;</span>  </span><br><span class="line">    ....  </span><br><span class="line">    <span class="comment">/*红黑树的根节点，这颗树的节点存储着epoll_fd所有要监听的文件描述符及该文件描述符关联的其他属性*/</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rb_root</span>  <span class="title">rbr</span>;</span>  </span><br><span class="line">    <span class="comment">/*双链表中则存放着将要通过epoll_wait返回给用户的满足条件文件描述符、事件类型*/</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">rdlist</span>;</span>  </span><br><span class="line">    <span class="comment">/*还有其他成员变量，这里省略了，因为我们更关注存放就绪事件链表和存放被监听的所有文件描述符的红黑树 */</span></span><br><span class="line">    ....  </span><br><span class="line">&#125;;  </span><br></pre></td></tr></table></figure><br>&#8195;&#8195;上面的rdllist是一个双向链表，用于存储就绪事件的文描述符。用户进程代码执行epoll_wait时，执行流阻塞（在底层，是内核让进程休眠），直到监听的文件描述符有中断事件发生，内核将这些就绪文件描述符添加到rdlist里面，最后返回用户的是events数组，数组包含就绪的fd及其事件类型。</p>
<p><strong>第三点</strong>：用列表存放大量项，但需要进行增或者删除操作时，列表时间复杂度为<br>&#8195;&#8195;时间复杂度O(N)，想象下10万个连接的时间复杂度<br>==真实设计==：Linux为epoll设计了一个红黑树的数据结构，当调用epoll_ctl函数用于添加或者删除一个文件描述符时，对应内核而已，都是在红黑树的节点去处理，而红黑树本身插入和删除性能比列表高，时间复杂度O(logN)，N为树的高度，太巧妙了。<br>&#8195;&#8195;这个红黑树上的节点存放什么数据呢？每个节点称为epoll item结构，里面的组成如下：<br>fd:被epoll对象监听的文件描述符（client发起的socket连接对应的文件描述符）<br>event：要监听该fd的就绪事件，例如前面说的EPOLLIN可读事件<br>file:在1.6章节提到的系统文件表中，一个文件描述符对应系统文件表中一个文件项，这个文件项就是file类型，用于指向inode）<br>ready_node:双向链表中一个就绪事件的节点，可指回双向链表。</p>
<p><img src="https://img-blog.csdnimg.cn/20200118225411426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;用户进程调用epoll_create函数，对应在内核就已经创建了一个全局唯一eventpoll实例（object），对应背后完整的数据结构如下，示意图来源于<a href="https://zhuanlan.zhihu.com/p/50984245">该文章</a>：<br>&#8195;&#8195;图中的list就是双向链表，可以清楚看出epoll主要用了两个struct类型（当然不止这2个成员变量，还有锁、等待队列）和两种数据结构，完成高并发IO模型的构建，设计巧妙。因此，如果大家提高个人开发能力以及设计能力，数据结构必须要精通！（这里涉及到内核对红黑树、链表的操作是线程安全的，源码用了锁保操作原子性）<br><img src="https://img-blog.csdnimg.cn/20200119210515593.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;关于更深入的epoll红黑树以及事件就绪链表等底层的代码实现和图解析，这里有两篇文章，写得很好，作者根据英文原版内容自行理解后的整理：<br><a href="https://www.cnblogs.com/sduzh/p/6714281.html">《Linux内核笔记：epoll实现原理（一）》</a><br><a href="https://www.cnblogs.com/sduzh/p/6793879.html">《Linux内核笔记：epoll实现原理（二）》</a><br>（英文原版的链接现无法访问，地址<a href="https://idndx.com/2014/09/01/the-implementation-of-epoll-1/">《the-implementation-of-epoll》</a> )</p>
<h5 id="4-3-level-trigger和edge-trigger"><a href="#4-3-level-trigger和edge-trigger" class="headerlink" title="4.3 level trigger和edge trigger"></a>4.3 level trigger和edge trigger</h5><p>&#8195;&#8195;epoll工作模式支持水平触发(level trigger，简称LT，又称普通模式)和边缘触发(edge trigger，简称ET，又称“高速模式”)，而select和poll只支持LT模式。这里说的触发需要通过以下详细的说明来体会其内涵：<br>==level trigger模式的触发条件：==</p>
<ul>
<li>对于读就绪事件，只要用户程序没有读完fd的数据，也即缓冲内容不为空，epoll_wait还会继续返回该fd，让用户程序继续读该fd</li>
<li>对于写就绪事件，只要用户程序未向fd写满数据，也即缓冲区还不满，epoll_wait还会继续返回该fd，让用户程序继续对该fd写操作</li>
</ul>
<p>原理解释：<br>&#8195;&#8195;假设当前用户进程添加监听的文件描述符为4，以下简称为4fd，当该4fd有可读可写就绪事件时，epoll_wait()有返回，于是用户程序去进行读写操作，如果当前这一轮用户程序没有把4fd数据一次性全部读写完，那么下一轮调用 epoll_wait()时，它还会返回4fd这个事件对象，让你继续把4fd的缓存区上读或者写。如果用户程序一直不去读写，它会一直通知返回4fd。<br>参考代码如下：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">　　<span class="keyword">int</span> epfd,nfds;</span><br><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>,<span class="title">events</span>[10];</span> <span class="comment">//ev用于注册事件，数组用于返回要处理的事件</span></span><br><span class="line">　　epfd = epoll_create(<span class="number">1</span>); <span class="comment">//监听标准输入描述符，用于做测试</span></span><br><span class="line">　　ev.data.fd = STDIN_FILENO; <span class="comment">//标准输入描述符绑定到用户data的fd变量</span></span><br><span class="line">　　ev.events = EPOLLIN; <span class="comment">//监听读事件，且默认为LT水平触发事件</span></span><br><span class="line">　　epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;ev); <span class="comment">//注册epoll事件</span></span><br><span class="line">　　<span class="keyword">for</span>(;;)</span><br><span class="line">　　&#123;</span><br><span class="line">　　　　nfds = epoll_wait(epfd, events, <span class="number">5</span>, <span class="number">-1</span>); <span class="comment">//内核返回就绪事件</span></span><br><span class="line">　　　　<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++)</span><br><span class="line">　　　　&#123;</span><br><span class="line">　　　　　　<span class="keyword">if</span>(events[i].data.fd==STDIN_FILENO) <span class="comment">//如果返回的事件对象的fd为标准输入fd，则打印字符串，注意到，用户程序没有在缓存区读取数据</span></span><br><span class="line">　　　　　　　　<span class="built_in">printf</span>(<span class="string">&quot;epoll LT mode&quot;</span>);</span><br><span class="line"></span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;上面代码最关键的地方：在<code>if(events[i].data.fd==STDIN_FILENO)</code>之后，用户程序没有在标准输入的缓存区读取数据，根据水平触发原理，epoll_wait一直返回STDIN_FILENO这个就绪读事件，该代码最终效果：屏幕标准输出一直打印”epoll LT mode”字符串。<br>&#8195;&#8195;上面的过程可以解释为何LT模式是epoll工作效率较低的模式，具体说明如下：<br>&#8195;&#8195;假设除了感兴趣监听的文件描述符4fd，还有另外100个我不需要读写文件描述符（监听它们不代表一定要处理他们的就绪读写事件），最终会出现这样场景：epoll_wait每次都把这100个fd返回，而我只想对4fd进行读写，因此导致程序必须从101个fd中检索出4fd，若这些100fd以更高的优先级返回，那么用户则更晚才能拿到4fd，最终降低业务处理效率。</p>
<p>==edge trigger模式的触发条件：==</p>
<ul>
<li><p>对于读就绪事件，常见触发条件：<br> 缓冲区由空变为不空的时候（有数据可读时）<br> 当有新增数据到达时，即缓冲区中的待读数据变多时</p>
<ul>
<li>对于写就绪事件，常见触发条件<br> 缓冲区由满变为空的时候（可写）<br>当有旧数据被发送走，即缓冲区中的内容变少的时</li>
</ul>
</li>
</ul>
<p>原理解释：<br>&#8195;&#8195;假设当前用户进程添加监听的文件描述符为4，以下简称为4fd，当该4fd有可读可写就绪事件时，epoll_wait()有返回，于是用户程序去读写操作，如果当前这一轮用户程序没有把4fd数据一次性全部读写完，那么下次调用epoll_wait()时，它不会再返回这个4fd就绪事件，直到在4fd上出现新的可读写事件才会通知你。这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。<br>参考代码如下：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">　　<span class="keyword">int</span> epfd,nfds;</span><br><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>,<span class="title">events</span>[10];</span> <span class="comment">//ev用于注册事件，数组用于返回要处理的事件</span></span><br><span class="line">　　epfd = epoll_create(<span class="number">1</span>); <span class="comment">//监听标准输入描述符，用于做测试</span></span><br><span class="line">　　ev.data.fd = STDIN_FILENO; <span class="comment">//标准输入描述符绑定到用户data的fd变量</span></span><br><span class="line">　　ev.events = EPOLLIN|EPOLLET; <span class="comment">//监听读事件，而且开启ET触发事件</span></span><br><span class="line">　　epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;ev); <span class="comment">//注册epoll事件</span></span><br><span class="line">　　<span class="keyword">for</span>(;;)</span><br><span class="line">　　&#123;</span><br><span class="line">　　　　nfds = epoll_wait(epfd, events, <span class="number">5</span>, <span class="number">-1</span>); <span class="comment">//内核返回就绪事件</span></span><br><span class="line">　　　　<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++)</span><br><span class="line">　　　　&#123;</span><br><span class="line">　　　　　　<span class="keyword">if</span>(events[i].data.fd==STDIN_FILENO) <span class="comment">//如果返回的事件对象的fd为标准输入fd，则打印字符串，注意到，用户程序没有在缓存区读取数据</span></span><br><span class="line">　　　　　　　　<span class="built_in">printf</span>(<span class="string">&quot;epoll ET mode&quot;</span>);</span><br><span class="line"></span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>&#8195;&#8195;程序运行效果：<code>ev.events = EPOLLIN|EPOLLET</code>，将epoll监听的标准输入文件描述符设为ET模式，当向stdin敲入字符串abc时，缓存区由空转为不空，触发epoll_wait返回就绪事件，而之后用户程序并没有把缓冲区读取数据，根据ET原理，程序只打印一次”epoll ET mode”后就被阻塞。因为epoll_wait只通知一次，下次不再通知用户该4fd事件。除非外界再向stdin敲入字符串以至缓存区新增了数据，epoll_wait就会通知用户这个4fd有就绪事件。</p>
<h5 id="4-4-水平触发和边缘触发的小结"><a href="#4-4-水平触发和边缘触发的小结" class="headerlink" title="4.4 水平触发和边缘触发的小结"></a>4.4 水平触发和边缘触发的小结</h5><p>&#8195;&#8195;以某个被监听的文件描述符发生读事件作为示例：</p>
<ul>
<li>a.对于某个监听的文件描述符fd，假设其指向的读缓冲区初始时刻为空</li>
<li>b. 假设内核拷贝了4KB数据到用户进程的读缓冲区<ul>
<li>c.不管水平触发还是边缘触发模式，epoll_wait此时都会返回可读就绪事件</li>
<li>d. 若采用水平触发方式，用户读取了2KB的数据，读缓冲区还剩余2KB数据，epoll_wait还会继续返回（通知）用户fd有可读就绪事件，直到读缓冲变为空为止。</li>
</ul>
</li>
<li>f.若采用边缘触发方式，用户读取了2KB的数据，读缓冲区还剩余2KB数据，epoll_wait不再返回（通知）用户fd有可读就绪事件，除非读缓存区被用户进程或者内核写入新增数据例如1KB（此时读取缓冲变为3KB数据)那么epoll_wait才会通知用户有可读就绪事件。</li>
</ul>
<p>&#8195;&#8195;到此，已经完成对epoll深入解析的内容，当你掌握这些底层原理后，再回看当前出色中间件或框架如redis、nginx、node.js、tornado等，真香！</p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>IO多路复用</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：深度解析jdk1.8的HashMap源代码关键逻辑（不含红黑树结构）</title>
    <url>/2020/11/22/Java%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90jdk1.8%E7%9A%84HashMap%E6%BA%90%E4%BB%A3%E7%A0%81%E5%85%B3%E9%94%AE%E9%80%BB%E8%BE%91%EF%BC%88%E4%B8%8D%E5%90%AB%E7%BA%A2%E9%BB%91%E6%A0%91%E7%BB%93%E6%9E%84%EF%BC%89/</url>
    <content><![CDATA[<p>jdk1.8的HashMap应该JSR-166 Java专家组（HashMap主要还是Dung Lea做了大量贡献）设计的一个极其出色且优秀的数据结构，无论是算法设计和程序代码设计都显得非常高水平，个人深入分析后，获益匪浅！</p>
<h4 id="一、HashMap底层结构图以及基本术语"><a href="#一、HashMap底层结构图以及基本术语" class="headerlink" title="一、HashMap底层结构图以及基本术语"></a>一、HashMap底层结构图以及基本术语</h4><p><img src="https://img-blog.csdnimg.cn/8ff5d42f17974049939ea07cdbd18770.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>​        这张结构图清晰展示HashMap由一维数组+链表+红黑树构成，数组在源码命名为table，是一个Node<K,V>[]泛型类型数组，table数组上的每个元素或者为null或者为单个node或者冲突链头节点或者为treeNode。桶位上可能形成冲突链表，红黑树tree bins是由node链表树化而来。</p>
<a id="more"></a>
<p>bins：多个key哈希碰撞在某个桶位上形成的冲突链，bins这个名词来源于源码的官方注释</p>
<p>tree  bins：某个桶位冲突链在HashMap扩容阶段被树化后的红黑树结构，tree  bins这个名词来源于源码的官方注释</p>
<p>从这张图即可提出以下几个经典问题：</p>
<p>1、put一个key后，HashMap是如何将这个key定位到数组对应的某个桶位（bucketIndex）？</p>
<p>2、put多个key后，若形成冲突链表，那么这个链表如何形成？</p>
<p>3、如果put的冲突key数量非常多，例如10万个，那么在数组中的某个位置将形成一个长度为10万的长链表，那么HashMap真的会在某个桶位形成这么长的冲突链吗？ 若不会，它是怎么设计的？</p>
<p>4、当数组个数超过扩容阈值时，它如何扩容？扩容后，数组的有些桶位上是一条冲链表，链表上节点会被rehash到新数组其他位置，这个过程如何实现？</p>
<p>5、为何数组长度要设计为2的N次方，关键作用在哪里？</p>
<p>6、为何数组某个位置元素是一个红黑树节点，有什么用？就不能使用链表替代吗？</p>
<p>7、jdk1.7（Java7）的HashMap没有红黑树结构，这与java8的有何区别?</p>
<p>8、很多文章说在多线程场景下，Java7版本下的HashMap因为使用了头插法，在扩容节点出现循环链表导致死循环出现，Java8则不存在，为何？</p>
<p>当然，还有其他更刁钻问题：loadfactor为何设计0.75而是0.55、0.65、0.85等</p>
<p>带上以上问题，通过解析java8的HashMap源码后，将可获得准确而易于理解的答案。</p>
<h4 id="二、java8的HashMap源码分析思路"><a href="#二、java8的HashMap源码分析思路" class="headerlink" title="二、java8的HashMap源码分析思路"></a>二、java8的HashMap源码分析思路</h4><p>分析路径参考：</p>
<p>静态常量=&gt;成员变量=&gt;构造方法=&gt;put方法（含putVal）=&gt;resize操作=&gt;get方法=&gt;remove方法等其他方法，当然还有<code>putTreeVal</code>方法、<code>treefyBin</code>方法，这两个属于HashMap的红黑树章节知识，涉及的原理也相对复杂，会在另外一篇文章会给出。</p>
<p>为何要先看构造方法和put方法？因为大家最熟悉以下用法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;String,Integer&gt; map=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">map.put(<span class="string">&quot;foo&quot;</span>,<span class="number">10</span>);</span><br><span class="line">map.put(<span class="string">&quot;bar&quot;</span>,<span class="number">20</span>);</span><br></pre></td></tr></table></figure>
<h5 id="1、继承关系"><a href="#1、继承关系" class="headerlink" title="1、继承关系"></a>1、继承关系</h5><p>在IDEA里面容易得出以下UML图<br><img src="https://img-blog.csdnimg.cn/19968e51f76b4c7a9bcf13bcc97df36e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;, <span class="title">Cloneable</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">362498820763181265L</span>;</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>HashMap&lt;K,V&gt;</code>：说明它是以<code>key-value</code>形式存储，K，V为泛型类型参数。</li>
<li><code>extends AbstractMap&lt;K,V&gt;</code>：继承了<code>AbstractMap</code>，通过该继承，实现复用。</li>
<li><code>implements Map&lt;K,V&gt;</code>：实现了<code>Map</code>接口，提供了所有可选的<code>Map</code>接口方法。</li>
<li><code>implements Cloneable</code>：表明其可以调用<code>clone()</code>方法来返回实例的<code>field-for-field</code>拷贝。</li>
<li><code>implements Serializable</code>：表明HashMap可以被序列化和反序列化。serialVersionUID用于验证反序列化后类版本是否一致。</li>
</ul>
<h5 id="2、静态常量"><a href="#2、静态常量" class="headerlink" title="2、静态常量"></a>2、静态常量</h5><p>重点看大写的静态变量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">// 上图中数组初始容量，长度必须为2的整数幂，默认大小为16，这里用了位运算符</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_INITIAL_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">4</span>; <span class="comment">// aka 16</span></span><br><span class="line"> <span class="comment">/*</span></span><br><span class="line"><span class="comment">* 若创建HashMap时指定容量超过2^31，则上限只能取到1 &lt;&lt; 30，能否1 &lt;&lt; 31？</span></span><br><span class="line"><span class="comment">首先int占用4个字节，数值范围在-2^31~2^31-1之间，也即[-2147483648,2147483647]，考虑到数组的容量设定值必须为2的整数幂，如果使用1 &lt;&lt; 31，那么得出数组的长度为：2147483648，显然大于int上界2147483647，产生了溢出。换句话说，最大值2147483647不是2的整数幂，而数组每次扩容只能选择2的整数幂，因此只能在0~2^31-1之间，选择一个恰好为2的整数幂的最大值，因此只能是：1&lt;&lt;30,也即1073741824,10亿长度！</span></span><br><span class="line"><span class="comment">* “关于HashMap数组容量的最大值只能右移30位，网上很多这种解释：因为第31位是符号位，0正，1为负，因此去掉符号位后只能选择30位长度作为最大值”，这种解释不够细致。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//默认负载因子大小</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> DEFAULT_LOAD_FACTOR = <span class="number">0.75f</span>;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 直译为树化阈值（桶的树化阈值），这里是指当数组里面的冲突链表长度达到8时，且数组size达到64时（MIN_TREEIFY_CAPACITY=64），才会执行treeify操作（注意不是treeifyBin操作），则需将这条链表转为tree bins。</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEIFY_THRESHOLD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 红黑树tree bins转为bins冲突链表的阈值，当在扩容resize时，此时HashMap的node所在桶位置会重新计算，在重新计算桶位（存储位置或者数组索引号）后，当原有的红黑树内数量 &lt; 6时，则将红黑树转换成链表（转换过程参考本人发布的红黑树部分的文章）</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNTREEIFY_THRESHOLD = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 直翻：触发最小树化操作的容量,换句话说：当数组size小于该值时，优先进行扩容，而不是树化</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TREEIFY_CAPACITY = <span class="number">64</span>;</span><br></pre></td></tr></table></figure>
<p>这里要解释下为何普通的冲突链表bins转为tree bins的阈值设为8，其实是考虑到统计情况，参考以下官方解释:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* Because TreeNodes are about twice the size of regular nodes, we</span><br><span class="line">* use them only when bins contain enough nodes to warrant use</span><br><span class="line">* (see TREEIFY_THRESHOLD). And when they become too small (due to</span><br><span class="line">* removal or resizing) they are converted back to plain bins.  In</span><br><span class="line">* usages with well-distributed user hashCodes, tree bins are</span><br><span class="line">* rarely used.  Ideally, under random hashCodes, the frequency of</span><br><span class="line">* nodes in bins follows a Poisson distribution</span><br><span class="line">* (http:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Poisson_distribution) with a</span><br><span class="line">* parameter of about 0.5 on average for the default resizing</span><br><span class="line">* threshold of 0.75, although with a large variance because of</span><br><span class="line">* resizing granularity. Ignoring variance, the expected</span><br><span class="line">* occurrences of list size k are (exp(-0.5) * pow(0.5, k) &#x2F;</span><br><span class="line">* factorial(k)). The first values are:</span><br><span class="line">*</span><br><span class="line">* 0:    0.60653066</span><br><span class="line">* 1:    0.30326533</span><br><span class="line">* 2:    0.07581633</span><br><span class="line">* 3:    0.01263606</span><br><span class="line">* 4:    0.00157952</span><br><span class="line">* 5:    0.00015795</span><br><span class="line">* 6:    0.00001316</span><br><span class="line">* 7:    0.00000094</span><br><span class="line">* 8:    0.00000006</span><br><span class="line">* more: less than 1 in ten million</span><br></pre></td></tr></table></figure>
<p>​        根据泊松分布计算的概率分布，在负载因子默认为0.75的时候，采用足够分布均匀的随机hashcode，在桶位上(hash槽)出现冲突链且元素个数达到8的概率为0.00000006，非常小，不到百万分之一。换句话说，设为0.75时，只有百万分之一概率桶位上节点出现8个元素，显然这能大概率减少触发树化操作，从而提升HashMap性能。</p>
<p>​        因此在日常开发中，put进去的key即使冲突（假设自定义的key哈希处理逻辑足够离散），也是大概率构成普通bins冲突链表（节点个数小于8），因此将树化阈值设为8可以减少频繁的树化操作。</p>
<p>​        或者我们可以逆向考察：假设树化阈值设为3，根据泊松分布算出的概率为0.0126，那么每put一个key，就有1.26%概率触发树化操作，这么频繁触发，势必影响HashMap的性能，但只要将树化阈值设为8时瞬间将普通概率事件变成小概率事件，显然合理，也有科学依据。</p>
<p>这里可以引出一个问题：</p>
<p>为何需要将普通链表树化为红黑树？</p>
<p>考察这种情况：若所put的key的哈希值离散性不合理，且需要put很多这样的key，则高频出现hash冲突，那么这些key将会出现以下情况：</p>
<p>​        假设这些key被定为到index=3的桶位置上，如果需要put 10万个这样的key，那么将会在这桶位上形成一条超长包含10万个节点的bins链表，这会出现什么问题？ 当使用get查询对应key时，将会使得原本o(1)的时间复杂度退化为0(n)，对于最坏查找情况：将需要遍历10万次，这是无法接受的查询性能。</p>
<p>​        加了树化阈值后，一旦bins链表超过阈值且数组容量达到64，那么一个包含10万个节点长链表将使用红黑树来组织，也能实现o(log(n))的查找效率。</p>
<h5 id="3、成员变量"><a href="#3、成员变量" class="headerlink" title="3、成员变量"></a>3、成员变量</h5><p>主要是Fields字段，这里重点是table数组和modCount真实含义及其改变时对应的操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">/* ---------------- Fields -------------- */</span></span><br><span class="line">  <span class="comment">// HashMap的底层数组，在第一次put的时候才会被初始化，里面每个元素都是Node&lt;K,V&gt;类型</span></span><br><span class="line">  <span class="keyword">transient</span> Node&lt;K,V&gt;[] table;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  键值对缓存集合</span></span><br><span class="line">  <span class="keyword">transient</span> Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//  key-value mappings 键值对的数量</span></span><br><span class="line">  <span class="keyword">transient</span> <span class="keyword">int</span> size;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//HashMap 结构发生改变的次数，对于key更新值不属于结构改变，引起结构改变的事件如：put、remove、clear、compute、merge，其实只要看源码里方法里面是否有出现++modCount或者modCount++</span></span><br><span class="line">  <span class="keyword">transient</span> <span class="keyword">int</span> modCount;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The next size value at which to resize (capacity * load factor).\</span></span><br><span class="line"><span class="comment">// 下一次resize扩容阈值，当前table中的元素超过此值时，触发扩容,对于初始默认capacity=16，loadFactor=0.75，那么当table的元素个数达到12时，table进行resize</span></span><br><span class="line">  <span class="keyword">int</span> threshold;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">float</span> loadFactor;</span><br></pre></td></tr></table></figure>
<h5 id="4、构造方法"><a href="#4、构造方法" class="headerlink" title="4、构造方法"></a>4、构造方法</h5><p>它有三个构造方法，这里只需解析<code>HashMap(int initialCapacity, float loadFactor)</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>这个构造方法目的是保证数组的初始容量必须为2的n次方，以及相关数值合法性检查</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Illegal initial capacity: &quot;</span> +</span><br><span class="line">                                           initialCapacity);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">        initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> (loadFactor &lt;= <span class="number">0</span> || Float.isNaN(loadFactor))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Illegal load factor: &quot;</span> +</span><br><span class="line">                                           loadFactor);</span><br><span class="line">    <span class="keyword">this</span>.loadFactor = loadFactor;</span><br><span class="line">    <span class="comment">/*这里很奇怪是吧？threshold就是触发扩容的阈值，在这里应该设定为capacity * load factor，对于默认容量16，则算出的threshold应该是12。但是这里threshold是tableSizeFor计算出，它返回是2的整数幂16。</span></span><br><span class="line"><span class="comment">    关于这里的解释：  </span></span><br><span class="line"><span class="comment">    (1) 如果使用类似new HashMap&lt;&gt;()构造方法，那么threshold初始化值为0,之后第一次put元素时，会被resize()创建table数组时，在resize方面里面threshold被重新赋值为：</span></span><br><span class="line"><span class="comment">    newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);</span></span><br><span class="line"><span class="comment">    threshold = newThr;//也即等于12</span></span><br><span class="line"><span class="comment"> 		(2) 如果使用类似new HashMap&lt;&gt;(10)构造方法或者new HashMap&lt;10,0.5&gt;构造方法时：</span></span><br><span class="line"><span class="comment"> 	那么threshold先被初始化值为16：</span></span><br><span class="line"><span class="comment"> 	this.threshold = tableSizeFor(initialCapacity=10);// 16</span></span><br><span class="line"><span class="comment"> 	之后第一次put元素时，会被resize()创建table数组时，threshold被重新赋值，过程如下：</span></span><br><span class="line"><span class="comment"> 				int newThr=0</span></span><br><span class="line"><span class="comment"> 				int oldThr=threshold; //16</span></span><br><span class="line"><span class="comment"> 			  ....</span></span><br><span class="line"><span class="comment">	  else if (oldThr &gt; 0) </span></span><br><span class="line"><span class="comment">	  				&#123;newCap = oldThr;&#125;// 而且创建新数组的容量直接使用oldThr=16</span></span><br><span class="line"><span class="comment">	  </span></span><br><span class="line"><span class="comment">	  if (newThr == 0) &#123;</span></span><br><span class="line"><span class="comment">            float ft = (float)newCap * loadFactor; 16*0.5=8</span></span><br><span class="line"><span class="comment">            newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ?</span></span><br><span class="line"><span class="comment">                      (int)ft : Integer.MAX_VALUE);  8</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    threshold = newThr; 8</span></span><br><span class="line"><span class="comment">    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; 前面newCap为16</span></span><br><span class="line"><span class="comment">    table = newTab;</span></span><br><span class="line"><span class="comment">    return newTab; 所以最终返回的是16个null元素的数组，其扩容阈值为threshold = newThr=8，而不是10*0.5</span></span><br><span class="line"><span class="comment">	</span></span><br><span class="line"><span class="comment"> 关于(2)的分析方法：</span></span><br><span class="line"><span class="comment"> Map&lt;String,Integer&gt; map=new HashMap&lt;String,Integer&gt;(10,0.5F);</span></span><br><span class="line"><span class="comment"> map.put(&quot;foo&quot;,10);//使用IDEA，在这里断点debug，然后观察在resize的执行过程即可</span></span><br><span class="line"><span class="comment"> 使用条件断点：new HashMap&lt;&gt;()-&gt;put-&gt;putVal-&gt;resize-&gt;观察threshold在不同构造方法下的赋值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">this</span>.threshold = tableSizeFor(initialCapacity);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​        关于this.threshold，在resize小节也会被再次提到。</p>
<p>​        这里tableSizeFor是重点解析方法，它会返回一个大于或等于设定值最小2的n次方数字，例如<code>new HashMap(12)</code>，那么经过tableSizeFor后，table的长度也会被设为16，而不是12，这里cap给定为12作为计算过程示例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// n=12-1=11，11对应二进制位0000 0000 0000 0000 0000 0000 0000 1011（32位），为了方便演示计算，这里去掉前面28位高位0，取1011</span></span><br><span class="line">    <span class="keyword">int</span> n = cap - <span class="number">1</span>; <span class="comment">// 为何将设定的容量值先减1？</span></span><br><span class="line">    <span class="comment">/*右移：最左补0，最右位删掉</span></span><br><span class="line"><span class="comment">    * 1011右移一位再和1011取或</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">1</span>; <span class="comment">//1011 | 0101 = 1111 </span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">2</span>; <span class="comment">//1111 | 0011 = 1111</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">4</span>; <span class="comment">//1111 | 0000 = 1111</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">8</span>; <span class="comment">//1111 | 0000 = 1111</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">16</span>;<span class="comment">//1111 | 0000 = 1111</span></span><br><span class="line">    <span class="comment">// 1111也即15，因此返回的是n+1，也即15+1=16，16显然满足大于等于12的最小2的n次方数字</span></span><br><span class="line">    <span class="keyword">return</span> (n &lt; <span class="number">0</span>) ? <span class="number">1</span> : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + <span class="number">1</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于12这个数字，到了 n &gt;&gt;&gt; 4之后就不需要再计算了。</p>
<p>这里有个trick：1+2+4+8+16=31，显然，只要右移动累计够31位，对于int四个字节无论再大的数，都可找到与n对应的二进制数且全部位都为1。那么在在它基础上加个1，即可算出大于等于这个数的最小2^n次幂整数（最大数为2^31次方），当然如果找到最大的数为2^31，它已经大于MAXIMUM_CAPACITY=2^30，因此即使给定数例如2^30+111,经过tableSizeFor后，也只能返回MAXIMUM_CAPACITY</p>
<p>此外`n = cap - 1，这里有个对设置值进行减1操作，设计原因如下:假定给定new HashMap(16)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> n = cap  <span class="comment">// 考察不减1的情况,这里cap=16，对应10000。</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">1</span>; <span class="comment">//10000 | 01000 = 11000 </span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">2</span>; <span class="comment">//11000 | 00110 = 11110</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">4</span>; <span class="comment">//11110 | 00000 = 11111</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">8</span>; <span class="comment">//11111 | 00000 = 11111</span></span><br><span class="line">    n |= n &gt;&gt;&gt; <span class="number">16</span>;<span class="comment">//11111 | 00000 = 11111</span></span><br><span class="line">    <span class="comment">// 11111对应31,这里返回的是n+1,也是31+1=32</span></span><br><span class="line"><span class="keyword">return</span> (n &lt; <span class="number">0</span>) ? <span class="number">1</span> : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + <span class="number">1</span>; </span><br></pre></td></tr></table></figure>
<p>既然new HashMap(16)给定的16设定值恰好满足2^4次方条件，可直接return，但因为采用<code>int n = cap</code>方式，导致返回的容量是32，显然浪费了一倍空间，因此<code>int n = cap-1</code>保证返回的值一定是大于等于cap的2^n次数。</p>
<p>5、Node节点定义</p>
<p>由于本次文章不包含HashMap里面的红黑树结构，因此只给出普通Node节点说明，定义相对简单：节点必须实现<code>hashCode</code>和<code>equals</code>方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> hash; <span class="comment">// 用于存放该节点的hash值，避免重复计算</span></span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    V value;</span><br><span class="line">    Node&lt;K,V&gt; next; <span class="comment">// 用于指向下一个节点</span></span><br><span class="line"></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.hash = hash;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> K <span class="title">getKey</span><span class="params">()</span>        </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getValue</span><span class="params">()</span>      </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> key + <span class="string">&quot;=&quot;</span> + value; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Objects.hashCode(key) ^ Objects.hashCode(value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">setValue</span><span class="params">(V newValue)</span> </span>&#123;</span><br><span class="line">        V oldValue = value;</span><br><span class="line">        value = newValue;</span><br><span class="line">        <span class="keyword">return</span> oldValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 节点与节点之间的判等</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="keyword">this</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">if</span> (o <span class="keyword">instanceof</span> Map.Entry) &#123;  <span class="comment">// 如果Object类型的o对象时Map.Entry类型，则先将o对象强制类型转换为 Map.Entry&lt;?,?&gt;类型，以便两个节点使用可以相同方法进行equals判等</span></span><br><span class="line">            Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;</span><br><span class="line">            <span class="keyword">if</span> (Objects.equals(key, e.getKey()) &amp;&amp;</span><br><span class="line">                Objects.equals(value, e.getValue()))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Node<K,V>内部静态类相对简单，这里不再累赘。</p>
<h5 id="5、理解hash扰动函数"><a href="#5、理解hash扰动函数" class="headerlink" title="5、理解hash扰动函数"></a>5、理解hash扰动函数</h5><p>如何将一个Node节点put到数组对应的位置（桶位）？</p>
<p>​        设计较为简单：由该节点的key的hashcode经过一个扰动函数之后再与table的长度进行与运算得出index，这个index对应桶位就是新增Node节点要插入的位置，当然需要调用put方法才能实现放入一个新增元素，计算公式为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n=capacity; <span class="comment">// 数组容量，2的整数幂</span></span><br><span class="line"><span class="keyword">int</span> hash=hash(key) <span class="comment">// 扰动哈希函数hash对给定的key计算出对应的哈希值</span></span><br><span class="line"><span class="keyword">int</span> i =  hash &amp; (n - <span class="number">1</span>);  <span class="comment">// 使用位与运算，算出桶位所在的数组的索引号</span></span><br><span class="line">Node&lt;K,V&gt; p=table[i];  </span><br></pre></td></tr></table></figure>
<p>接下来看看<code>i =  hash &amp; (n - 1)</code>工作原理：假设目前数组为默认初始容量16，put四个key到HashMap里面</p>
<p>虽然四个key的hash值都不相同，但四个key都定位到同一个桶位上，为何会这样？观察一下计算过程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// 注意这里不是使用hashCode方法计算，而是hash扰动方法，该方法见后面解释</span></span><br><span class="line">System.out.println(hash(<span class="string">&quot;foo00&quot;</span>)); <span class="comment">// 97614999</span></span><br><span class="line">    System.out.println(hash(<span class="string">&quot;foo11&quot;</span>)); <span class="comment">// 97615031</span></span><br><span class="line">    System.out.println(hash(<span class="string">&quot;foo22&quot;</span>)); <span class="comment">// 97614935</span></span><br><span class="line">    System.out.println(hash(<span class="string">&quot;foo33&quot;</span>)); <span class="comment">// 97614967</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/8d105b02ec2844ffb40defe81a7cb6a8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>​        以上四个hash值对应的二进制和(16-1)进行与运算后算出的桶位索引号都是7，桶位冲突，显然这四个key对应的节点将在table[7]形成一条冲突链表。此外，从计算过程可以观察出数组长度-1对应的二进制1111恰好形成一个对高位的掩码，“与”操作后，key的高位全部置为零，只保留低4位，这个值就就是给定key的桶位索引号7。</p>
<p>这样设计有什么好处？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">不管给定的key多大，例如key是一个整数1073741821，通过以上与运算，在计算过程中，永远只有key的低n位参与到位运算（这里n就是数组长度2^n里面的n次幂数字。例如上面的foo00例子，若数组长度16，n就是4），这这方式屏蔽了高位，能非常快速计算出index，利用底层cpu的位运算能力来加速得到桶位索引号，这个设计确实很巧妙，这也是为何数组长度要设计2的整数幂原因之一。</span><br></pre></td></tr></table></figure>
<p>但这里是不是有个担心的地方？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">即使给定的key有多大或者可以分布均匀，按照上面的算法，运算后仅有低n位的值，这不就加大了桶位冲突吗？原本想让均布分别的key能够均匀分别到数组大部分桶位上，结果反而更加冲突了？ </span><br></pre></td></tr></table></figure>
<p>对于这个问题，好在HashMap已经想到这一点，不是直接使用<code>i =  hashCode &amp; (n - 1)</code>，更好的处理方法：</p>
<p><code>i =  hash &amp; (n - 1)，这里hash就是扰动函数</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">既然如果直接用key的hashCode和table.length-1进行与运算后，key高位信息就会被置0，导致key无法合理离散分布。那么可以再这样设计，让key右移16位，和原key的hashCode进行异或运算，使得key的高位信息特征能够“融入”到低位中，经过这种“扰动”处理，在进行&#96;i &#x3D;  hash &amp; (n - 1)&#96;后，能进一步降低哈希冲突。</span><br></pre></td></tr></table></figure>
<p>如何使得key的高位信息特征能够“融入”到低位中？以下就是hash(key)内部实现为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="comment">/*假设现在有个一key对应的hashCode如下</span></span><br><span class="line"><span class="comment">    * 0100 1010 0000 0000 0101 0000 1011 1101，h</span></span><br><span class="line"><span class="comment">    * 0000 0000 0000 0000 0100 1010 0000 0000 ，h&gt;&gt;16，</span></span><br><span class="line"><span class="comment">    * 0100 1010 0000 0000 0001 1010 1011 1101，h^h&gt;&gt;16 ,异或运算：同为0，异为1</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)</code>能够把key的高16位信息“融入”到低16位中，当进行桶位bucketIndex计算时，被16位全为1的table长度&amp;运行后，后得到低16位是包含高16key高位信息，以期减少哈希冲突。</p>
<p>6、如何快速制造桶位冲突的键</p>
<p>假设让所加的key都定位到数组索引号10，数组初始容量为16，生成过程如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Util</span> </span>&#123;</span><br><span class="line">          <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> collidedIndex  指定桶位冲突的位置，也即定位元素碰撞的数组索引号</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> tableSize  指定数组容量</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> num  指定生成冲突key的个数</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> </span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;Integer&gt; <span class="title">makeKeyCollision</span><span class="params">(<span class="keyword">int</span> collidedIndex, <span class="keyword">int</span> tableSize, <span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> target = hash(collidedIndex) &amp; tableSize - <span class="number">1</span>;</span><br><span class="line">            List&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Integer.MAX_VALUE; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (list.size() == num) <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">int</span> bucket_index = hash(i) &amp; tableSize - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span> (bucket_index == target) &#123;</span><br><span class="line">                    list.add(i);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> list.subList(<span class="number">0</span>, num);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> h;</span><br><span class="line">            <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Map&lt;Integer, String&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="comment">// 要求在桶位10上生成10个hash冲突节点</span></span><br><span class="line">        List&lt;Integer&gt; list = Util.makeKeyCollision(<span class="number">10</span>, <span class="number">16</span>, <span class="number">10</span>);</span><br><span class="line">        <span class="comment">// [10, 26, 42, 58, 74, 90, 106, 122, 138, 154]</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> key:list)&#123;</span><br><span class="line">            map.put(key,<span class="string">&quot;foo&quot;</span>+key);</span><br><span class="line">        &#125;</span><br><span class="line">      </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其实这个冲突的key很好找，如果给定一个key为10，那么其他key生成方式为nextKey=10+tableSize*n，n=0,1,2,3,4…，因此生成冲突key另外一种方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;Integer&gt; <span class="title">KeyCollision</span><span class="params">(<span class="keyword">int</span> collidedIndex, <span class="keyword">int</span> tableSize, <span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> i;</span><br><span class="line">       List&lt;Integer&gt; list= <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;N;i++)&#123;</span><br><span class="line">           list.add(collidedIndex+tableSize*i);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> list;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h5 id="6、put（putVal）方法"><a href="#6、put（putVal）方法" class="headerlink" title="6、put（putVal）方法"></a>6、put（putVal）方法</h5><p>在前面第5点已经知道如何将一个新增节点定位到数组里面对应的桶位，那么找到桶位后，如何放置呢？</p>
<p>putVal的分析要结合HashMap是由数组、链表和红黑树构成的原理进行理解，以下源码是针对类似这样的操作<code>map.put(&quot;foo&quot;,10)</code>：key为”foo”,value为10。</p>
<p>以下代码总体思路：<br><img src="https://img-blog.csdnimg.cn/6f3d05b8d4574adba1d2ef05a40113ad.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>以下为源码对应的实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/* </span></span><br><span class="line"><span class="comment">         tab表示当前HashMap的table，临时变量</span></span><br><span class="line"><span class="comment">         p表示table的桶位节点（或者称为桶位头节点、桶位首节点），临时变量</span></span><br><span class="line"><span class="comment">         n表示table哈希表（数组）的长度，临时变量</span></span><br><span class="line"><span class="comment">         i表示key定位到的桶位下标位置（简称桶位位置），临时变量</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line">        <span class="comment">/* 只有第一次put时才会触发初始化逻辑，这时HashMap的talbe才会被resize()方法在内存创建出来,换句话说在new HashMap()时，table不会被创建到内存。这种叫惰性创建。</span></span><br><span class="line"><span class="comment">         if ((tab = table) == null || (n = tab.length) == 0)  // 将这句代码分开写：</span></span><br><span class="line"><span class="comment">				 tab=table;</span></span><br><span class="line"><span class="comment">				 n=tab.length; </span></span><br><span class="line"><span class="comment">				 if((tab==null)|| n==0) // 显然如果table为null或者table数组长度为0，说明是第一次put</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">         <span class="comment">/*tab = resize() //调用resize初次创建table</span></span><br><span class="line"><span class="comment">         * n=tab.length; //初次创建tabel,数组长度显然为n=16</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">            n = (tab = resize()).length;</span><br><span class="line">  			</span><br><span class="line"> 			<span class="comment">/* 对应第2条思路:如果定位到数组的桶位上的元素table[i]恰好为null,直接将该新节点放入该桶位，而且next指向null</span></span><br><span class="line"><span class="comment">  		* Node&lt;K,V&gt; p = tab[i = (n - 1) &amp; hash];</span></span><br><span class="line"><span class="comment">  		  if(p==null)&#123;</span></span><br><span class="line"><span class="comment">  		   		tab[i] = newNode(hash, key, value, null); </span></span><br><span class="line"><span class="comment">  		  &#125;</span></span><br><span class="line"><span class="comment">  		*/</span></span><br><span class="line">  		<span class="comment">// p指向的是桶位的头节点，若该头节点为null，则说明桶位是空，则放入新节点即可完成put操作</span></span><br><span class="line">        <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)</span><br><span class="line">            tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">        <span class="comment">// p指向的是桶位的头节点，若该头节点不为空null，则说明桶位上可能是红黑树节点或者冲突链表</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//  Node&lt;K,V&gt; e节点临时变量，如果给定的key不为null，且当前桶位的节点key与给定的key一致，将将当前桶位节点p赋给临时变量e，k表示一个key临时变量，用于后续k=p.key赋值操作</span></span><br><span class="line">           <span class="comment">/*</span></span><br><span class="line"><span class="comment">           K k=p.key;</span></span><br><span class="line"><span class="comment">           Node&lt;K,V&gt; p = tab[i = (n - 1) &amp; hash];</span></span><br><span class="line"><span class="comment">           if (p.hash== hash &amp;&amp; ((k==key) || (key != null &amp;&amp; key.equals(k))) ) </span></span><br><span class="line"><span class="comment">           // p.hash == hash &amp;&amp; ((k==key) 用于判断新增节点key为null或者key为基本类型的情况</span></span><br><span class="line"><span class="comment">    			 // p.hash == hash &amp;&amp; (key.equals(k)) 用于判断新增节点key是不为null的引用类型情况,这样才能保证key.equals(k)不会出现NullPointerException</span></span><br><span class="line"><span class="comment">           */</span></span><br><span class="line">            </span><br><span class="line">            Node&lt;K,V&gt; e; K k;</span><br><span class="line">            <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">                 <span class="comment">// 新节点Node与p刚好哈希碰撞,且两者的key相等。</span></span><br><span class="line">                ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k)))) <span class="comment">// 这句能够证明java8的HashMap支持key为null插入。</span></span><br><span class="line">                e = p;</span><br><span class="line">           <span class="comment">// 对应第3思路的的2)方向：</span></span><br><span class="line">          <span class="comment">// 如果新节点Node与p刚好哈希碰撞，但两者的key不等，当p为TreeNode，那么只需将新增节点Node放入到这课以p为根节点的红黑树。</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 对应第3思路的的3)方向：说明定位到该桶位的第一个节点：</span></span><br><span class="line">                <span class="comment">// Node&lt;K,V&gt; p = tab[i = (n - 1) &amp; hash]是链表上的节点,使用链表遍历，将新节点插入到合适位置。</span></span><br><span class="line">                <span class="comment">// binCount用于计算出该桶位上链表的长度</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line">                    <span class="comment">// 若遍历到链表尾，直接p.next指向newNode完成新增节点</span></span><br><span class="line">                    <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        p.next = newNode(hash, key, value, <span class="keyword">null</span>);  <span class="comment">//也即所谓的尾插法</span></span><br><span class="line">                        <span class="comment">// 判断新增节点后，链表长度（桶节点个数）是否大于树化该桶链表的阈值8-1</span></span><br><span class="line">                        <span class="comment">//因为binCount从0开始，因此binCount等于7时，说明该桶位上链表长度为8，触发树化操作</span></span><br><span class="line">                        <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></span><br><span class="line">                            treeifyBin(tab, hash);</span><br><span class="line">                        <span class="keyword">break</span>; <span class="comment">// 完成以上操作后，退出链表遍历。</span></span><br><span class="line">                    &#125;</span><br><span class="line">     <span class="comment">// 若遍历链表还未到尾部，例如在链表中间恰好找到一个链表上的节点e，它与给定key有哈希碰撞，且两者key相同，那么将退出该桶位上的链表遍历。</span></span><br><span class="line">                    <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                        ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    p = e; <span class="comment">// 用于链表下一轮遍历</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          </span><br><span class="line">            <span class="comment">/* 以下操作是针对找到与给定key相同的节点三种情况进行更新值操作</span></span><br><span class="line"><span class="comment">            1) 当前桶位table[i]的p节点恰好与给定的key相同</span></span><br><span class="line"><span class="comment">            Node&lt;K,V&gt; p = tab[i = (n - 1) &amp; hash]</span></span><br><span class="line"><span class="comment">            e=p;</span></span><br><span class="line"><span class="comment">            2) 当前桶位table[i]的p为treeNode，在红黑树里面找到一个与给定key相同的e节点</span></span><br><span class="line"><span class="comment">            TreeNode&lt;K,V&gt; p = tab[i = (n - 1) &amp; hash]</span></span><br><span class="line"><span class="comment">            以p为根节点的红黑树在遍历过程中找到一个与给定key相同的e节点：</span></span><br><span class="line"><span class="comment">            e=p;</span></span><br><span class="line"><span class="comment">            </span></span><br><span class="line"><span class="comment">            3) 当前桶位table[i]的p为链表，在该链表中找到一个与给定key相同的e节点</span></span><br><span class="line"><span class="comment">            Node&lt;K,V&gt; p = tab[i = (n - 1) &amp; hash]</span></span><br><span class="line"><span class="comment">            以p为根节点的链表在遍历过程中找到一个与给定key相同的e节点：</span></span><br><span class="line"><span class="comment">            e=p;</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></span><br><span class="line">                V oldValue = e.value;</span><br><span class="line">                <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line">                    e.value = value; <span class="comment">// 更新valu</span></span><br><span class="line">                afterNodeAccess(e); <span class="comment">// 在新增节点操作之后，需要对遍历过程的临时节点e做什么操作，源码未实现任何操作。</span></span><br><span class="line">                <span class="keyword">return</span> oldValue; <span class="comment">// 显然更新值putVal就直接return了，不会执行后面++modCount这一句。这里证明更新值不会对HashMap结构产生变化。</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">  			<span class="comment">/*</span></span><br><span class="line"><span class="comment">  			若程序能运行到++modCount这一句，说明一定是以下三种情况之一：</span></span><br><span class="line"><span class="comment">  			1、在tab[i]为null时为其新增一个节点</span></span><br><span class="line"><span class="comment">  			2、在treeNode增加一个节点</span></span><br><span class="line"><span class="comment">  			3、在冲突链表上新增节点后</span></span><br><span class="line"><span class="comment">  			*/</span></span><br><span class="line">        ++modCount;</span><br><span class="line">        <span class="keyword">if</span> (++size &gt; threshold) <span class="comment">// 对于思路4：数组size长度加1，并判断是否大于threshold，大于则扩容</span></span><br><span class="line">            resize();</span><br><span class="line">        afterNodeInsertion(evict);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>; <span class="comment">// 若新增节点被成功插入，则put方法的返回值为null</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h5 id="7、resize"><a href="#7、resize" class="headerlink" title="7、resize()"></a>7、resize()</h5><h6 id="原理与图解分析"><a href="#原理与图解分析" class="headerlink" title="原理与图解分析"></a>原理与图解分析</h6><font color=red> (注意：本节内容重点放在HashMap数组、数组上的非treeNode节点、桶位上的链表，所作图也不包括tree bins 红黑树，所有关于红黑树的内容在另外一篇文章给出）</font>

<p>首先需明白为何要resize扩容？</p>
<p>先从逆向思维进行思考：若不对数组进行resize扩容，那么<code>Node&lt;K,V&gt;[] table</code>数组长度将固定在16。</p>
<p>考察一种情况：</p>
<blockquote>
<p>不断往这个HashMap put元素后，例如随机put10万个元素到这个map，由于其底层table长度固定为16，根据抽屉原理也可以推出：至少99984个元素会在找桶位时发生哈希冲突，不妨假设会出现这种情况：数组的15个桶位位置形成15条bin链表（长度为7），总共可放置7*15个元素，剩下一个桶位将有(100000-7*15)个元素构成一课庞大的红黑树</p>
</blockquote>
<p>（1）在查找一个key时，若该给定的key位于15个桶位位置形成15条bin链表的某种链表，时间复杂度最差也是只是查询7次，还可接受。</p>
<p>（2）在查找一个key时，若该给定的key位于这颗大的红黑树里面，那么查询时间复杂度最差为O(2logn)次，也即23次。</p>
<p>​    目前固定table的长度看起来“查询需求”的效率还可接受，但仍然无法达到0(1)效率，还不如使用TreeMap</p>
<p>若采用扩容，虽然数组的容量变得更大，数组占用内存多，但带来额外的收益：</p>
<p>考察一种情况：假设还是put入10万个，对应的扩容数组容量为131072,也即2^17，若10万个元素都能平均分布在每个桶位上，也即这个HashMap底层的table数组每个元素就是节点，那么完全可以达到o(1)性能，且几乎无哈希冲突。</p>
<p>当然：table长度扩容到2^17，占用不少内存空间，这是典型的用空间换时间的例子。</p>
<p>实际情况不会出现10万个元素完全分布在不同的桶位上，更多常见的是：一部分的元素位于桶位第一个位置上，一部分元素在链表上，一部分元素在tree bin红黑树上。</p>
<p>扩容后，桶位上元素的索引在原表和新表上区别：the elements from each bin must either stay at same index, or move with a power of two offset in the new table.意思是说，扩容后，桶位上元素的索引号要么跟之前相同，要么就是元素所在的新表位置在索引号+oldTable.length</p>
<p>以下源码解析部分：</p>
<p>在前面的putVal方法，出现<code>resize</code>调用的场合：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line">    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">        n = (tab = resize()).length;  <span class="comment">// 第一次put时，用resize初始化了一个长度为16的空table</span></span><br></pre></td></tr></table></figure>
<p>它是如何初始化？</p>
<p>这里需要可通过IDEA里面debug相关过程即可知道resize的设计过程：</p>
<ul>
<li>1) 第一种情况：针对第一次put key时，在putVal方法里面：<code>n = (tab = resize()).length;</code></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;Integer, String&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">map.put(<span class="number">5</span>,<span class="string">&quot;foo&quot;</span>+<span class="number">5</span>); <span class="comment">// 对其设置断点</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>通过debug观察resize的初始化流程Force Step Into先进入内建方法，再使用Step out 返回，直到出现resize的调用后，使用Force Step Into进去方法内部，接着即可单步debug也即Step Into。</p>
<p>当然也可以使用condition条件设置debug直接跳到这里的条件成立：  if ((tab = table) == null || (n = tab.length) == 0)</p>
</blockquote>
<p>resize内部参与执行的语句，不满足执行条件的代码被忽略</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    Node&lt;K,V&gt;[] oldTab = table; <span class="comment">// null</span></span><br><span class="line">    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length; <span class="comment">// oldCap=0</span></span><br><span class="line">    <span class="keyword">int</span> oldThr = threshold; <span class="comment">// oldThr=threshold=0</span></span><br><span class="line">    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>; </span><br><span class="line">....</span><br><span class="line">   <span class="keyword">else</span> &#123;               <span class="comment">// zero initial threshold signifies using defaults</span></span><br><span class="line">        newCap = DEFAULT_INITIAL_CAPACITY; <span class="comment">// newCap使用默认容量16</span></span><br><span class="line">        newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); <span class="comment">// newThr=16*0.75=12</span></span><br><span class="line">    &#125;</span><br><span class="line">......</span><br><span class="line">threshold = newThr;<span class="comment">// threshold = newThr=12</span></span><br><span class="line">    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap]; <span class="comment">// 空Node数组，容量16</span></span><br><span class="line">    table = newTab;<span class="comment">// table=Node[16]</span></span><br><span class="line">......</span><br><span class="line">    <span class="keyword">return</span> newTab;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看出，如果第1次往map里面put key，对应resize的执行流程相对简单</p>
<ul>
<li>2) 第2种情况，也是第一次put key，只是使用的构造方法不同</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;Integer, String&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">10</span>,<span class="number">0.5F</span>); <span class="comment">//自行定义了初始数组容量和负载因子</span></span><br><span class="line">map.put(<span class="number">5</span>,<span class="string">&quot;foo&quot;</span>+<span class="number">5</span>); <span class="comment">// 对其设置断点</span></span><br></pre></td></tr></table></figure>
<p>resize内部参与执行的语句，不满足执行条件的代码被忽略</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    Node&lt;K,V&gt;[] oldTab = table; <span class="comment">// null </span></span><br><span class="line">    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length; <span class="comment">// // oldCap=0</span></span><br><span class="line">    <span class="keyword">int</span> oldThr = threshold; <span class="comment">// 这里threshold=16，因为 new HashMap&lt;&gt;(10,0.5F)里面调用了this.threshold = tableSizeFor(10)</span></span><br><span class="line">    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</span><br><span class="line">......</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) <span class="comment">// 前面已知oldThr=threshold=16</span></span><br><span class="line">        newCap = oldThr; <span class="comment">// 新数组容量newCap被置为16</span></span><br><span class="line">......</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">if</span> (newThr == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor; <span class="comment">// 16*0.5=8.0</span></span><br><span class="line">        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?</span><br><span class="line">                  (<span class="keyword">int</span>)ft : Integer.MAX_VALUE); <span class="comment">// 新扩容阈值为8</span></span><br><span class="line">    &#125;</span><br><span class="line">    threshold = newThr; <span class="comment">// threshold = newThr=8</span></span><br><span class="line">    <span class="meta">@SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)</span></span><br><span class="line">    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap]; <span class="comment">// 正式初始化table数组 Node[16]</span></span><br><span class="line">    table = newTab; <span class="comment">// table=Node[16]</span></span><br><span class="line">.......</span><br><span class="line">    <span class="keyword">return</span> newTab; </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用<code>new HashMap&lt;&gt;(10)</code>构造方法同上。可以看出这两种构造方法目的也是为了确认根据给定容量和负载因子值（得到原数组容量0和原扩容阈值16），来计算出新数组容量和新扩容阈值。</p>
<p>3) 第3三种需要resize的情况最复杂：</p>
<p>根据putVal源码可知，若table数组本身已有元素，当新插入一个节点后HashMap的元素数量超过容量时，需进行扩容，参考以下put的逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params">                   <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">        Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line">        <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">            n = (tab = resize()).length; <span class="comment">// 对应前面1）2）的resize情况</span></span><br><span class="line">        ...... </span><br><span class="line">        <span class="keyword">if</span> (++size &gt; threshold) <span class="comment">// map的里面的元素个数超过扩容阈值</span></span><br><span class="line">            resize(); <span class="comment">// 对应本次resize情况</span></span><br><span class="line">        afterNodeInsertion(evict);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure>
<p>先给出扩容过程原理图，假设现在给map 放入以下的key:</p>
<p>第一组key：0，1</p>
<p>第二组key：5，21，37，133，309，3189</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    Map&lt;Integer, String&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();       </span><br><span class="line">map.put(<span class="number">0</span>,<span class="string">&quot;foo0&quot;</span>);</span><br><span class="line">    map.put(<span class="number">1</span>,<span class="string">&quot;foo1&quot;</span>);</span><br><span class="line"></span><br><span class="line">    map.put(<span class="number">5</span>,<span class="string">&quot;foo5&quot;</span>);</span><br><span class="line">    map.put(<span class="number">21</span>,<span class="string">&quot;foo21&quot;</span>);</span><br><span class="line">    map.put(<span class="number">37</span>,<span class="string">&quot;foo37&quot;</span>);</span><br><span class="line">    map.put(<span class="number">133</span>,<span class="string">&quot;foo133&quot;</span>);</span><br><span class="line">    map.put(<span class="number">309</span>,<span class="string">&quot;foo309&quot;</span>);</span><br><span class="line">    map.put(<span class="number">3189</span>,<span class="string">&quot;foo3189&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>根据putVal的桶位(定位到数组的index位置)计算方法:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getIndex</span><span class="params">(<span class="keyword">int</span> key,<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> hash(key)&amp;(n-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一组key：0，1，对应的桶位为0，1</p>
<p>第二组key：5，21，37，133，309，3189，对应的桶位都是5</p>
<h6 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h6><p>因此以上的key在HashMap的分布如下图所示（注意，为方便作图，已忽略节点里面的hash、value、next以及tree bins 红黑树）：<br><img src="https://img-blog.csdnimg.cn/0f123835a64c46ce89fab999f5fac433.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h6 id="第一次扩容至32"><a href="#第一次扩容至32" class="headerlink" title="第一次扩容至32"></a>第一次扩容至32</h6><p>假设该HashMap元素个数已达12个，触发扩容（阈值threshold=16*0.75=12），进行扩容后，HashMap内部结构图如下：<br><img src="https://img-blog.csdnimg.cn/6787ecd9b08e410782a206d9080bbeda.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>对比前后图可以发现：新数组长度为32，key：21、309、3189对应的节点去到新数组的第21个桶位上，且形成链表。</p>
<h6 id="第二次扩容至64"><a href="#第二次扩容至64" class="headerlink" title="第二次扩容至64"></a>第二次扩容至64</h6><p>又再假设假设该HashMap元素个数已达个24个，再次触发扩容（阈值threshold=32*0.75=24），进行扩容后，HashMap内部结构图如下：<br><img src="https://img-blog.csdnimg.cn/e652f4f3c379456dbddabee0b7088f42.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>对比前后图可以发现：新数组长度为64，</p>
<p>key为5的节点原位置5，在新数组也是5，位置没变</p>
<p>key为133的节点原位置5，在新数组也是5，位置没变</p>
<p>key为21的节点原位置21，在新数组也是21，位置没变</p>
<p>key为37的节点原位置5，扩容后脱离了原链表，落在新数组第37桶位上</p>
<p>key为309、3189的节点原位置21，扩容后脱离了原链表，落在新数组第53桶位上</p>
<p>以上节点在扩容后，如何确定哪些节点应该落到新数组的哪个位置呢？在以下关于数组扩容（resize又称rehash）知识点，可以再次说明数组容量设为2的整数幂是有多方便。</p>
<h6 id="扩容前后，节点迁移过程"><a href="#扩容前后，节点迁移过程" class="headerlink" title="扩容前后，节点迁移过程"></a>扩容前后，节点迁移过程</h6><p>根据桶位计算方法：hash(key)&amp;(table.size-1)，对所有key：0，1，5，21，37，133，309，3189使用二进制方式计算</p>
<p>从HashMap内置的hash方法可以得出，对数值key采用hash方法计算后还是这个数值key本身，因此相当于所有key的值，与15进行与运算</p>
<p>对于0，1这两个key，很简单:<br><img src="https://img-blog.csdnimg.cn/3246391442514355809a8d6a7cb7909a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>对于5，21，37，133，309，3189这5个key，你会发现这些key的低4位都是0101，如下图红色标记所示，和(16-1)与后，高位直接置0，仅留低4位，因此5个key的桶位索引号都为5，换句话说这些key在桶位为5的位置发生冲突，放入HashMap后，就形成一条长链表如前面图示。<br><img src="https://img-blog.csdnimg.cn/8d75e1fcebd044ba895d5e8d83e8a900.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ENvfwVJ1-1621500146350)(/Users/kevent/Desktop/temp/java_all/HashMap结构图/截屏2021-01-31 22.49.10.png)\]"></p>
<ul>
<li>数组被扩容后，容量为32的节点分布计算</li>
</ul>
<p>此时<code>hash(key)&amp;(table.size-1)</code>里面的table.size-1的值为32-1，也即31，对应二进制位11111,</p>
<p>对于0，1这两个key，很简单:<br><img src="https://img-blog.csdnimg.cn/6f78100066eb42639bdea8e6f8ccf6db.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>对于5，21，37，133，309，3189这5个key，对比数组容量16的情况，和(32-1)与后，高位直接置0，仅留低5位。key的低4位还是0101，对于key第5位要么0要么1，因此和11111与运算后，高位（第5位）有1的key，落在21号桶位上，高位（第5位）有0的key还是落在5号桶位上，因此5、37、133最终保持在index=5位置上，而21、309、3189落在index=21位置上，如下图所示：<br><img src="https://img-blog.csdnimg.cn/d93f8fb85d9544568ec3e0bf64d57cea.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<p>以上得出重要的扩容节点位置定位逻辑，如下图所示：该图能可视化解释了resize源码里面关于低位节点和高位节点如何挪动的设计，图的左边是扩容前容量16的HashMap结构，图的右边是扩容后容量32的新HashMap结构<br><img src="https://img-blog.csdnimg.cn/65d45409fe3541dba3526dc2e72e64b0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<ul>
<li><p>数组被扩容后，容量为64的节点分布计算</p>
<p>这里不再给出，逻辑跟上面一致。</p>
</li>
</ul>
<h6 id="理解高节点hiNode和低节点loNode"><a href="#理解高节点hiNode和低节点loNode" class="headerlink" title="理解高节点hiNode和低节点loNode"></a>理解高节点hiNode和低节点loNode</h6><p>有两种方式可以定义，这里以高节点hiNode看看它具备什么特征：</p>
<p>方式1（从新数组容量的角度出发）：假设新数组容量-1对应的二进制位数为n，key哈希值的二进制位从左往右数的第n位为1的话，那么key所属节点即为高节点，例如数组32-1，对应二进制11111位数为5，对于key为21，其hash值也为21，二进制为10101，第5位为1，那么这个21所属节点则称为“高节点”（或高位节点）</p>
<p>同理：假设新数组容量-1对应的二进制值位数为n，key哈希值的二进制位从左往右数的第n位为0的话，那么这key所属节点则称为“低节点”（或低位节点），例如对于对于数组32-1，对应二进制11111位数为5，对于key为5，其hash值也为5，二进制为00101，第5位为0，所以key为5的节点则称为“低节点”（或低位节点）</p>
<p>方式2（从旧数组容量的角度出发）：旧表容量对应的二进制和key哈希值&amp;后的值，该值最高位为1，那么key所属节点即为高节点</p>
<p>(这里要小心了，方式2说的是旧数组容量，不是方式1所提的新数组容量，新数组容量=旧数组容量&lt;&lt;1)</p>
<p>实例说明：旧数组容量16的二进制10000，key为21的哈希值二进制为10101，两者&amp;后，10000，最高位为1，所以这个21所属节点就是高节点。</p>
<p>同理：假设旧容量对应的二进制和key哈希值&amp;后的结果（注意这个结果会是0），该值最高位为0，那么key所属节点即为低节点</p>
<p>实例说明：旧数组容量16的二进制10000，key为133的哈希值二进制为100 0101，两者&amp;后，值为000 0000，当然最高位为0，所以这个133所属节点就是低节点。</p>
<p>16 &amp; 133 =&gt; 001 0000 &amp;100 0101，结果为0</p>
<h6 id="HashMap哪些位置上节点才是高节点和低节点对应的特征呢？"><a href="#HashMap哪些位置上节点才是高节点和低节点对应的特征呢？" class="headerlink" title="HashMap哪些位置上节点才是高节点和低节点对应的特征呢？"></a>HashMap哪些位置上节点才是高节点和低节点对应的特征呢？</h6><p>经过前面的原理分析，HashMap的结构无非就是数组+冲突链表(bins)+红黑树(tree bins)，所以高节点和低节点的分布如下：</p>
<p>1）数组桶位上的节点：要么是高位节点，要么是低位节点</p>
<p>2）若桶位上已形成冲突链表，那么链表上节点类型可能有三种情况：A、全部节点都是低节点 （低位节点）B、全部节点都是高节点（高位节点）C、低节点和高节点都有</p>
<p>3) 桶位上是一个tree bins红黑树，树上节点类似跟2）情况类似：A、全部树节点都是低节点 （低位节点）B、全部树节点都是高节点（高位节点）C、树所有节点既有含低节点又有高节点</p>
<p>以上三点内容，结合节点扩容迁移图可知：</p>
<p>1）数组桶位、冲突链、红黑树上的高节点，扩容后，在新数组的位置index发生改变，index=原index+原数组容量</p>
<p>2）数组桶位、冲突链、红黑树上的低节点，在新数组的位置index和原数组的位置一样，没有改变，即index=原index</p>
<p>费这么大劲去理解所谓“高节点和低节点”有何用？因为它就是resize内部最核心的设计，理解了它，resize工作原理才能真正理解，也能真正理解为何数组容量要设计为2的整数幂。</p>
<h6 id="resize-核心内容"><a href="#resize-核心内容" class="headerlink" title="resize()核心内容"></a>resize()核心内容</h6><p>有了前面原理性分析的铺垫，针对旧数组已有节点的情况下的resize的源码分析将“易如反掌”，resize就是把所有的Node节点重新Hash挪（迁移）到新数组new table上，挪动节点的位置需要按一定规则去挪。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line">   			<span class="comment">// 临时变量oldTab：表示旧数组，</span></span><br><span class="line">        Node&lt;K,V&gt;[] oldTab = table;</span><br><span class="line">        <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</span><br><span class="line"> 				<span class="comment">// ....省略部分</span></span><br><span class="line">        Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap]; <span class="comment">// newTab容量一定是2倍旧数组容量</span></span><br><span class="line">        table = newTab;</span><br><span class="line">   </span><br><span class="line">        <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123;  <span class="comment">// 旧数组有节点的情况下才进行扩容</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;  <span class="comment">// 遍历数组每个元素（或者说遍历每一个桶位（槽位））</span></span><br><span class="line">                Node&lt;K,V&gt; e; <span class="comment">// 临时变量，oldTab[j]表示key定位到的桶位头节点，赋给这个临时变量e</span></span><br><span class="line">                 <span class="comment">/* </span></span><br><span class="line"><span class="comment">                 e = oldTab[j];</span></span><br><span class="line"><span class="comment">                 if (e != null)&#123; 如果旧数组桶位j节点不为空</span></span><br><span class="line"><span class="comment">                 		oldTab[j] = null; //  则先该桶位指向null，用于GC回收</span></span><br><span class="line"><span class="comment">                 &#125;</span></span><br><span class="line"><span class="comment">                 */</span> </span><br><span class="line">                <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    oldTab[j] = <span class="keyword">null</span>;</span><br><span class="line">                  	 <span class="comment">//1、桶位单个节点迁移</span></span><br><span class="line">                    <span class="keyword">if</span> (e.next == <span class="keyword">null</span>) <span class="comment">// oldTab[j].next为空，说明这是个单个头节点，不是一条链表，先计算其在新数组的位置i=e.hash &amp; (newCap - 1)，然后newTab[i]=e，完成了这个单个头节点挪到新数组上。注意新数组的位置不是使用该计算方式：e.hash &amp; oldCap</span></span><br><span class="line">                        newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                    <span class="comment">//2、红黑树节点迁移</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 如果oldTab[j]是一个TreeNode，那么进行红黑树的扩容处理，本章暂不给出相关分析，在另外一篇文章给出。</span></span><br><span class="line">                        ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">                   <span class="comment">//3、冲突链节点迁移</span></span><br><span class="line">                    <span class="keyword">else</span> &#123; <span class="comment">// 到了这里oldTab[j]上有一条冲突链，需要将高节点和低节点分别取出，放到新数组对应位置。</span></span><br><span class="line">                      </span><br><span class="line">                        <span class="comment">// preserve order,这是源码注释：1.8的HashMap扩容后还可以链表上节点前后指向不变。乍一看好像没啥，其实以下代码说表达就是面试过程最核心的一个问题： 头插法和尾插法、为何什么java 7的头插法在扩容阶段容易因此死循环？ </span></span><br><span class="line"></span><br><span class="line">                     </span><br><span class="line"><span class="comment">// 低节点构成新链表的头、尾节点临时变量：loHead-&gt;node-&gt;node-...-&gt;loTail-&gt;null, 存放旧数组oldTab[j]冲突链上的低节点</span></span><br><span class="line">                        Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line"><span class="comment">// 高节点构成新链表的头、尾节点临时变量：hiHead-&gt;node-&gt;node-...-&gt;hiTail-&gt;null, 存放旧数组oldTab[j]冲突链上的高节点</span></span><br><span class="line">                        Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                        Node&lt;K,V&gt; next; <span class="comment">// 临时变量，存放遍历原冲突链表的下一个节点,和e搭配，形成循环</span></span><br><span class="line">                        <span class="comment">/*</span></span><br><span class="line"><span class="comment">                        do&#123;</span></span><br><span class="line"><span class="comment">                        		next=e.next;</span></span><br><span class="line"><span class="comment">                        		//低高链表处理</span></span><br><span class="line"><span class="comment">                        		e=next;</span></span><br><span class="line"><span class="comment">                        &#125; while(e != null)</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                        <span class="keyword">do</span> &#123;</span><br><span class="line">                            next = e.next;</span><br><span class="line">                            <span class="comment">// 从前面低节点特征定义可知，冲突链表的低节点和旧数组容量&amp;后为0，例如</span></span><br><span class="line">                            <span class="comment">// *** 0 0101 &amp; 10000 = 0 0000 =&gt;0</span></span><br><span class="line">                            <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123;</span><br><span class="line">                              <span class="keyword">if</span> (loTail == <span class="keyword">null</span>) <span class="comment">// 如果loTail为空，说明还没创建低节点链表</span></span><br><span class="line">                                    loHead = e; <span class="comment">// 因此将当前遍历的e节点作为低节点链表的头节点</span></span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                   <span class="comment">// 说明已存在低节点链表，只需将当前遍历的e节点作为该链的尾节点即可。</span></span><br><span class="line">                                    loTail.next = e;</span><br><span class="line">                                loTail = e; <span class="comment">// 用于后面：if (loTail != null) loTail.next = null;</span></span><br><span class="line">                            &#125;</span><br><span class="line">                            <span class="keyword">else</span> &#123;</span><br><span class="line">                             <span class="comment">//从前面低节点特征定义可知，冲突链表的高节点和旧数组容量&amp;后为1 </span></span><br><span class="line">                                <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>) <span class="comment">// 如果hiTail为空，说明还没创建高节点链表</span></span><br><span class="line">                                    hiHead = e; <span class="comment">// 因此将当前遍历的e节点作为高节点链表的头节点</span></span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                  <span class="comment">// 说明已存在高节点链表，只需将当前遍历的e节点作为该链的尾节点即可。</span></span><br><span class="line">                                    hiTail.next = e;</span><br><span class="line">                                hiTail = e; <span class="comment">// 用于高节点链表遍历</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</span><br><span class="line">                        <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123; <span class="comment">// loTail不空，说明从旧桶位j的冲突链表上“分离出”低位节点链表</span></span><br><span class="line">                            loTail.next = <span class="keyword">null</span>; <span class="comment">// 这里表明java8的HashMap扩容阶段的冲突链上采用尾插法</span></span><br><span class="line">                            newTab[j] = loHead; <span class="comment">// 低节点链表放在新数组的（原index）对应的位置</span></span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123; <span class="comment">// hiTail不为空，明从旧桶位j的冲突链表上“分离出”高位节点链表</span></span><br><span class="line">                            hiTail.next = <span class="keyword">null</span>;</span><br><span class="line">                            newTab[j + oldCap] = hiHead; <span class="comment">// 高节点链表放在新数组的（原index+原数组容量）对应的位置</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> newTab; <span class="comment">// 返回扩容后的新数组引用</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里的难点在 ：loHead、loTail 以及hiHead、hiTail的工作原理：</p>
<p>旧数组桶位号index=5上的冲突链：</p>
<p><code>5-&gt;21-&gt;37-&gt;133-&gt;309-&gt;3189-&gt;null</code>，节点迁移到新数组，被<code>if (e.hash &amp; oldCap) == 0)</code>分成以下两条链</p>
<p>新数组桶位号index=5上的低节点链：</p>
<p><code>loHead(5)-&gt;node(37)-&gt;loTail(133)-&gt;null</code></p>
<p>新数组桶位号index=5+旧数组capacity上的高节点链：</p>
<p><code>hiHead(21)-&gt;node(309)-&gt;hiTail(3189)-&gt;null</code></p>
<p>可以看出，在old table已经有元素的情况下，resize的过程会触发较多操作（创建新数组、红黑树spit、原冲突链分开为高低节点链等），换句话说，若频繁触发resize，会拖慢HashMap实际使用性能，因此在使用过程中，可以预估好容量再new HashMap</p>
<h5 id="8、get方法"><a href="#8、get方法" class="headerlink" title="8、get方法"></a>8、get方法</h5><p>有了前面putVal和resize的深度解析，关于get、remove、replace方法都很容易理解，从注释可以看出HashMap的get方法是实现了Map接口的get方法，从源码可以知，getNode才是内部实现。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implements Map.get and related methods.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> hash, Object key)</span> </span>&#123;</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">   tab:当前HashMap里面数组table的引用</span></span><br><span class="line"><span class="comment">   first：桶位中的头节点</span></span><br><span class="line"><span class="comment">   n：table的长度</span></span><br><span class="line"><span class="comment">   e：临时Node节点</span></span><br><span class="line"><span class="comment">   k：key的临时变量</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; <span class="keyword">int</span> n; K k;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      hash=hash(key);</span></span><br><span class="line"><span class="comment">    	tab=table;</span></span><br><span class="line"><span class="comment">  		n=tab.length;</span></span><br><span class="line"><span class="comment">  		first = tab[(n - 1) &amp; hash;</span></span><br><span class="line"><span class="comment">  		if(tab != null &amp;&amp; n&gt;0 &amp;&amp; first !=null) 如果table不为空 且长度&gt;0且所定位到的桶位节点不为空</span></span><br><span class="line"><span class="comment">  	*/</span>	</span><br><span class="line">    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">      	</span><br><span class="line">      <span class="comment">// 先判断桶位上的first节点key是否与给定key一致，若一致则返回该桶位头节点first</span></span><br><span class="line">        <span class="keyword">if</span> (first.hash == hash &amp;&amp; </span><br><span class="line">            ((k = first.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            <span class="keyword">return</span> first;</span><br><span class="line">      <span class="comment">// 给定key所定位到的桶位节点的next指针不为空，说明该桶位上要么有一条冲突链，要么有一棵红黑树</span></span><br><span class="line">        <span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 1、针对当前桶位是一个红黑树节点的情况</span></span><br><span class="line">            <span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 如果first是TreeNode类型，那么使用getTreeNode继续去查找key</span></span><br><span class="line">                <span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);</span><br><span class="line">            <span class="keyword">do</span> &#123;</span><br><span class="line">             <span class="comment">// 2、针对当前桶位是一条冲突链的情况。</span></span><br><span class="line">               <span class="comment">// 否则在冲突链表上进行遍历，若当前遍历节点e的hash与给定key的hash一致且值相等，说明已找到，返回该e即可。</span></span><br><span class="line">                <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                    ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                    <span class="keyword">return</span> e;</span><br><span class="line">            &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getNode逻辑比较简单（红黑树的getTreeNode逻辑会更复杂）。</p>
<h5 id="9、remove方法"><a href="#9、remove方法" class="headerlink" title="9、remove方法"></a>9、remove方法</h5><p>内部逻辑由removeNode实现。</p>
<p>整体思想：先根据key数组去找到node（node可能位于数组索引位置上（桶位）头节点，或者冲突链上，或者红黑树上），然后再使用内部的removeNode方法删除该node</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = removeNode(hash(key), key, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">true</span>)) == <span class="keyword">null</span> ?</span><br><span class="line">        <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implements Map.remove and related methods.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> hash hash for key，给定key的哈希值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> key the key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value the value to match if matchValue, else ignored</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> matchValue if true only remove if value is equal，要求key和value相同时才删除该节点</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> movable if false do not move other nodes while removing，是否可删除节点，</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the node, or null if none</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">			</span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">removeNode</span><span class="params">(<span class="keyword">int</span> hash, Object key, Object value,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">boolean</span> matchValue, <span class="keyword">boolean</span> movable)</span> </span>&#123;</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">   tab:当前HashMap数组table的引用</span></span><br><span class="line"><span class="comment">   p：桶位中的头节点,p=tab[index = (n - 1) &amp; hash]</span></span><br><span class="line"><span class="comment">   n：table的长度,n = tab.length</span></span><br><span class="line"><span class="comment">   index：数组元素索引号(key定位到的桶位置)，index = (n - 1) &amp; hash</span></span><br><span class="line"><span class="comment">   */</span>        </span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, index;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      hash=hash(key);</span></span><br><span class="line"><span class="comment">    	tab=table;</span></span><br><span class="line"><span class="comment">  		n=tab.length;</span></span><br><span class="line"><span class="comment">  		index = (n - 1) &amp; hash;</span></span><br><span class="line"><span class="comment">  		p = tab[index];</span></span><br><span class="line"><span class="comment">  		if(tab != null &amp;&amp; n&gt;0 &amp;&amp; first !=null) 如果table不为空 且长度&gt;0且所定位到的桶位节点p不为空</span></span><br><span class="line"><span class="comment">  	*/</span>	      </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (p = tab[index = (n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        Node&lt;K,V&gt; node = <span class="keyword">null</span>, e; K k; V v;</span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">         Node&lt;K,V&gt; node = null,临时节点，用于放置找到与给定key相同的节点</span></span><br><span class="line"><span class="comment">         Node&lt;K,V&gt; e,遍历冲突链时的临时节点</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         </span><br><span class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">            <span class="comment">// 当前桶位上的节点p与给定的key相同，说明p就是要删除的节点。</span></span><br><span class="line">            ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            node = p;</span><br><span class="line">        <span class="comment">// 若key不同，则桶位上p节点要么是tree bin红黑树根结点，要么是冲突链bins的根节点</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((e = p.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">              <span class="comment">// 若p节点是TeeNode类型，则在红黑树找出与key相同的节点</span></span><br><span class="line">                node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key);</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="comment">// 否则在冲突链上找到与给定key相同的节点</span></span><br><span class="line">                <span class="keyword">do</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                        ((k = e.key) == key ||</span><br><span class="line">                         (key != <span class="keyword">null</span> &amp;&amp; key.equals(k)))) &#123;</span><br><span class="line">                        node = e; </span><br><span class="line">                        <span class="keyword">break</span>; <span class="comment">// 在冲突链上找到要删除的节点，则结束链表遍历</span></span><br><span class="line">                    &#125;</span><br><span class="line">                    p = e;</span><br><span class="line">                &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 上面根据给定key找到这个节点若不为空，且不需要value匹配</span></span><br><span class="line">  <span class="comment">/* 以下两句表达式一个是判断给定key与当前定位的节点key是否相等</span></span><br><span class="line"><span class="comment">      一个是判断给定value与当前定位的节点的value是否相等</span></span><br><span class="line"><span class="comment">  (e.hash == hash &amp;&amp;((k = e.key) == key ||(key != null &amp;&amp; key.equals(k))))</span></span><br><span class="line"><span class="comment">  (v = node.value) == value || (value != null &amp;&amp; value.equals(v))</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">        <span class="keyword">if</span> (node != <span class="keyword">null</span> &amp;&amp; (!matchValue || (v = node.value) == value ||</span><br><span class="line">                             (value != <span class="keyword">null</span> &amp;&amp; value.equals(v)))) &#123;</span><br><span class="line">          	<span class="comment">// 如果找到的node节点是红黑树类型节点，则对应执行红黑树节点删除方法</span></span><br><span class="line">            <span class="keyword">if</span> (node <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                ((TreeNode&lt;K,V&gt;)node).removeTreeNode(<span class="keyword">this</span>, tab, movable);</span><br><span class="line">            <span class="comment">// 如果找到的node节点就是桶位上的头节点p，则node下一个节点放到桶位上即可</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (node == p)</span><br><span class="line">                tab[index] = node.next;</span><br><span class="line">            <span class="comment">// 如果找到的node节点是冲突链上节点，将node下一个节点放到p的next，完成node节点本身的删除</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                p.next = node.next;</span><br><span class="line">            ++modCount; <span class="comment">// 因为前面已经删除了一个节点，所以HashMap结构发生了一次变化，需要记录</span></span><br><span class="line">            --size;</span><br><span class="line">            afterNodeRemoval(node);<span class="comment">// 删除一个节点后回调方法，默认为空操作。</span></span><br><span class="line">            <span class="keyword">return</span> node;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对于三种类型节点的删除逻辑代码设计如下图所示：<br><img src="https://img-blog.csdnimg.cn/70feb1200b544a62b354c0879ae4489b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeWllbGQtYnl0ZXM=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h5 id="10、putAll"><a href="#10、putAll" class="headerlink" title="10、putAll"></a>10、putAll</h5><p>将一个HashMap放入到另外一个HashMap里面</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;Integer, Object&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">Map&lt;Integer,String&gt; map1=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">map.put(<span class="number">0</span>,<span class="string">&quot;foo0&quot;</span>);</span><br><span class="line">map.put(<span class="number">1</span>,<span class="string">&quot;foo1&quot;</span>);</span><br><span class="line">map.put(<span class="number">5</span>,<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">map1.put(<span class="number">21</span>,<span class="string">&quot;foo21&quot;</span>);</span><br><span class="line">map1.put(<span class="number">37</span>,<span class="string">&quot;foo37&quot;</span>);</span><br><span class="line">map.putAll(map1);</span><br><span class="line">System.out.println(map); <span class="comment">// &#123;0=foo0, 1=foo1, 5=10, 21=foo21, 37=foo37&#125;</span></span><br></pre></td></tr></table></figure>
<p>源码实现放在putMapEntries方法里面：</p>
<p>(总体设计：将新map里面元素放入到旧map里面，例如Map<Integer, Object>可以容纳Map<Integer,String>类型、入Map<Integer,Interger>类型)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 注意Map的key和value是上界通配符，注意类型兼容情况</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">putAll</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span> </span>&#123;</span><br><span class="line">      putMapEntries(m, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">putMapEntries</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m, <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">int</span> s = m.size(); <span class="comment">// 新map的元素个数</span></span><br><span class="line">      <span class="keyword">if</span> (s &gt; <span class="number">0</span>) &#123; <span class="comment">// </span></span><br><span class="line">          <span class="keyword">if</span> (table == <span class="keyword">null</span>) &#123; <span class="comment">// 新map的数组容量有无超过旧map设定的扩容阈值，其实需带入相关默认值即可了解下面两句代码的意义，例如旧map使用new HashMap&lt;&gt;(),那么默认容量为16，loadFactor=0.75，threshold=12</span></span><br><span class="line">            <span class="comment">//若新map目前有3个元素，则s=3,因此ft=3.0F/0.75F+1.0F=5.0F</span></span><br><span class="line">            <span class="comment">//t= 5,显然t小于旧map的扩容阈值，因此无需进行tableSizeFor</span></span><br><span class="line">            <span class="comment">// 简单说，旧map容量为16，新map才3个元素，当然可以直接放入，无需扩容等操作。</span></span><br><span class="line">              <span class="keyword">float</span> ft = ((<span class="keyword">float</span>)s / loadFactor) + <span class="number">1.0F</span>;</span><br><span class="line">              <span class="keyword">int</span> t = ((ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY) ?</span><br><span class="line">                       (<span class="keyword">int</span>)ft : MAXIMUM_CAPACITY);</span><br><span class="line">              <span class="keyword">if</span> (t &gt; threshold)</span><br><span class="line">                  threshold = tableSizeFor(t);</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> (s &gt; threshold) <span class="comment">// 新map的元素个数超过旧map的扩容阈值，则直接先对旧map进行扩容操作</span></span><br><span class="line">              resize();</span><br><span class="line">          <span class="comment">// 当旧map的容量足够容纳新map的元素个数，则遍历新map元素，复用putVal方法，将其放入到旧map里面</span></span><br><span class="line">          <span class="keyword">for</span> (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123;</span><br><span class="line">              K key = e.getKey();</span><br><span class="line">              V value = e.getValue();</span><br><span class="line">              putVal(hash(key), key, value, <span class="keyword">false</span>, evict);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125; <span class="comment">// 若新map的元素个数为空，则不做任何操作</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="三、小结"><a href="#三、小结" class="headerlink" title="三、小结"></a>三、小结</h4><p>关于java8的HashMap的其他方法如：<code>clear()</code>、’containsKey’、’containsValue’、’replace’等方法不再讲解，这些方法实现相对简单，关于HashMap红黑树部分的核心方法将在下一篇文章给出。</p>
<h4 id="四、部分经典问题讨论"><a href="#四、部分经典问题讨论" class="headerlink" title="四、部分经典问题讨论"></a>四、部分经典问题讨论</h4><h5 id="1、在桶位计算里面，hash方法为何采用异或运算而不是“或”“或非”等逻辑运算？"><a href="#1、在桶位计算里面，hash方法为何采用异或运算而不是“或”“或非”等逻辑运算？" class="headerlink" title="1、在桶位计算里面，hash方法为何采用异或运算而不是“或”“或非”等逻辑运算？"></a>1、在桶位计算里面，hash方法为何采用异或运算而不是“或”“或非”等逻辑运算？</h5><p>这个概念非常巧妙和重要：</p>
<ul>
<li><p>找出两个数有差异的位，a^b得到的结果中，1表示在该位两数存在差别，0表示无差别，回看hash的计算方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">h = key.hashCode() ^ (h &gt;&gt;&gt; <span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<p>这种方式能找出key1有差异的位，同理也可以找出key2有差异的位，那么这种方式计算出来key1的哈希值和key2的哈希值将不同，从而把key1和key2的hash值“尽量分散”。</p>
<p>（1）异或运算的作用能够识别出key的hash高16位只要有一位不同，最终使得不同key对应不同的hash值</p>
<p>（2）hash计算采用异或运算能够让两个hash整数运算后，不会出现溢出：不进位有什么好处，例如有个key的hash值为2^31-1，它和(2^31-2) 异或后，还是31位，且能找出有差异的位</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">异或其实就是是不带进位的加法器（又称半加法器）</span><br><span class="line">0+0&#x3D;0</span><br><span class="line">0+1&#x3D;1</span><br><span class="line">1+0&#x3D;1</span><br><span class="line">1+1&#x3D;0（不进位） </span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="2、HashMap的构造方法里面为何要将初始容量设为16，而不是2、4、8或者32或者其他奇数值？"><a href="#2、HashMap的构造方法里面为何要将初始容量设为16，而不是2、4、8或者32或者其他奇数值？" class="headerlink" title="2、HashMap的构造方法里面为何要将初始容量设为16，而不是2、4、8或者32或者其他奇数值？"></a>2、HashMap的构造方法里面为何要将初始容量设为16，而不是2、4、8或者32或者其他奇数值？</h5><p>1）首先数组容量长度设计为2的整数幂有两个收益</p>
<p>第一个收益：扩容阶段，能快速计算节点在新数组的桶位</p>
<p>第二个收益：扩容阶段，能快速将原数组的冲突链上的高位节点和低位节点“分离出来”，然后方便放到新数组相应桶位上</p>
<p>2）如果设为2、4、8或者32的情况：</p>
<p>对于数组容量设为2，4，8当然可以，但也导致了put的前期节点更加频繁扩容，而初始容量为32，对于又会有点点浪费内存，所以16算是一个相对平衡的值，这也是一个trade-off，空间和时间的权衡</p>
<h5 id="3、HashMap的构造方法里面为何要将load-factor设为0-75？而不是0-73、0-5、0-65、0-85？等，以下有三种解释："><a href="#3、HashMap的构造方法里面为何要将load-factor设为0-75？而不是0-73、0-5、0-65、0-85？等，以下有三种解释：" class="headerlink" title="3、HashMap的构造方法里面为何要将load factor设为0.75？而不是0.73、0.5、0.65、0.85？等，以下有三种解释："></a>3、HashMap的构造方法里面为何要将load factor设为0.75？而不是0.73、0.5、0.65、0.85？等，以下有三种解释：</h5><p>1）第一种是本人对其解释：0.75化为分数形式为3/4，那么基于数组容量会被tableSizeFor设为2的整数幂，分母的4，那么扩容阈值threshold=2的整数幂*3/4算出的值也是整数。按这种说法设为0.5也可以吗，当然可以，但是0.5降低扩容阈值，会让HashMap更早（更高概率）触发resize操作，影响HashMap性能。  对于0.65、0.85写成分数分别为13/20、17/20，显然对于数组容量为2^n来说，无法整除分母，计算threshold得出非整数，这种方式若放在JDK源码里面，显得很不严谨，也不合理（因为人类喜欢看到类似整数的东西，若出现小数即显得“繁琐”又不好”记忆”）</p>
<p>2）基于泊松分布概率</p>
<p>如果key的hash算法离散性好，那么不同key定位到某个桶位的这一事件将变成随机事件，恰好能用泊松分布来观察key的分布情况，而且当load factor设为0.75时，由于采用足够分布均匀的随机hashcode算法，那么在桶位上(hash槽)出现冲突链且元素个数达到8的概率为0.00000006，非常小，不到百万分之一。</p>
<p>换句话说，设为0.75时，只有百万分之一概率桶位上节点出现8个元素，显然这能大概率减少触发树化操作，从而提升HashMap性能。</p>
<p>3）官方说明</p>
<p>首先是 <a href="https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html">Oracle官方的Java docs</a></p>
<blockquote>
<p>An instance of <code>HashMap</code> has two parameters that affect its performance: <em>initial capacity</em> and <em>load factor</em>. </p>
<p>…..</p>
<p>As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the <code>HashMap</code> class, including <code>get</code> and <code>put</code>). </p>
</blockquote>
<p>As a general rule 也即根据经验（软件工程实践），0.75可以在时间成本和空间成本上取得最佳平衡，高于0.75，map填得越满，碰撞概率越高（想想抽屉原理或者结合上面的知识点即可理解）</p>
<p>其次来自wiki的一个中文解释，还不错，<a href="https://bk.tw.lvfukeji.com/baike-哈希表?wprov=srpw1_4">链接</a></p>
<blockquote>
<p>对于开放定址法，荷载因子是特别重要因素，应严格限制在0.7-0.8以下。超过0.8，查表时的CPU缓存不命中（cache missing）按照指数曲线上升。因此，一些采用开放定址法的hash库，例如Java的系统库限制了荷载因子为0.75。</p>
</blockquote>
<h5 id="4、java开发手册中为何建议将-int-float-expectedSize-0-75F-1-0F"><a href="#4、java开发手册中为何建议将-int-float-expectedSize-0-75F-1-0F" class="headerlink" title="4、java开发手册中为何建议将(int) ((float) expectedSize / 0.75F + 1.0F)"></a>4、java开发手册中为何建议将<code>(int) ((float) expectedSize / 0.75F + 1.0F)</code></h5><p>先看个简单计算：例如new HashMap(6)，在内部因为有tableSizeFor方法，会将数组给定容量从6变为8，那么下次扩容阈值就变成8*0.75=6，也即数组元素个数到达6时就触发resize，这个操作我们知道它让HashMap有性能损耗。</p>
<p>如果采用 expectedSize / 0.75F + 1.0计算出给定的初始容量，那么对于上面的6来说，initialCapacity=6/0.75+1.0=9，</p>
<p>也即使用new HashMap(9)做初始化，这样tableSizeFor方法会将其table容量设为16，于是扩容阈值变为12，也即数组元素个数到达12时再触发resize，显然比前面new HashMap(6)情况，已经降低一半的扩容概率。</p>
<p>当然代价就是牺牲了空间，原来占用8，现在占用16，占用空间增加一倍，但扩容概率降低一倍，典型的trade-off。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：基于AQS实现的ReentrantReadWriteLock源代码深入分析</title>
    <url>/2021/08/07/%E5%9F%BA%E4%BA%8EAQS%E5%AE%9E%E7%8E%B0%E7%9A%84ReentrantReadWriteLock%E6%BA%90%E4%BB%A3%E7%A0%81%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>在ReentrantLock的独占模式下，当需要在一个读写高度竞争场景中使用它的<code>lock.lock()</code>时，你会发现这是独占锁，它会让大量同时请求锁的线程们都不得不进入CLH阻塞队列中等待，如果在“读多写少”的场景中，ReentrantLock的这种独占锁方式显然会降低并发性能，因此ReentrantReadWriteLock就是为了解决这种“读多写少”的场景：一个线程正在请求锁进行读操作可以不影响其他线程同时请求读锁（共享锁），意味着多个线程可以并发读。</p>
<p>这里首先通过Doug Lea在源码注释给出的demo作为对ReentrantReadWriteLock一般用法说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CachedData</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 读线程需要对data读取，写线程可以对data进行更新，那么data显然是线程不安全的，需要借助锁进行相关操作</span></span><br><span class="line">  Object data;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">boolean</span> cacheValid; <span class="comment">// 判断data是否已经被缓存</span></span><br><span class="line">  <span class="comment">// 1、创建一个读写锁</span></span><br><span class="line">  <span class="keyword">final</span> ReentrantReadWriteLock rwl = <span class="keyword">new</span> ReentrantReadWriteLock(); </span><br><span class="line"> <span class="comment">//该方式的功能就是对已经缓存的数据进行预处理 </span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">processCachedData</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 2、由于一开始，我们乐观认为data可能已经被缓存，因此我们不急着申请独占锁，而是申请并发的共享读锁</span></span><br><span class="line">    rwl.readLock().lock();</span><br><span class="line">    <span class="comment">// 3、结果发现：我们太乐观了，原来data没缓存，因此还不能读取数据（如果非得去读，只能读到旧数据）</span></span><br><span class="line">    <span class="keyword">if</span> (!cacheValid) &#123;</span><br><span class="line">      <span class="comment">// Must release read lock before acquiring write lock</span></span><br><span class="line">      <span class="comment">// 4、既然乐观错估了情况，那么只能用上“强大的独占锁”，保证自己能独占的实施“写操作、更新操作、删除操作”：因为读写锁是互斥的，需先释放读锁，然后在升级为写锁。</span></span><br><span class="line">      <span class="comment">// </span></span><br><span class="line">      rwl.readLock().unlock();</span><br><span class="line">      <span class="comment">// 这就是所谓的“锁升级”。虽然说升级，但这里是当前线程请求独占锁，若同一时刻有其他线程已经拿到写锁，那么在这里当前线程会被阻塞在AQS里面的CLH阻塞队列</span></span><br><span class="line">      rwl.writeLock().lock(); </span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// Recheck state because another thread might have</span></span><br><span class="line">        <span class="comment">// acquired write lock and changed state before we did.</span></span><br><span class="line">        <span class="comment">// 5、由于我们获取写锁的时机也许比其他线程晚一步拿到，因此在这里拿到独占锁后，还得重新检查是否data已经提前被其他线程更新过</span></span><br><span class="line">        <span class="keyword">if</span> (!cacheValid) &#123;</span><br><span class="line">          data = ... <span class="comment">// 例如从数据库重新读取最新的data，然后将cacheValid标记为true，表示data已经更新（或已经更新缓存）</span></span><br><span class="line">          cacheValid = <span class="keyword">true</span>; </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 6、由于我们还得使用更新后的data，因此可以申请读锁。注意，由于当前线程还持有写锁，因此其他线程不可能获得写锁进行写操作，因此当前线程此时可以申请读锁</span></span><br><span class="line">        <span class="comment">// Downgrade by acquiring read lock before releasing write lock</span></span><br><span class="line">        rwl.readLock().lock();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 7、显然data已经被我“独占式”地更新过，可以释放写锁了</span></span><br><span class="line">        rwl.writeLock().unlock(); <span class="comment">// Unlock write, still hold read</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 8、在第6点获得共享的读锁后，在这里可以使用已经缓存的新data</span></span><br><span class="line">      use(data);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="comment">// 9、释放读锁</span></span><br><span class="line">      rwl.readLock().unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="关于读锁、写锁、读线程、写线程的一些说明"><a href="#关于读锁、写锁、读线程、写线程的一些说明" class="headerlink" title="关于读锁、写锁、读线程、写线程的一些说明"></a>关于读锁、写锁、读线程、写线程的一些说明</h4><p><code>rwl.readLock().lock()</code> `:很明显，线程在请求读锁，此锁是AQS的共享模式</p>
<p><code>rwl.writeLock().lock()</code>:很明显，线程在请求写锁，此锁是AQS的独占模式</p>
<p>严格来说：对于ReentrantReadWriteLock这种读写锁的设计，请求读锁的线程直接称为“读线程”可以吗，例如，请求写锁的线程，由于已经获取独占锁的线程可以去请求读锁，那么这个线程是应该称为“写线程”？“读线程”？还是“写、读线程”？还是“读写线程”呢？ 这么看来，请求到读锁的线程似乎也不适合称为读线程？成功请求到写锁的线程也不适合称为写线程。也许换一个角度可以更容易区分：</p>
<p>在用户代码层面的角度出发：</p>
<blockquote>
<p>1、用户设计此方法具有明显“读取”数据的逻辑时，该方法首先使用<code>`rwl.readLock().lock()</code> `，过程中不管是否需要再次请求写锁，都可以把执行用户方法的线程称为“读线程”</p>
<p>2、用户设计此方法具有明显“更新、删除、改、插入”数据的逻辑时，该方法首先使用<code>`rwl.writeLock().lock()</code> `，过程中不管是否需要再次请求写锁、还是读锁，都可以把执行用户方法的线程称为“写线程”</p>
</blockquote>
<p>但是从ReentrantReadWriteLock内部设计来看，可能按以下思路去想会更清晰：</p>
<blockquote>
<p>1、仅在ReentrantReadWriteLock内部调用tryAcquire方法来看，调用tryAcquire方法的线程可以看成是写线程，因为它是从<code>rwl.writeLock().lock()</code>来的</p>
<p>2、仅在ReentrantReadWriteLock内部调用tryAcquireShared方法来看，调用tryAcquireShared方法的线程可以看成是读线程，因为它是从<code>rwl.readLock().lock()</code>来的</p>
</blockquote>
<a id="more"></a>
<p>如果不按”用户代码”或者不按<code>ReentrantReadWriteLock</code>内部设计的角度，那么也可以按以下说明进行统一化描述：</p>
<p>“请求读锁的线程”、“持有读锁的线程”</p>
<p>“请求写锁的线程”、“持有写锁的线程”</p>
<p>在持有写锁的情况下，请求读锁的线程。</p>
<p>不存在“持有读锁的情况下，还能同时持有写锁的线程”的情形！  （否则此读锁已不是共享锁，而是变相成为了独占锁）或者说不满足“读写互斥设计”</p>
<h4 id="关于同步状态值的设计"><a href="#关于同步状态值的设计" class="headerlink" title="关于同步状态值的设计"></a>关于同步状态值的设计</h4><p>在ReentrantLock中，如何记录线程已经获得锁资源的次数呢？回顾代码设计如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">final</span> Thread current = Thread.currentThread();</span><br><span class="line">    <span class="keyword">int</span> c = getState();</span><br><span class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>) &#123; <span class="comment">// 只有state为0时，其他线程才有机会争抢独占锁</span></span><br><span class="line">        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) &#123;</span><br><span class="line">            setExclusiveOwnerThread(current);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;  <span class="comment">// 同一线程再次获取独占锁，同一线程重入锁</span></span><br><span class="line">        <span class="keyword">int</span> nextc = c + acquires;</span><br><span class="line">        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">        setState(nextc);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ReentrantLock仅有一个state值，对于同一线程执行<code>lock.lock()</code>n次，那么state变为n，表示此线程获取独占锁n次或者说重入n次（当然也要求此线程要释放n次），既然是独占锁，那么就不会出现这种情况：有多个不同线程同时成功更改state，也即同一个时刻，仅能有个线程CAS更改成功。</p>
<p>而在ReentrantReadWriteLock的同步状态值中，state的设计非常巧妙，使用高16位作为读锁重入的计数，低16位作为线程写锁重入的计数</p>
<blockquote>
<p>对于一个int值通过位运算实现不同场景下的计数，在Doug Lea的并发设计里面一个高级的手段：例如在ConcurrentHashMap里面的resizeStamp也采用这种移位计算状态的策略，还有ConcurrentHashMap的TreeBin读写锁设计也是采用位运算策略。</p>
</blockquote>
<p>高16位作为读锁重入的计数有两种情况：</p>
<p>（1）此高16位的数值可表示同一读线程的请求的读锁总数（总重入数）</p>
<p>（2）此高16位的数值可表示多个读线程同时请求到读锁总数</p>
<p>低16位作为写锁计数的情况仅有一种：</p>
<p>因为写锁是独占锁，因此低16位一定是表示同一写线程请求的总独占锁数量（总重入数）</p>
<p>以下是读写锁的同步计数移位计算的设计</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">SHARED_SHIFT   = 16</span><br><span class="line">（1）读锁设计</span><br><span class="line">static final int SHARED_UNIT    = (1 &lt;&lt; SHARED_SHIFT);  用于获取state高16的读锁总数</span><br><span class="line">0000 0000 0000 0000 0000 0000 0000 0001</span><br><span class="line">// 1右移16位，对应的就是SHARED_UNIT</span><br><span class="line">0000 0000 0000 0001 0000 0000 0000 0000</span><br><span class="line"></span><br><span class="line">如何获取读锁总数？</span><br><span class="line">0000 0000 0000 0110 0000 0000 0000 0010;某一时刻state的值</span><br><span class="line">对其左移16位：</span><br><span class="line">0000 0000 0000 0000 0000 0000 0000 0110</span><br><span class="line">因此可以快速得出读锁数量：6个读锁（当然也同时存在某个线程重入2次的写锁）</span><br><span class="line">以上就是sharedCount的设计</span><br><span class="line">static int sharedCount(int c)    &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125;</span><br><span class="line"></span><br><span class="line">如何实现读锁加1呢？如果直接state+1，那么这个加1是加在低位上，显然不合理，其实也很简单：</span><br><span class="line">state+(1&lt;&lt;SHARED_SHIFT)，这就实现在在高16位的读锁加1 </span><br><span class="line"></span><br><span class="line">（2） 写锁设计</span><br><span class="line">static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; 用于获取state低16位的写线程数量</span><br><span class="line">任意的state值和这个独占锁掩码相与后即可得到state的低16位置,例如下面</span><br><span class="line">0000 0000 0000 0110 0000 0000 0000 0010 ；;某一时刻state的值</span><br><span class="line">0000 0000 0000 0000 1111 1111 1111 1111 ；独占锁掩码</span><br><span class="line">两者取与后，得到写线程数量：</span><br><span class="line">0000 0000 0000 0000 0000 0000 0000 0010 </span><br><span class="line">因此可以快速得出写锁数量：2个写锁（当然也同时存在6个读锁）</span><br><span class="line">以上exclusiveCount的设计</span><br><span class="line">static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125;</span><br><span class="line"></span><br><span class="line">写锁如何加1？</span><br><span class="line">state+(1 &amp; EXCLUSIVE_MASK),显然state+1就是在state的低16位上做累加，计算公式合理</span><br><span class="line">写锁如何加n?</span><br><span class="line">state+(n &amp; EXCLUSIVE_MASK),显然state+n就是在state的低16位上做累加，计算公式合理</span><br><span class="line"></span><br><span class="line">（3）读锁最大可重入数量为65535，当然写锁也是</span><br><span class="line">static final int MAX_COUNT      = (1 &lt;&lt; SHARED_SHIFT) - 1</span><br><span class="line">0000 0000 0000 0001 0000 0000 0000 0000 减1 也即：</span><br><span class="line">0000 0000 0000 0000 1111 1111 1111 1111 </span><br></pre></td></tr></table></figure>
<h4 id="写锁实现"><a href="#写锁实现" class="headerlink" title="写锁实现"></a>写锁实现</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1、创建一个读写锁</span></span><br><span class="line"><span class="keyword">final</span> ReentrantReadWriteLock rwl = <span class="keyword">new</span> ReentrantReadWriteLock(); </span><br><span class="line"><span class="comment">// 2、请求写锁</span></span><br><span class="line">rwl.writeLock().lock(); </span><br></pre></td></tr></table></figure>
<p>原来写锁的获取并不像ReentrantLock使用<code>lock.lock()</code>的方式，而是通过<code>writeLock().lock()</code>获取，从方法名就知道这样设计为了方便使用者知道当前使用什么类型的锁。</p>
<p>首先看其构造器：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Creates a new &#123;<span class="doctag">@code</span> ReentrantReadWriteLock&#125; with</span></span><br><span class="line"><span class="comment">   * default (nonfair) ordering properties.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ReentrantReadWriteLock</span><span class="params">()</span> </span>&#123;  <span class="comment">// 默认构造器使用的是非公平模式</span></span><br><span class="line">      <span class="keyword">this</span>(<span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Creates a new &#123;<span class="doctag">@code</span> ReentrantReadWriteLock&#125; with</span></span><br><span class="line"><span class="comment">   * the given fairness policy.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> fair &#123;<span class="doctag">@code</span> true&#125; if this lock should use a fair ordering policy</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ReentrantReadWriteLock</span><span class="params">(<span class="keyword">boolean</span> fair)</span> </span>&#123;</span><br><span class="line">      sync = fair ? <span class="keyword">new</span> FairSync() : <span class="keyword">new</span> NonfairSync();</span><br><span class="line">      readerLock = <span class="keyword">new</span> ReadLock(<span class="keyword">this</span>);  <span class="comment">// new时就已经实例化一个readerLock 对象和 writerLock 对象</span></span><br><span class="line">      writerLock = <span class="keyword">new</span> WriteLock(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">	</span><br><span class="line"><span class="comment">// Sync当然是ReentrantReadWriteLock内部核心实现类，实现了AQS的tryAcquire、tryRelease、tryAcquireShared、tryReleaseShared的关键逻辑，以及其他辅助方法</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Sync</span> <span class="keyword">extends</span> <span class="title">AbstractQueuedSynchronizer</span></span></span><br><span class="line"><span class="class"><span class="title">static</span> <span class="title">final</span> <span class="title">class</span> <span class="title">NonfairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span></span></span><br><span class="line"><span class="class"><span class="title">static</span> <span class="title">final</span> <span class="title">class</span> <span class="title">FairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span></span></span><br><span class="line"><span class="class">    </span></span><br><span class="line"><span class="class">    </span></span><br><span class="line"><span class="class">// <span class="title">WriteLock</span>和 <span class="title">ReadLock</span> 都是<span class="title">ReentrantReadWriteLock</span>内部类</span></span><br><span class="line"><span class="class">  <span class="title">public</span> <span class="title">ReentrantReadWriteLock</span>.<span class="title">WriteLock</span> <span class="title">writeLock</span>() </span>&#123; <span class="keyword">return</span> writerLock; &#125; </span><br><span class="line">  <span class="keyword">public</span> ReentrantReadWriteLock.<span class="function">ReadLock  <span class="title">readLock</span><span class="params">()</span>  </span>&#123; <span class="keyword">return</span> readerLock; &#125; </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadLock</span> <span class="keyword">implements</span> <span class="title">Lock</span>, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span></span><br><span class="line"><span class="class">	<span class="title">public</span> <span class="title">static</span> <span class="title">class</span> <span class="title">WriteLock</span> <span class="keyword">implements</span> <span class="title">Lock</span>, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure>
<h5 id="WriteLock实现类"><a href="#WriteLock实现类" class="headerlink" title="WriteLock实现类"></a>WriteLock实现类</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteLock</span> <span class="keyword">implements</span> <span class="title">Lock</span>, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4992448646407690164L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Sync sync;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Constructor for use by subclasses</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lock the outer lock object</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> NullPointerException if the lock is null</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// WriteLock的调用点是在ReentrantReadWriteLock的构造器中：</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        public ReentrantReadWriteLock(boolean fair) &#123;</span></span><br><span class="line"><span class="comment">        sync = fair ? new FairSync() : new NonfairSync();</span></span><br><span class="line"><span class="comment">        readerLock = new ReadLock(this);  </span></span><br><span class="line"><span class="comment">        writerLock = new WriteLock(this);</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    */</span>     </span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">WriteLock</span><span class="params">(ReentrantReadWriteLock lock)</span> </span>&#123;</span><br><span class="line">        sync = lock.sync;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Acquires the write lock.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Acquires the write lock if neither the read nor write lock</span></span><br><span class="line"><span class="comment">     * are held by another thread</span></span><br><span class="line"><span class="comment">     * and returns immediately, setting the write lock hold count to</span></span><br><span class="line"><span class="comment">     * one.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;If the current thread already holds the write lock then the</span></span><br><span class="line"><span class="comment">     * hold count is incremented by one and the method returns</span></span><br><span class="line"><span class="comment">     * immediately.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;If the lock is held by another thread then the current</span></span><br><span class="line"><span class="comment">     * thread becomes disabled for thread scheduling purposes and</span></span><br><span class="line"><span class="comment">     * lies dormant until the write lock has been acquired, at which</span></span><br><span class="line"><span class="comment">     * time the write lock hold count is set to one.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 从这里可以看出：类似的用户程序使用rwl.writeLock().lock()时，其实就是调用AQS的独占锁的获取锁逻辑：acquire(1)，显然既然是独占锁，那么获取逻辑自然跟ReentrantLock的lock()是类似的，也即AQS的acquire方法.</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        sync.acquire(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  	...<span class="comment">//WriteLock类其他省略部分</span></span><br><span class="line">              </span><br></pre></td></tr></table></figure>
<p>显然<code>sync.acquire(1)</code>对应内部的AQS逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// tryAcquire由ReentrantReadWriteLock的sync实现</span></span><br><span class="line">    <span class="keyword">if</span> (!tryAcquire(arg) &amp;&amp;  </span><br><span class="line">        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class="line">        selfInterrupt();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="写锁获取-tryAcquire"><a href="#写锁获取-tryAcquire" class="headerlink" title="写锁获取-tryAcquire"></a>写锁获取-tryAcquire</h5><p>要知道所谓的写锁获取，其实就是独占锁的请求，要么仅能一个线程拥有此独占锁，要么同一线程重入独占锁多次。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">    <span class="comment">/* Walkthrough:攻略或玩法，也即tryAcquire的成功与否的策略</span></span><br><span class="line"><span class="comment">     * Walkthrough:</span></span><br><span class="line"><span class="comment">     * 1. If read count nonzero or write count nonzero</span></span><br><span class="line"><span class="comment">     *    and owner is a different thread, fail.</span></span><br><span class="line"><span class="comment">     * 2. If count would saturate, fail. (This can only</span></span><br><span class="line"><span class="comment">     *    happen if count is already nonzero.)</span></span><br><span class="line"><span class="comment">     * 3. Otherwise, this thread is eligible for lock if</span></span><br><span class="line"><span class="comment">     *    it is either a reentrant acquire or</span></span><br><span class="line"><span class="comment">     *    queue policy allows it. If so, update state</span></span><br><span class="line"><span class="comment">     *    and set owner.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    Thread current = Thread.currentThread();</span><br><span class="line">    <span class="keyword">int</span> c = getState(); <span class="comment">// 当前同步状态值</span></span><br><span class="line">    <span class="keyword">int</span> w = exclusiveCount(c); <span class="comment">// 当前写锁计数（需要利用移位来计算出）</span></span><br><span class="line">  	<span class="comment">// 由于同步状态可以表征读锁计数和写锁计数，因为当前线程想获取写锁，因此它需要判断：同步状态值不为0到底是指有其他线程获取到了写锁还是获取了读锁还是都被获取？</span></span><br><span class="line">    <span class="keyword">if</span> (c != <span class="number">0</span>) &#123;</span><br><span class="line">      	<span class="comment">// 1、c != 0且写锁计数为0，说明当前线程持有读锁，不允许请求写锁（否则就是持有读锁情况下拿到写锁，那么这个读锁就不是共享模式读锁，而是变相的独占锁，显然违背了ReentrantReadWriteLock的功能设计初衷）；或者说“当前线程持有读锁且不释放读锁的情况下，不允许升级为写锁，否则读锁就变相的成为了独占锁”</span></span><br><span class="line">       <span class="comment">// 2、写锁计数不为了0，说明是当前线程正在重入独占锁，如果不是同一线程请求，写锁当然请求失败，返回false</span></span><br><span class="line">        <span class="comment">// (Note: if c != 0 and w == 0 then shared count != 0)</span></span><br><span class="line">        <span class="keyword">if</span> (w == <span class="number">0</span> || current != getExclusiveOwnerThread())</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      	<span class="comment">// 3、是当前线程重入的写锁请求，但重入次数超过最大值，直接抛出异常提示</span></span><br><span class="line">        <span class="keyword">if</span> (w + exclusiveCount(acquires) &gt; MAX_COUNT)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">        <span class="comment">// Reentrant acquire</span></span><br><span class="line">      	<span class="comment">// 4、说明是同一线程的写锁重入，累加计数后，允许通过请求锁。</span></span><br><span class="line">        setState(c + acquires);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// 5、执行流来到这里说明c等于0，说明此刻暂时还没其他线程在竞争读、写锁，那么当前线程可以去CAS抢锁</span></span><br><span class="line">  <span class="comment">/* writerShouldBlock() 是用于是否使用公平模式抢锁策略，对于写锁获取策略来说，显然是false，如下所示</span></span><br><span class="line"><span class="comment">static final class NonfairSync extends Sync &#123;</span></span><br><span class="line"><span class="comment">	private static final long serialVersionUID = -8159625535654395037L;</span></span><br><span class="line"><span class="comment">  final boolean writerShouldBlock() &#123; return false; // writers can always barge ，写锁获取只能直接争抢</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">	因此这里的if第一个writerShouldBlock()返回false然后第二个条件就是去跟其他写线程CAS抢锁了，失败那么只能返回false</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">    <span class="keyword">if</span> (writerShouldBlock() ||</span><br><span class="line">        !compareAndSetState(c, c + acquires))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  <span class="comment">// 6、对于c != 0，说明此写线程重入成功，对于c=0，说明此线程是第一个获得独占锁的线程</span></span><br><span class="line">    setExclusiveOwnerThread(current);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="写锁释放-tryRelease"><a href="#写锁释放-tryRelease" class="headerlink" title="写锁释放-tryRelease"></a>写锁释放-tryRelease</h5><p>用户代码层面，只需调用<code>rwl.writeLock().unlock()</code>来释放写锁，需要注意的是，这里指代独占锁的释放，意味着要么已经拥有独占锁的写线程释放此锁，要么已经重入独占锁的写线程退出重入锁 </p>
<p>（读锁的释放也会使用此方法）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Attempts to release this lock.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;If the current thread is the holder of this lock then</span></span><br><span class="line"><span class="comment">     * the hold count is decremented. If the hold count is now</span></span><br><span class="line"><span class="comment">     * zero then the lock is released.  If the current thread is</span></span><br><span class="line"><span class="comment">     * not the holder of this lock then &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * IllegalMonitorStateException&#125; is thrown.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IllegalMonitorStateException if the current thread does not</span></span><br><span class="line"><span class="comment">     * hold this lock</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">//按照注释提示：</span></span><br><span class="line"><span class="comment">// 1、如果当前线程是重入的，那么每次unlock就是对重入次数减1，也即此线程内部写锁计数hold count 减1 （注意，如果hold count减1后不为0，此写线程是不会释放独占锁的）</span></span><br><span class="line"><span class="comment">// 2、如果当前线程的不是重入，仅是获取独占锁一次，那么当它调用unlock时，显然state变为0，意味着是释放独占锁，其他线程可以争抢此独占锁了。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        sync.release(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * Note that tryRelease and tryAcquire can be called by</span></span><br><span class="line"><span class="comment">     * Conditions. So it is possible that their arguments contain</span></span><br><span class="line"><span class="comment">     * both read and write holds that are all released during a</span></span><br><span class="line"><span class="comment">     * condition wait and re-established in tryAcquire.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 释放锁的逻辑</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!isHeldExclusively())</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();</span><br><span class="line">        <span class="keyword">int</span> nextc = getState() - releases; <span class="comment">// 扣减释放量</span></span><br><span class="line">        <span class="keyword">boolean</span> free = exclusiveCount(nextc) == <span class="number">0</span>; <span class="comment">// 这里的free是指写锁释放完全释放，如果等于0，说写锁完全被释放。</span></span><br><span class="line">      	<span class="comment">// 如果为0，说明写锁完全被释放，其他线程（含读、写线程）可以争抢了</span></span><br><span class="line">        <span class="keyword">if</span> (free)</span><br><span class="line">            setExclusiveOwnerThread(<span class="keyword">null</span>);</span><br><span class="line">      	<span class="comment">// free不为0，说明是同一线程重入写锁后，现在是释放自己重入的写锁，也即“重入-退出”操作，将state设为扣减后的值</span></span><br><span class="line">        setState(nextc);</span><br><span class="line">        <span class="keyword">return</span> free;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="读锁实现"><a href="#读锁实现" class="headerlink" title="读锁实现"></a>读锁实现</h4><p>读锁的实现类最关键的还是<code>lock</code>和<code>unlock</code>方法，具体实现由<code>ReentrantReadWriteLock</code>  内部的<code>Sync</code>类的<code>tryAcquireShared</code>和<code>tryReleaseShared</code>对应</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The lock returned by method &#123;<span class="doctag">@link</span> ReentrantReadWriteLock#readLock&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadLock</span> <span class="keyword">implements</span> <span class="title">Lock</span>, <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">5992448646407690164L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Sync sync;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Constructor for use by subclasses</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lock the outer lock object</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> NullPointerException if the lock is null</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">ReadLock</span><span class="params">(ReentrantReadWriteLock lock)</span> </span>&#123;</span><br><span class="line">        sync = lock.sync;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Acquires the read lock.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Acquires the read lock if the write lock is not held by</span></span><br><span class="line"><span class="comment">     * another thread and returns immediately.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;If the write lock is held by another thread then</span></span><br><span class="line"><span class="comment">     * the current thread becomes disabled for thread scheduling</span></span><br><span class="line"><span class="comment">     * purposes and lies dormant until the read lock has been acquired.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        sync.acquireShared(<span class="number">1</span>); <span class="comment">// 具体获取读锁的逻辑在tryAcquireShared实现</span></span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  	</span><br><span class="line">    <span class="comment">// 响应中断的获取读锁方式</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lockInterruptibly</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        sync.acquireSharedInterruptibly(<span class="number">1</span>); </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 即时返回是否成功获取读锁，显然如果写锁别其他线程持有，那么此时tryLock() 立即返回false</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sync.tryReadLock();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">(<span class="keyword">long</span> timeout, TimeUnit unit)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sync.tryAcquireSharedNanos(<span class="number">1</span>, unit.toNanos(timeout));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否读锁，具体释放读锁的逻辑在tryReleaseShared实现</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        sync.releaseShared(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Throws &#123;<span class="doctag">@code</span> UnsupportedOperationException&#125; because</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@code</span> ReadLocks&#125; do not support conditions.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UnsupportedOperationException always</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Condition <span class="title">newCondition</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns a string identifying this lock, as well as its lock state.</span></span><br><span class="line"><span class="comment">     * The state, in brackets, includes the String &#123;<span class="doctag">@code</span> &quot;Read locks =&quot;&#125;</span></span><br><span class="line"><span class="comment">     * followed by the number of held read locks.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> a string identifying this lock, as well as its lock state</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> r = sync.getReadLockCount();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">super</span>.toString() +</span><br><span class="line">            <span class="string">&quot;[Read locks = &quot;</span> + r + <span class="string">&quot;]&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="读锁获取-tryAcquireShared"><a href="#读锁获取-tryAcquireShared" class="headerlink" title="读锁获取(tryAcquireShared)"></a>读锁获取(tryAcquireShared)</h5><p>调用者使用<code>rwl.readLock().lock()</code> 获取读锁，内部调用是acquireShared方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Acquires the read lock.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Acquires the read lock if the write lock is not held by</span></span><br><span class="line"><span class="comment"> * another thread and returns immediately.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;If the write lock is held by another thread then</span></span><br><span class="line"><span class="comment"> * the current thread becomes disabled for thread scheduling</span></span><br><span class="line"><span class="comment"> * purposes and lies dormant until the read lock has been acquired.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    sync.acquireShared(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>acquireShared当然是AQS的模板方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquireShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (tryAcquireShared(arg) &lt; <span class="number">0</span>) <span class="comment">// 只要ReentrantReadWriteLock的tryAcquireShared获取读锁方法返回-1，就说明当前线程没能马上获得读锁，得去CLH排队等待</span></span><br><span class="line">        doAcquireShared(arg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>tryAcquireShared</code>则是<code>ReentrantReadWriteLock</code>实现请求读锁的关键逻辑</p>
<p>其实<code>tryAcquireShared</code>总逻辑分为4部分：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">tryAcquireShared</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    Thread current = Thread.currentThread();</span><br><span class="line">    <span class="keyword">int</span> c = getState();</span><br><span class="line">    <span class="comment">// 第1部分：全局存在写锁且不是当前线程持有，因此当前线程不能获取读锁</span></span><br><span class="line">    <span class="keyword">if</span> (exclusiveCount(c) != <span class="number">0</span> &amp;&amp;</span><br><span class="line">        getExclusiveOwnerThread() != current)</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> r = sharedCount(c);</span><br><span class="line">    <span class="comment">// 第2部分：全局层面获得读锁</span></span><br><span class="line">    <span class="keyword">if</span> (!readerShouldBlock() &amp;&amp;</span><br><span class="line">        r &lt; MAX_COUNT &amp;&amp;</span><br><span class="line">        compareAndSetState(c, c + SHARED_UNIT)) &#123;</span><br><span class="line">	 </span><br><span class="line">      <span class="comment">// 第3部分:&#123;第2部分的全局层面获得读锁后，还得对当前线程自己持有的读锁计数进行相关管理&#125;，这部分最为核心，也是最难理解的部分。</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>; <span class="comment">// 第2部分的全局层面获得读锁，必然返回1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 第4部分：在全局层面获取读锁失败，使用以下全面尝试获取读锁的逻辑去获取读锁，增加当前线程获取读锁的概率。</span></span><br><span class="line">    <span class="keyword">return</span> fullTryAcquireShared(current);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>完整解析：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">tryAcquireShared</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>&#123;</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * Walkthrough:</span></span><br><span class="line"><span class="comment">             * 1. If write lock held by another thread, fail.</span></span><br><span class="line"><span class="comment">             * 2. Otherwise, this thread is eligible for</span></span><br><span class="line"><span class="comment">             *    lock wrt state, so ask if it should block</span></span><br><span class="line"><span class="comment">             *    because of queue policy. If not, try</span></span><br><span class="line"><span class="comment">             *    to grant by CASing state and updating count.</span></span><br><span class="line"><span class="comment">             *    Note that step does not check for reentrant</span></span><br><span class="line"><span class="comment">             *    acquires, which is postponed to full version</span></span><br><span class="line"><span class="comment">             *    to avoid having to check hold count in</span></span><br><span class="line"><span class="comment">             *    the more typical non-reentrant case.</span></span><br><span class="line"><span class="comment">             * 3. If step 2 fails either because thread</span></span><br><span class="line"><span class="comment">             *    apparently not eligible or CAS fails or count</span></span><br><span class="line"><span class="comment">             *    saturated, chain to version with full retry loop.</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            Thread current = Thread.currentThread();</span><br><span class="line">            <span class="keyword">int</span> c = getState();</span><br><span class="line">          	<span class="comment">//1、 当前state存在写锁计数但不是当前线程持有，那么不容许获取读锁，因为两者被设计为是互斥的，换句话说：在有写锁存在的条件下，只能是已经持有写锁的当前线程才能获取当前读锁，当然这种情况被称为“锁降级”。想想为何？</span></span><br><span class="line">           <span class="comment">//  因为如果存在写锁的条件下但不是当前线程持有，还去允许当前线程获取读锁的话，这不就等价于将别人占有的写锁“抢了过来”，这显然是不符合ReentrantReadWriteLock的读写互斥设计的。</span></span><br><span class="line">            <span class="keyword">if</span> (exclusiveCount(c) != <span class="number">0</span> &amp;&amp;</span><br><span class="line">                getExclusiveOwnerThread() != current)</span><br><span class="line">                <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">          	<span class="comment">//2、当前state拥有的读锁数量</span></span><br><span class="line">            <span class="keyword">int</span> r = sharedCount(c);</span><br><span class="line">          	<span class="comment">//3、如果此时读线程是非公平模式且读锁计数小于最大值且CAS更新读锁计数加1成功，那么就执行此if里面的条件</span></span><br><span class="line">            <span class="keyword">if</span> (!readerShouldBlock() &amp;&amp;</span><br><span class="line">                r &lt; MAX_COUNT &amp;&amp;</span><br><span class="line">                compareAndSetState(c, c + SHARED_UNIT)) &#123; </span><br><span class="line"><span class="comment">// 在state的高16位累加1，也即使得所有线程总的读锁计数加1,执行流来到这里，说明当前线程已经成功获得读锁，下面就是要完成当前线程持有读锁的计数管理相关逻辑。</span></span><br><span class="line">              </span><br><span class="line">              	<span class="comment">//4、在2读取的r值如果为0，说明当前线程是第一个进来请求读锁的线程，记为firstReader</span></span><br><span class="line">                <span class="keyword">if</span> (r == <span class="number">0</span>) &#123;</span><br><span class="line">                    firstReader = current;</span><br><span class="line">                    firstReaderHoldCount = <span class="number">1</span>; <span class="comment">// 记录第一个读线程的重入数量，首次当然是1 </span></span><br><span class="line">               <span class="comment">//  5、 如果在第2点读取的读锁数量不为0且当前线程就是之前的第一个读线程，说明是重入，对第一个读线程的重入进行计数加1 </span></span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (firstReader == current) &#123;</span><br><span class="line">                    firstReaderHoldCount++;</span><br><span class="line">               <span class="comment">// 6、以下的读锁设计比较难理解，参考后面的内容分析。</span></span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 读取“全局缓存计数器”，注意到此“全局缓存计数器”只缓存“最新成功获取读锁的那个线程”：The hold count of the last thread to successfully acquire readLock，目的是This saves ThreadLocal lookup，避免回到ThreadLocalHoldCounter的readHolds去查找。</span></span><br><span class="line">                    HoldCounter rh = cachedHoldCounter;</span><br><span class="line">                    <span class="keyword">if</span> (rh == <span class="keyword">null</span> || rh.tid != getThreadId(current))</span><br><span class="line">                   <span class="comment">/* 6.1 </span></span><br><span class="line"><span class="comment">                  	情况1 如果此“全局缓存计数器”变量为空，说明在此刻之前没有“最新成功获取读锁的那个线程””出现，那么当前线程自然可以将自己设为“最新成功获取读锁的那个线程”这个角色</span></span><br><span class="line"><span class="comment">                  	情况2 如果此“全局缓存计数器”变量不为空但已经缓存“最新成功获取读锁的那个线程”，而这个缓存线程又不是当前线程，说明当前线程从此刻起将成为“最新成功获取读锁的线程”角色。</span></span><br><span class="line"><span class="comment">                  	针对情况1和情况2，既然当前线程在此刻已经成为“最新成功获取读锁的线程”角色，那么当前线程取出自己的计数器并放入“全局缓存计数器”：cachedHoldCounter = rh = readHolds.get()。这样就保证了cachedHoldCounter会一直指向“最新成功获取读锁的线程”  </span></span><br><span class="line"><span class="comment">                  	*/</span></span><br><span class="line">                        cachedHoldCounter = rh = readHolds.get();</span><br><span class="line">                   <span class="comment">/*6.2</span></span><br><span class="line"><span class="comment">                   说明6.1两个条件都不成立，说明“全局缓存计数器”缓存的恰好是当前线程，如果缓存的读锁计数为0，那么说明这个线程是在上一刻释放了自己持有的最后一个读锁且将“全局缓存计数器”计数减至0（并且它会调用readHolds.remove()移除了rh计数器对象，这一操作可以发生在tryReleaseShared中），现在这一刻此线程又再次进来作为“最新成功获取读锁的线程”，而此刻当前线程自己readHolds并没有放置计时器，于是作为“最新成功获取读锁的线程”，当前线程再将计数器放入到自己的“ThreadLocalHoldCounter的readHolds中”。</span></span><br><span class="line"><span class="comment">                   */</span> </span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (rh.count == <span class="number">0</span>)</span><br><span class="line">                        readHolds.set(rh);</span><br><span class="line">                    <span class="comment">// </span></span><br><span class="line">                    rh.count++;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> fullTryAcquireShared(current);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h5 id="关于“当前线程自己持有的读锁计数进行相关管理”的设计（ReentrantReadWriteLock最核心的设计）"><a href="#关于“当前线程自己持有的读锁计数进行相关管理”的设计（ReentrantReadWriteLock最核心的设计）" class="headerlink" title="关于“当前线程自己持有的读锁计数进行相关管理”的设计（ReentrantReadWriteLock最核心的设计）"></a>关于“当前线程自己持有的读锁计数进行相关管理”的设计（ReentrantReadWriteLock最核心的设计）</h5><p>首先，读锁的计数需要在两个层面上进行去理解：</p>
<p>所有线程请求的总读锁计数的加1操作，使用以下方式记录：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">     <span class="comment">// 也即每个线程，只要请求读锁成功，那么读锁总数对应加1（state的高16位加1）  </span></span><br><span class="line"><span class="keyword">if</span> (!readerShouldBlock() &amp;&amp;</span><br><span class="line">           r &lt; MAX_COUNT &amp;&amp;</span><br><span class="line">           compareAndSetState(c, c + SHARED_UNIT)) &#123; </span><br></pre></td></tr></table></figure>
<p>其次，还需要记录每个线程自己请求读锁的数量（或者重入读锁的次数），也即“当前线程自己持有的读锁计数进行相关管理”。这里解释为何每个线程需要记录自己读锁的数量，这是因为ReentrantReadWriteLock设计的读锁是可重入的，那么每个线程管理自己的读锁数量后，可以方便进行且准确的加锁、释放锁操作（自己重入多少次，就需要释放多少次）以及是否需要进入排队操作。<code>而compareAndSetState(c, c + SHARED_UNIT)</code>是记录所有线程的总读锁数量的设计，显然无法实现“当前线程自己持有的读锁计数进行相关管理”的功能。</p>
<p>这种要实现“每个线程自己的状态管理和线程关联起来”的场景，ThreadLocal无疑是最适合的，这在“ThreadLocal的源代码设计的深度解析”的文章中已提及过。在这里通过设计一个计数器HoldCounter并放入到ThreadLocal，也即ThreadLocalHoldCounter，即可实现线程安全方式实现“当前线程自己持有的读锁计数进行相关管理”的需求。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 理解本节内容的前提是掌握ThreadLocal的底层设计原理及其源代码实现，否则无法掌握其设计内涵。	</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * A counter for per-thread read hold counts.</span></span><br><span class="line"><span class="comment">     * Maintained as a ThreadLocal; cached in cachedHoldCounter</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 1、此辅助对象就是为了记录当前线程请求读锁的次数，目的是为了知道此线程第一有没有持有读锁，第二持有读锁情况下重入多少次。交由ThreadLocal来维护。</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HoldCounter</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;  <span class="comment">// 记录当前线程的读锁请求次数</span></span><br><span class="line">        <span class="comment">// Use id, not reference, to avoid garbage retention</span></span><br><span class="line">      	<span class="comment">// 当前线程的内存地址ID号，用于一些特别场景例如下面的cachedHoldCounter</span></span><br><span class="line">		<span class="comment">// 使用线程ID号而不是对象可以有效优化GC</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> tid = getThreadId(Thread.currentThread());</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 2、通过ThreadLocal维护HoldCounter的更新，实现每个线程能线程安全的方式去管理持有读锁的计数。</span></span><br><span class="line"><span class="comment">// 这里是设定HoldCounter的初始化值</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadLocalHoldCounter</span></span></span><br><span class="line"><span class="class">        <span class="keyword">extends</span> <span class="title">ThreadLocal</span>&lt;<span class="title">HoldCounter</span>&gt; </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> HoldCounter <span class="title">initialValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> HoldCounter();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The number of reentrant read locks held by current thread.</span></span><br><span class="line"><span class="comment">     * Initialized only in constructor and readObject.</span></span><br><span class="line"><span class="comment">     * Removed whenever a thread&#x27;s read hold count drops to 0.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> 3、当前线程持有读锁的重入次数，如果不为0，说明当前线程肯定持有读锁,需要注意它的灵活用法:</span></span><br><span class="line"><span class="comment"> 总体上一定要遵守“当前线程不再持有读锁时，当前线程的ThreadLocal对象一定不能还存放着HoldCounter对象，否则HoldCounter是强引用，会造成当前线程的ThreadLocal出现内存泄露的风险”</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> 3.1 一旦当前线程释放自己持有的最后一个读锁，说明当前线程不再需要持有“读锁计时器”引用来给自己计算读锁数量，那么当前线程还需要做多一个操作： readHolds.remove(),保证当前线程不再引用此“ThreadLocal变量”的读锁计数器，从而避免当前线程的ThreadLocal内存泄露，对应的代码逻辑（此逻辑一般在释放锁的操作中实施）：</span></span><br><span class="line"><span class="comment">       if (count &lt;= 1) &#123;</span></span><br><span class="line"><span class="comment">                readHolds.remove();</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> readHolds = new ThreadLocalHoldCounter() ,当然此初始化已经被上面initialValue取代了。</span></span><br><span class="line"><span class="comment"> 每个线程持有的readHolds初始值：count=0，tid=此线程内存地址ID</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"> 3.2 如果当前线程查询自己readHolds的rh.count=0,说明当前线程是第一次成功获取读锁，考虑当前线程以后可能有重入读锁，那么此时可以将自己“读锁计数器” rh放到readHolds里面，对应的代码逻辑（此逻辑一般在请求读锁的操作中实施）：</span></span><br><span class="line"><span class="comment">         else if (rh.count == 0)</span></span><br><span class="line"><span class="comment">                    readHolds.set(rh);</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ThreadLocalHoldCounter readHolds;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The hold count of the last thread to successfully acquire</span></span><br><span class="line"><span class="comment">     * readLock. This saves ThreadLocal lookup in the common case</span></span><br><span class="line"><span class="comment">     * where the next thread to release is the last one to</span></span><br><span class="line"><span class="comment">     * acquire. This is non-volatile since it is just used</span></span><br><span class="line"><span class="comment">     * as a heuristic, and would be great for threads to cache.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Can outlive the Thread for which it is caching the read</span></span><br><span class="line"><span class="comment">     * hold count, but avoids garbage retention by not retaining a</span></span><br><span class="line"><span class="comment">     * reference to the Thread.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Accessed via a benign data race; relies on the memory</span></span><br><span class="line"><span class="comment">     * model&#x27;s final field and out-of-thin-air guarantees.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意此cachedHoldCounter从名字可以直到它是一个缓存的读锁计数器，那么它是缓存哪个线程的读锁计时器呢？</span></span><br><span class="line"><span class="comment">// 缓存的是“最近（最新）获得读锁的那个线程的读锁计数器”，例如Thread-0、Thread-1、Thread-2三个线程并发竞争读锁，如果Thread-1是最后一个成功获取读锁的线程，那么cachedHoldCounter就会缓存Thread-1的读锁计数count=1和Thread-1对应的内存地址ID号。当然如果后面还有最新的来的Thread-xx，那么cachedHoldCounter就会马上更新缓存Thread-xx的信息。</span></span><br><span class="line"><span class="comment">/* 这个缓存cachedHoldCounter有何作用呢？ </span></span><br><span class="line"><span class="comment">（1）在前面ThreadLocalHoldCounter中，每个线程HoldCounter由ThreadLocal维护，如果此线程需要查询自己的读锁计数，需要使用ThreadLocal里面的get方法，而此get方法会调用ThreadLocal里面ThreadLocalMap的getEntry方法，了解ThreadLocal的底层设计都知道这一get操作还可能引起ThreadLocalMap的额外清理操作，这无疑降低查询效率，因此干脆设计用于“具有缓存功能、快速查询”的cachedHoldCounter，如果当前线程恰好就是设置cachedHoldCounter的线程，那么它直接在cachedHoldCounter就可以拿到自己的读锁数，完全不需要调用ThreadLocal里面get方法来查询（避免了在ThreadLocalMap内部的一系列操作）通过这种方式，当前线程在读锁重入时能提高加锁效率。</span></span><br><span class="line"><span class="comment">  （2）当前如果读锁竞争激烈，那么假设线程加锁后缓存了Thread-1，在Thread-1释放锁前，又有其他线程Thread-2、Thread-3请求了读锁，那么cachedHoldCounter就会一直更新，这种情况导致Thread-1需要调用ThreadLocal查询自己的读锁计数。可以说Thread-1在请求读锁重入锁期间没有“享受到缓存带来的性能提升”</span></span><br><span class="line"><span class="comment">  正如源码注释里面的说的：This is non-volatile since it is just used as a heuristic, and would be great for threads to cache.</span></span><br><span class="line"><span class="comment">  因为这是被设计为一个试探性的缓存，因此不需要设为volatile类型，如果当前线程试探性去查询发现此缓存是自己加锁后设置的缓存，那么就可以提高此线程的本次加锁和释放读锁的性能，如果不是自己设置缓存，那就稍微牺牲一点性能ThreadLocal（在内部的ThreadLocalMap）去查询。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HoldCounter cachedHoldCounter;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * firstReader is the first thread to have acquired the read lock.</span></span><br><span class="line"><span class="comment">     * firstReaderHoldCount is firstReader&#x27;s hold count.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;More precisely, firstReader is the unique thread that last</span></span><br><span class="line"><span class="comment">     * changed the shared count from 0 to 1, and has not released the</span></span><br><span class="line"><span class="comment">     * read lock since then; null if there is no such thread.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Cannot cause garbage retention unless the thread terminated</span></span><br><span class="line"><span class="comment">     * without relinquishing its read locks, since tryReleaseShared</span></span><br><span class="line"><span class="comment">     * sets it to null.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Accessed via a benign data race; relies on the memory</span></span><br><span class="line"><span class="comment">     * model&#x27;s out-of-thin-air guarantees for references.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;This allows tracking of read holds for uncontended read</span></span><br><span class="line"><span class="comment">     * locks to be very cheap.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">/* 以下两个成员也是为了提升读锁加速的性能。认真看其源码注释说：更准确地说，firstReader是指向一个线程———这个线程是让总读锁计数从0变为1的线程。注意需要用动态的思维看待这样的解释。简单来说就是第一个获得读锁的线程，firstReaderHoldCount显然就是第一个获得读锁线程的重入计数</span></span><br><span class="line"><span class="comment">那么在什么场景下firstReader和firstReaderHoldCount起到提升性能的作用呢？</span></span><br><span class="line"><span class="comment">对于这种有序获取读锁的场景，优化效果明显，例如，</span></span><br><span class="line"><span class="comment">Thread-0加锁-释放锁后，接着Thread-1才开始请求读锁，Thread-1释放读锁，然后Thread-2才开始请求读锁....</span></span><br><span class="line"><span class="comment">可以这么理解：</span></span><br><span class="line"><span class="comment">当Thraed-0作为第一个请求读锁的线程，它从加锁到释放锁的代码执行期间，此时Thread-1还没开始启动请求读锁。那么当Thread-0再次重入锁时（而且此刻还没其他线程来请求读锁），显然只需查询firstReaderHoldCount自己的读锁计数即可，无需进入自己的ThreadLocal里面去查询读锁计数，因为进入ThreadLocal去查询意味着要去ThreadLocalMap查询（get操作还可能引起清理stale entry）意味着降低查询效率。</span></span><br><span class="line"><span class="comment">这就是注释所说的：This allows tracking of read holds for uncontended read locks to be very cheap. 能够使用最小代价去追踪那种非竞争线程获取的读锁计数</span></span><br><span class="line"><span class="comment">uncontended read locks：一个线程拥有读锁的时候，没有其他线程企图获得读锁（也就是非并发竞争的）</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> Thread firstReader = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">int</span> firstReaderHoldCount;</span><br></pre></td></tr></table></figure>
<p>可以看到，读锁的获取设计确实较为复杂，因为Doug Lea为了实现在某些非竞争情况下请求锁性能的提升，采用了非常高技巧的“fast path”策略——先试探性执行某个“快速”操作，如果没命中那就再执行那个“相对耗时”的操作。（这种设计思想其实在ConcurrentHashMap里面的<code>addCount</code>和<code>fullAddCount</code>已经体现过）</p>
<p>其次可以通过一个内部方法快速理解线程自己管理的读锁计数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 获取当前线程想获取自己持有读锁的数量</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">int</span> <span class="title">getReadHoldCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    		<span class="comment">// 1. 如果总读锁数量为0，那么当前线程持有的读锁数量肯定也为0</span></span><br><span class="line">        <span class="keyword">if</span> (getReadLockCount() == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">		</span><br><span class="line">        Thread current = Thread.currentThread();</span><br><span class="line">    		<span class="comment">// 2. 看看自己是否为第一个获取读锁的角色，如果是，好办直接使用firstReaderHoldCount存储的数量即可，无需进入到ThreadLocal里面查询</span></span><br><span class="line">        <span class="keyword">if</span> (firstReader == current)</span><br><span class="line">            <span class="keyword">return</span> firstReaderHoldCount;</span><br><span class="line">		<span class="comment">// 3. 说明当前线程不是“第一个获取读锁”的线程，那就看看“全局缓存计时器”缓存的是哪个线程的</span></span><br><span class="line">        HoldCounter rh = cachedHoldCounter;</span><br><span class="line">    		<span class="comment">// 4. 如果存在“全局缓存计时器”且缓存的就是当前线程自己，那么说明当前线程一定是“最新获取读锁的那个线程”，返回缓存的计数即可</span></span><br><span class="line">        <span class="keyword">if</span> (rh != <span class="keyword">null</span> &amp;&amp; rh.tid == getThreadId(current))</span><br><span class="line">            <span class="keyword">return</span> rh.count;</span><br><span class="line">        <span class="comment">// 5. 说明自己不是“最新获取读锁的那个线程”，当然也不是“第一个获取读锁的角色”，因此要读取自己持有的读锁计数，显然要先取出自己在ThreadLocal放置的计时器</span></span><br><span class="line">        <span class="keyword">int</span> count = readHolds.get().count;</span><br><span class="line">    		<span class="comment">// 6. 自己在ThreadLocal放置的计时器的读锁数量已经变为0，说明当前线程接下来已经不再持有读锁，也即不需要再借助“计数器”，那么应该及时在自己的ThreadLocal变量删除此“计数器对象”——ThreadLocalHoldCounter</span></span><br><span class="line">        <span class="keyword">if</span> (count == <span class="number">0</span>) readHolds.remove();</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>有了以上缓存和快速试探的算法设计以及线程自己是如何管理自己持有的读锁的分析后，那么<code>fullTryAcquireShared</code>的算法流程才相对好理解。</p>
<h5 id="fullTryAcquireShared"><a href="#fullTryAcquireShared" class="headerlink" title="fullTryAcquireShared"></a>fullTryAcquireShared</h5><p>处理“快速尝试tryAcquireShared中CAS失败更新总读锁”的情况</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Full version of acquire for reads, that handles CAS misses</span></span><br><span class="line"><span class="comment">     * and reentrant reads not dealt with in tryAcquireShared.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 正如注释里面说的：专为快速尝试tryAcquireShared中CAS失败更新总读锁、重入读锁的情况</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">int</span> <span class="title">fullTryAcquireShared</span><span class="params">(Thread current)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * This code is in part redundant with that in</span></span><br><span class="line"><span class="comment">         * tryAcquireShared but is simpler overall by not</span></span><br><span class="line"><span class="comment">         * complicating tryAcquireShared with interactions between</span></span><br><span class="line"><span class="comment">         * retries and lazily reading hold counts.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">      	<span class="comment">// 注释也说到fullTryAcquireShared的总体代码实现跟tryAcquireShared大体类似（有部分冗余）</span></span><br><span class="line">        HoldCounter rh = <span class="keyword">null</span>;</span><br><span class="line">      	<span class="comment">// 自旋+内部CAS，保证了当前线程一定会在某个时机上成功拿到读锁，或在某个时机拿不到锁然后被放入CLH阻塞队列里面排队</span></span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="keyword">int</span> c = getState();</span><br><span class="line">          	<span class="comment">// 1、同tryAcquireShared的设计，同步状态存在写锁，且是其他线程持有的写锁，显然当前线程无法获取读锁，因为不同线程的写、读锁请求是互斥的。</span></span><br><span class="line">            <span class="keyword">if</span> (exclusiveCount(c) != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// A</span></span><br><span class="line">                <span class="keyword">if</span> (getExclusiveOwnerThread() != current)</span><br><span class="line">                    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">              	<span class="comment">// else return -1</span></span><br><span class="line">              	<span class="comment">//补充技术点：注意到这里有个死锁的解释：假设给A配一个else分支，返回-1，那么就使得持有写锁的当前线程进入阻塞队列，它持有的写锁一直不会释放，而外面的线程又等着这个写锁的释放，因此进入了死锁困境，所以不能在这里return -1。换句话说，不要试图阻塞已持有的独占锁线程，否则会出现死锁意外。其实这种合理的设计在AQS的条件队列await里面的“addConditionWaiter然后fullyRelease”有提到（进了条件队列后，阻塞前把自己持有的所有独占锁完全释放掉）</span></span><br><span class="line">                <span class="comment">// else we hold the exclusive lock; blocking here</span></span><br><span class="line">                <span class="comment">// would cause deadlock.</span></span><br><span class="line">              <span class="comment">// 2、来到这里说明此时写锁计数为0，暂时没有其他线程请求写锁，当前线程可以去请求读锁但遗憾的是readerShouldBlock为true时，表示当前线程需要被阻塞，而if里面的逻辑并没有关于“进入CLH阻塞队列排队的操作”，原来这里的if是这么设计的：既然当前线程待会要进去阻塞队列，那么在它进去之前，要求当前线程先处理好自己的读锁计数情况</span></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (readerShouldBlock()) &#123;</span><br><span class="line">                <span class="comment">// Make sure we&#x27;re not acquiring read lock reentrantly</span></span><br><span class="line">              </span><br><span class="line">                <span class="comment">// 如果存在这样的场合：假设某线程AA在某次tryAcquireShared请求读锁成功，然后在某次重入读锁时tryAcquireShared没有抢到读锁只能调用fullTryAcquireShared来处理重入锁时，线程AA来到if (readerShouldBlock()) 为true情况，接下里怎么安排呢？ 以下就是对应的处理策略：                 </span></span><br><span class="line">              	<span class="comment">// 3.1 如果当前线程就是第一个加读锁的线程，说明是重入读锁，即使readerShouldBlock为true，当前线程也不需要去阻塞队列排队（也即不需要在这里return -1），因为对于重入锁情况，直接它进入下面的第11位置执行重入锁逻辑</span></span><br><span class="line">                <span class="keyword">if</span> (firstReader == current) &#123;</span><br><span class="line">                  	<span class="comment">// 并没有直接返回-1</span></span><br><span class="line">                    <span class="comment">// assert firstReaderHoldCount &gt; 0;</span></span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 3.2 说明当前线程不是firstReader也不是firstReader的重入，而是此线程是第一次竞争读锁，但因为大前提readerShouldBlock是true，因此此线程一定会进入CLH阻塞队列。</span></span><br><span class="line">                <span class="comment">// 4、 </span></span><br><span class="line">                    <span class="keyword">if</span> (rh == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        rh = cachedHoldCounter;</span><br><span class="line">                      	<span class="comment">//5、</span></span><br><span class="line">                      	<span class="comment">// 5.1 当前线程（显然是非firstReader角色）如果是第一次请求读锁，发现cachedHoldCounter还是空或者里面缓存不是自己的，从自己的ThreadLocal取出HoldCounter对象,然后进入6.1</span></span><br><span class="line">                      	<span class="comment">// 5.2 当前线程是重入锁的情况，显然rh不是null，且6.1的判断不成立,会走入7.2逻辑</span></span><br><span class="line">                        <span class="keyword">if</span> (rh == <span class="keyword">null</span> || rh.tid != getThreadId(current)) &#123;</span><br><span class="line">                            rh = readHolds.get();</span><br><span class="line">                          <span class="comment">// 6、</span></span><br><span class="line">                          <span class="comment">//  6.1 对接5.1 由于当前线程是第一次请求读锁，但此时并没有拿到读锁，需要清空自己的ThreadLocal里面的HoldCounter对象，然后会来到7.1。因为这个线程将被送入CLH阻塞队列中，既然它准备要在阻塞队列了，那么它的ThreadLocal存放的HoldCounter对象要移除，避免ThreadLocal造成此线程的内存泄露。然后会进入7.1步骤</span></span><br><span class="line">                            <span class="keyword">if</span> (rh.count == <span class="number">0</span>)</span><br><span class="line">                                readHolds.remove();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                  	<span class="comment">// 7、</span></span><br><span class="line">                  	<span class="comment">// 7.1 当前线程第一次请求读锁,此时它持有读锁数量是0，由于readerShouldBlock为true，因此当前线程不能直接竞争读锁要去阻塞队列排队等待，因此返回-1</span></span><br><span class="line">                  	<span class="comment">// 7.2 当前线程是重入情况，显然rh.count不等于0，因此会进入第12的重入锁累加</span></span><br><span class="line">                    <span class="keyword">if</span> (rh.count == <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//8、 读锁总数达到最大值</span></span><br><span class="line">            <span class="keyword">if</span> (sharedCount(c) == MAX_COUNT)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">          	<span class="comment">// 9、如果当前线程CAS对总读锁加1成功，那么结果一定是返回1，中间逻辑就是线程对自己的计数器加1</span></span><br><span class="line">            <span class="keyword">if</span> (compareAndSetState(c, c + SHARED_UNIT)) &#123;</span><br><span class="line">                <span class="comment">//10、发现此时读锁为0，那么当前线程就变成firstReader角色。</span></span><br><span class="line">                <span class="keyword">if</span> (sharedCount(c) == <span class="number">0</span>) &#123;</span><br><span class="line">                    firstReader = current;</span><br><span class="line">                    firstReaderHoldCount = <span class="number">1</span>;</span><br><span class="line">                <span class="comment">// 11、firstReader的重入  </span></span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (firstReader == current) &#123;</span><br><span class="line">                    firstReaderHoldCount++;</span><br><span class="line">                <span class="comment">// 12、 当前线程（其他线程，非firstReader角色）的第一次请求读锁或者重入锁的情况 </span></span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                  	<span class="comment">// 12.1 rh为空，说明准备要将缓冲计时器cachedHoldCounter指向当前“最新成功获取读锁的线程”</span></span><br><span class="line">                    <span class="keyword">if</span> (rh == <span class="keyword">null</span>)</span><br><span class="line">                        rh = cachedHoldCounter;</span><br><span class="line">            <span class="comment">/* 12.2 </span></span><br><span class="line"><span class="comment">              	情况1 如果此“全局缓存计数器”变量为空，说明在此刻之前没有“最新成功获取读锁的那个线程””出现，那么当前线程自然可以将自己设为“最新成功获取读锁的那个线程”这个角色</span></span><br><span class="line"><span class="comment">              	情况2 如果此“全局缓存计数器”变量不为空但已经缓存“最新成功获取读锁的那个线程”，而这个缓存线程又不是当前线程，说明当前线程从此刻起将成为“最新成功获取读锁的线程”角色。</span></span><br><span class="line"><span class="comment">              	针对情况1和情况2，既然当前线程在此刻已经成为“最新成功获取读锁的线程”角色，那么当前线程取出自己的计数器并放入“全局缓存计数器”：放入的前提是rh.count == 0，说明当前线程是“最新成功获取读锁的线程”且不是重入的，cachedHoldCounter = rh = readHolds.get()。这样就保证了cachedHoldCounter会一直指向“最新成功获取读锁的线程”  。</span></span><br><span class="line"><span class="comment">              	如果当前线程是重入请求读锁，那么这里if (rh.count == 0)就不会成立，自然不会有readHolds.set(rh)</span></span><br><span class="line"><span class="comment">              	*/</span></span><br><span class="line">                    <span class="keyword">if</span> (rh == <span class="keyword">null</span> || rh.tid != getThreadId(current))</span><br><span class="line">                        rh = readHolds.get();</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (rh.count == <span class="number">0</span>)</span><br><span class="line">                        readHolds.set(rh);</span><br><span class="line">                <span class="comment">// 当前线程（其他线程，非firstReader角色）增加自己的读锁计数器，并将缓存设置为自己的 </span></span><br><span class="line">                    rh.count++;</span><br><span class="line">                  	</span><br><span class="line">                    cachedHoldCounter = rh; <span class="comment">// cache for release</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其实<code>fullTryAcquireShared</code>考虑到两种角色的线程处理，而每种角色有对应着“第一次请求读锁”和“重入读锁”的两种情况，这导致了<code>fullTryAcquireShared</code>设计的复杂性</p>
<p>第一种角色的线程：<code>firstReader</code></p>
<p>当<code>readerShouldBlock</code>为true时（意味着此线程要进行排队，但有例外：重入锁情况不需要排队）：</p>
<p>（1）如果<code>firstReader</code> 是重入的，不需要返回-1，也即不需要进入CLH阻塞排队，直接进入第11位置的重入锁加1逻辑</p>
<p>第二种角色的线程：非<code>firstReader</code>角色，也即不是第一个请求读锁的线程</p>
<p>当<code>readerShouldBlock</code>为true时（意味着此线程要进行排队，但有例外：重入锁情况不需要排队）：</p>
<p>（1）如果当前线程（非<code>firstReader</code>角色）是重入的，此时<code>rh.count!=0</code>不需要返回-1，也即不需要进入CLH阻塞排队，直接进入第11位置的重入锁加1逻辑</p>
<p>（2）如果当前线程（非<code>firstReader</code>角色）是第一次请求读锁，但因为<code>readerShouldBlock</code>为true，表示需要进入阻塞队列排队，因此会满足<code>rh.count=0</code>返回-1，此线程进入阻塞队列</p>
<p>或者用更为简洁的话：</p>
<p>如果其他线程是第一次获取读锁，因为<code>readerShouldBlock</code>为true，因此就要返回-1，这样这种线程才会被安排到CLH队列去排队，符合“ShouldBlock的要求”。又因为这些线程第一次获取读锁就需要被安排到CLH排队，因此他们持有的计数器对象如果rh.count=0,就需要readHolds.remove()，这样保证了入队后，这个线程首先没有持有读锁且能对应没有持有读锁计数器，保证了这些线程不会发生“ThreadLocal内存泄露”。</p>
<p>如果是<code>firstReader</code>是重入锁或者其他线程做重入锁，那么对自己的HoldCount进行加1计数后返回1，表示重入锁成功。</p>
<h5 id="关于读锁的非公平设计readerShouldBlock"><a href="#关于读锁的非公平设计readerShouldBlock" class="headerlink" title="关于读锁的非公平设计readerShouldBlock"></a>关于读锁的非公平设计readerShouldBlock</h5><p>如果<code>ReentrantReadWriteLock</code>在初始化给定的是非公平模式，那么其设计思路如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Nonfair version of Sync</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NonfairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">8159625535654395037L</span>;</span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">writerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>; <span class="comment">// writers can always barge</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">readerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">/* As a heuristic to avoid indefinite writer starvation,</span></span><br><span class="line"><span class="comment">         * block if the thread that momentarily appears to be head</span></span><br><span class="line"><span class="comment">         * of queue, if one exists, is a waiting writer.  This is</span></span><br><span class="line"><span class="comment">         * only a probabilistic effect since a new reader will not</span></span><br><span class="line"><span class="comment">         * block if there is a waiting writer behind other enabled</span></span><br><span class="line"><span class="comment">         * readers that have not yet drained from the queue.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">return</span> apparentlyFirstQueuedIsExclusive();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要到这里的readerShouldBlock方法都会在<code>tryAcquireShared</code>和<code>fullTryAcquireShared</code>使用，具体实现是<code>apparentlyFirstQueuedIsExclusive</code>,注释说它可以“试探性”的（或者说在一定概率上）防止在阻塞队列等待的写线程节点发生“锁饥饿”情况。</p>
<p>为何会出现“阻塞队列等待的写线程节点发生“锁饥饿”情况”?</p>
<p>假设现在有一个写锁节点已经在阻塞队列，结构如下：</p>
<p>head(null,-1)&lt;-&gt;node(写线程,0)</p>
<p>在<code>fullTryAcquireShared</code>中，如果<code>readerShouldBlock</code>的非公平模式设计为直接返回false：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">readerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（1）假设现在有新来的10个线程Thread-1~Thread-10并发请求读锁的线程，那么按照<code>readerShouldBlock</code>返回false的逻辑，他们可以直接竞争第一次请求锁或者重入锁，不妨假设这些请求读锁的10个线程都恰好领先于阻塞队列的写线程获取锁（毕竟新来的线程拿到CPU时间片的概率要比阻塞队列里面的写线程要高），那么就会导致此写线程根本无机会去请求锁，只能一直“委屈的在阻塞队列里面等待”，也即发生了“锁饥饿”情况。</p>
<p>那么如何优化这种极限情况？这就是<code>apparentlyFirstQueuedIsExclusive</code>方法要解决的。</p>
<p>（2）从方法的命名也可以看出它的含义：阻塞队列的第一个排队节点是否就是独占模式节点（在本文可以理解为：阻塞队列的第一个排队节点是否就是写线程节点），如果是独占模式节点，那么<code>readerShouldBlock</code>就会返回true，就会使得那些新来的请求读锁的线程们不能直接去竞争锁资源，而是被安排到阻塞队列去排队等候，这就使得阻塞队列的第一个写线程节点能够出队去竞争锁资源了，从而避免在（1）提到的“锁饥饿”情况。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns &#123;<span class="doctag">@code</span> true&#125; if the apparent first queued thread, if one</span></span><br><span class="line"><span class="comment"> * exists, is waiting in exclusive mode.  If this method returns</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@code</span> true&#125;, and the current thread is attempting to acquire in</span></span><br><span class="line"><span class="comment"> * shared mode (that is, this method is invoked from &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * #tryAcquireShared&#125;) then it is guaranteed that the current thread</span></span><br><span class="line"><span class="comment"> * is not the first queued thread.  Used only as a heuristic in</span></span><br><span class="line"><span class="comment"> * ReentrantReadWriteLock.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">apparentlyFirstQueuedIsExclusive</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Node h, s;</span><br><span class="line">    <span class="comment">// 1、存在阻塞队列</span></span><br><span class="line">  	<span class="comment">// 2、阻塞队列至少有1个线程节点在排队</span></span><br><span class="line">  	<span class="comment">// 3、且第一个线程节点是独占模式节点</span></span><br><span class="line">  	<span class="comment">// 4、且第一个线程节点线程未取消</span></span><br><span class="line">  	<span class="comment">// 如果以上4个条件同时成立，那么就会返回true，也即readerShouldBlock返回true</span></span><br><span class="line">    <span class="keyword">return</span> (h = head) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        (s = h.next)  != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">        !s.isShared()         &amp;&amp;</span><br><span class="line">        s.thread != <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上逻辑可以结合以下两种情况来理解：</p>
<p>第一种情况：</p>
<p>外面的新来的线程们：Thread-1~Thread10</p>
<p>阻塞队列的情况：head(null,-1)&lt;-&gt;node(写线程,0)</p>
<p>因此,根据<code>apparentlyFirstQueuedIsExclusive</code>的第三个条件，阻塞队列的第一个节点是独占模式节点（请求写锁的线程），那么<code>readerShouldBlock</code>就会返回true，要求外面的新来的线程们去阻塞队列排队等待，保证了此写线程能够出队抢锁。</p>
<p>第二种情况：</p>
<p>外面的新来的线程们：Thread-1~Thread10</p>
<p>阻塞队列的情况：head(null,-1)&lt;-&gt;node(读线程,-1)&lt;-&gt;node(写线程,0)</p>
<p>或者：head(null,-1)&lt;-&gt;node(读线程,-1)…….&lt;-&gt;node(写线程,0)</p>
<p>可以看到，阻塞队列里面的第一个线程节点为读线程节点，写线程排在第二或者其他更后面的位置，这时<code>apparentlyFirstQueuedIsExclusive</code> 显然因为<code>!s.isShared()</code>是共享模式节点而返回false，也即<code>`readerShouldBlock</code>就会返回false，那么外面新来的10个线程就会直接争抢锁资源而不需要排队，这会导致排在第二位置或者更后位置的写线程发生了一定程度的“锁饥饿”，因此只能说这种设计能够在一定程度上减少<code>writer starvation</code>，但不能保证一定杜绝<code>writer starvation</code>。</p>
<p>因此，正如<code>readerShouldBlock</code>里面的注释：a probabilistic effect 或者As a heuristic to avoid indefinite writer starvation，也即有一定概率性的。</p>
<p>这里不得不赞叹Dung Lea设计的读锁请求算法是如此的精妙和高超！</p>
<h5 id="读锁释放"><a href="#读锁释放" class="headerlink" title="读锁释放"></a>读锁释放</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1、用户代码</span></span><br><span class="line">rwl.readLock().unlock();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、ReentrantReadWriteLock的unlock</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Attempts to release this lock.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * &lt;p&gt;If the number of readers is now zero then the lock</span></span><br><span class="line"><span class="comment">         * is made available for write lock attempts.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            sync.releaseShared(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3、调用的是AQS的releaseShared：共享锁释放</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">releaseShared</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tryReleaseShared(arg)) &#123;</span><br><span class="line">            doReleaseShared();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、ReentrantReadWriteLock内部Sync定义的释放读锁逻辑</span></span><br><span class="line"><span class="comment">// 从读锁的加锁分析中可以推理出：有几种释放锁情况</span></span><br><span class="line"><span class="comment">// 第一：请求读锁然后马上释放锁的情况，</span></span><br><span class="line"><span class="comment">// 第二：已经是重入锁的情况去释放自己的重入锁	</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryReleaseShared</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>&#123;</span><br><span class="line">            Thread current = Thread.currentThread();</span><br><span class="line">          	<span class="comment">// 1、如果当前线程恰好是第一个拿到读锁的线程，那么对于首次加锁那么释放锁就是直接将firstReader置为null，否则说明是重入锁的释放，只需将自己持有的读锁计数减1</span></span><br><span class="line">            <span class="keyword">if</span> (firstReader == current) &#123;</span><br><span class="line">                <span class="comment">// assert firstReaderHoldCount &gt; 0;</span></span><br><span class="line">                <span class="keyword">if</span> (firstReaderHoldCount == <span class="number">1</span>)</span><br><span class="line">                    firstReader = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    firstReaderHoldCount--;</span><br><span class="line">              </span><br><span class="line">            <span class="comment">// 2、 非firstReader角色释放自己持有的读锁 </span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                HoldCounter rh = cachedHoldCounter;</span><br><span class="line">              	<span class="comment">// 2.1 如果缓存为空或者缓存的HoldCounter对象不是当前线程，那么就从当前线程的ThreadLocal取出它的HoldCounter对象赋给rh变量</span></span><br><span class="line">                <span class="keyword">if</span> (rh == <span class="keyword">null</span> || rh.tid != getThreadId(current))</span><br><span class="line">                    rh = readHolds.get();</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">int</span> count = rh.count;</span><br><span class="line">              	<span class="comment">// 2.2 如果原来持有的读锁&lt;=1,那么因为本次释放就会使得自己持有读锁计数变为0，对应要清除自己ThreadLocal存放的HoldCounter对象，这是因为考虑到ThreadLocal里面的entry是WeakReference类型，而HoldCounter是放在entry中，那么显然调用remove后，使得这个entry没有对象引用，加速GC回收。</span></span><br><span class="line">               <span class="comment">// 此外，当前线程因此已经准备要释放自己持有的最后一个读锁，说明释放后它就不不再需要计数，因此通过主动调用remove来避免ThreadLocal出现内存泄漏。</span></span><br><span class="line">                <span class="keyword">if</span> (count &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">                    readHolds.remove();  <span class="comment">// 务必已经掌握了ThreadLocal的底层源码设计，否则这段将无法理解为何要使用remove。</span></span><br><span class="line">                    <span class="keyword">if</span> (count &lt;= <span class="number">0</span>)</span><br><span class="line">                        <span class="keyword">throw</span> unmatchedUnlockException();</span><br><span class="line">                &#125;</span><br><span class="line">              	<span class="comment">// 2.3 如果原来持有的读锁计数大于1，那么当前线程的本次释放并不是释放自己持有的最后一个读锁，而是重入锁的退出，只需对自己持有的重入锁计数器减1即可</span></span><br><span class="line">                --rh.count;</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">//3、以上1和2步骤都是为了完成线程自己持有读锁的计数减1，那么下面使用自旋保证当前线程一定能使得总读锁计数state-1，如果释放后总锁（含读、写锁）剩余量刚好为0，那么当前下线程就会使用doReleaseShared去唤醒阻塞队里面正在等待的线程节点，尤其是被阻塞的写线程节点。</span></span><br><span class="line">            <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                <span class="keyword">int</span> c = getState();</span><br><span class="line">                <span class="keyword">int</span> nextc = c - SHARED_UNIT;</span><br><span class="line">                <span class="keyword">if</span> (compareAndSetState(c, nextc))</span><br><span class="line">                    <span class="comment">// Releasing the read lock has no effect on readers,</span></span><br><span class="line">                    <span class="comment">// but it may allow waiting writers to proceed if</span></span><br><span class="line">                    <span class="comment">// both read and write locks are now free.</span></span><br><span class="line">					<span class="comment">/* 这里也说到：如果是读线程释放读锁，对于持有读锁的线程来说是无影响的，如何理解这句话？</span></span><br><span class="line"><span class="comment">          想象一下：假设有65535个线程并发使用lock请求读锁，并假设都可以拿到读锁（意味着每个线程拿到1个读锁）,此时state的高16位计数为65535，假设在下一刻有65534个线程同一时刻释放自己的读锁，可以容易推出：每次有线程成功CAS：compareAndSetState(c, nextc)，且nextc不等于0，对应tryReleaseShared返回false，那么在外面此线程并不需要进入doReleaseShared()逻辑，这其实在说明：65534个线程在释放锁的过程中，tryReleaseShared都是返回false，显然都不影响其他读线程，这就是“Releasing the read lock has no effect on readers”的含义。</span></span><br><span class="line"><span class="comment">          当tryReleaseShared返回true时，说明第65535个线程也释放自己的锁，此时nextc==0成立，这时当前线程才会进入doReleaseShared()唤醒阻塞队列里面的线程节点。 因此这一刻，读锁和写锁都没人请求，阻塞队列的线程节点当然可以出队去请求锁。</span></span><br><span class="line"><span class="comment">                   */</span></span><br><span class="line">                    <span class="keyword">return</span> nextc == <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h4 id="读写锁的公平性"><a href="#读写锁的公平性" class="headerlink" title="读写锁的公平性"></a>读写锁的公平性</h4><p>我们知道，<code>ReentrantReadWriteLock</code>在创建时默认使用的非公平模式</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认构造器使用的是非公平模式</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReentrantReadWriteLock</span><span class="params">()</span> </span>&#123; </span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReentrantReadWriteLock</span><span class="params">(<span class="keyword">boolean</span> fair)</span> </span>&#123;</span><br><span class="line">    sync = fair ? <span class="keyword">new</span> FairSync() : <span class="keyword">new</span> NonfairSync();</span><br><span class="line">    readerLock = <span class="keyword">new</span> ReadLock(<span class="keyword">this</span>);  <span class="comment">// new时就已经实例化一个readerLock 对象和 writerLock 对象</span></span><br><span class="line">    writerLock = <span class="keyword">new</span> WriteLock(<span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>非公平模式实现：</p>
<p>请求写锁时：当前准备请求写锁的线程可以直接CAS写锁，不需要询问CLH阻塞队列是否有线程在等待拿锁。</p>
<p>请求读锁时：返回apparentlyFirstQueuedIsExclusive的策略，解析参考上文。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Nonfair version of Sync</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NonfairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">8159625535654395037L</span>;</span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">writerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>; <span class="comment">// writers can always barge</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">readerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">/* As a heuristic to avoid indefinite writer starvation,</span></span><br><span class="line"><span class="comment">         * block if the thread that momentarily appears to be head</span></span><br><span class="line"><span class="comment">         * of queue, if one exists, is a waiting writer.  This is</span></span><br><span class="line"><span class="comment">         * only a probabilistic effect since a new reader will not</span></span><br><span class="line"><span class="comment">         * block if there is a waiting writer behind other enabled</span></span><br><span class="line"><span class="comment">         * readers that have not yet drained from the queue.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">return</span> apparentlyFirstQueuedIsExclusive();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>公平模式：</p>
<p>请求写锁时：当前准备请求写锁的线程要去查询CLH阻塞队列是否有线程正在排队</p>
<p>请求读锁时：当前准备请求读锁的线程要去查询CLH阻塞队列是否有线程正在排队</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Fair version of Sync</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">FairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">2274990926593161451L</span>;</span><br><span class="line">      <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">writerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          <span class="keyword">return</span> hasQueuedPredecessors();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">readerShouldBlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          <span class="keyword">return</span> hasQueuedPredecessors();</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Queries whether any threads have been waiting to acquire longer</span></span><br><span class="line"><span class="comment">   * than the current thread.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &lt;p&gt;An invocation of this method is equivalent to (but may be</span></span><br><span class="line"><span class="comment">   * more efficient than):</span></span><br><span class="line"><span class="comment">   *  &lt;pre&gt; &#123;<span class="doctag">@code</span></span></span><br><span class="line"><span class="comment">   * getFirstQueuedThread() != Thread.currentThread() &amp;&amp;</span></span><br><span class="line"><span class="comment">   * hasQueuedThreads()&#125;&lt;/pre&gt;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &lt;p&gt;Note that because cancellations due to interrupts and</span></span><br><span class="line"><span class="comment">   * timeouts may occur at any time, a &#123;<span class="doctag">@code</span> true&#125; return does not</span></span><br><span class="line"><span class="comment">   * guarantee that some other thread will acquire before the current</span></span><br><span class="line"><span class="comment">   * thread.  Likewise, it is possible for another thread to win a</span></span><br><span class="line"><span class="comment">   * race to enqueue after this method has returned &#123;<span class="doctag">@code</span> false&#125;,</span></span><br><span class="line"><span class="comment">   * due to the queue being empty.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &lt;p&gt;This method is designed to be used by a fair synchronizer to</span></span><br><span class="line"><span class="comment">   * avoid &lt;a href=&quot;AbstractQueuedSynchronizer#barging&quot;&gt;barging&lt;/a&gt;.</span></span><br><span class="line"><span class="comment">   * Such a synchronizer&#x27;s &#123;<span class="doctag">@link</span> #tryAcquire&#125; method should return</span></span><br><span class="line"><span class="comment">   * &#123;<span class="doctag">@code</span> false&#125;, and its &#123;<span class="doctag">@link</span> #tryAcquireShared&#125; method should</span></span><br><span class="line"><span class="comment">   * return a negative value, if this method returns &#123;<span class="doctag">@code</span> true&#125;</span></span><br><span class="line"><span class="comment">   * (unless this is a reentrant acquire).  For example, the &#123;<span class="doctag">@code</span></span></span><br><span class="line"><span class="comment">   * tryAcquire&#125; method for a fair, reentrant, exclusive mode</span></span><br><span class="line"><span class="comment">   * synchronizer might look like this:</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   *  &lt;pre&gt; &#123;<span class="doctag">@code</span></span></span><br><span class="line"><span class="comment">   // 这里给出当前线程请求独占锁前需要询问阻塞队列的是否有线程正在排队情况	</span></span><br><span class="line"><span class="comment">   * protected boolean tryAcquire(int arg) &#123;</span></span><br><span class="line"><span class="comment">   *   if (isHeldExclusively()) &#123;</span></span><br><span class="line"><span class="comment">   *     // A reentrant acquire; increment hold count</span></span><br><span class="line"><span class="comment">   *     return true;</span></span><br><span class="line"><span class="comment">   *   &#125; else if (hasQueuedPredecessors()) &#123;</span></span><br><span class="line"><span class="comment">   *     return false;</span></span><br><span class="line"><span class="comment">   *   &#125; else &#123;</span></span><br><span class="line"><span class="comment">   *     // try to acquire normally</span></span><br><span class="line"><span class="comment">   *   &#125;</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&lt;/pre&gt;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if there is a queued thread preceding the</span></span><br><span class="line"><span class="comment">   *         current thread, and &#123;<span class="doctag">@code</span> false&#125; if the current thread</span></span><br><span class="line"><span class="comment">   *         is at the head of the queue or the queue is empty</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@since</span> 1.7</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 如果当前准备请求读写锁的线程调用hasQueuedPredecessors()返回false，说明当前线程要么已经在队列且作为第一个排队节点，要么就是没有CLH阻塞队列，也即没有其他线程在排队，这两种情况都可以说明当前线程不需要排队可以直接去竞争锁。</span></span><br><span class="line"> <span class="comment">//  2. s = h.next且s.thread = Thread.currentThread()，就是说明CLH阻塞队列里面第一个排队线程就是当前线程本身。</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">hasQueuedPredecessors</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// The correctness of this depends on head being initialized</span></span><br><span class="line">      <span class="comment">// before tail and on head.next being accurate if the current</span></span><br><span class="line">      <span class="comment">// thread is first in queue.</span></span><br><span class="line">      Node t = tail; <span class="comment">// Read fields in reverse initialization order</span></span><br><span class="line">      Node h = head;</span><br><span class="line">      Node s;</span><br><span class="line">      <span class="keyword">return</span> h != t &amp;&amp;</span><br><span class="line">          ((s = h.next) == <span class="keyword">null</span> || s.thread != Thread.currentThread());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>基于以上全文，显然<code>ReentrantReadWriteLock</code>的一些特定如下：</p>
<p>读读不互斥：持有读锁的线程A读数据显然不影响持有读锁的线程B读数据，这样的<code>读读不互斥</code>才是<code>ReentrantReadWriteLock</code>的高性能设计，如果采用<code>ReentrantLock</code>，那么持有<code>lock</code>的线程A读数据显然会导致线程B需要等待线程A释放了<code>ReentrantLock的lock</code>才能读数据，这就是为何<code>ReentrantLock</code>读读互斥不再符合<code>读多写少</code>的高并发需求的场景。</p>
<p>读写互斥：设想，如果允许读写不互斥，那么持有写锁的线程A正在写数据时，持有读锁的线程B读数据就会读到旧数据，这不是<code>ReentrantReadWriteLock</code>的设计初衷。因此持有写锁的线程A正在写数据时，线程B将无法请求到读锁或者写锁。持有读锁的线程A正在读数据时，线程B将无法请求到写锁来执行相关写操作，这样就保证了线程A在写时，其他线程不会读到乱数据。</p>
<p>写写互斥：持有写锁的线程A正在写数据时，线程B显然无法请求到写锁。</p>
<p>对于同一线程持有读锁的情况下，不允许持有写锁：否则就是持当前线程有读锁情况下竟然可以请求到写锁，那么这个读锁就不是共享模式读锁，而是变相称为了独占锁。</p>
<p>对于同一线程持有写锁的情况下，允许持有读锁：这是允许的，因此例如持有写锁的线程A正在写数据（外面的其他线程无法请求读锁和写锁），写完数据后，可以请求读锁再去读数据，这样不管是线程A自己还是外面的线程都能正确的读取到正确的数据。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：jdk1.8的ConcurrentHashMap源码深入分析（二）</title>
    <url>/2021/06/19/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E7%B3%BB%E5%88%97%EF%BC%9Ajdk1.8%E7%9A%84ConcurrentHashMap%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>在前面文章中，addCount方法分为两部分：</p>
<p>第一部分是put一个节点后，需要对size加1计数，这部分交由fullAddCount完成，它的设计和逻辑可谓精妙，非常值得在实际项目参考其代码实现。</p>
<p>第二部分是加1计数后，需要判断是否需要对table进行扩容，扩容思想设计及其源代码实现同样非常精妙，值得多次阅读和学以致用！</p>
<p>本文将重点深入分析CHM核心扩容逻辑：transfer、helpTransfer、以及resizeStamp。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b8f6d46f059f62cc486c59068a58e948.png" alt="CHM的sc设置基数图示"></p>
<p>《gitee 博客文章封面》</p>
<h4 id="1、addCount的扩容判断设计"><a href="#1、addCount的扩容判断设计" class="headerlink" title="1、addCount的扩容判断设计"></a>1、addCount的扩容判断设计</h4><h5 id="第1个执行扩容线程"><a href="#第1个执行扩容线程" class="headerlink" title="第1个执行扩容线程"></a>第1个执行扩容线程</h5><p>本章节最精彩的地方：分析Doug Lea 如何安排“每个加入扩容任务线程对sc进行cas加1计数”、“每个结束自己扩容任务线程对sc进行减1”、以及“最后一个结束扩容线程要干些什么收尾工作”。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// putVal调用addCount(1L, binCount)，因此这里x就是1，check即binCount，对于put入一个key（key已存在,则binCount=0，新key,binCount&gt;0），那么binCount自然是&gt;=0</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">      CounterCell[] as; <span class="keyword">long</span> b, s;</span><br><span class="line">    	<span class="comment">// 主分支1：完成加1计数逻辑，之前文章fullAddCount已经详细分析，本文重点讲解addCount主分支2</span></span><br><span class="line">    	<span class="comment">// ......忽略部分</span></span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line">     	<span class="comment">// 主分支2：当前线程检查是否需要扩容，若需要，则执行transfer扩容逻辑</span></span><br><span class="line">     	<span class="comment">// check即binCount,每次新增节点，当然要检查是否需要扩容</span></span><br><span class="line">      <span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">          Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">        	<span class="comment">// 这里的s就是主分支1里面的 s = sumCount()，含义：完成加1计数后，统计当前CHM节点总数量s，看看s有无达到扩容阈值sizeCtl</span></span><br><span class="line">          <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                 (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">              <span class="keyword">int</span> rs = resizeStamp(n); <span class="comment">// jdk 1.8这里的写法是一个bug，后面有指出原因。</span></span><br><span class="line">            	<span class="comment">//分支2.1：sc的值为负数时，表明CHM还在扩容期，原因参考后面小节的sc、resizeStamp方法的解析</span></span><br><span class="line">              <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                 <span class="comment">// 分支2.1.1：while循环扩容结束点，扩容结束条件有5个，(nt = nextTable) == null 以及transferIndex &lt;= 0条件再看完transfer源码解析后，可以很容易理解，但前面3个条件目前理解会很困难，需要理解后面小节的sc、resizeStamp方法解析后才能准确理解其含义，也即这里先跳过这个5个条件的解释。</span></span><br><span class="line">                  <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                      sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                      transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                      <span class="keyword">break</span>;</span><br><span class="line">             <span class="comment">// 分支2.1.2：第1个执行扩容线程在分支2.2将sc设置基数值后，以后每进来一个扩容线程都会对sc进行cas加1</span></span><br><span class="line">                  <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                      transfer(tab, nt);</span><br><span class="line">              &#125;</span><br><span class="line">            	<span class="comment">//分支2.2：第1个执行扩容线程会执行此逻辑，将sc（sizeCtl）设为一个基础数（该数为负数），为什么设置一个负数呢？后面resizeStamp方法给出非常完整解答！没有充分积累，此处看起来将很难理解其设计意图。</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                           (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">                  transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">              s = sumCount();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>对于分支2.1：因为表示有其他线程在扩容，说明是并发状态，这部分逻辑先放着，我们先考察分支2.2的逻辑。</p>
<p>分支2.2: 对于ConcurrentHashMap 第1个进入扩容的线程会执行此逻辑，会将sc（sizeCtl）设为一个基础负数：<code>(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2</code></p>
<p>“第1个执行扩容线程”:当ConcurrentHashMap首次扩容时，负责首次扩容任务的线程就是“第1个执行扩容线程”，这个概念很重要！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//分支2.2：当ConcurrentHashMap首次扩容时，第1个执行扩容的线程会先执行此分支</span></span><br><span class="line"> <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                              (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line"> <span class="comment">//第1个负责扩容的线程执行以下扩容逻辑  </span></span><br><span class="line"> &#123;transfer(tab, <span class="keyword">null</span>);&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>正是因为有了“第1个执行扩容线程”将sc（sizeCtl）设为一个基础负数，此时回看分支2.1即可明白其含义：</p>
<p>此时sc是个基础负数，因此sc&lt;0必然成立，换句话说：只要有第2个以及后续更多的线程进入whlie，sc&lt;0成立，表明当前CHM正在扩容状态（或者有线程正在对CHM进行扩容处理）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="comment">//分支2.1    </span></span><br><span class="line"><span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">//...省略</span></span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>有了以上解析的铺垫，现在我们重新理顺以下执行流程：假设当前CHM已经达到扩容阈值时，“第1个执行扩容线程”以及“第2个以及以后更多线程加入到扩容”在进入addCount后的执行流，从序号①开始，按升序推进，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="comment">//新put一个节点后，addCount(x=1, check=binCount=1)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">      CounterCell[] as; <span class="keyword">long</span> b, s;</span><br><span class="line">		<span class="comment">// 主分支1：完成fullAddCount加1计算后，计算</span></span><br><span class="line">    	 s = sumCount();</span><br><span class="line">      <span class="comment">// 主分支2：当前线程检查是否需要扩容，若需要，就执行transfer扩容逻辑</span></span><br><span class="line">      <span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">          Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">        	<span class="comment">//① 我们已假设需要扩容，也即整个CHM的节点数量s已经达到扩容阈值sc=sizeCtl,“第1个执行扩容线程”go to ②</span></span><br><span class="line">        	<span class="comment">//④ 由于我们已经假设：第2个以及以后更多线程加入到扩容,这些线程能进入while，go to ⑤</span></span><br><span class="line">          <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                 (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">              <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">            	<span class="comment">//② “第1个执行扩容线程”进来后，此时sc还是正值(扩容阈值sc=sizeCtl)，go to ③</span></span><br><span class="line">            	<span class="comment">//⑤ 在上一轮“第1个执行扩容线程”执行③后，sc就是一个基数负值，因此“第2个以及以后更多线程加入扩容”的线程们会go to ⑥</span></span><br><span class="line">              <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                	<span class="comment">// 这里的条件先放着，等看完后面的transfer方法解析，这些条件自然能迎刃而解</span></span><br><span class="line">                  <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                      sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                      transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                      <span class="keyword">break</span>;</span><br><span class="line">                	<span class="comment">//⑥ “第2个以及以后更多线程加入到扩容”的线程们：在首轮中第1个线程将sc设置基数值后，以后每进来一个扩容线程，该线程对sc进行cas加1，为什么这么安排，请看下一节内容：sc设置为一个基础负数的实际意义</span></span><br><span class="line">                  <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                    	<span class="comment">//  “第2个以及以后更多线程执行扩容”真正进入扩容处理的逻辑</span></span><br><span class="line">                      transfer(tab, nt);</span><br><span class="line">              &#125;</span><br><span class="line">            	<span class="comment">//③ “第1个执行扩容线程”用cas将sc设为一个基数负值</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                           (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">                	<span class="comment">// “第1个执行扩容线程”真正进入扩容处理的逻辑</span></span><br><span class="line">                  transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">              s = sumCount();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="sc设置为一个基础负数的实际意义"><a href="#sc设置为一个基础负数的实际意义" class="headerlink" title="sc设置为一个基础负数的实际意义"></a>sc设置为一个基础负数的实际意义</h5><p>sizeCtl在CHM扩容期间的用处大有来头：如下图所示</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b8f6d46f059f62cc486c59068a58e948.png" alt="CHM的sc设置基数图示"></p>
<p>此图可以清晰解释Doug Lea设计<code>sc=(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)</code>的意图：</p>
<p>第1个线程将sc设置基数值后，以后每进来一个扩容线程，该线程对sc进行cas加1（结合上图），代码实现是在上面的addCount的分支2.1.2</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// 分支2.1.2：第1个扩容线程将sc设置基数值后，以后每进来一个扩容线程，那个线程就会对sc进行cas加1后，再执行transfer扩容</span></span><br><span class="line">    <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">        transfer(tab, nt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对sc加1有什么用？其实到这里应该可以理解了：<font color=red>Doug Lea用这种方式记录当前参与扩容线程的数量，</font>为何要记录参与扩容线程的数量？</p>
<p>两个目的，而且都是非常“聪明”的目的！</p>
<ul>
<li>第一个目的：为了限制参与扩容的总线程数</li>
</ul>
<p>因为一个JVM进程开启太过多的线程数量（例如10万个线程）参与到扩容，意味着当前OS大量线程在运行，高并发下的线程上下文切换、大量线程栈占用空间，一定程度上导致CHM性能不增加反降低，甚至影响到“系统层面的稳定性”，至于为何设计最大扩容线程数量为65535（对应二进制<code>1111 1111 1111 1111</code>）,需从文章后面的resizeStemp解析中找到答案。</p>
<blockquote>
<p>题外话：65535是不是很熟悉？windows 最大可开启的tcp端口号数量？想想为何是这个数2^16-1？或者思考：一个Java进程到底能创建多少线程，注意到最大线程理论值=进程的用户地址空间除以线程栈的大小，用户地址空间和线程栈又取决于操作系统、内存、jvm参数等，具体可参考这篇文章<a href="https://developer.aliyun.com/article/67090">JVM源码分析之一个Java进程究竟能创建多少线程</a></p>
<ul>
<li>JVM：<code>Xmx</code>，<code>Xss</code>，<code>MaxPermSize</code>，<code>MaxDirectMemorySize</code>，<code>ReservedCodeCacheSize</code>等</li>
<li>Kernel：<code>max_user_processes</code>，<code>max_map_count</code>，<code>max_threads</code>，<code>pid_max</code>等</li>
</ul>
</blockquote>
<p>如果参与扩容的线程数量达到了最大值，后面再来第65536、65537、65538个等更多线程，这些线程不会参与到扩容逻辑代码，控制实现在addCount的分支2.1.1的条件：如果sc == rs + MAX_RESIZERS，也即当前参与到扩容队伍的总线程数量达到设定的最大值，再进来的线程不再安排扩容，这些线程会直接break返回。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">         Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">       	<span class="comment">// 完成加1计数后，算下当前CHM节点总数量s，看看s有无达到扩容阈值sizeCtl</span></span><br><span class="line">         <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">             <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">           	<span class="comment">//分支2.1：这里是sc=sizeCtl，如果sizeCtl是一个负数（因为第1个进来扩容的线程将sc设置为一个基数负值），说明有其他线程正在处理table扩容，那么当前线程自然要看看自己能否参与到扩容逻辑中</span></span><br><span class="line">             <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// 分支2.1.1：如果扩容线程达到最大值，那么后续进来再多扩容线程都会直接break掉，也即不参与扩容逻辑。</span></span><br><span class="line">                 <span class="keyword">if</span> ((条件<span class="number">1</span>|| 条件<span class="number">2</span> ||sc == rs + MAX_RESIZERS || 条件<span class="number">4</span> || 条件<span class="number">5</span>)</span><br><span class="line">                     <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>第二个目的：为了确定到底哪个线程是“最后一个完成扩容的线程”？</li>
</ul>
<p>为了确定到底哪个线程是“最后一个完成扩容的线程”，并让它来告知外界整个CHM已经完成了扩容，具体如何实现？</p>
<font color=red>Doug Lea这么设计：每结束一个扩容线程，那个线程就对sc进行cas减1（结合上图理解），直到有一个线程对sc进行cas减1时恰好使得sc就是一开始设置的基础值，那么这个线程就是要找的“最后一个完成扩容的线程”，于是可以将finishing置为true，表示整个CHM的扩容已经完成，对应的源码如下，在transfer方法里面</font>：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">            Node&lt;K,V&gt; f; <span class="keyword">int</span> fh;</span><br><span class="line">     <span class="keyword">while</span> (advance) &#123;  </span><br><span class="line">       			<span class="comment">//...省略部分        </span></span><br><span class="line">          <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">						<span class="comment">//① 这里的逻辑被可改为以下的②、③写法</span></span><br><span class="line">            <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">            i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">        &#125;</span><br><span class="line">       <span class="comment">//...省略部分</span></span><br></pre></td></tr></table></figure>

可把`(sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT` 改写为`sc = (resizeStamp(n) << RESIZE_STAMP_SHIFT) +2`，所以以上①逻辑可以改写下面的②和③写法：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">            Node&lt;K,V&gt; f; <span class="keyword">int</span> fh;</span><br><span class="line">     <span class="keyword">while</span> (advance) &#123;  </span><br><span class="line">       			<span class="comment">//...省略部分</span></span><br><span class="line">            <span class="comment">//  线程完成自己管辖的桶位节点转移到新表后,那么这个线程就对sc进行cas减1</span></span><br><span class="line">              <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">                    <span class="comment">//② 直到有一个线程对sc进行cas减1时恰好使得sc就是一开始设置的基础值，</span></span><br><span class="line">                    <span class="keyword">if</span> (sc==(resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) +<span class="number">2</span>)&#123;</span><br><span class="line">                    <span class="comment">// 说明这个线程就是“最后一个完成扩容的线程”，由它来结束整个CHM的扩容流程</span></span><br><span class="line">                      finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">                      i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">                   	<span class="comment">//③   </span></span><br><span class="line">                    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="comment">// 注意这里在transfer解析中给出，请看完下面transfer方法解析再回来看这个return</span></span><br><span class="line">                      <span class="keyword">return</span>;</span><br><span class="line">                    &#125;            </span><br><span class="line">                &#125;</span><br><span class="line">       <span class="comment">//...省略部分</span></span><br></pre></td></tr></table></figure>

这下逻辑清晰了：

执行`finishing = advance = true;i = n `后，“最后一个完成扩容的线程”在transfer内部就会按以下执行流程走（按序号升序来看执行流，从底下的①开始）

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">    Node&lt;K,V&gt; f; <span class="keyword">int</span> fh;</span><br><span class="line">    <span class="comment">//④ 因为③的advance=true，进入while，go to⑤</span></span><br><span class="line">  	<span class="comment">//⑥ 因为⑤进入if后，advance会改为false，故这里退出while,go to⑦</span></span><br><span class="line">    <span class="keyword">while</span> (advance) &#123;</span><br><span class="line">        <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">      	<span class="comment">//⑤ 因为③的finishing=true，进入if</span></span><br><span class="line">        <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">          	<span class="comment">//回到while⑥</span></span><br><span class="line">            advance = <span class="keyword">false</span>;</span><br><span class="line">      	<span class="comment">// 此分支与本次解析无关</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            i = -<span class="number">1</span>;</span><br><span class="line">            advance = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">// 此分支与本次解析无关</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                 (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                  nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                               nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">            bound = nextBound;</span><br><span class="line">            i = nextIndex - <span class="number">1</span>;</span><br><span class="line">            advance = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//⑦ 因为③的i=n,因此满足条件2：i&gt;=n，进入if</span></span><br><span class="line">    <span class="keyword">if</span> (i &lt; <span class="number">0</span> || i &gt;= n || i + n &gt;= nextn) &#123;</span><br><span class="line">        <span class="keyword">int</span> sc;</span><br><span class="line">        <span class="comment">//⑧ 因为③的finishing=true，进入if</span></span><br><span class="line">        <span class="keyword">if</span> (finishing) &#123;</span><br><span class="line">          	<span class="comment">// 回收nextTable引用</span></span><br><span class="line">            nextTable = <span class="keyword">null</span>;</span><br><span class="line">          	<span class="comment">// 将迁移好的新表赋给原来的table引用</span></span><br><span class="line">            table = nextTab;</span><br><span class="line">          	<span class="comment">//设置下次扩容阈值</span></span><br><span class="line">            sizeCtl = (n &lt;&lt; <span class="number">1</span>) - (n &gt;&gt;&gt; <span class="number">1</span>);</span><br><span class="line">          	<span class="comment">//⑨ 退出for循环，代表：整个CHM已完成所有节点的迁移</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">//① 线程完成自己管辖的桶位节点转移到新表后,那么这个线程就对sc进行cas减1</span></span><br><span class="line">        <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">          <span class="comment">//② 直到有一个线程对sc进行cas减1时恰好使得sc就是一开始设置的基础值,也即addCount分支2里面的：sc==(rs &lt;&lt; RESIZE_STAMP_SHIFT) +2)，go to ③</span></span><br><span class="line">            <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">         		<span class="comment">//③ go to ④</span></span><br><span class="line">            finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">            i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

当线程进入了transfer方法后，这些线程们是怎么知道自己负责第几号到第几号的桶位迁移呢、以及如何将自己管辖的桶位节点转移到新表中呢？这就是tranfer方法里面第二层要深入解析的内容。

#### 2、transfer内部精密的并发设计

##### 2.1 为每个扩容线程分配“桶位区间”

假设现有4个线程并发同一时刻进入transfer逻辑，那么每个线程如何协同转移节点呢？  

首先：为每个线程准确划分线程自己要管辖的“桶位区间”，既然要确定桶位区间`[left,right]`，就必然要在并发环境下计算每个线程自己区间的左边界下标和右边界下标，而右区间边界-左区间边界就是步长：stride，也即线程要完成转移的桶位个数（或者领取任务的个数/步长），如下：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = tab.length, stride;</span><br><span class="line">  	<span class="comment">// 计算领取任务的步长，根据cpu个数来计算，如果计算值小于默认步长16，那么就采用默认步长16，否则步长采用计算值。本文为方便作图，将stride设为4</span></span><br><span class="line">    <span class="keyword">if</span> ((stride = (NCPU &gt; <span class="number">1</span>) ? (n &gt;&gt;&gt; <span class="number">3</span>) / NCPU : n) &lt; MIN_TRANSFER_STRIDE)</span><br><span class="line">        stride = MIN_TRANSFER_STRIDE; <span class="comment">// subdivide range</span></span><br></pre></td></tr></table></figure>

以下以4个线程为例，table长度16，为了方便作图，本文的stride长度设为4，那么四个线程各自分配到桶位范围如下图所以

![WX20210705-212539](https://img-blog.csdnimg.cn/img_convert/f84cad5ea7c6c4d721c6e3a65cb41900.png)

划分好每个线程自己的桶位区间有什么用？当然是各司其职，将自己管辖区间内的桶位节点迁移到新table上。

那么每个线程分配的桶位区间是如何以并发方式（无锁方式）计算出? 

考察现在有线程1，2，3，4共4个线程同一时刻执行到`for (int i = 0, bound = 0;;) ` ，基于当前table=16和本文stride长度设为4。不妨假设线程1先执行，而且线程1是作为“第1个扩容线程”进入到transfer方法里面：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> ((stride = (NCPU &gt; <span class="number">1</span>) ? (n &gt;&gt;&gt; <span class="number">3</span>) / NCPU : n) &lt; MIN_TRANSFER_STRIDE)</span><br><span class="line">        stride = MIN_TRANSFER_STRIDE; <span class="comment">// subdivide range</span></span><br><span class="line"><span class="comment">// 首次扩容，transfer(table,null),也即nextTab为null，所以进入if</span></span><br><span class="line">    <span class="keyword">if</span> (nextTab == <span class="keyword">null</span>) &#123;            <span class="comment">// initiating</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">          	<span class="comment">// 线程1先创建一个新表，两倍于旧表容量</span></span><br><span class="line">            Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node&lt;?,?&gt;[n &lt;&lt; <span class="number">1</span>];</span><br><span class="line">            nextTab = nt;</span><br><span class="line">          <span class="comment">// 在本节中，我们假设旧table是16，因此线程1创建的新表为32,因此不会进入catch逻辑</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable ex) &#123;      <span class="comment">// try to cope with OOME</span></span><br><span class="line">          	<span class="comment">// 如果线程本次扩容的新表容量恰好达到最大，就不再扩容，直接返回。</span></span><br><span class="line">            sizeCtl = Integer.MAX_VALUE;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        nextTable = nextTab;</span><br><span class="line">        transferIndex = n;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

线程1上面流程走完后有：`transferIndex=n=16,stride=4`

线程1：从for循环开始，因为i = 0，bound = 0，因此分支1条件不成立，因为nextIndex = transferIndex=16，显然分支2的条件也不成立，那么只能执行分支3，假设4个线程此时都来到分支3，不妨假设此时线程1抢到CAS，那么对于线程1就会进入分支3逻辑，其他3个线程继续`while`自旋再回到分支3的CAS。线程1执行流程如下：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="keyword">boolean</span> advance = <span class="keyword">true</span>;</span><br><span class="line">   <span class="keyword">boolean</span> finishing = <span class="keyword">false</span>; </span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">         <span class="comment">// ....省略部分        </span></span><br><span class="line">		<span class="keyword">while</span> (advance) &#123;</span><br><span class="line">             <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">           	<span class="comment">//分支1：线程1刚进入for循环时，i=0，bound=0，显然不满足分支1条件</span></span><br><span class="line">             <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">                 advance = <span class="keyword">false</span>;</span><br><span class="line">           	<span class="comment">//分支2:这里的transferIndex的值就是前面transferIndex = n，nextIndex = transferIndex=16，此分支作用：判断CHM剩余可分配给线程桶位的数量，如果&lt;=0,说明CHM所有桶位都分配给线程们了。</span></span><br><span class="line">             <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                 i = -<span class="number">1</span>;</span><br><span class="line">                 advance = <span class="keyword">false</span>;</span><br><span class="line">             &#125;                </span><br><span class="line">	<span class="comment">//分支3：我们已经假设线程1首先抢到cas，故进入if。cas比较前有：nextIndex=transferIndex=16</span></span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                       <span class="comment">// nextBound=(16&gt;4?) 16-4:0=12</span></span><br><span class="line">                       nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                    nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">               	<span class="comment">// cas成功后，那么TRANSFERINDEX(transferIndex)就从16被设置成12.</span></span><br><span class="line">               	<span class="comment">// bound=nextBound=12</span></span><br><span class="line">                 bound = nextBound;</span><br><span class="line">               	<span class="comment">// i=16-1=15</span></span><br><span class="line">                 i = nextIndex - <span class="number">1</span>;</span><br><span class="line">            			<span class="comment">// 这里advance=false后，线程1就可以退出while循环，表明线程1分配到了桶位区间[12,15]分配</span></span><br><span class="line">                 advance = <span class="keyword">false</span>;</span><br><span class="line">             &#125;</span><br></pre></td></tr></table></figure>

线程2：线程1完成cas逻辑后退出while，其他3个线程还得继续竞争分支3的cas，不妨假设线程2成功操作cas（线程3、线程4只能继续循环cas），那么线程2会执行以下逻辑：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">       <span class="comment">// ....省略部分        </span></span><br><span class="line"><span class="keyword">while</span> (advance) &#123;</span><br><span class="line">           <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">         	<span class="comment">//分支1：线程2刚进入for循环时，i=0，bound=0，显然不满足分支1条件</span></span><br><span class="line">           <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">         	<span class="comment">//分支2:注意这里的transferIndex已经被线程1在上面的分支3cas阶段设置成12，含义：旧表16个桶位，目前还剩12个桶位未分配出去（线程1分配了4个）。nextIndex = transferIndex=12</span></span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">               i = -<span class="number">1</span>;</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">           &#125;                </span><br><span class="line">			<span class="comment">//分支3：已经假设本次由线程2抢到cas</span></span><br><span class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                    (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                     <span class="comment">// nextBound=(12&gt;4?) 12-4:0=8</span></span><br><span class="line">                     nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                  nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">             	<span class="comment">// 线程2cas成功后，那么TRANSFERINDEX(transferIndex)就从12被设置成8</span></span><br><span class="line">             	<span class="comment">// bound=nextBound=8</span></span><br><span class="line">               bound = nextBound;</span><br><span class="line">             	<span class="comment">// i=12-1=11</span></span><br><span class="line">               i = nextIndex - <span class="number">1</span>;</span><br><span class="line">             	<span class="comment">// 这里advance=false后，线程2就可以退出while循环，表示线程2现在分配了对应的桶位区间[8,11]</span></span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure>

线程3：线程2完成cas逻辑后退出while，其他2个线程还是同时竞争分支3的cas，不妨假设线程3成功操作cas（此时线程4只能继续循环cas），那么线程3会执行以下逻辑：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">       <span class="comment">// ....省略部分        </span></span><br><span class="line"><span class="keyword">while</span> (advance) &#123;</span><br><span class="line">           <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">         	<span class="comment">//分支1：线程3刚进入for循环时，i=0，bound=0，显然不满足分支1条件</span></span><br><span class="line">           <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">         	<span class="comment">//分支2:注意这里的transferIndex已经被线程2在上面的分支3cas阶段设置成8，含义：旧表16个桶位，目前还剩8个桶位未分配出去（线程1分配了4个，线程2分配了4个）。nextIndex = transferIndex=8</span></span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">               i = -<span class="number">1</span>;</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">           &#125;                </span><br><span class="line">			<span class="comment">//分支3：已经假设本次由线程3抢到cas</span></span><br><span class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                    (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                     <span class="comment">// nextBound=(8&gt;4?) 8-4:0=4</span></span><br><span class="line">                     nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                  nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">             	<span class="comment">// 线程3 cas成功后，那么TRANSFERINDEX(transferIndex)就从8被设置成4</span></span><br><span class="line">             	<span class="comment">// bound=nextBound=4</span></span><br><span class="line">               bound = nextBound;</span><br><span class="line">             	<span class="comment">// i=8-1=7</span></span><br><span class="line">               i = nextIndex - <span class="number">1</span>;</span><br><span class="line">             	<span class="comment">// 这里advance=false后，线程3就可以退出while循环，表示线程3现在分配了对应的桶位区间[4,7]</span></span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure>

线程4：线程3完成cas逻辑后退出while，目前只有线程4操作分支3的cas，肯定能进入cas，那么线程4会执行以下逻辑：

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">       <span class="comment">// ....省略部分        </span></span><br><span class="line"><span class="keyword">while</span> (advance) &#123;</span><br><span class="line">           <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">         	<span class="comment">//分支1：线程4刚进入for循环时，i=0，bound=0，显然不满足分支1条件</span></span><br><span class="line">           <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">         	<span class="comment">//分支2:注意这里的transferIndex已经被线程3在上面的分支3cas阶段设置成4，含义：旧表16个桶位，目前还剩4个桶位未分配出去（线程1分配了4个、线程2分配了4个、线程3分配了4个）。nextIndex = transferIndex=4</span></span><br><span class="line">           <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">               i = -<span class="number">1</span>;</span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">           &#125;                </span><br><span class="line">			<span class="comment">//分支3：本次由线程4抢到cas</span></span><br><span class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                    (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                     <span class="comment">// nextBound=(4&gt;4?) 4-4:0=0</span></span><br><span class="line">                     nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                  nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">             	<span class="comment">// 线程4 cas成功后，那么TRANSFERINDEX(transferIndex)就从4被设置成0，表示旧表16个桶位中，已经全部分配出去（给到各个线程）</span></span><br><span class="line">             	<span class="comment">// bound=nextBound=0</span></span><br><span class="line">               bound = nextBound;</span><br><span class="line">             	<span class="comment">// i=4-1=3</span></span><br><span class="line">               i = nextIndex - <span class="number">1</span>;</span><br><span class="line">             	<span class="comment">// 这里advance=false后，线程4就可以退出while循环，表示线程4现在分配了对应的桶位区间[0,3]</span></span><br><span class="line">               advance = <span class="keyword">false</span>;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure>

<font color=red>经过以上四轮while推演，可以发现四个线程并发同一时刻进入`for (int i = 0, bound = 0;;) `并来到`whlie(advance)` 的实际目的：就是为扩容线程们分配好各自要迁移的桶位区间，分配方式是从table尾部往头部方向的倒序分配，正如上面计算的线程1=>15到12，线程2=>11到8，线程3=>7到4，线程4=>3到0，而且巧妙的是：每个线程分配的桶位区间都不会重叠，原因在于：分支3需要线程自己抢到cas权才能对transferIndex扣减步长值</font>

<p>在4个线程分配好桶位区间的情况下，此时若存在第5个线程、第6个线程…..它们能分配到桶位区间吗？答案是不能，可以分开两种线程时刻讨论：</p>
<ul>
<li>第一种线程时刻：</li>
</ul>
<p>不妨假设第5个线程、第6个线程…都是和线程1、线程2、线程3、线程4一起进入到<code>for (int i = 0, bound = 0;;)</code>并来到<code>whlie(advance)</code>，由于前面4个线程分配好了桶位区间（但这个4个线程正准备扩容），此时考察第5个线程，不妨假设从while的①步骤开始</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">    <span class="comment">// ....省略部分</span></span><br><span class="line">    <span class="keyword">boolean</span> advance = <span class="keyword">true</span>;</span><br><span class="line">		<span class="keyword">boolean</span> finishing = <span class="keyword">false</span>;</span><br><span class="line">   <span class="comment">//① 线程5的首次advance为true，进入while</span></span><br><span class="line">   <span class="comment">//⑤ 来自④的advance=false,go to ⑥</span></span><br><span class="line">    <span class="keyword">while</span> (advance) &#123;</span><br><span class="line">        <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">      	<span class="comment">//② 线程5首次进入for循环时，i=0，bound=0，（且前面4个线程还没扩容完），显然不满足分支1条件finishing</span></span><br><span class="line">        <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">            advance = <span class="keyword">false</span>;</span><br><span class="line">      	<span class="comment">//③ 从线程4的分支3可知，transferIndex已经被设置为0，含义：旧表16个桶位已经全部分配出去，所以线程5会进入该分支</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">//④ 回到⑤while</span></span><br><span class="line">            i = -<span class="number">1</span>;</span><br><span class="line">            advance = <span class="keyword">false</span>;</span><br><span class="line">        &#125;                </span><br><span class="line"><span class="comment">// 线程5不会到此次分支</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                 (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                  nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                               nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">            bound = nextBound;</span><br><span class="line">            i = nextIndex - <span class="number">1</span>;</span><br><span class="line">            advance = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="comment">//⑥ 因为④的i=-1，满足条件1，进入⑦  </span></span><br><span class="line">    <span class="keyword">if</span> (i &lt; <span class="number">0</span> || i &gt;= n || i + n &gt;= nextn) &#123;</span><br><span class="line">        <span class="keyword">int</span> sc;</span><br><span class="line">      	<span class="comment">// 前面4个线程还没扩容完，不会进入此if</span></span><br><span class="line">        <span class="keyword">if</span> (finishing) &#123;</span><br><span class="line">            nextTable = <span class="keyword">null</span>;</span><br><span class="line">            table = nextTab;</span><br><span class="line">            sizeCtl = (n &lt;&lt; <span class="number">1</span>) - (n &gt;&gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">//⑦ 线程5对sc进行cas减1计数：表示在整个CHM有1个线程正准备离开扩容队伍</span></span><br><span class="line">        <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">          	<span class="comment">//⑧ 由“前面4个线程还没扩容完”可知，线程5在⑦中对sc进行减1后，sc肯定还不是基础值，故去到⑨（在这里我们也可以知道，线程5不是“最后一个完成扩容的线程”）</span></span><br><span class="line">            <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">              	<span class="comment">//⑨ 线程5返回到外部的addCount方法⑪。</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">            i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>由上面的①到⑨执行流，可以看出，由于transferIndex已经被线程4设置为0后（表示16个桶位都分配给前面4个线程），第5个线程就会进入③代码片区，最终从⑨结束并返回到外部addCount方法，可以清楚看到线程5在整个过程即没有“分配到桶位区间”也没有参与“桶位节点迁移工作”就直接返回了，继续看线程5返回点，从⑪开始：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//新put一个节点后，addCount(x=1, check=binCount=1)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">     CounterCell[] as; <span class="keyword">long</span> b, s;</span><br><span class="line">   	<span class="comment">// .......省略部分</span></span><br><span class="line">   </span><br><span class="line">		<span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">           Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">         	<span class="comment">//⑫ 因为“前面4个线程还没扩容完”，且sc还是一个负数，因此s必然&gt;=sc = sizeCtl,故线程5go to ⑬</span></span><br><span class="line">           <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                  (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">               <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">             	<span class="comment">//⑬ 因为“前面4个线程还没扩容完”，那么sc还是个负值，故线程5go to ⑭</span></span><br><span class="line">               <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                 	<span class="comment">//⑭ 在上面tansfer方法的③步骤中已经知道，transferIndex已经是等于0，故满足条件5：transferIndex &lt;= 0，所以线程5会被break并退出了while循环。</span></span><br><span class="line">                   <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                       sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                       transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                       <span class="keyword">break</span>;</span><br><span class="line">                   <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                       transfer(tab, nt);</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                            (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">                   transfer(tab, <span class="keyword">null</span>); <span class="comment">//⑪ 线程5返回点，接着线程5会回到while</span></span><br><span class="line">               s = sumCount();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>在这里⑭步骤可以知道，线程5就退出了。</p>
<font color=red>由此，我们终于搞清楚这样的扩容安排：对于旧表16开始扩容时，已经有4个线程分配好各自待迁移桶位区间后，如果此时后续再来第5个线程、第6个线程等更多线程进入到扩容逻辑transfer方法，这些后来者线程（或者在分配桶位区间竞争失败的线程）最终即没有“分配到桶位区间”也没有参与“桶位节点迁移工作”，直接结束addCount方法调用。</font>

<ul>
<li>第二种线程时刻：</li>
</ul>
<p>假设线程1、线程2、线程3、线程4已经分配好了桶位区间，并假设此时第5个线程、第6个线程等其他线程开始进入AddCount方法内部的while位置：此时考察第5个线程，由于transferIndex已经等于0可知，其实此时线程5（线程6等）就会回到上面第一种情况分析的⑭步骤：<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//⑬ 因为“前面4个线程还没扩容完”，那么sc还是个负值，故线程5go to ⑭</span></span><br><span class="line"> <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">   	<span class="comment">//⑭ 在上面tansfer方法的③步骤中已经知道，transferIndex已经是等于0，故满足条件5：transferIndex &lt;= 0，所以线程5会被break并退出了while循环。</span></span><br><span class="line">     <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">         sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">         transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">         <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure><br>结论跟第一种情况一样，此处不再累赘。</p>
<p>关于以上四个线程并发分配桶位区间的计算过程，总结如下图</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/f47502051e91952d11b6083955357035.png" alt="四个线程桶位分配过程"></p>
<h5 id="2-2-桶位区间迁移节点的设计"><a href="#2-2-桶位区间迁移节点的设计" class="headerlink" title="2.2 桶位区间迁移节点的设计"></a>2.2 桶位区间迁移节点的设计</h5><p>前面给4个线程分配好各自的桶位区间后，接下来四个线程要完成自己管辖的桶位区间节点迁移任务。</p>
<p>不妨以线程1作为考察：线程1管辖的桶位区间是<code>[12,15]</code>，从第i=15号桶位开始进行节点迁移，直到4个桶位节点都迁移完毕的图示流程：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c8943d787e863bb0df0f6bc57f8fcf95.png" alt="线程1迁移桶位区间过程"></p>
<p>图中的fwd节点：线程每完成一个节点迁移，就在旧表桶位放置一个fwd节点</p>
<p>以下的桶位区间节点迁移过程是基于i=15桶位开始：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="comment">// Forwarding中文译为：转发、转投，因此ForwardingNode表示“转发节点”，这个类也是jdk1.8CHM一个关键设计，起着“通告作用”，只要有写线程进行put、replace、clear、compute看到桶位是一个fwd节点，那么这些线程就会转而去加入扩容队伍（帮助扩容helpTransfer）、而读线程约到fwd节点就会“转到”新表去读数据节点</span></span><br><span class="line">ForwardingNode&lt;K,V&gt; fwd = <span class="keyword">new</span> ForwardingNode&lt;K,V&gt;(nextTab);</span><br><span class="line">		<span class="comment">// 例如，线程1管辖的桶位区间是`[bound,i]`，第i个捅位节点迁移完毕后，advance为true时，表示线程1向前移动1步（i指针向右移动1步）以便继续迁移第--i个桶位节点</span></span><br><span class="line">      <span class="keyword">boolean</span> advance = <span class="keyword">true</span>;</span><br><span class="line">      <span class="keyword">boolean</span> finishing = <span class="keyword">false</span>; <span class="comment">// to ensure sweep before committing nextTab        </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">          Node&lt;K,V&gt; f; <span class="keyword">int</span> fh;</span><br><span class="line">        	<span class="comment">// 逻辑1：线程1首次拿到桶位区间[12,15]会退出while，走到逻辑2</span></span><br><span class="line">          <span class="keyword">while</span> (advance) &#123;</span><br><span class="line">              <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">            	<span class="comment">// 分支1：接续下面的逻辑3、逻辑4、逻辑5，请看完逻辑3、逻辑4、逻辑5后再回来分支1的条件</span></span><br><span class="line">              <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">                  advance = <span class="keyword">false</span>;</span><br><span class="line">            	<span class="comment">// 分支2</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                  i = -<span class="number">1</span>;</span><br><span class="line">                  advance = <span class="keyword">false</span>;</span><br><span class="line">              &#125;</span><br><span class="line">             <span class="comment">// 分支3：</span></span><br><span class="line">              <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                       (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                        nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                     nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">                  bound = nextBound;</span><br><span class="line">                  i = nextIndex - <span class="number">1</span>;</span><br><span class="line">                  advance = <span class="keyword">false</span>;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        	<span class="comment">// 逻辑2：接逻辑1，i=15，不满足这三个判断条件，因此会跳过逻辑2走到逻辑3</span></span><br><span class="line">          <span class="keyword">if</span> (i &lt; <span class="number">0</span> || i &gt;= n || i + n &gt;= nextn) &#123;</span><br><span class="line">              <span class="keyword">int</span> sc;</span><br><span class="line">            	<span class="comment">// 分支2.1</span></span><br><span class="line">              <span class="keyword">if</span> (finishing) &#123;</span><br><span class="line">                  nextTable = <span class="keyword">null</span>;</span><br><span class="line">                  table = nextTab;</span><br><span class="line">                  sizeCtl = (n &lt;&lt; <span class="number">1</span>) - (n &gt;&gt;&gt; <span class="number">1</span>);</span><br><span class="line">                  <span class="keyword">return</span>;</span><br><span class="line">              &#125;</span><br><span class="line">            	<span class="comment">// 分支2.2</span></span><br><span class="line">              <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">                  <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">                      <span class="keyword">return</span>;</span><br><span class="line">                  finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">                  i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        	<span class="comment">/* 逻辑3：旧表中不是每个桶位都有节点数据，当线程1发现桶位15为null时，就在此放入一个ForwardingNode节点，告知外界：当前桶位正在迁移节点，这样当其他线程看到桶位节点是fwd时，就转去执行帮助扩容逻辑，如下所示</span></span><br><span class="line"><span class="comment">  		    还记得putVal方法里面的以下设计吗:</span></span><br><span class="line"><span class="comment">  		    其他线程看到桶位节点是fwd时，就转去执行帮助扩容的逻辑。</span></span><br><span class="line"><span class="comment">  		    else if ((fh = f.hash) == MOVED)</span></span><br><span class="line"><span class="comment">              	tab = helpTransfer(tab, f);   </span></span><br><span class="line"><span class="comment">  		*/</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i)) == <span class="keyword">null</span>)</span><br><span class="line">              advance = casTabAt(tab, i, <span class="keyword">null</span>, fwd);</span><br><span class="line">        	</span><br><span class="line">        	<span class="comment">// 逻辑4：如果逻辑3条件不成立，就会走到逻辑4，也即线程1发现第15号桶位已经是一个fwd节点，说明当前桶位节点已经被其他线程迁移完毕，那么线程1可以跳过第15号桶位继续向前走1步，advance = true，也即去第14桶号桶，然后回到while逻辑1里面的分支1</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)</span><br><span class="line">              advance = <span class="keyword">true</span>; <span class="comment">// already processed</span></span><br><span class="line">        	<span class="comment">// 逻辑5：若线程1走到这里，说明当前桶位节点是一个正常的数据节点（链表、红黑树），直接给当前桶位头节点加独占锁后，执行迁移节点到新表的逻辑</span></span><br><span class="line">          <span class="keyword">else</span> &#123;</span><br><span class="line">            		<span class="keyword">synchronized</span> (f) </span><br><span class="line">                <span class="comment">// 省略部分</span></span><br><span class="line">                <span class="comment">//将i位置低位节点链迁移到新表的i位置 </span></span><br><span class="line">                 setTabAt(nextTab, i, ln);</span><br><span class="line">             		<span class="comment">//将i位置高位节点链迁移到新表的i+n位置 </span></span><br><span class="line">                 setTabAt(nextTab, i + n, hn);</span><br><span class="line">            		<span class="comment">//在当前桶位放入fwd，告知外界：当前桶位已经迁移完</span></span><br><span class="line">                 setTabAt(tab, i, fwd);</span><br><span class="line">            		<span class="comment">//advance = true让线程1回到逻辑1while里面的分支1，表示继续处理第14号（--i）桶位节点</span></span><br><span class="line">                 advance = <span class="keyword">true</span>;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure>
<p>有了以上扩容线程1对i=15桶节点迁移流程的说明后，那么接下里继续讨论线程1如何知道已经完成<code>[bound=12,i=15]</code>桶位区间所有节点迁移且退出for循环的流程。</p>
<p>不妨考察线程1当前迁移的桶位是第i=13桶号，之后的执行流程按以下序列号进行，从底部的①开始按升序知道第⑰步的return，以下逻辑一定耐心看完，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="comment">//addCount调用transfer(tab, null);</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab)</span> </span>&#123;			</span><br><span class="line">  		<span class="comment">// ......省略部分</span></span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, bound = <span class="number">0</span>;;) &#123;</span><br><span class="line">            Node&lt;K,V&gt; f; <span class="keyword">int</span> fh;</span><br><span class="line">          	<span class="comment">//③ 从底部②的advance为ture，故进入while，go to ④</span></span><br><span class="line">          	<span class="comment">//⑥ 从⑤的advance为false，故退出while，go to ⑦</span></span><br><span class="line">          	<span class="comment">//⑪ 从底部⑩的advance为ture，故进入while，go to ⑫</span></span><br><span class="line">          	<span class="comment">//⑮ 从⑭的advance为false，故退出while，go to ⑯</span></span><br><span class="line">            <span class="keyword">while</span> (advance) &#123;</span><br><span class="line">                <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">              <span class="comment">//④ --i等于12&gt;=bound=12,进入if</span></span><br><span class="line">              <span class="comment">//⑫ --i等于11已经越过左边界bound=12，不进入if,go to ⑬</span></span><br><span class="line">                <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">                  	<span class="comment">//⑤ 回到while⑥</span></span><br><span class="line">                    advance = <span class="keyword">false</span>;</span><br><span class="line">              <span class="comment">//⑬ 由于16个桶位已全部分配出去，因此transferIndex=0，满足条件，进入if</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> ((nextIndex = transferIndex) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    i = -<span class="number">1</span>;</span><br><span class="line">                    <span class="comment">//⑭ 说明线程1已经迁移完自己管辖的4个桶位节点，不需要继续前进处理下一个桶位了，go to⑮</span></span><br><span class="line">                    advance = <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">               <span class="comment">// 线程1已经分配好桶位区间，故分支与本次解析无关，可忽略</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt</span><br><span class="line">                         (<span class="keyword">this</span>, TRANSFERINDEX, nextIndex,</span><br><span class="line">                          nextBound = (nextIndex &gt; stride ?</span><br><span class="line">                                       nextIndex - stride : <span class="number">0</span>))) &#123;</span><br><span class="line">                    bound = nextBound;</span><br><span class="line">                    i = nextIndex - <span class="number">1</span>;</span><br><span class="line">                    advance = <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">//⑯ 因为⑭已经将i设为-1，这里满足条件i&lt;0,进入if，go to ⑰ </span></span><br><span class="line">            <span class="keyword">if</span> (i &lt; <span class="number">0</span> || i &gt;= n || i + n &gt;= nextn) &#123;</span><br><span class="line">                <span class="keyword">int</span> sc;</span><br><span class="line">              	<span class="comment">// 我们假设线程1不是“最后一个完成扩容的线程”，故跳过逻辑</span></span><br><span class="line">                <span class="keyword">if</span> (finishing) &#123;</span><br><span class="line">                    nextTable = <span class="keyword">null</span>;</span><br><span class="line">                    table = nextTab;</span><br><span class="line">                    sizeCtl = (n &lt;&lt; <span class="number">1</span>) - (n &gt;&gt;&gt; <span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">              <span class="comment">//⑰ 线程1对sc进行cas减1操作：表示在整个CHM有1个线程正准备离开扩容队伍</span></span><br><span class="line">                <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">                  	<span class="comment">// 由于我们假设线程1不是“最后一个完成扩容的线程”，满足条件，执行return，退出for循环</span></span><br><span class="line">                    <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">                    i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">//⑦ 在这里我们假设第12号桶不为空，因此go to ⑧</span></span><br><span class="line">          <span class="comment">// (如果第12号（i）桶为空，就直接给该桶位设为fwd表示已处理，将advance设为true：表示让线程1继续迁移下一个（第--i个）桶位节点。回到上面while循环)</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i)) == <span class="keyword">null</span>)</span><br><span class="line">                advance = casTabAt(tab, i, <span class="keyword">null</span>, fwd);</span><br><span class="line">          <span class="comment">//⑧ 在这里我们假设第12号桶不是fwd节点而是正常节点，因此go to ⑨</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)</span><br><span class="line">              <span class="comment">//(如果第12号（i）桶是一个fwd节点，表示有其他“协助型线程”处理过（所以源码注释为already processed），那么线程1更省功夫了，直接将advance设为true：表示让线程1继续迁移下一个（第--i个）桶位节点。回到上面while循环)</span></span><br><span class="line">                advance = <span class="keyword">true</span>; </span><br><span class="line">          	<span class="comment">//① 准备迁移i=13号桶位，去setTabAt等逻辑</span></span><br><span class="line">          	<span class="comment">//⑨ 准备迁移i=12号桶位，去setTabAt等逻辑</span></span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">                  	<span class="comment">// 如果是桶位节点是普通链表</span></span><br><span class="line">                    <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">							<span class="comment">// 将低位链表、高位链表迁移到新表对应的i、i+n位置</span></span><br><span class="line">                            setTabAt(nextTab, i, ln);</span><br><span class="line">                            setTabAt(nextTab, i + n, hn);</span><br><span class="line">                      		<span class="comment">// 在第i=13号桶位放置fwd节点，表明该桶位已经处理完</span></span><br><span class="line">                            setTabAt(tab, i, fwd);</span><br><span class="line">                      		<span class="comment">// ② 将advance设为true，回到while③</span></span><br><span class="line">                      		<span class="comment">// ⑩ 再次将advance设为true，回到while⑪</span></span><br><span class="line">                            advance = <span class="keyword">true</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                  		<span class="comment">// （类似链表的逻辑）如果是桶位节点是TreeBin（也即桶位是一棵红黑树）</span></span><br><span class="line">                     <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">														<span class="comment">//（类似链表的逻辑）</span></span><br><span class="line">                            setTabAt(nextTab, i, ln);</span><br><span class="line">                            setTabAt(nextTab, i + n, hn);</span><br><span class="line">                            setTabAt(tab, i, fwd);</span><br><span class="line">                            advance = <span class="keyword">true</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>现在考察线程1执行完⑰的return后，它会在回到⑲while处，然后流程如下：最后发现线程1完成自己桶位区间节点迁移后，会在以下的㉑步骤退出while循环结束addCount方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// addCount(long x, int check)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">addCount</span><span class="params">(<span class="keyword">long</span> x, <span class="keyword">int</span> check)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// ......省略部分</span></span><br><span class="line">     <span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">         Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">       	<span class="comment">//⑲ 由于线程2、3、4还在迁移桶位节点中，因此这里sc = sizeCtl还是负值，而s=sumCount()是正值，因此能进入while，go to ⑳</span></span><br><span class="line">         <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">             <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">           	<span class="comment">//⑳ 由于线程2、3、4还在迁移桶位节点中，因此这里sc = sizeCtl还是负值</span></span><br><span class="line">             <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">               	<span class="comment">//㉑  基于上面线程1到线程4的桶位分配完毕，因此transferIndex还是等于0，故线程1在这里就会break退出addCount方法</span></span><br><span class="line">                 <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                     sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                     transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                     <span class="keyword">break</span>;</span><br><span class="line">                 <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                     transfer(tab, nt);</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                          (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">               	<span class="comment">//⑱ 来自内部transfer第⑰步的return，线程1继续返回到while</span></span><br><span class="line">                 transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">             s = sumCount();</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-3-桶位节点转移到新表"><a href="#2-3-桶位节点转移到新表" class="headerlink" title="2.3 桶位节点转移到新表"></a>2.3 桶位节点转移到新表</h5><p>2.1关注的是线程自己如何一个桶位一个桶位迁移的控制逻辑，那么接下来就要解释扩容线程如何将旧表桶位节点转移到新表中的过程，其实只要掌握jdk1.8的HashMap双向链表到树和jdk1.7的ConcurrentHashMap链表转移设计思想，以下逻辑则很好理解：</p>
<p>针对两种数据节点类型对应两种迁移设计：</p>
<ul>
<li>链表迁移思路：</li>
</ul>
<p>1、synchronized锁住当前桶位头节点f，保证当前线程独占操作。<font color=red>换句话说：只能有1个线程负责迁移当前桶位节点到新数组，请别误解为：jdk1.8多线程并发协同扩容是指多个线程一起在同一个桶位上“并发协同”迁移桶位节点。</font></p>
<p>2、如果桶位头节点是一条链表，先找出lastRun节点为首的子链，然后根据高位节点（p.hash &amp; n 不等于0）和低位节点特征（p.hash &amp; n等于0）分别构建一条低位节点链ln和一条高位节点链hn</p>
<p>（如果你已经掌握jdk1.7CHM的Segment数组扩容算法，你会发现jdk1.8的处理方式比1.7更优，因为jdk1.8引入了低位节点链和高位节点链作为迁移前的中间容器，而这里的低位链和高位链的设计需要你掌握jdk1.8HashMap的数组扩容算法）</p>
<p>3、链表除去lastRun子链，还剩下位于lsatRun节点前面的节点，将这些节点使用头插法插入到ln或者hn中</p>
<p>4、此时冲突链被完整的拆开成低位链和高位链，接下来就好办了：使用cas方式，低位链放在新表的第i位置，高位链放在新表的第i+n位置</p>
<p>5、第4点迁移完后，线程在当前桶位放置一个fwd，表示当前桶位已迁移完。</p>
<p>6、线程将advance设为true，回到while继续处理“桶位区间”的下一个待迁移桶位</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (advance) &#123;</span><br><span class="line">    <span class="keyword">int</span> nextIndex, nextBound;</span><br><span class="line">    <span class="keyword">if</span> (--i &gt;= bound || finishing)</span><br><span class="line">        advance = <span class="keyword">false</span>;</span><br></pre></td></tr></table></figure>
<p>这里再次提醒：“桶位区间”的迁移顺序是倒序的，例如当前线程负责的桶位区间为[12,15]，当第15号桶位迁移完后，—i继续处理下一个第14号桶位节点，以此类推，直到—i等于11越过左边界12，说明此时桶位区间包含的4个桶位节点全部被当前线程迁移到新表里面。</p>
<p>lastRun子链是什么？</p>
<p>就是从链表的尾部节点开始，找到具有相同的（e.hash &amp; n）值的连续最大子链，图示如下：</p>
<p>其中e.hash &amp; n的计算公式可找出低位节点和高位节点。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2ed6f93efc10facd087b82560a4c75f4.png" alt="lastRun示意图.001"></p>
<ul>
<li>TreeBin（红黑树）迁移思路</li>
</ul>
<p>因为TreeBin持有红黑树的root节点，关于红黑树扩容的分析在在jdk1.8的HashMap文章有详细的过程，CHM也类似逻辑，这里不再累赘。</p>
<p>以上两种迁移思路就是下面transfer方面里面：<code>synchronized (f)</code>代码片段要实现的逻辑</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab)</span> </span>&#123;</span><br><span class="line">		<span class="comment">//......省略部分</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((f = tabAt(tab, i)) == <span class="keyword">null</span>)</span><br><span class="line">            advance = casTabAt(tab, i, <span class="keyword">null</span>, fwd);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((fh = f.hash) == MOVED)</span><br><span class="line">            advance = <span class="keyword">true</span>; <span class="comment">// already processed</span></span><br><span class="line">  </span><br><span class="line">  			<span class="comment">// 线程若能进入以下分支，说明此桶位头节点既不是null也不是转发节点fwd，那么该桶位头节点要么是一条冲突链表、要么是一个TreeBin节点（也即一棵红黑树）</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (f) &#123;</span><br><span class="line">                <span class="comment">// 链表的迁移逻辑</span></span><br><span class="line">                <span class="keyword">if</span> (tabAt(tab, i) == f) &#123;</span><br><span class="line">                  	<span class="comment">// ln:用于存放低位节点链，hn:用于存放高位节点链</span></span><br><span class="line">                    Node&lt;K,V&gt; ln, hn;</span><br><span class="line">                    <span class="keyword">if</span> (fh &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="keyword">int</span> runBit = fh &amp; n;</span><br><span class="line">                        Node&lt;K,V&gt; lastRun = f;</span><br><span class="line">                      	<span class="comment">//①</span></span><br><span class="line">                        <span class="keyword">for</span> (Node&lt;K,V&gt; p = f.next; p != <span class="keyword">null</span>; p = p.next) &#123;</span><br><span class="line">                            <span class="keyword">int</span> b = p.hash &amp; n;</span><br><span class="line">                          	<span class="comment">// 找出有相同特征的、以lastRun节点为首的子链</span></span><br><span class="line">                            <span class="keyword">if</span> (b != runBit) &#123;</span><br><span class="line">                                runBit = b;</span><br><span class="line">                                lastRun = p;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                      <span class="comment">// runBit也即p.hash &amp; n=0，说明是低位节点</span></span><br><span class="line">                        <span class="keyword">if</span> (runBit == <span class="number">0</span>) &#123;</span><br><span class="line">                          	<span class="comment">// runBit == 0，说明lastRun子链是低位节点构成的，因此理应将lastRun放在低位链ln，此时hn还是设为null，hn在第②步骤会被用到</span></span><br><span class="line">                            ln = lastRun;</span><br><span class="line">                            hn = <span class="keyword">null</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                      <span class="comment">// 也即runBit不为0，p.hash &amp; n !=0，说明是高位节点</span></span><br><span class="line">                        <span class="keyword">else</span> &#123;</span><br><span class="line">                            hn = lastRun;</span><br><span class="line">                            ln = <span class="keyword">null</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                      </span><br><span class="line">                        <span class="comment">//②  </span></span><br><span class="line">                        <span class="keyword">for</span> (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123;</span><br><span class="line">                            <span class="keyword">int</span> ph = p.hash; K pk = p.key; V pv = p.val;</span><br><span class="line">                          	<span class="comment">//(ph &amp; n) == 0的节点是低位特征节点</span></span><br><span class="line">                            <span class="keyword">if</span> ((ph &amp; n) == <span class="number">0</span>)</span><br><span class="line">                              	<span class="comment">// 将剩余的低位节点使用头插法插入到低位链中</span></span><br><span class="line">                                ln = <span class="keyword">new</span> Node&lt;K,V&gt;(ph, pk, pv, ln);</span><br><span class="line">                            <span class="comment">//(ph &amp; n) !=0的节点是高位特征节点</span></span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                               <span class="comment">// 将剩余的低位节点使用头插法插入到高位链中</span></span><br><span class="line">                                hn = <span class="keyword">new</span> Node&lt;K,V&gt;(ph, pk, pv, hn);</span><br><span class="line">                        &#125;</span><br><span class="line">                      </span><br><span class="line">                        <span class="comment">//使用cas机制：低位链放在新表的第i位置，</span></span><br><span class="line">                        setTabAt(nextTab, i, ln);</span><br><span class="line">                      	<span class="comment">//使用cas机制：高位链放在新表的第i+n位置</span></span><br><span class="line">                        setTabAt(nextTab, i + n, hn);</span><br><span class="line">                      	<span class="comment">//在旧表当前i桶位放置fwd节点，表示已迁移完</span></span><br><span class="line">                        setTabAt(tab, i, fwd);</span><br><span class="line">                      	<span class="comment">// 回到while继续处理下一个桶位</span></span><br><span class="line">                        advance = <span class="keyword">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">                        TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span><br><span class="line">                        <span class="comment">// 用于存放红黑树中低位节点的单向链表</span></span><br><span class="line">                        TreeNode&lt;K,V&gt; lo = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">                      	<span class="comment">// 用于存放红黑树中高位节点的单向链表</span></span><br><span class="line">                        TreeNode&lt;K,V&gt; hi = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                      	<span class="comment">// lc计数器，用于判断低位节点链是否需要树化，hc同理</span></span><br><span class="line">                        <span class="keyword">int</span> lc = <span class="number">0</span>, hc = <span class="number">0</span>;</span><br><span class="line">                      </span><br><span class="line">                        <span class="comment">//③ jdk1.8的CHM红黑树本身也是一条单向链表</span></span><br><span class="line">                        <span class="keyword">for</span> (Node&lt;K,V&gt; e = t.first; e != <span class="keyword">null</span>; e = e.next) &#123;</span><br><span class="line">                            <span class="keyword">int</span> h = e.hash;</span><br><span class="line">                            TreeNode&lt;K,V&gt; p = <span class="keyword">new</span> TreeNode&lt;K,V&gt;</span><br><span class="line">                                (h, e.key, e.val, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">                          	<span class="comment">// 构建低位节点单向链表，并计数</span></span><br><span class="line">                            <span class="keyword">if</span> ((h &amp; n) == <span class="number">0</span>) &#123;</span><br><span class="line">                            <span class="comment">//如果p.prev和hiTail是空，说明当前p节点是头节点，放在lo位置</span></span><br><span class="line">                                <span class="keyword">if</span> ((p.prev = loTail) == <span class="keyword">null</span>)</span><br><span class="line">                                    lo = p;</span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                  <span class="comment">// 其他非头节点，按尾插法插入到链表尾部</span></span><br><span class="line">                                    loTail.next = p;</span><br><span class="line">                              <span class="comment">//遍历下一个节点</span></span><br><span class="line">                                loTail = p;</span><br><span class="line">                              <span class="comment">//链表长度计数</span></span><br><span class="line">                                ++lc;</span><br><span class="line">                            &#125;</span><br><span class="line">                            <span class="comment">// 构建高位节点单向链表，并计数</span></span><br><span class="line">                            <span class="keyword">else</span> &#123;</span><br><span class="line">                                <span class="keyword">if</span> ((p.prev = hiTail) == <span class="keyword">null</span>)</span><br><span class="line">                                    hi = p;</span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                    hiTail.next = p;                                  	</span><br><span class="line">                                hiTail = p;</span><br><span class="line">                                ++hc;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="comment">// 低位链表是否需要树化，计数小于等于6不进行树化，如果不存在高位链表，</span></span><br><span class="line">                      	<span class="comment">/* 低位链表需要树化:</span></span><br><span class="line"><span class="comment">                      	 1、若hc!=0，将低位节点链表包装成TreeBin类型（内部会重新构建树），以便放入到新表对应的桶位头节点位置</span></span><br><span class="line"><span class="comment">                      	 2、若hc=0，说明新构建的低位链还是原来红黑树本身的链表，无需改动，直接返回ln=t。   TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span></span><br><span class="line"><span class="comment">                      	*/</span> </span><br><span class="line">                        ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) :</span><br><span class="line">                            (hc != <span class="number">0</span>) ? <span class="keyword">new</span> TreeBin&lt;K,V&gt;(lo) : t;</span><br><span class="line">                      	<span class="comment">// 同上，不再赘述。</span></span><br><span class="line">                        hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) :</span><br><span class="line">                            (lc != <span class="number">0</span>) ? <span class="keyword">new</span> TreeBin&lt;K,V&gt;(hi) : t;</span><br><span class="line">                        setTabAt(nextTab, i, ln);</span><br><span class="line">                        setTabAt(nextTab, i + n, hn);</span><br><span class="line">                        setTabAt(tab, i, fwd);</span><br><span class="line">                        advance = <span class="keyword">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="2-4-transfer扩容并发度讨论"><a href="#2-4-transfer扩容并发度讨论" class="headerlink" title="2.4 transfer扩容并发度讨论"></a>2.4 transfer扩容并发度讨论</h5><p>在jdk1.7的CHM中，它的并发度设计是相对受限的，在创建CHM中，可以指定CONCURRENCY_LEVEL的值来设并发度，遗憾的是：一旦创建好，CHM支持的并发度就不能更改。并且在jdk1.7的CHM中，指定的并发度设计的太大或者太小也会有问题：</p>
<ul>
<li>并发太小时，多个线程都在竞争数量不多的segment加剧竞争，可谓“僧多粥少”</li>
<li>并发度设为太大时（例如segment并发度设为65535最大值），使得key原本可定位在同一个segment的读、写操作就会被分配到不同的segment中，结果是cpu cache命中率会下降，会在一定程度上降低CHM性能</li>
</ul>
<p>而在jdk1.8创建CHM中，它不会限制并行度：</p>
<blockquote>
<p>考察put/transfer方法，内部在桶位迁移用时<code>synchronized</code>锁住桶位头节点f的设计，也即锁粒度为桶位，那么随着transfer方法扩容CHM使得底层table更大后，因为锁粒度为桶位，因此并发度也随着变大，换句话说，jdk1.8的CHM的并发度是动态变化：并发度大小等于底层数组长度，底层数组每扩容1次，并发度就变大1倍，而不是像jdk1.7那样再创建期间并发度就被限制死了。</p>
</blockquote>
<p>当然jdk1.8CHM的并发度因为是动态变大，当table容量太大时，也会面临cpu cache命中率会下降的性能问题。</p>
<h5 id="2-5-CHM扩容时，可以同时支持读取get节点吗？"><a href="#2-5-CHM扩容时，可以同时支持读取get节点吗？" class="headerlink" title="2.5 CHM扩容时，可以同时支持读取get节点吗？"></a>2.5 CHM扩容时，可以同时支持读取get节点吗？</h5><p>答案是支持的，CHM扩容完全不影响其他读线程读取key，为何？在“transfer方法桶位节点转移的处理逻辑”小节可以得到答案：</p>
<p>例如桶位的冲突链正在扩容，你发现它扩容时只是复制该链表的节点（ln = lastRun），也就是原链表还在桶位上，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// runBit也即p.hash &amp; n=0，说明是低位节点</span></span><br><span class="line">  <span class="keyword">if</span> (runBit == <span class="number">0</span>) &#123;</span><br><span class="line">    	<span class="comment">// runBit == 0，说明lastRun子链是低位节点构成的，因此理应将lastRun放在低位链ln，此时hn还是设为null，hn在第②步骤会被用到</span></span><br><span class="line">      ln = lastRun;</span><br><span class="line">      hn = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 也即runBit不为0，p.hash &amp; n !=0，说明是高位节点</span></span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">      hn = lastRun;</span><br><span class="line">      ln = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>从红黑树的迁移更容易找到答案：</p>
<p>迁移线程将红黑树的节点复制到一条（lo—-&gt;loTail或者hi—-&gt;hiTail）或者两条新链表上（lo—-&gt;loTail以及hi—-&gt;hiTail），桶位上原链表结构（TreeNode的next属性构成）未发生任何改变，因此若此时有“读线程”来读桶位上key是完全OK的，不受扩容线程影响。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (f <span class="keyword">instanceof</span> TreeBin) &#123;</span><br><span class="line">    TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f;</span><br><span class="line">    <span class="comment">// 用于存放红黑树中低位节点的单向链表</span></span><br><span class="line">    TreeNode&lt;K,V&gt; lo = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">  	<span class="comment">// 用于存放红黑树中高位节点的单向链表</span></span><br><span class="line">    TreeNode&lt;K,V&gt; hi = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">  	<span class="comment">// lc计数器，用于判断低位节点链是否需要树化，hc同理</span></span><br><span class="line">    <span class="keyword">int</span> lc = <span class="number">0</span>, hc = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//③ 将原链节点拷贝到hi/lo链上，这里可以证明：CHM扩容时也支持并发get原链表</span></span><br><span class="line">    <span class="keyword">for</span> (Node&lt;K,V&gt; e = t.first; e != <span class="keyword">null</span>; e = e.next) &#123;</span><br><span class="line">        <span class="keyword">int</span> h = e.hash;</span><br><span class="line">        TreeNode&lt;K,V&gt; p = <span class="keyword">new</span> TreeNode&lt;K,V&gt;</span><br><span class="line">            (h, e.key, e.val, <span class="keyword">null</span>, <span class="keyword">null</span>); <span class="comment">// 用原节点的key和value创建一个新的TreeNode，不就相当于拷贝节点，且原链表结构没有收到影响。</span></span><br><span class="line">      	<span class="comment">// 构建低位节点单向链表，并计数</span></span><br><span class="line">        <span class="keyword">if</span> ((h &amp; n) == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">//如果p.prev和hiTail是空，说明当前p节点是头节点，放在lo位置</span></span><br><span class="line">            <span class="keyword">if</span> ((p.prev = loTail) == <span class="keyword">null</span>)</span><br><span class="line">                lo = p;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">              <span class="comment">// 其他非头节点，按尾插法插入到链表尾部</span></span><br><span class="line">                loTail.next = p;</span><br><span class="line">          <span class="comment">//遍历下一个节点</span></span><br><span class="line">            loTail = p;</span><br><span class="line">          <span class="comment">//链表长度计数</span></span><br><span class="line">            ++lc;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3、helpTransfer方法解析"><a href="#3、helpTransfer方法解析" class="headerlink" title="3、helpTransfer方法解析"></a>3、helpTransfer方法解析</h4><p>有了上面transfer详细的多线程协同扩容细节铺垫后，那么helpTransfer方法则相当好理解：</p>
<p>该方法在CHM的相关“写操作”方法被调用，如：put、remove、replace、clear、compute、computeIfAbsent、merge，实际含义为：</p>
<blockquote>
<p>当一个线程去对CHM进行写操作时（put、remove、replace、clear、compute、computeIfAbsent、merge）”，如果恰好遇到桶位节点是一个ForwardingNode节点，那么这个线程先转而去帮助当前CHM扩容。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**留意源码官方注释</span></span><br><span class="line"><span class="comment"> * Helps transfer if a resize is in progress.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123;</span><br><span class="line">    Node&lt;K,V&gt;[] nextTab; <span class="keyword">int</span> sc;</span><br><span class="line">  	<span class="comment">//① 条件1和条件3都是判断底层table还在扩容中，条件2再次检查当前桶位是否是一个ForwardingNode节点</span></span><br><span class="line">    <span class="keyword">if</span> (tab != <span class="keyword">null</span> &amp;&amp; (f <span class="keyword">instanceof</span> ForwardingNode) &amp;&amp;</span><br><span class="line">        (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">int</span> rs = resizeStamp(tab.length);</span><br><span class="line">      	<span class="comment">//③  while自旋配合cas进制是一套很常用的逻辑。三个条件都是能够判断“只要底层table还在扩容”</span></span><br><span class="line">      	<span class="comment">//条件1：nextTab == nextTable，说明nextTable还未指向table变量，</span></span><br><span class="line">      	<span class="comment">//条件2：table == tab，table变量还是指向旧表tab</span></span><br><span class="line">      	<span class="comment">//条件3：有了前面小节内容的铺垫，这里sizeCtl是负数表示CHM在扩容中</span></span><br><span class="line">        <span class="keyword">while</span> (nextTab == nextTable &amp;&amp; table == tab &amp;&amp;</span><br><span class="line">               (sc = sizeCtl) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">          	<span class="comment">//④ 这4个条件都是判断是否协助扩容，任意一个条件成立都使得线程不需要再去参与扩容，直接break返回，当然前面3个判断条件的理解目前相对困难，可看完后面章节即可立即。</span></span><br><span class="line">            <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                sc == rs + MAX_RESIZERS || transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">          	<span class="comment">//⑤ 每个进入扩容的线程都要对sc进行加1计数，由于采用cas方法，一次cas不成功就继续回到③while重试，直到sc加1成功或者满足④判断条件才不会继续while重试</span></span><br><span class="line">            <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>)) &#123;</span><br><span class="line">                transfer(tab, nextTab);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">//⑥ 只要③的while条件成立，说明底层table已经扩容完成，可直接返回新table引用：nextTab给到外部调用点：tab = helpTransfer(tab, f)，这是tab指向的是扩容后的table</span></span><br><span class="line">        <span class="keyword">return</span> nextTab;</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">//② 如果①有个一个条件不成立，说明已经完成底层数组扩容，那当前线程就不需要“帮助扩容”，直接返回table引用（注意此table引用其实是指向了newTab，也即扩容后的那个table）</span></span><br><span class="line">    <span class="keyword">return</span> table;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4、addCount主分支2为何使用while循环"><a href="#4、addCount主分支2为何使用while循环" class="headerlink" title="4、addCount主分支2为何使用while循环"></a>4、addCount主分支2为何使用while循环</h4><h5 id="4-1-背景"><a href="#4-1-背景" class="headerlink" title="4.1 背景"></a>4.1 背景</h5><p>只要CHM节点数量size达到扩容阈值且数组长度还未到最大值MAXIMUM_CAPACITY就可以进行扩容操作，这一点其实跟HashMap的扩容判断是一致的。</p>
<p>主分支2里面为何使用<code>while</code> 循环判断是否需要扩容呢？ 就不能用<code>if</code>来取代吗？答案：不能，因为CHM是并发扩容，不是单线程的HashMap扩容操作，如果将主分支2的<code>while</code>改为<code>if</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">//addCount的主分支2  </span></span><br><span class="line"><span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">         Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">			<span class="comment">//将while改成if</span></span><br><span class="line">         <span class="keyword">if</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">        			<span class="comment">//扩容逻辑</span></span><br><span class="line">             s = sumCount();</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>考察并发量高两种情形：</p>
<ul>
<li><p>情况1：使得扩容完成的线程能继续加入未完成的扩容任务</p>
<p>在原table还有桶位未分配出去时，也即transferIndex&gt;0，那么某线程A完成自己的桶位区间的节点迁移后，主分支2使用<code>while</code>语义就可以保证线程A继续加入到还未完成的扩容任务，而不是退出，显然线程A得到复用，而不需要创建新的线程，提高cpu效率。若使用<code>if</code>语义，当线程A完成自己的桶位区间的节点迁移后会直接退出，未充分利用已在执行态的线程A。</p>
</li>
<li><p>情况2：上一阶段刚扩容完因并发量高导致节点数量立即达到下一阶段扩容阈值</p>
<p>在CHM容量n0扩容到容量n1，有一扩容线程A完成扩容后退出，并发的其他线程很快把线程A扩容的容量n1的table写满，此时需要继续将容量n1的table扩容到容量n2（但线程A已经退出扩容处理），结果容量n2的新表很快又被并发的其他线程put满，此时急需第3次扩容（但线程A已经退出扩容处理），结果容量n3的新表很快又被并发的其他线程put满，以此类推…</p>
<font color=red>所以你会发现：既然线程第1次扩容的n1很快会被并发的其他线程put满，那么可以这么设计：线程完成扩容transfer逻辑后先不结束线程，而是使用`while`强制安排线程继续判断CHM是否需要下一阶段扩容，如果需要下一阶段扩容，线程立即设置基础值然后进入transfer开始下一阶段扩容，复用已经处在执行态的线程，而不是去创建新线程。</font>

</li>
</ul>
<h5 id="4-2-线程会不会无限次扩容"><a href="#4-2-线程会不会无限次扩容" class="headerlink" title="4.2 线程会不会无限次扩容"></a>4.2 线程会不会无限次扩容</h5><p>按情况2的解释，线程岂不是无限次扩容？这就是次分支1要解决的情况，里面有一个<code>break</code>，显然这就是<code>while</code>循环结束点，所以线程不会无限扩容下去，那么满足什么条件线程才会<code>break</code>，这里就需要第5节的<code>resizeStamp</code> 技术点来解释，请参考之。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//分支1</span></span><br><span class="line"> <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 次分支1</span></span><br><span class="line">     <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">         sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">         transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">         <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<h4 id="5、resizeStamp-n-以及sc变量的解析（CHM最关键的设计之一）"><a href="#5、resizeStamp-n-以及sc变量的解析（CHM最关键的设计之一）" class="headerlink" title="5、resizeStamp(n)以及sc变量的解析（CHM最关键的设计之一）"></a>5、resizeStamp(n)以及sc变量的解析（CHM最关键的设计之一）</h4><h5 id="5-1-rs设计意图？"><a href="#5-1-rs设计意图？" class="headerlink" title="5.1 rs设计意图？"></a>5.1 rs设计意图？</h5><p>resizeStamp方法和sc变量在transfer方法里面给我们造成很大困扰，它们的设计意图不好理解，resizeStamp是CHM的最关键设计之一，因此需要一步一步用位运算去探索和推演Doug Lea为何采用二进制的角度来实现sizeCtl支持多种状态标记以及rs扩容印记的设计，具体如下：</p>
<p>首先，使用场景肯定是用于扩容，不管从其方法命名还是从下面源码官方注释：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* ---------------- Table Initialization and Resizing -------------- */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the stamp bits for resizing a table of size n.</span></span><br><span class="line"><span class="comment"> * Must be negative when shifted left by RESIZE_STAMP_SHIFT.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">resizeStamp</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Integer.numberOfLeadingZeros(n) | (<span class="number">1</span> &lt;&lt; (RESIZE_STAMP_BITS - <span class="number">1</span>));</span><br></pre></td></tr></table></figure>
<p>第二点，可以用resizeStamp的值和sc变量来判断整个CHM扩容是否结束：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">        sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">        transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>第三点，sc变量可以记录进入扩容逻辑的线程数量</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                   transfer(tab, nt);</span><br></pre></td></tr></table></figure>
<p>还可以实现对退出扩容逻辑线程的计数以及找出最后一个完成扩容的线程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">    i = n; <span class="comment">// recheck before commit</span></span><br></pre></td></tr></table></figure>
<p>带着以上思路，现在以table长度n=16需要扩容进行逐步深入分析，以下全部计算过程采用位计算方式，Int类型为32位，分为高16位和低16位，第32位是符号位。</p>
<p>对于resizeStamp(16)有以下计算结果，其中<code>Integer.numberOfLeadingZeros(n)</code> 返回n对应的二进制高位0的个数，例如n=16，其高位0个数为32-5=27，假设现有一个线程A作为首个扩容线程，开始执行AddCount分支2流程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">//AddCount方法的分支2</span></span><br><span class="line"><span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">        <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">               (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">            <span class="comment">// 从这里开始推演相关位运算的逻辑</span></span><br><span class="line">            <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">            <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                    sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                    transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc, sc + <span class="number">1</span>))</span><br><span class="line">                    transfer(tab, nt);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                         (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">                transfer(tab, <span class="keyword">null</span>);</span><br><span class="line">            s = sumCount();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>计算<code>int rs = resizeStamp(n=16);</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000 0000 0000 0000 0000 0000 0001 1011  # 27</span><br><span class="line">0000 0000 0000 0000 1000 0000 0000 0000  # 1&lt;&lt;15</span><br><span class="line"></span><br><span class="line">或运算结果如下,得到容量16的rs：</span><br><span class="line">0000 0000 0000 0000 1000 0000 0001 1011  # 32795</span><br></pre></td></tr></table></figure>
<p>可以看出：rs的低16位记录了“table长度16信息”，其设计意图是？</p>
<p>目的就是为了给table等于16这一容量值打上一个“戳（印记）”，或者称为”table长度为16的扩容印记”，简称“扩容印记”。<br>需要注意的是：下面小节的sc变量的“table长度16的信息”是记录在sc的高16位上，区别于rs的低16位记录“table长度16信息”。</p>
<p>作为对比，假设table容量32需要扩容处理，那么table此时对应的“扩容印记”rs：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000 0000 0000 0000 0000 0000 0001 1010  # 26</span><br><span class="line">0000 0000 0000 0000 1000 0000 0000 0000  # 32768</span><br><span class="line">或运算结果如下,得到容量32的rs：</span><br><span class="line">0000 0000 0000 0000 1000 0000 0001 1010 # 32794</span><br></pre></td></tr></table></figure>
<p>通过对比容量16和容量32的table扩容印记可知，不同容量下的扩容阶段都有其唯一“扩容印记”rs，那么只要线程在扩容时发现：前后读取的rs值不同，说明整个CHM已经完成上一阶段的扩容且正在进行下一阶段扩容，那么线程就可以直接退出，无需进入transfer方法，对应下面的代码片段的条件1用法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (check &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab, nt; <span class="keyword">int</span> n, sc;</span><br><span class="line">    <span class="keyword">while</span> (s &gt;= (<span class="keyword">long</span>)(sc = sizeCtl) &amp;&amp; (tab = table) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">           (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;</span><br><span class="line">        <span class="comment">// 从这里开始推演相关位运算的逻辑</span></span><br><span class="line">        <span class="keyword">int</span> rs = resizeStamp(n);</span><br><span class="line">        <span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">/*后面时刻读取的rs：sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT</span></span><br><span class="line"><span class="comment">          前面时刻读取的rs: rs</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">         <span class="comment">//后面时刻读取的rs与前面时刻读取的rs不等,说明整个CHM已经完成了扩容，那么线程就可以直接退出，无需进入transfer方法	// 条件1</span></span><br><span class="line">            <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || 其他<span class="number">4</span>个条件)</span><br><span class="line">                <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>因此<code>(sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs</code> 是表达这种场景：线程进入CHM扩容任务发现CHM已经结束了本阶段的扩容，因此线程无需参与到扩容任务中，退出即可。</p>
<h5 id="5-2-sc相关位运算的设计意图？"><a href="#5-2-sc相关位运算的设计意图？" class="headerlink" title="5.2 sc相关位运算的设计意图？"></a>5.2 sc相关位运算的设计意图？</h5><p>条件1：<code>sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs</code> 已经由上面的小节详细分析，此小节尝试回答条件2：<code>sc == rs + 1</code> 和条件3：<code>sc == rs + MAX_RESIZERS</code>   设计目的？</p>
<p>不妨做个猜测：通过对比条件2和条件3发现，条件2像是sc变量的最小值，条件3像是sc变量允许的最大值，从最大值的MAX_RESIZERS含义可推出：扩容线程数量最大值，说明条件3可用于限制同时参与扩容线程的最大数量。根据条件3设计了扩容线程的上限，我们可以大胆推理出：条件2指的是当前CHM已经没有扩容线程在扩容了，也即所有扩容线程已经退出扩容任务，此时再来新的线程即可break</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;		</span><br><span class="line">  			<span class="comment">// 条件2：sc == rs + 1</span></span><br><span class="line">  			<span class="comment">// 条件3：sc == rs + MAX_RESIZERS</span></span><br><span class="line">       <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">           sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">           transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">           <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>将上面基本猜测进行以下位计算的原理论证：</p>
<p>在第1节addCount给出的重要设计点“第一个执行扩容的线程”，它进入transfer方法前，将sc设为一个值：sc=(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，下面看看这个二进制串隐藏了什么“隐秘信息”，以容量16的戳印记rs为例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000 0000 0000 0000 1000 0000 0001 1011 #  </span><br><span class="line">右移RESIZE_STAMP_SHIFT&#x3D;16位后如下：</span><br><span class="line">1000 0000 0001 1011 0000 0000 0000 0000 # 第32位是符号位，1表示负值</span><br></pre></td></tr></table></figure>
<p>先看高16的含义：对比原来rs二进制串可知，“容量16的扩容印记”放在sc的高16位上。</p>
<p>再看低16位的含义：</p>
<p>第1个进入扩容的线程执行：sc=(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，对应二进制串：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0010 # 低16位+2</span><br></pre></td></tr></table></figure>
<p>第2个进入扩容transfer方法前，线程都会对sc加1操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0011 # 低16位+3</span><br></pre></td></tr></table></figure>
<p>第3个进入扩容transfer方法前，线程都会对sc加1操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0100 # 低16位+4</span><br></pre></td></tr></table></figure>
<p>易归纳：<br>当n个线程加1时使得sc的低16位达到以下值时，参与扩容线程总数达到最大值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 1111 1111 1111 1111 # 低16位&#x3D;65535</span><br></pre></td></tr></table></figure>
<p>此时，低16位的值65535不就是Doug Lea设计的最大扩容线程数量：<code>MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1</code>的值吗</p>
<p>以上流程可以得出这样一个规则：同一阶段扩容期，sc的高16位保持不变，对于sc低16位，除去第1个线程，后续每进一个扩容线程，低16位加1计数</p>
<p>综上，可以得出这样的结论：</p>
<p>① sc高16位是存放当前table容量的印记（扩容戳）信息，目的：只要是同一容量下CHM扩容，那么高16位的值固定不变</p>
<p>② sc低16位用于对参与扩容线程的计数。计数范围为： 0000 0000 0000 0010到1111 1111 1111 1111</p>
<p>在CHM还是扩容状态时，由高16位扩容印记的第32位1符号位可知，不管低16位现在记录了多少个扩容下线程，sc一定是负数，</p>
<p>因此在源码设计才有以下<code>if (sc &lt; 0)</code>  成立就能判断出CHM还在扩容状态中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">            <span class="keyword">int</span> rs = resizeStamp(n); <span class="comment">// rs是正值：0000 0000 0000 0000 1000 0000 0001 1011 </span></span><br><span class="line">            		<span class="comment">//①</span></span><br><span class="line"><span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            		<span class="comment">//② 这里也有rs，是正值还是负值呢？</span></span><br><span class="line">                <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                    sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                    transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">              	<span class="comment">//③</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                         (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h5 id="5-3-解析sc-rs-1的真实含义（这是官方bug）"><a href="#5-3-解析sc-rs-1的真实含义（这是官方bug）" class="headerlink" title="5.3 解析sc == rs + 1的真实含义（这是官方bug）"></a>5.3 解析<code>sc == rs + 1</code>的真实含义（这是官方bug）</h5><p>这个条件很诡异，首先线程进入<code>if (sc &lt; 0)</code> 分支，说明sc是负数，在此之前<code>rs = resizeStamp(n)</code>扩容印记的计算结果是正值，那么<code>sc == rs+1</code> 以及<code>sc == rs + MAX_RESIZERS</code> （负数==正数）永远都不会成立，这个正是open jdk1.8的CHM出现一个重点bug。</p>
<p>本人一开始对此条件成立情况很是费解，但因为一直认为Doug Lea以及JSR-166 expert group在JUC造诣很深，因此还不曾怀疑此处设计有逻辑问题，接着出于要对resizeStamp方法和sc移位计算的设计彻底掌握的要求，去查阅了相关文章，恰好找到一篇高质量文章，它提到了这其实一个官方的bug：<a href="https://mp.weixin.qq.com/s/6HWJDKnNaceIEZQMOLZkog">文章连接</a>，到此本人才真正对<code>sc == rs + 1</code>诡异设计得到最官方的解决方案！！ <a href="https://bugs.openjdk.java.net/browse/JDK-8214427">这是官方bug描述的链接</a>，bug已经java 12版本得到修复。本人在另外一篇文章中也详细给出了官方对这个bug的处理过程。</p>
<font color=red>（ [这篇关于CHM源码研究的文章](https://mp.weixin.qq.com/s/6HWJDKnNaceIEZQMOLZkog)，作者水平确实不错，解析也足够细腻，但本人还是建议那些想要彻底掌握或者提升高级技术的人，切勿抄袭别人文章，务必独立完成相关源码的核心设计分析才能真正提升个人能力）</font>

<p>在去查阅了resizeStamp讨论的相关文章的过程中，你发下绝大部分的文章给出有效讨论真的很少，而且你会发现以下现象：</p>
<font color=red>当前CSDN大量文章、微信公众号文章、以及所谓的JAVA进阶培训班绝大部分是基于针对jdk1.8CHM的源码解析，而且他们也不能解释`sc == rs + 1`的上下文计算过程及其设计目的，他们无法发现或者感知如此隐秘而有意义的bug，所以他们一般都会停留在表面类似这样源码解释：当前已经没有扩容线程在扩容或者参与扩容线程总数量达到最大值” </font>

<p>如果你已经吃透本文的所有分析，那么你应该可以猜出Doug Lea写这两行代码的含义如下：</p>
<p><code>sc=rs+1</code> 其实是想表达<code>sc=(resizeStamp(n)&lt;&lt;RESIZE_STAMP_SHIFT)+1</code>，</p>
<p>而<code>sc=rs+MAX_RESIZERS</code> 其实是想表达<code>sc=(resizeStamp(n)&lt;&lt;RESIZE_STAMP_SHIFT)+MAX_RESIZERS</code></p>
<p>用代码来表示其真正的写法如下所示</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> rs = resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT;</span><br><span class="line"><span class="keyword">if</span>(sc&lt;<span class="number">0</span>)</span><br><span class="line"> 		<span class="keyword">if</span>(条件<span class="number">1</span>||sc == rs + <span class="number">1</span> || sc == rs + MAX_RESIZERS)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>所以rs其实应该是这样的二进制串，显然是一个负数，这样才能使得<code>sc == rs + 1</code>在某个时刻成立</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0000</span><br></pre></td></tr></table></figure>
<p>此外，因为rs已经是负值，那么在transfer有个地方不能直接用一开始计算的rs值，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">  	<span class="comment">/* 大家第一次代码解析到这里时：应该会无法理解，为何不用rs，而是要用resizeStamp(n) 重新计算rs：</span></span><br><span class="line"><span class="comment">  ((sc - 2) != rs &lt;&lt; RESIZE_STAMP_SHIFT)</span></span><br><span class="line"><span class="comment">  	*/</span></span><br><span class="line">    <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">        <span class="keyword">return</span>;</span><br></pre></td></tr></table></figure>
<p>因为transfer方法执行过程中，表示扩容状态下，rs的值已经是<code>rs=resizeStamp(16) &lt;&lt; RESIZE_STAMP_SHIFT</code>  ，也即rs位于sc的高16位上了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0000</span><br></pre></td></tr></table></figure>
<p>如果使用<code>((sc - 2) != rs &lt;&lt; RESIZE_STAMP_SHIFT)</code>  来判断，rs右移16位后，就变成0，此时sc没有实际含义：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0000 0000 0000 0000 0000 0000 0000 0000</span><br></pre></td></tr></table></figure>
<p>因此这里不能用原来的rs值来判断条件，而是使用<code>resizeStamp(n)</code>重新计算rs值得出sc值是否等于原始基数，以此判断当前线程是否是最后一个退出扩容的线程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">等价于</span><br><span class="line"><span class="keyword">if</span> (sc != （resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT） +<span class="number">2</span> ) <span class="comment">//</span></span><br></pre></td></tr></table></figure>
<p>基于以上详实的分析，才能真正从背后移位设计的原理去理解<code>sc==rs+1</code>含义：表示当前CHM已经没有扩容线程在扩容了，也即所有扩容线程已经退出扩容任务，此时再来新的线程即可break。</p>
<h5 id="5-4-为何用sc-rs-1来判断所有扩容线程已经退出而不是用sc-rs-0或者sc-rs-2呢？"><a href="#5-4-为何用sc-rs-1来判断所有扩容线程已经退出而不是用sc-rs-0或者sc-rs-2呢？" class="headerlink" title="5.4 为何用sc=rs+1来判断所有扩容线程已经退出而不是用sc=rs+0或者sc=rs+2呢？"></a>5.4 为何用sc=rs+1来判断所有扩容线程已经退出而不是用sc=rs+0或者sc=rs+2呢？</h5><p>假设当前有1个线程是最后一个完成扩容的线程，它在transfer方法中准备执行以下逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ① 假设当前线程是最后一个扩容线程并持有sc值为rs+2，然后通过cas将主存上sizeCtl设置为sc-1，也即此时主存上的sizeCtl=（rs+2）-1=rs+1。</span></span><br><span class="line"><span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc = sizeCtl, sc - <span class="number">1</span>)) &#123;</span><br><span class="line">                <span class="comment">// ② 注意下面的sc不是主存的上值，而是线程在①持有的sc=rs+2的值，因此仅当sc-2=rs，其中rs= resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT 那么这个线程就是要找的最后一个扩容线程。</span></span><br><span class="line">                <span class="keyword">if</span> ((sc - <span class="number">2</span>) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT)</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                finishing = advance = <span class="keyword">true</span>;</span><br><span class="line">                i = n; <span class="comment">// recheck before commit</span></span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<p>以上逻辑等价于<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//当前线程</span></span><br><span class="line"><span class="keyword">int</span> sc=getAndAddInt(<span class="keyword">this</span>,sizeCtl,-<span class="number">1</span>) <span class="comment">//先取出sizeCtl的值：rs+2，然后再将cas设置sizeCtl在主存的值为rs+1</span></span><br><span class="line"><span class="comment">//其他线程：</span></span><br><span class="line"><span class="comment">//当其他线程进入transfer方法时，读取的sc就是rs+1这个值，满足扩容完成的退出条件`sc==rs+1`</span></span><br></pre></td></tr></table></figure></p>
<p>因此如果当前线程最后一个完成扩容的线程，那么它执行完以上逻辑后，主存中sizeCtl的值为rs+1，因此之后的线程读取sc的值并来到以下逻辑就会发现② 位置的<code>sc == rs + 1</code>成立，说明<font color=red>“当前CHM所有扩容线程已经退出，本阶段的CHM扩容已完成，因此可直接break退出transfer”</font><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">            <span class="keyword">int</span> rs = resizeStamp(n); <span class="comment">// rs是正值：0000 0000 0000 0000 1000 0000 0001 1011 </span></span><br><span class="line">            		</span><br><span class="line"><span class="keyword">if</span> (sc &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            		<span class="comment">//② 这里也有rs，是正值还是负值呢？</span></span><br><span class="line">                <span class="keyword">if</span> ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + <span class="number">1</span> ||</span><br><span class="line">                    sc == rs + MAX_RESIZERS || (nt = nextTable) == <span class="keyword">null</span> ||</span><br><span class="line">                    transferIndex &lt;= <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                                         (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<h5 id="5-5-首次扩容时sc的基础值为何不是加1而是加2？"><a href="#5-5-首次扩容时sc的基础值为何不是加1而是加2？" class="headerlink" title="5.5 首次扩容时sc的基础值为何不是加1而是加2？"></a>5.5 首次扩容时sc的基础值为何不是加1而是加2？</h5><p>刚开始解析addCount源码时，让人无法理解的为何首个扩容线程进入扩容逻辑时是加2开始而不是加1开始计算呢，有点反正常思维。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (U.compareAndSwapInt(<span class="keyword">this</span>, SIZECTL, sc,</span><br><span class="line">                             (rs &lt;&lt; RESIZE_STAMP_SHIFT) + <span class="number">2</span>))</span><br><span class="line">    transfer(tab, <span class="keyword">null</span>);</span><br></pre></td></tr></table></figure>
<p>考察<code>sizeCtl=-1</code> 这个特殊的-1，换成有符号的二进制串：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0000 0000 0000 0000 0000 0001 &#x2F;&#x2F; ①低16位的最低位为1</span><br></pre></td></tr></table></figure>
<p>对比容量16且从加1开始线程计数：</p>
<p>第1个扩容线程进入transfer时：sc=rs &lt;&lt; RESIZE_STAMP_SHIFT) + 1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0001 &#x2F;&#x2F; ②低16位的最低位为1</span><br></pre></td></tr></table></figure>
<p>第2个扩容线程进入transfer时：sc+1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0002</span><br></pre></td></tr></table></figure>
<p>对比①<code>sizeCtl=-1</code>和②的二进制串可知，两者低16位的最低位都是1，状态标记冲突了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xxxx xxxx xxxx xxxx 0000 0000 0000 0001 </span><br></pre></td></tr></table></figure>
<p>考虑到：sc是用来扩容状态标记、sizeCtl也是用在initTable对sizeClt cas置-1的初始化标记，如果两者都取-1，就不好判断当前CHM的状态，因此Doug Lea为避免这种状态位的冲突，因此将扩容线程计数从加2开始计数。</p>
<p>虽然本文给出这样相对”牵强“的解释，但无法理解为何Doug Lea不在源码中给大家注释清楚？ </p>
<h4 id="6、sizeCtl状态小结"><a href="#6、sizeCtl状态小结" class="headerlink" title="6、sizeCtl状态小结"></a>6、sizeCtl状态小结</h4><p>有了以上第4节和第5节深入的细节分析，现在可以总结sizeCtl也即sc取不同值的实际含义</p>
<p><strong>sizeCtl为正值部分好理解：</strong></p>
<p>① sizeCtl=0，默认值，在反序列化CHM对象时，如果CHM的size为0，那么此时sizeCtl设为0</p>
<p>② 当new CHM阶段，sizeCtl=table的初始默认容量或者指定的容量（会被修正为2的整数幂）</p>
<p>③ 当CHM扩容结束后，sizeCtl=下一次扩容阈值</p>
<p><strong>对于负值部分的理解需要结合resizeStamp章节内容：</strong></p>
<p>以table容量等于16作为分析，sizeCtl主要是以下四种特殊情况的负值：</p>
<p>①  sizeCtl=-1。在new CHM后，第一次put入key，table为null，需要使用tab = initTable()方法初始化，多线程环境下，线程使用cas将 sizeCtl设为-1加锁做初始化工作。</p>
<p>② 第一个进入扩容线程将sizeCtl设为一个基础值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0010 &#x2F;&#x2F; 最高位1，所以此值为负值</span><br></pre></td></tr></table></figure>
<p>③ 达到最大扩容线程数量sizeCtl的值为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 1111 1111 1111 1111 &#x2F;&#x2F; 最高位1，所以此值为负值</span><br></pre></td></tr></table></figure>
<p>也即当CHM处于并发线程扩容时，sizeCtl的取值范围为：<code>[rs+2,rs+65535]</code> </p>
<p>④ 所有扩容线程退出扩容时，对应sc=rs+1：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1000 0000 0001 1011 0000 0000 0000 0001  &#x2F;&#x2F; 最高位1，所以此值为负值</span><br></pre></td></tr></table></figure>
<p>以上sizeCtl用于管理多线程并发操作与状态标记的背后原理。</p>
<h4 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h4><p>本文研究的重点放在CHM扩容设计逻辑上，不得不说，其并发源码写的确实高级，尤其是关于sizeCtl移位计数的逻辑设计让人佩服，也值得在项目尝试应用相关并发控制思想！<br>下一篇文章将重点讨论：jdk1.8 ConcurrentHashMap的TreeBin读写锁竞争机制</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title>Java高级主题：基于AQS驱动的ReentrantLock公平锁和非公平锁实现原理解析</title>
    <url>/2021/04/25/%E5%9F%BA%E4%BA%8EAQS%E9%A9%B1%E5%8A%A8%E7%9A%84ReentrantLock%E5%85%AC%E5%B9%B3%E9%94%81%E5%92%8C%E9%9D%9E%E5%85%AC%E5%B9%B3%E9%94%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>本文是入门和理解AQS框架的重要文章，尽管AQS还有共享模式以及条件Condition等设计，但重入锁仍然是最适合理解AQS底层数据结构及其算法设计的切入点。</p>
<h4 id="单线程使用可重入锁的内部简单工作机制"><a href="#单线程使用可重入锁的内部简单工作机制" class="headerlink" title="单线程使用可重入锁的内部简单工作机制"></a>单线程使用可重入锁的内部简单工作机制</h4><p>分别在以下两个断点位置进行debug，断点条件i==5，并且在variables窗口watch一个特殊的变量<code>state</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.locks.ReentrantLock;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReentrantLockDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ReentrantLock lock=<span class="keyword">new</span> ReentrantLock();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">5</span>; i++) &#123;</span><br><span class="line">            lock.lock(); <span class="comment">// 断点位置</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">5</span>; i++) &#123;</span><br><span class="line">            lock.unlock();<span class="comment">// 断点位置</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到<code>lock.lock()</code>的内部执行流程如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1、java.util.concurrent.locks.ReentrantLock$NonfairSync@29444d75[State = 4, empty queue]</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;                  </span><br><span class="line">          sync.lock();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、java.util.concurrent.locks.ReentrantLock</span></span><br><span class="line">			<span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment">// 重点：使用cas更新state的值</span></span><br><span class="line">                setExclusiveOwnerThread(Thread.currentThread());</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                acquire(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3、java.util.concurrent.locks.AbstractQueuedSynchronizer</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">compareAndSetState</span><span class="params">(<span class="keyword">int</span> expect, <span class="keyword">int</span> update)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// See below for intrinsics setup to support this</span></span><br><span class="line">        <span class="keyword">return</span> unsafe.compareAndSwapInt(<span class="keyword">this</span>, stateOffset, expect, update);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、java.util.concurrent.locks.AbstractQueuedSynchronizer</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!tryAcquire(arg) &amp;&amp; <span class="comment">// 调用下面5在ReentrantLock定义的tryAcquire方法</span></span><br><span class="line">            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class="line">            selfInterrupt();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5、java.util.concurrent.locks.ReentrantLock</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> nonfairTryAcquire(acquires);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6、java.util.concurrent.locks.ReentrantLock</span></span><br><span class="line">        <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">nonfairTryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">final</span> Thread current = Thread.currentThread();</span><br><span class="line">            <span class="keyword">int</span> c = getState();<span class="comment">// 重点：获取state变量当前值</span></span><br><span class="line">            <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) &#123;</span><br><span class="line">                    setExclusiveOwnerThread(current);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class="line">                <span class="keyword">int</span> nextc = c + acquires;</span><br><span class="line">                <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span></span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">                setState(nextc); <span class="comment">// 重点：更新state变量值</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>经过这么一轮debug，可以观察到原来<code>lock.lock()</code> 通过cas更改state变量值来实现“可重入性”，例如，这里for循环5次使得该线程在同一锁对象上加锁了5次，可以看到对应的state累加计数等于5，基于此可以推出<code>lock.unlock()</code>操作则是每次<code>unlock()</code>就是对state进行cas减1操作，如下：</p>
<a id="more"></a>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//1、java.util.concurrent.locks.ReentrantLock</span></span><br><span class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        sync.release(<span class="number">1</span>); </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、java.util.concurrent.locks.AbstractQueuedSynchronizer</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">release</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (tryRelease(arg)) &#123; </span><br><span class="line">            Node h = head;</span><br><span class="line">            <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h.waitStatus != <span class="number">0</span>)</span><br><span class="line">                unparkSuccessor(h);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、java.util.concurrent.locks.ReentrantLock</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> c = getState() - releases;  <span class="comment">// 当i=5次循环时，这里getState=1,releases=1</span></span><br><span class="line">            <span class="keyword">if</span> (Thread.currentThread() != getExclusiveOwnerThread())</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();</span><br><span class="line">            <span class="keyword">boolean</span> free = <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="number">0</span>) &#123; <span class="comment">// 所有的重入都退出后，此刻不再有线程持有独占锁。</span></span><br><span class="line">                free = <span class="keyword">true</span>;</span><br><span class="line">                setExclusiveOwnerThread(<span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            setState(c); <span class="comment">// 当i=5次循环时，state被置为0</span></span><br><span class="line">            <span class="keyword">return</span> free;  </span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>其实这个state变量是一个非常重要的“同步状态”，记录了当前持有独占锁的线程的重入加锁次数，它在AbstractQueuedSynchronizer内部定义：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * The synchronization state.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> state;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从<code>ReentrantLock</code>以上两个流程来看，重点需要解析<code>sync.lock()</code>、<code>sync.release(1)</code>，而这里sync实例是来自<code>ReentrantLock</code>内部定义<code>Sync</code>类，该类继承至<code>AbstractQueuedSynchronizer</code>，因此如果要真正理解<code>ReentrantLock</code>的可重入性，则需要深入底层的`AbstractQueuedSynchronizer，这是JUC众多锁工具的底层实现。</p>
<h4 id="从Sync类了解AQS"><a href="#从Sync类了解AQS" class="headerlink" title="从Sync类了解AQS"></a>从Sync类了解AQS</h4><p>在真正解析AQS之前，可以先看看在ReentrantLock内部定义的Sync，以下按<code>ReentrantLock</code>默认构造器进入分析流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    sync = <span class="keyword">new</span> NonfairSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>NonfairSync</code>是实现非公平锁的主要逻辑</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sync object for non-fair locks</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NonfairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">7316153563782823691L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     # 这里说，当前线程当前直接无需排队去争抢锁资源，也即不是先到先到，所以是才称为非公平模式。抢不到才去阻塞队列里面排队</span></span><br><span class="line"><span class="comment">     * Performs lock.  Try immediate barge, backing up to normal</span></span><br><span class="line"><span class="comment">     * acquire on failure.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">            setExclusiveOwnerThread(Thread.currentThread());</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            acquire(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> nonfairTryAcquire(acquires);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Sync类定义了获取锁的分配逻辑，这里涉及到对同步状态state更改、同一线程的锁重入、锁重入释放</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	</span></span><br><span class="line"><span class="comment"> * Base of synchronization control for this lock. Subclassed</span></span><br><span class="line"><span class="comment"> * into fair and nonfair versions below. Uses AQS state to</span></span><br><span class="line"><span class="comment"> * represent the number of holds on the lock.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">			</span><br><span class="line"><span class="keyword">abstract</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Sync</span> <span class="keyword">extends</span> <span class="title">AbstractQueuedSynchronizer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">5179523762034025860L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Performs &#123;<span class="doctag">@link</span> Lock#lock&#125;. The main reason for subclassing</span></span><br><span class="line"><span class="comment">     * is to allow fast path for nonfair version.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Performs non-fair tryLock.  tryAcquire is implemented in</span></span><br><span class="line"><span class="comment">     * subclasses, but both need nonfair try for trylock method.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">nonfairTryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Thread current = Thread.currentThread();</span><br><span class="line">        <span class="keyword">int</span> c = getState();</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) &#123;</span><br><span class="line">                setExclusiveOwnerThread(current);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextc = c + acquires;</span><br><span class="line">            <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">            setState(nextc);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;      </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> c = getState() - releases;</span><br><span class="line">        <span class="keyword">if</span> (Thread.currentThread() != getExclusiveOwnerThread())</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();</span><br><span class="line">        <span class="keyword">boolean</span> free = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">            free = <span class="keyword">true</span>;</span><br><span class="line">            setExclusiveOwnerThread(<span class="keyword">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        setState(c);</span><br><span class="line">        <span class="keyword">return</span> free;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">isHeldExclusively</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// While we must in general read state before owner,</span></span><br><span class="line">        <span class="comment">// we don&#x27;t need to do so to check if current thread is owner</span></span><br><span class="line">        <span class="keyword">return</span> getExclusiveOwnerThread() == Thread.currentThread();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">final</span> ConditionObject <span class="title">newCondition</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ConditionObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Methods relayed from outer class</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">final</span> Thread <span class="title">getOwner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getState() == <span class="number">0</span> ? <span class="keyword">null</span> : getExclusiveOwnerThread();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">int</span> <span class="title">getHoldCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> isHeldExclusively() ? getState() : <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">isLocked</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getState() != <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Reconstitutes the instance from a stream (that is, deserializes it).</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">readObject</span><span class="params">(java.io.ObjectInputStream s)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> java.io.IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">        s.defaultReadObject();</span><br><span class="line">        setState(<span class="number">0</span>); <span class="comment">// reset to unlocked state</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="1、AQS工作原理"><a href="#1、AQS工作原理" class="headerlink" title="1、AQS工作原理"></a>1、AQS工作原理</h4><p>源码给出的官方设计思路：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Even though this class is based on an internal FIFO queue, it does not automatically enforce FIFO acquisition policies. The core of exclusive synchronization takes the form:</span><br><span class="line">   Acquire:</span><br><span class="line">       while (!tryAcquire(arg)) &#123;</span><br><span class="line">          enqueue thread if it is not already queued;</span><br><span class="line">          possibly block current thread;</span><br><span class="line">       &#125;</span><br><span class="line">  </span><br><span class="line">   Release:</span><br><span class="line">       if (tryRelease(arg))</span><br><span class="line">          unblock the first queued thread;</span><br><span class="line">   </span><br><span class="line">(Shared mode is similar but may involve cascading signals.)</span><br></pre></td></tr></table></figure>
<p>等待队列（又称阻塞队列、阻塞链表、）内部工作原理：我们建议此工作原理在深度掌握AQS设计之前预读几遍，在深度掌握AQS源代码设计之后再回头理解它，你会发现能完全掌握AQS的整体设计。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Wait queue node class.</span><br><span class="line">		The wait queue is a variant of a &quot;CLH&quot; (Craig, Landin, and Hagersten) lock queue. CLH locks are normally used for spinlocks. We instead use them for blocking synchronizers, but use the same basic tactic of holding some of the control information about a thread in the predecessor of its node. A &quot;status&quot; field in each node keeps track of whether a thread should block. A node is signalled when its predecessor releases. Each node of the queue otherwise serves as a specific-notification-style monitor holding a single waiting thread. The status field does NOT control whether threads are granted locks etc though. A thread may try to acquire if it is first in the queue. But being first does not guarantee success; it only gives the right to contend. So the currently released contender thread may need to rewait.</span><br><span class="line">		To enqueue into a CLH lock, you atomically splice it in as new tail. To dequeue, you just set the head field.</span><br><span class="line">            +------+  prev +-----+       +-----+</span><br><span class="line">       head |      | &lt;---- |     | &lt;---- |     |  tail</span><br><span class="line">            +------+       +-----+       +-----+</span><br><span class="line">       </span><br><span class="line">		Insertion into a CLH queue requires only a single atomic operation on &quot;tail&quot;, so there is a simple atomic point of demarcation from unqueued to queued. Similarly, dequeuing involves only updating the &quot;head&quot;. However, it takes a bit more work for nodes to determine who their successors are, in part to deal with possible cancellation due to timeouts and interrupts.</span><br><span class="line">		The &quot;prev&quot; links (not used in original CLH locks), are mainly needed to handle cancellation. If a node is cancelled, its successor is (normally) relinked to a non-cancelled predecessor. For explanation of similar mechanics in the case of spin locks, see the papers by Scott and Scherer at http:&#x2F;&#x2F;www.cs.rochester.edu&#x2F;u&#x2F;scott&#x2F;synchronization&#x2F;</span><br><span class="line">		We also use &quot;next&quot; links to implement blocking mechanics. The thread id for each node is kept in its own node, so a predecessor signals the next node to wake up by traversing next link to determine which thread it is. Determination of successor must avoid races with newly queued nodes to set the &quot;next&quot; fields of their predecessors. This is solved when necessary by checking backwards from the atomically updated &quot;tail&quot; when a node&#39;s successor appears to be null. (Or, said differently, the next-links are an optimization so that we don&#39;t usually need a backward scan.)</span><br><span class="line">		Cancellation introduces some conservatism to the basic algorithms. Since we must poll for cancellation of other nodes, we can miss noticing whether a cancelled node is ahead or behind us. This is dealt with by always unparking successors upon cancellation, allowing them to stabilize on a new predecessor, unless we can identify an uncancelled predecessor who will carry this responsibility.</span><br><span class="line">		CLH queues need a dummy header node to get started. But we don&#39;t create them on construction, because it would be wasted effort if there is never contention. Instead, the node is constructed and head and tail pointers are set upon first contention.</span><br><span class="line">Wait queue node class.</span><br><span class="line"></span><br><span class="line">		Threads waiting on Conditions use the same nodes, but use an additional link. Conditions only need to link nodes in simple (non-concurrent) linked queues because they are only accessed when exclusively held. Upon await, a node is inserted into a condition queue. Upon signal, the node is transferred to the main queue. A special value of status field is used to mark which queue a node is on.</span><br><span class="line">		Thanks go to Dave Dice, Mark Moir, Victor Luchangco, Bill Scherer and Michael Scott, along with members of JSR-166 expert group, for helpful ideas, discussions, and critiques on the design of this class.</span><br></pre></td></tr></table></figure>
<p>这里当然最权威的方式是解析AQS源代码的开发注释</p>
<blockquote>
<p>Provides a framework for implementing blocking locks and related synchronizers (semaphores, events, etc) that rely on first-in-first-out (FIFO) wait queues. This class is designed to be a useful basis for most kinds of synchronizers that rely on a single atomic int value to represent state. Subclasses must define the protected methods that change this state, and which define what that state means in terms of this object being acquired or released. Given these, the other methods in this class carry out all queuing and blocking mechanics. Subclasses can maintain other state fields, but only the atomically updated int value manipulated using methods getState, setState and compareAndSetState is tracked with respect to synchronization.</p>
<p>AQS为众多同步器（例如semaphores，events，reentrantLock）提供了一个实现框架，该框架的底层是基于变体的FIFO等待队列（注意：此等待队列又称为阻塞队列）实现的，大部分同步器只需通过原子更改一个变量名为state值的方式即可完成相关同步状态控制，AQS的子类（也即自行设计的同步器）必须重新定义相关的protected方法——以更新state状态变量，从而实现state值得变化能够表征获得锁或者释放锁，AQS的其他所有方法则实现了排队和阻塞机制。AQS的子类可以维护其他状态字段，但只有子类使用getState、setState和compareAndSetState方法去原子更新这个state值才能追踪到同步状态的变化。</p>
</blockquote>
<p>上面所说的FIFO等待队列其实就是CLH队列（Craig, Landin, Hagersten这三个人发明的数据结构，因此用他们名字命名），它在AQS内部使用双向链表队列+CAS原子锁实现，功能如下：</p>
<ul>
<li><p>FIFO的设计，保证线程在阻塞队列中的公平性，也即先进去阻塞队列等待抢锁资源的线程，也将是最先被唤醒出队</p>
</li>
<li><p>未成功拿到独占锁的线程们将通过自旋和CAS插入到队尾，显然是非阻塞设计，在短时间内能够实现无锁并发插入这些线程。</p>
</li>
</ul>
<h4 id="2、从构造方法解析"><a href="#2、从构造方法解析" class="headerlink" title="2、从构造方法解析"></a>2、从构造方法解析</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractQueuedSynchronizer</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">AbstractOwnableSynchronizer</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">7373984972572414691L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a new &#123;<span class="doctag">@code</span> AbstractQueuedSynchronizer&#125; instance</span></span><br><span class="line"><span class="comment">     * with initial synchronization state of zero.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">AbstractQueuedSynchronizer</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Wait queue node class.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     # 这里就解释了CLH阻塞队列的设计细节</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;The wait queue is a variant of a &quot;CLH&quot; (Craig, Landin, and</span></span><br><span class="line"><span class="comment">     * Hagersten) lock queue. CLH locks are normally used for</span></span><br><span class="line"><span class="comment">     * spinlocks.  We instead use them for blocking synchronizers, but</span></span><br><span class="line"><span class="comment">     * use the same basic tactic of holding some of the control</span></span><br><span class="line"><span class="comment">     * information about a thread in the predecessor of its node.  A</span></span><br><span class="line"><span class="comment">     * &quot;status&quot; field in each node keeps track of whether a thread</span></span><br><span class="line"><span class="comment">     * should block.  A node is signalled when its predecessor</span></span><br><span class="line"><span class="comment">     * releases.  Each node of the queue otherwise serves as a</span></span><br><span class="line"><span class="comment">     * specific-notification-style monitor holding a single waiting</span></span><br><span class="line"><span class="comment">     * thread. The status field does NOT control whether threads are</span></span><br><span class="line"><span class="comment">     * granted locks etc though.  A thread may try to acquire if it is</span></span><br><span class="line"><span class="comment">     * first in the queue. But being first does not guarantee success;</span></span><br><span class="line"><span class="comment">     * it only gives the right to contend.  So the currently released</span></span><br><span class="line"><span class="comment">     * contender thread may need to rewait.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;To enqueue into a CLH lock, you atomically splice it in as new</span></span><br><span class="line"><span class="comment">     * tail. To dequeue, you just set the head field.</span></span><br><span class="line"><span class="comment">     * &lt;pre&gt;</span></span><br><span class="line"><span class="comment">     *      +------+  prev +-----+       +-----+</span></span><br><span class="line"><span class="comment">     * head |      | &lt;---- |     | &lt;---- |     |  tail</span></span><br><span class="line"><span class="comment">     *      +------+       +-----+       +-----+</span></span><br><span class="line"><span class="comment">     * &lt;/pre&gt;</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Insertion into a CLH queue requires only a single atomic</span></span><br><span class="line"><span class="comment">     * operation on &quot;tail&quot;, so there is a simple atomic point of</span></span><br><span class="line"><span class="comment">     * demarcation from unqueued to queued. Similarly, dequeuing</span></span><br><span class="line"><span class="comment">     * involves only updating the &quot;head&quot;. However, it takes a bit</span></span><br><span class="line"><span class="comment">     * more work for nodes to determine who their successors are,</span></span><br><span class="line"><span class="comment">     * in part to deal with possible cancellation due to timeouts</span></span><br><span class="line"><span class="comment">     * and interrupts.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;The &quot;prev&quot; links (not used in original CLH locks), are mainly</span></span><br><span class="line"><span class="comment">     * needed to handle cancellation. If a node is cancelled, its</span></span><br><span class="line"><span class="comment">     * successor is (normally) relinked to a non-cancelled</span></span><br><span class="line"><span class="comment">     * predecessor. For explanation of similar mechanics in the case</span></span><br><span class="line"><span class="comment">     * of spin locks, see the papers by Scott and Scherer at</span></span><br><span class="line"><span class="comment">     * http://www.cs.rochester.edu/u/scott/synchronization/</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;We also use &quot;next&quot; links to implement blocking mechanics.</span></span><br><span class="line"><span class="comment">     * The thread id for each node is kept in its own node, so a</span></span><br><span class="line"><span class="comment">     * predecessor signals the next node to wake up by traversing</span></span><br><span class="line"><span class="comment">     * next link to determine which thread it is.  Determination of</span></span><br><span class="line"><span class="comment">     * successor must avoid races with newly queued nodes to set</span></span><br><span class="line"><span class="comment">     * the &quot;next&quot; fields of their predecessors.  This is solved</span></span><br><span class="line"><span class="comment">     * when necessary by checking backwards from the atomically</span></span><br><span class="line"><span class="comment">     * updated &quot;tail&quot; when a node&#x27;s successor appears to be null.</span></span><br><span class="line"><span class="comment">     * (Or, said differently, the next-links are an optimization</span></span><br><span class="line"><span class="comment">     * so that we don&#x27;t usually need a backward scan.)</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Cancellation introduces some conservatism to the basic</span></span><br><span class="line"><span class="comment">     * algorithms.  Since we must poll for cancellation of other</span></span><br><span class="line"><span class="comment">     * nodes, we can miss noticing whether a cancelled node is</span></span><br><span class="line"><span class="comment">     * ahead or behind us. This is dealt with by always unparking</span></span><br><span class="line"><span class="comment">     * successors upon cancellation, allowing them to stabilize on</span></span><br><span class="line"><span class="comment">     * a new predecessor, unless we can identify an uncancelled</span></span><br><span class="line"><span class="comment">     * predecessor who will carry this responsibility.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;CLH queues need a dummy header node to get started. But</span></span><br><span class="line"><span class="comment">     * we don&#x27;t create them on construction, because it would be wasted</span></span><br><span class="line"><span class="comment">     * effort if there is never contention. Instead, the node</span></span><br><span class="line"><span class="comment">     * is constructed and head and tail pointers are set upon first</span></span><br><span class="line"><span class="comment">     * contention.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Threads waiting on Conditions use the same nodes, but</span></span><br><span class="line"><span class="comment">     * use an additional link. Conditions only need to link nodes</span></span><br><span class="line"><span class="comment">     * in simple (non-concurrent) linked queues because they are</span></span><br><span class="line"><span class="comment">     * only accessed when exclusively held.  Upon await, a node is</span></span><br><span class="line"><span class="comment">     * inserted into a condition queue.  Upon signal, the node is</span></span><br><span class="line"><span class="comment">     * transferred to the main queue.  A special value of status</span></span><br><span class="line"><span class="comment">     * field is used to mark which queue a node is on.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;Thanks go to Dave Dice, Mark Moir, Victor Luchangco, Bill</span></span><br><span class="line"><span class="comment">     * Scherer and Michael Scott, along with members of JSR-166</span></span><br><span class="line"><span class="comment">     * expert group, for helpful ideas, discussions, and critiques</span></span><br><span class="line"><span class="comment">     * on the design of this class.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">        <span class="comment">/** Marker to indicate a node is waiting in shared mode */</span></span><br><span class="line">        <span class="comment">// SHARED变量表示节点被标记为共享模式，可以看到是一个空的Node()对象，用于Semaphore这种共享模式的加锁工具</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> Node SHARED = <span class="keyword">new</span> Node();</span><br><span class="line">        <span class="comment">/** Marker to indicate a node is waiting in exclusive mode */</span></span><br><span class="line">       <span class="comment">// EXCLUSIVE为null表示节点被标记为独占模式，用于类似ReentrantLock这种独占锁工具</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> Node EXCLUSIVE = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** waitStatus value to indicate thread has cancelled */</span></span><br><span class="line">      	<span class="comment">// waitStatus变量值若为1，表示当前线程节点已经被取消排队（注意虽然被标记为取消状态，但还未出队）</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> CANCELLED =  <span class="number">1</span>;</span><br><span class="line">        <span class="comment">/** waitStatus value to indicate successor&#x27;s thread needs unparking */</span></span><br><span class="line">        <span class="comment">// waitStatus变量值若为-1，表示当前线程节点的后驱线程节点需要被唤醒</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SIGNAL    = -<span class="number">1</span>;</span><br><span class="line">        <span class="comment">/** waitStatus value to indicate thread is waiting on condition */</span></span><br><span class="line">        <span class="comment">// waitStatus变量值若为-2，表示当前线程节点处在条件等待当中</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> CONDITION = -<span class="number">2</span>;</span><br><span class="line">				</span><br><span class="line">      	<span class="comment">// 用于共享模式的锁工具的连续唤醒操作设计，ReentrantLock用不上此设计，Semaphore这种工具可以用上。</span></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> PROPAGATE = -<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 以下是关于waitStatus变量取不同值时对应的场景说明</span></span><br><span class="line"><span class="comment">         * waitStatus用来表示当前线程节点的状态，仅取以下5个值</span></span><br><span class="line"><span class="comment">         * Status field, taking on only the values:</span></span><br><span class="line"><span class="comment">         *   SIGNAL:     The successor of this node is (or will soon be)</span></span><br><span class="line"><span class="comment">         *               blocked (via park), so the current node must</span></span><br><span class="line"><span class="comment">         *               unpark its successor when it releases or</span></span><br><span class="line"><span class="comment">         *               cancels. To avoid races, acquire methods must</span></span><br><span class="line"><span class="comment">         *               first indicate they need a signal,</span></span><br><span class="line"><span class="comment">         *               then retry the atomic acquire, and then,</span></span><br><span class="line"><span class="comment">         *               on failure, block.</span></span><br><span class="line"><span class="comment">         * SIGNAL的说明：当前线程节点的后驱节点发出SIGNAL唤醒的通知，当前线程需要在释放资源后或者自身被标记为取消状态后去唤醒挂在身后的后驱节点，为了避免无畏线程竞争，实现acquire功能的方法必须首先给出自己需要被通知，然后再重试“atomic acquire”,如果获取失败，则阻塞自己（而不是一直去CAS重试）</span></span><br><span class="line"><span class="comment">         *   CANCELLED:  This node is cancelled due to timeout or interrupt.</span></span><br><span class="line"><span class="comment">         *               Nodes never leave this state. In particular,</span></span><br><span class="line"><span class="comment">         *               a thread with cancelled node never again blocks.</span></span><br><span class="line"><span class="comment">         * SIGNAL的说明：由于超时或中断，节点被取消。此类节点不会脱落此取消状态。取消节点的线程不会再次阻塞。</span></span><br><span class="line"><span class="comment">         *   CONDITION:  This node is currently on a condition queue.</span></span><br><span class="line"><span class="comment">         *               It will not be used as a sync queue node</span></span><br><span class="line"><span class="comment">         *               until transferred, at which time the status</span></span><br><span class="line"><span class="comment">         *               will be set to 0. (Use of this value here has</span></span><br><span class="line"><span class="comment">         *               nothing to do with the other uses of the</span></span><br><span class="line"><span class="comment">         *               field, but simplifies mechanics.)</span></span><br><span class="line"><span class="comment">         * CONDITION的说明：节点当前位于“condition queue”，在被放到CLH队列之前，它不会用作同步队列节点，此时状态将设置为0。表示节点在等待队列上，当其他线程调用了Condition的signal方法后，CONDITION状态的节点将从等待队列转移到同步队列中，等待获取资源。</span></span><br><span class="line"><span class="comment">         *   PROPAGATE:  A releaseShared should be propagated to other</span></span><br><span class="line"><span class="comment">         *               nodes. This is set (for head node only) in</span></span><br><span class="line"><span class="comment">         *               doReleaseShared to ensure propagation</span></span><br><span class="line"><span class="comment">         *               continues, even if other operations have</span></span><br><span class="line"><span class="comment">         *               since intervened.</span></span><br><span class="line"><span class="comment">         * PROPAGATE的说明：应将releaseShared操作传播到其他节点，这是在doReleaseShared中设置的（仅针对头部节点），以确保releaseShared操作能够传播继续进行，即使其他操作已经介入。</span></span><br><span class="line"><span class="comment">         * 换句话说：在共享模式下，前驱节点线程节点不仅要唤醒其后驱线程节点，同时也会唤醒后驱线程节点的后驱线程节点，类似一路唤醒下去。</span></span><br><span class="line"><span class="comment">         *   0:          None of the above</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * The values are arranged numerically to simplify use.</span></span><br><span class="line"><span class="comment">         * Non-negative values mean that a node doesn&#x27;t need to</span></span><br><span class="line"><span class="comment">         * signal. So, most code doesn&#x27;t need to check for particular</span></span><br><span class="line"><span class="comment">         * values, just for sign.</span></span><br><span class="line"><span class="comment">         * waitStatus采用这几个值得设计是为了方便使用，非负值表示节点不需要signal，所以，大多数代码不需要检查特定的值，只需要检查符号即可。</span></span><br><span class="line"><span class="comment">         * The field is initialized to 0 for normal sync nodes, and</span></span><br><span class="line"><span class="comment">         * CONDITION for condition nodes.  It is modified using CAS</span></span><br><span class="line"><span class="comment">         * (or when possible, unconditional volatile writes).</span></span><br><span class="line"><span class="comment">         * waitStatus状态值，对于正常同步节点，它会被初始化为0，而对于condition nodes，则会被置为“CONDITION”，一般使用CAS去更新waitStatus的值。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">volatile</span> <span class="keyword">int</span> waitStatus;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Link to predecessor node that current node/thread relies on</span></span><br><span class="line"><span class="comment">         * for checking waitStatus. Assigned during enqueuing, and nulled</span></span><br><span class="line"><span class="comment">         * out (for sake of GC) only upon dequeuing.  Also, upon</span></span><br><span class="line"><span class="comment">         * cancellation of a predecessor, we short-circuit while</span></span><br><span class="line"><span class="comment">         * finding a non-cancelled one, which will always exist</span></span><br><span class="line"><span class="comment">         * because the head node is never cancelled: A node becomes</span></span><br><span class="line"><span class="comment">         * head only as a result of successful acquire. A</span></span><br><span class="line"><span class="comment">         * cancelled thread never succeeds in acquiring, and a thread only</span></span><br><span class="line"><span class="comment">         * cancels itself, not any other node.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        结合前面的prev指针用于处理线程被取消的情形。</span></span><br><span class="line"><span class="comment">     The &quot;prev&quot; links (not used in original CLH locks), are mainly</span></span><br><span class="line"><span class="comment">     needed to handle cancellation. If a node is cancelled, its</span></span><br><span class="line"><span class="comment">     successor is (normally) relinked to a non-cancelled</span></span><br><span class="line"><span class="comment">     predecessor. </span></span><br><span class="line"><span class="comment">          non-callcelled node -&gt; a node -&gt;successor</span></span><br><span class="line"><span class="comment">          non-callcelled node -&gt;successor</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">         </span><br><span class="line">        <span class="keyword">volatile</span> Node prev;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Link to the successor node that the current node/thread</span></span><br><span class="line"><span class="comment">         * unparks upon release. Assigned during enqueuing, adjusted</span></span><br><span class="line"><span class="comment">         * when bypassing cancelled predecessors, and nulled out (for</span></span><br><span class="line"><span class="comment">         * sake of GC) when dequeued.  The enq operation does not</span></span><br><span class="line"><span class="comment">         * assign next field of a predecessor until after attachment,</span></span><br><span class="line"><span class="comment">         * so seeing a null next field does not necessarily mean that</span></span><br><span class="line"><span class="comment">         * node is at end of queue. However, if a next field appears</span></span><br><span class="line"><span class="comment">         * to be null, we can scan prev&#x27;s from the tail to</span></span><br><span class="line"><span class="comment">         * double-check.  The next field of cancelled nodes is set to</span></span><br><span class="line"><span class="comment">         * point to the node itself instead of null, to make life</span></span><br><span class="line"><span class="comment">         * easier for isOnSyncQueue.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">      next指针被设计为“We also use &quot;next&quot; links to implement blocking mechanics”。 已经被取消的节点会将next指针指向自己而不是把next指向null，这一点设计很像SkipList里面的remove设计逻辑这种设计有利于GC。</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">volatile</span> Node next;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * The thread that enqueued this node.  Initialized on</span></span><br><span class="line"><span class="comment">         * construction and nulled out after use.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        用于指向获取锁的线程，可以看到其实Node节点就是包装需要获取锁的线程，因此也可以称为线程节点。</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        <span class="keyword">volatile</span> Thread thread;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Link to next node waiting on condition, or the special</span></span><br><span class="line"><span class="comment">         * value SHARED.  Because condition queues are accessed only</span></span><br><span class="line"><span class="comment">         * when holding in exclusive mode, we just need a simple</span></span><br><span class="line"><span class="comment">         * linked queue to hold nodes while they are waiting on</span></span><br><span class="line"><span class="comment">         * conditions. They are then transferred to the queue to</span></span><br><span class="line"><span class="comment">         * re-acquire. And because conditions can only be exclusive,</span></span><br><span class="line"><span class="comment">         * we save a field by using special value to indicate shared</span></span><br><span class="line"><span class="comment">         * mode.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">      ReentrantLock用不到此属性，CountDownLatch、CyclicBarrier等这种共享模式锁工具可以用到此属性。</span></span><br><span class="line"><span class="comment">nextWaiter特殊标记,Node在CLH队列时，nextWaiter表示共享式或独占式标记，也即nextWaiter=SHARED;Node在条件队列时，nextWaiter表示下个Node节点指针</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        Node nextWaiter;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Returns true if node is waiting in shared mode.</span></span><br><span class="line"><span class="comment">         判断线程节点是否处于共享模式</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">isShared</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> nextWaiter == SHARED;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Returns previous node, or throws NullPointerException if null.</span></span><br><span class="line"><span class="comment">         * Use when predecessor cannot be null.  The null check could</span></span><br><span class="line"><span class="comment">         * be elided, but is present to help the VM.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> the predecessor of this node</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">final</span> Node <span class="title">predecessor</span><span class="params">()</span> <span class="keyword">throws</span> NullPointerException </span>&#123;</span><br><span class="line">            Node p = prev;</span><br><span class="line">            <span class="keyword">if</span> (p == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">        Node() &#123;    <span class="comment">// Used to establish initial head or SHARED marker</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      	<span class="comment">// 没有成功获取资源（例如锁）的线程就会使用此包装为一个Node，此节点会被addWaiter方法使用，也即将节点入队（此队列称为wait queue）</span></span><br><span class="line">        Node(Thread thread, Node mode) &#123;     <span class="comment">// Used by addWaiter</span></span><br><span class="line">            <span class="keyword">this</span>.nextWaiter = mode;</span><br><span class="line">            <span class="keyword">this</span>.thread = thread;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Node(Thread thread, <span class="keyword">int</span> waitStatus) &#123; <span class="comment">// Used by Condition</span></span><br><span class="line">            <span class="keyword">this</span>.waitStatus = waitStatus;</span><br><span class="line">            <span class="keyword">this</span>.thread = thread;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>其他属性</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Head of the wait queue, lazily initialized.  Except for</span></span><br><span class="line"><span class="comment">   * initialization, it is modified only via method setHead.  Note:</span></span><br><span class="line"><span class="comment">   * If head exists, its waitStatus is guaranteed not to be</span></span><br><span class="line"><span class="comment">   * CANCELLED.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">// 阻塞队列（双向链表）的头结点，惰性创建，仅能在setHead方法能够对head指向做调整</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> Node head;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Tail of the wait queue, lazily initialized.  Modified only via</span></span><br><span class="line"><span class="comment">   * method enq to add new wait node.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="comment">// 阻塞队列（双向链表）的尾结点，惰性创建，仅能在enq方法能够对tail指向做调整</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">volatile</span> Node tail;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The synchronization state.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 这个state变量就是AQS子类需要去更爱的，用于控制资源</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> state;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns the current value of synchronization state.</span></span><br><span class="line"><span class="comment">   * This operation has memory semantics of a &#123;<span class="doctag">@code</span> volatile&#125; read.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> current state value</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">getState</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> state;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3、关于线程是如何被”Queuing”-入队操作"><a href="#3、关于线程是如何被”Queuing”-入队操作" class="headerlink" title="3、关于线程是如何被”Queuing” 入队操作"></a>3、关于线程是如何被”Queuing” 入队操作</h4><p>AQS本身并不会触发线程去入队的操作，那么是在什么时机才会有这个操作发生？这里不妨还是以</p>
<p><code>ReentrantLock</code>使用为例：根据基本的demo用法，如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> ReentrantLock lock=<span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">        <span class="comment">// 创建新线程执行任务</span></span><br><span class="line">    &#125;   <span class="keyword">catch</span> ( Exception e)&#123;</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从构造器可以看出，默认创建的是非公平的同步锁，具体逻辑由<code>NonfairSync</code>类实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    sync = <span class="keyword">new</span> NonfairSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-1-NonfairSync"><a href="#3-1-NonfairSync" class="headerlink" title="3.1 NonfairSync"></a>3.1 NonfairSync</h5><p>Sync object for non-fair locks，也即<code>NonfairSync</code>，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NonfairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">7316153563782823691L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Performs lock.  Try immediate barge, backing up to normal</span></span><br><span class="line"><span class="comment">     * acquire on failure.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 线程执行lock.lock()也即对应以下逻辑</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 这个动词短语immediate barge用得很贴切：线程可以立即&quot;乱闯&quot;,也即新线程使用lock.lock()时，可以不排队直接参加竞争资源，如何竞争？ 只要将state状态CAS更新为1则抢占成功，这就是所谓的非公平锁。</span></span><br><span class="line">        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">          	<span class="comment">//该线程成功竞争后，将自己独占线程</span></span><br><span class="line">            setExclusiveOwnerThread(Thread.currentThread());</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">          	<span class="comment">// 新来的线程compareAndSetState(0, 1)失败，那么进入逻辑</span></span><br><span class="line">            acquire(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> nonfairTryAcquire(acquires);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-2-acquire-1"><a href="#3-2-acquire-1" class="headerlink" title="3.2 acquire(1)"></a>3.2 acquire(1)</h5><p><code>acquire(1)</code> 是AQS内部的方法，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Acquires in exclusive mode, ignoring interrupts.  Implemented</span></span><br><span class="line"><span class="comment">     * by invoking at least once &#123;<span class="doctag">@link</span> #tryAcquire&#125;,</span></span><br><span class="line"><span class="comment">     * returning on success.  Otherwise the thread is queued, possibly</span></span><br><span class="line"><span class="comment">     * repeatedly blocking and unblocking, invoking &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * #tryAcquire&#125; until success.  This method can be used</span></span><br><span class="line"><span class="comment">     * to implement method &#123;<span class="doctag">@link</span> Lock#lock&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arg the acquire argument.  This value is conveyed to</span></span><br><span class="line"><span class="comment">     *        &#123;<span class="doctag">@link</span> #tryAcquire&#125; but is otherwise uninterpreted and</span></span><br><span class="line"><span class="comment">     *        can represent anything you like.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">      从该方法可以看出：线程在NonfairSync里面抢占更新state失败，也还有机会再来去申请资源，</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">也即上面的tryAcquire，如果成功，那么就返回true，如果线程还是tryAcquire失败，那么此时线程就会被排队：acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ，然后进入selfInterrupt()</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class="line">            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class="line">            selfInterrupt();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>由于AQS的<code>tryAcquire</code>需要子类实现具体的逻辑，也即ReentrantLock内部的tryAcquire，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> nonfairTryAcquire(acquires);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-3-nonfairTryAcquire"><a href="#3-3-nonfairTryAcquire" class="headerlink" title="3.3 nonfairTryAcquire"></a>3.3 nonfairTryAcquire</h5><p>ReentrantLock内部的tryAcquire内部是由<code>ReentrantLock</code>内部的Sync类的<code>nonfairTryAcquire</code>方法实现，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Base of synchronization control for this lock. Subclassed</span></span><br><span class="line"><span class="comment"> * into fair and nonfair versions below. Uses AQS state to</span></span><br><span class="line"><span class="comment"> * represent the number of holds on the lock.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Sync</span> <span class="keyword">extends</span> <span class="title">AbstractQueuedSynchronizer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">5179523762034025860L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Performs &#123;<span class="doctag">@link</span> Lock#lock&#125;. The main reason for subclassing</span></span><br><span class="line"><span class="comment">     * is to allow fast path for nonfair version.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// NonfairSync类的lock()方法实现了该抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Performs non-fair tryLock.  tryAcquire is implemented in</span></span><br><span class="line"><span class="comment">     * subclasses, but both need nonfair try for trylock method.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">// 这里就是AQS的tryAcquire在子类Sync的具体实现逻辑</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">nonfairTryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Thread current = Thread.currentThread();</span><br><span class="line">      	<span class="comment">// 获取同步状态state值</span></span><br><span class="line">        <span class="keyword">int</span> c = getState();</span><br><span class="line">      	<span class="comment">//①如果同步状态此时为0，说明没有其他线程在竞争，那么当前线程果断使用CAS尝试将state从0更新为1，这里也再次证明新线程使用lock.lock()第一次if (compareAndSetState(0, 1))不成功，在tryAcquire里面还有机会再尝试一次compareAndSetState，如果本次成功CAS那么将自己设为独占线程，获得锁资源，返回true。</span></span><br><span class="line">        <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) &#123;</span><br><span class="line">                setExclusiveOwnerThread(current);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      	<span class="comment">/*</span></span><br><span class="line"><span class="comment">      	② 如果当前线程就是之前设置的独占线程，说明当前线程再次“重入”获取锁，那么只要重入次数最大值只要不溢出，那么就可以让当前线程再次“重入”并更新同步状态state的值，这就是ReentrantLock可重入锁设计的原理,最多可以重入多少次？ 首先state是int类型，因此最大值为：</span></span><br><span class="line"><span class="comment">      	max=(1&lt;&lt;31)-1</span></span><br><span class="line"><span class="comment">      	所以才会有以下的用法：</span></span><br><span class="line"><span class="comment">    double tooLarge=Math.pow(2,31);</span></span><br><span class="line"><span class="comment">    for (int i = 1; i &lt;= tooLarge; i++) &#123;</span></span><br><span class="line"><span class="comment">        lock.lock();  // 同一线程，重入加锁到最大值，其实内部就是对state进行CAS累加，也即setState(nextc)，直到state值累加到超过最大值则抛出&quot;Maximum lock count exceeded&quot;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    for (int i = 1; i &lt;= 5; i++) &#123;</span></span><br><span class="line"><span class="comment">        lock.unlock();</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">      	</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextc = c + acquires;</span><br><span class="line">            <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">            setState(nextc);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">/*③ 若无法满足①、②条件，说明当前线程面临激烈锁资源竞争，那么只能返回false，接着就被安排去入队，也即AQS里面的</span></span><br><span class="line"><span class="comment">if (!tryAcquire(arg) &amp;&amp;</span></span><br><span class="line"><span class="comment">        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span></span><br><span class="line"><span class="comment">        selfInterrupt();      </span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  到此我们已经找出了线程从开始使用<code>lock.lock()</code> 到被<code>Queuing</code>入队的时机，因此接下里入队操作就是核心逻辑了</p>
<h4 id="4、AQS-的acquireQueued-addWaiter-Node-EXCLUSIVE-arg"><a href="#4、AQS-的acquireQueued-addWaiter-Node-EXCLUSIVE-arg" class="headerlink" title="4、AQS 的acquireQueued(addWaiter(Node.EXCLUSIVE), arg))"></a>4、AQS 的acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</h4><p>这里接着第3章节的内容，此时线程需要被入队，执行<code>acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</code> ,这里的arg就是<code>acquire(1)</code>里面的1，表示需要对state加1或者需要获取1个锁资源</p>
<h5 id="4-1-addWaiter-Node-EXCLUSIVE"><a href="#4-1-addWaiter-Node-EXCLUSIVE" class="headerlink" title="4.1 addWaiter(Node.EXCLUSIVE)"></a>4.1 addWaiter(Node.EXCLUSIVE)</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Creates and enqueues node for current thread and given mode.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@return</span> the new node</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">/*对于独占模式来说，addWaiter的功能就是将当前线程包装为线程节点，并使用CAS将自己添加到阻塞队列的尾部，分两个步骤：</span></span><br><span class="line"><span class="comment"> 1、优先快速尝试将自己通过CAS加到阻塞队列尾部，如果入队失败就进入2</span></span><br><span class="line"><span class="comment"> 2、使用enq(node)确保自己最后可以加入到阻塞队列尾部（或者入队成功）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> Node <span class="title">addWaiter</span><span class="params">(Node mode)</span> </span>&#123;</span><br><span class="line">      Node node = <span class="keyword">new</span> Node(Thread.currentThread(), mode);</span><br><span class="line">      <span class="comment">// Try the fast path of enq; backup to full enq on failure</span></span><br><span class="line">      Node pred = tail;</span><br><span class="line">      <span class="comment">//1、 node&lt;=&gt;node&lt;=&gt;pred 变成 node&lt;=&gt;node&lt;=&gt;pred&lt;=&gt;new Node</span></span><br><span class="line">      <span class="keyword">if</span> (pred != <span class="keyword">null</span>) &#123;</span><br><span class="line">        	<span class="comment">// 新线程节点的前驱节点指针指向pred，如果pred节点没有被其他线程更改，那么CAS成功就会将pred的next指针指向新线程节点，从而使得新线程节点成功入队。</span></span><br><span class="line">          node.prev = pred;</span><br><span class="line">          <span class="keyword">if</span> (compareAndSetTail(pred, node)) &#123;</span><br><span class="line">              pred.next = node;</span><br><span class="line">            	<span class="comment">//返回线程节点给到外部调用方，也即acquireQueued方法</span></span><br><span class="line">              <span class="keyword">return</span> node;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    	<span class="comment">//2、enq内部使用自旋（for循环）保证线程一定能入队</span></span><br><span class="line">      enq(node);</span><br><span class="line">     <span class="comment">//返回线程节点给到外部调用方，也即acquireQueued方法</span></span><br><span class="line">      <span class="keyword">return</span> node;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-2-enq-node"><a href="#4-2-enq-node" class="headerlink" title="4.2 enq(node)"></a>4.2 enq(node)</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Inserts node into queue, initializing if necessary. See picture above.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> node the node to insert</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> node&#x27;s predecessor</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">	<span class="comment">// 将线程节点插入双向链表（阻塞队列）尾部，如果链表为空，则需要先初始化后再插入节点</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">enq</span><span class="params">(<span class="keyword">final</span> Node node)</span> </span>&#123;</span><br><span class="line">      	<span class="comment">// 自旋，保证线程一定能在某次循环中CAS入队成功</span></span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            Node t = tail;</span><br><span class="line">          	<span class="comment">// 如果链表尾节点为空，说明阻塞队列还未创建，因此需要初始化，这时新建一个非线程节点放在链表头部。</span></span><br><span class="line">            <span class="keyword">if</span> (t == <span class="keyword">null</span>) &#123; <span class="comment">// Must initialize</span></span><br><span class="line"> <span class="comment">/* 这里非常关键：双向链表的头节点不是线程节点，而是一个无参构造方法的节点，这里可以称为辅助节点，就像ConcurrentSkipList底层的数据链表头节点(BASE_HEADER)也是辅助节点的设计:</span></span><br><span class="line"><span class="comment"> head = new HeadIndex&lt;K,V&gt;(new Node&lt;K,V&gt;(null, BASE_HEADER, null),null, null, 1);</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">                <span class="keyword">if</span> (compareAndSetHead(<span class="keyword">new</span> Node()))</span><br><span class="line">                    tail = head;</span><br><span class="line">            <span class="comment">// 如果双向链表已经存在，则线程节点尝试使用CAS将自己添加到队列尾部</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                node.prev = t;</span><br><span class="line">                <span class="keyword">if</span> (compareAndSetTail(t, node)) &#123;</span><br><span class="line">                    t.next = node;</span><br><span class="line">                  	<span class="comment">// 返回线程节点给到外部调用方，也即acquireQueued方法</span></span><br><span class="line">                    <span class="keyword">return</span> t;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>未能获取的锁资源（未能对state更新成功的）新线程通过<code>addWaiter</code>或者<code>enq</code>方法让自己添加到阻塞队列尾部，或者这样理解：在<code>addWaiter</code>里面首先尝试，也就代码里面的注释所表达的意思：Try the fast path of enq; backup to full enq on failure</p>
<p>接下里还需要做什么？线程加入阻塞队列以后，是马上把自己阻塞起来，还是刚好位于队列第一个线程节点位置然后马上尝试获取锁资源呢？这就是<code>acquireQueued(addWaiter(Node.EXCLUSIVE),1))</code>核心设计</p>
<h5 id="4-3-acquireQueued-addWaiter-Node-EXCLUSIVE-1"><a href="#4-3-acquireQueued-addWaiter-Node-EXCLUSIVE-1" class="headerlink" title="4.3 acquireQueued(addWaiter(Node.EXCLUSIVE),1))"></a>4.3 acquireQueued(addWaiter(Node.EXCLUSIVE),1))</h5><p>从上面4.1和4.2可知，阻塞队列里面放的都是未能成功获取锁资源的线程节点（除了头节点辅助节点），这些节点已经入队，但总不能放着它们在队列里面就不管了，接下来还要做一件事情：</p>
<p>从队列取出线程节点，让它重新去尝试获取已经被外面活动线程释放的锁资源</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> 已在队列中的线程以独占且不可中断的模式获取锁资源</span></span><br><span class="line"><span class="comment"> * Acquires in exclusive uninterruptible mode for thread already in</span></span><br><span class="line"><span class="comment"> * queue. Used by condition wait methods as well as acquire.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arg the acquire argument</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if interrupted while waiting</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">acquireQueued</span><span class="params">(<span class="keyword">final</span> Node node, <span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">// failed表示是否成功获得锁资源</span></span><br><span class="line">    <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      	<span class="comment">// 表示线程是否被中断</span></span><br><span class="line">        <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">          	<span class="comment">// 核心逻辑：取出当前入队线程节点的前驱节点</span></span><br><span class="line">            <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">          	<span class="comment">/*</span></span><br><span class="line"><span class="comment">          	（注意区分阻塞队列的head节点（辅助节点又称哨兵节点）和阻塞队列的第一个线程节点）</span></span><br><span class="line"><span class="comment">          	如果当前线程节点前驱节点恰好是head节点，说明当前线程节点就是阻塞队列的第一个线程节点，当然有资格且第一个先去尝试获取锁资源，如下结构</span></span><br><span class="line"><span class="comment">          	head(辅助节点) &lt;-&gt; node（第一个线程节点） -&gt; null</span></span><br><span class="line"><span class="comment">          	*/</span> </span><br><span class="line">            <span class="keyword">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</span><br><span class="line">             <span class="comment">// 这里的tryAcquire表示你（阻塞队列的第一个线程节点）有资格去抢锁资源，但也要看看能否抢赢外界线程</span></span><br><span class="line">            <span class="comment">/* 如果线程节点获取资源成功，那么就可以做出队操作，这里出队很巧妙:</span></span><br><span class="line"><span class="comment">            经过setHead之后，线程节点node.thread不再指向线程，而是变成了辅助节点而且成为阻塞队列的新head，原head节点通过p.next = null 完成GC，这就是为何阻塞队列将head节点设计辅助节点new Node()的原因：第一个线程节点获取锁资源后需要出队以及方便回收设计</span></span><br><span class="line"><span class="comment">                    private void setHead(Node node) &#123;</span></span><br><span class="line"><span class="comment">                    head = node;</span></span><br><span class="line"><span class="comment">                    node.thread = null;</span></span><br><span class="line"><span class="comment">                    node.prev = null;</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">                setHead(node);</span><br><span class="line">                p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">                failed = <span class="keyword">false</span>;</span><br><span class="line">                <span class="keyword">return</span> interrupted;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">/* </span></span><br><span class="line"><span class="comment">          	两种情况会进入if逻辑：</span></span><br><span class="line"><span class="comment">          	①如果当前节点是第一个线程节点或者说其前驱节点是head节点，但在tryAcquire(arg)竞争失败（因为外面有新的线程在lock.lock()里面CAS成功）</span></span><br><span class="line"><span class="comment">          	②当前线程节点不是第一个线程节点</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">                parkAndCheckInterrupt())</span><br><span class="line">                interrupted = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (failed)</span><br><span class="line">            cancelAcquire(node);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-4-shouldParkAfterFailedAcquire-p-node"><a href="#4-4-shouldParkAfterFailedAcquire-p-node" class="headerlink" title="4.4 shouldParkAfterFailedAcquire(p, node)"></a>4.4 shouldParkAfterFailedAcquire(p, node)</h5><p>接4.3内容，这部分内容可以解析阻塞队列为何被称为“阻塞”队列的原因，<code>shouldParkAfterFailedAcquire</code>从方法名字也可以看出其设计目的：既然第一个线程节点获取锁资源失败，那么就不能一直无限去<code>Acquire</code>，而是在阻塞队列是自己变成“阻塞状态”去等待，这样就不会白白消耗cpu。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Checks and updates status for a node that failed to acquire.</span></span><br><span class="line"><span class="comment">    * Returns true if thread should block. This is the main signal</span></span><br><span class="line"><span class="comment">    * control in all acquire loops.  Requires that pred == node.prev.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> pred node&#x27;s predecessor holding status</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if thread should block</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">	<span class="comment">/*非常重要的源码注释：根据4.3，如果线程节点未能p == head &amp;&amp; tryAcquire(arg)，则检查并更新该节点里面的Node.SIGNAL状态值。如果线程节点应该被阻塞，则返回true。</span></span><br><span class="line"><span class="comment">为了理解以下设计逻辑，这里不妨先假设当前链表结构为：</span></span><br><span class="line"><span class="comment">		head(waitStatus=0) &lt;-&gt; node(waitStatus=0) -&gt; null</span></span><br><span class="line"><span class="comment">		那么这里pred显然指向head(waitStatus=0) ，node指向第一个线程节点node（waitStatus=0）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">shouldParkAfterFailedAcquire</span><span class="params">(Node pred, Node node)</span> </span>&#123;</span><br><span class="line">   <span class="comment">// waitStatus也即线程节点的等待状态，新线程节点创建时waitStatus默认为0， </span></span><br><span class="line">       <span class="keyword">int</span> ws = pred.waitStatus;</span><br><span class="line">   <span class="comment">// ①waitStatus变量值若为Node.SIGNAL也即等于-1，说明对于pred(-1)&lt;-&gt;node(ws=0)来说，pred节点其实已经是阻塞状态了，那么node作为pred的后驱节点也肯定要被阻塞，因此返回true后，在parkAndCheckInterrupt里面node就会被阻塞起来</span></span><br><span class="line">       <span class="keyword">if</span> (ws == Node.SIGNAL)</span><br><span class="line">           <span class="comment">/*</span></span><br><span class="line"><span class="comment">            * This node has already set status asking a release</span></span><br><span class="line"><span class="comment">            * to signal it, so it can safely park.</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   <span class="comment">/* ② 如果waitStatus的值&gt;0,也即waitStatus=Node.CANCELLED=1,说明pred线程节点取消排队，考察以下阻塞队列结构</span></span><br><span class="line"><span class="comment">  head(ws=-1)&lt;-&gt;node1(ws=-1)&lt;-&gt;node2(ws=1)&lt;-&gt;node3(ws=0)-&gt; null</span></span><br><span class="line"><span class="comment">  假设pred=node2,node=node3,显然node2已经取消排队，那么node3不能跟在它后面，因此需要将node2出队，也即node.prev = pred = pred.prev，有：</span></span><br><span class="line"><span class="comment">  head(ws=-1)&lt;-&gt;node1(ws=-1)&lt;-&gt;node3(ws=0)-&gt; null</span></span><br><span class="line"><span class="comment">  </span></span><br><span class="line"><span class="comment">  如果链表中不止一个node2取消排队，还有很多节点也是处于“取消排队状态”，那么就使用前向遍历，直到找到有一个前驱线程节点不是取消排队的节点（说明此节点状态可靠），然后把它作为node3的前驱节点。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">       <span class="keyword">if</span> (ws &gt; <span class="number">0</span>) &#123;</span><br><span class="line">           <span class="comment">/*</span></span><br><span class="line"><span class="comment">            * Predecessor was cancelled. Skip over predecessors and</span></span><br><span class="line"><span class="comment">            * indicate retry.</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">           <span class="keyword">do</span> &#123;</span><br><span class="line">             <span class="comment">//将原来写法 node.prev = pred = pred.prev拆分为下面写法</span></span><br><span class="line">               pred=pred.prev;</span><br><span class="line">               node.prev = pred</span><br><span class="line">           &#125; <span class="keyword">while</span> (pred.waitStatus &gt; <span class="number">0</span>);</span><br><span class="line">         	<span class="comment">// 经过前向遍历后，所有取消状态的节点都被出队，那么到这里就可以找到一个可靠状态的pred前驱节点，将要处理的node挂在它后面即可</span></span><br><span class="line">           pred.next = node;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">/*注意这里的注释，非常重要：</span></span><br><span class="line"><span class="comment">            * waitStatus must be 0 or PROPAGATE.  Indicate that we</span></span><br><span class="line"><span class="comment">            * need a signal, but don&#x27;t park yet.  Caller will need to</span></span><br><span class="line"><span class="comment">            * retry to make sure it cannot acquire before parking.</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">         	<span class="comment">//③ 执行流来到这里说明前驱节点waitStatus要么是0要么是PROPAGATE，此时需要将待处理的node节点前驱节点pred的waitStatus设为SIGNAL值，以表示我作为pred的后驱节点需要等待被唤醒，但还没进入park阻塞状态。</span></span><br><span class="line">           compareAndSetWaitStatus(pred, ws, Node.SIGNAL);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>流程：</p>
<p>1、构造器使用非公平同步类对象new NonfairSync()</p>
<p>2、lock.lock()调用1定义的lock()方法，尝试对父类AQS的state变量使用CAS从0更新为1，成功就将自己设为独占锁，失败就进入3</p>
<p>3、使用acquire(1)继续去竞争资源，其内部调用AQS的acquire方法，</p>
<p>4、AQS的acquire方法是模板方法，因此会调用子类的tryAcquire(arg)再次尝试更新state</p>
<p>5、子类tryAcquire里面的nonfairTryAcquire(1)，再次读取state，如果为0就尝试CAS加1，如果成功就跟2类似，失败CAS且当前线程是独占线程，就可以再次对state加1</p>
<p>6、否则AQS的tryAcquire(1)返回false，表示当前线程两次获取锁资源失败，那么就开始7</p>
<p>7、将当前线程包装为线程节点，将在addWaiter尝试加入双向链表：如果链表已经存在，快速执行CASTail，成功就返回，失败则使用enq使用for循环入队：第一次循环判断是否需要初始化双向链表，如果需要则新建一个辅助节点头结点，第二次循环才是CASTail，由于使用自旋，因此enq一定可以使得线程节点加入双向链表尾部。</p>
<p>8、在acquireQueued中，刚入队的线程节点如果恰好又是第一个线程节点就会马上尝试tryAcquire(1)，如果成功获取锁资源，就会将setHead操作，并将当前线程节点设为辅助节点（也即线程出队），如果tryAcquire(1)失败来到9</p>
<p>9、使用shouldParkAfterFailedAcquire(pred, node)  ，第一个线程节点尝试tryAcquire(1)失败后不能无限循环再获取资源，因此将它的前驱节点设为SIGNAL状态（要求：pred == node.prev 读一致性），表示当前节点等待阻塞中，需要被唤醒</p>
<p>10、由于当前节点的前驱节点可能处于“取消排队状态”，因此当前节点不能将此类节点作为自己的前驱节点，因此需要不断前向遍历，直到找到waitStatus&lt;=0的前驱节点 pred(waitStatus&lt;=0)（找到一个可以“挂靠”的、非取消状态的前驱节点）由于使用</p>
<p><code>node.prev = pred = pred.prev</code> 因此只要处于“取消排队状态”的前驱节点都会被删掉而不在链表中。</p>
<p>11、只要将前驱节点的ws设为Node.SIGNAL，那么就可以返回true，来到parkAndCheckInterrupt</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">    parkAndCheckInterrupt())</span><br><span class="line">    interrupted = <span class="keyword">true</span>;</span><br></pre></td></tr></table></figure>
<p>线程节点就可以自己阻塞了</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Convenience method to park and then check if interrupted</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if interrupted</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">parkAndCheckInterrupt</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    LockSupport.park(<span class="keyword">this</span>);</span><br><span class="line">    <span class="keyword">return</span> Thread.interrupted();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>12、回到acquire(1)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Performs lock.  Try immediate barge, backing up to normal</span></span><br><span class="line"><span class="comment"> * acquire on failure.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        setExclusiveOwnerThread(Thread.currentThread());</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        acquire(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上流程完成了 线程入队、尝试获取锁资源、阻塞自己变成等待唤醒，接下来</p>
<p>需要了解清楚，在线程节点执行阻塞自己的时候使用shouldParkAfterFailedAcquire，里面能够向前遍历阻塞队列并会有side-effect作用：沿途清除“处在取消状态的线程节点”，那么为何会出现又取消状态的节点呢？以及在什么时机变成取消状态节点？</p>
<h5 id="4-5-acquireQueued里面的cancelAcquire-node"><a href="#4-5-acquireQueued里面的cancelAcquire-node" class="headerlink" title="4.5  acquireQueued里面的cancelAcquire(node)"></a>4.5  acquireQueued里面的cancelAcquire(node)</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">acquireQueued</span><span class="params">(<span class="keyword">final</span> Node node, <span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">            <span class="keyword">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</span><br><span class="line">                setHead(node);</span><br><span class="line">                p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">                failed = <span class="keyword">false</span>;</span><br><span class="line">                <span class="keyword">return</span> interrupted;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">                parkAndCheckInterrupt())</span><br><span class="line">                interrupted = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (failed)</span><br><span class="line">            cancelAcquire(node);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从队列中释放节点的疑虑打消了，那么又有新问题了：</p>
<ul>
<li>shouldParkAfterFailedAcquire中取消节点是怎么生成的呢？什么时候会把一个节点的waitStatus设置为-1？</li>
<li>是在什么时间释放节点通知到被挂起的线程呢？</li>
</ul>
<p>通过cancelAcquire方法，将Node的状态标记为CANCELLED。接下来，我们逐行来分析这个方法的原理：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Utilities for various versions of acquire</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Cancels an ongoing attempt to acquire.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">cancelAcquire</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">  	<span class="comment">/* cancelAcquire流程将结合以下链表结构作为解释，其中node4就是当前要被设为取消排队的线程节点，如果node5是null，那么说明node4是tail节点，如果node5是线程节点，说明node4不在链表尾部：</span></span><br><span class="line"><span class="comment">  	head&lt;-&gt;node1(非取消)&lt;-&gt;node2(CANCELLED)&lt;-&gt;node3(CANCELLED)&lt;-&gt;node4(非取消)&lt;-&gt;node5</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">      <span class="comment">// Ignore if node doesn&#x27;t exist</span></span><br><span class="line">  	<span class="comment">// 准备执行时发现node4已结被回收，则可以直接返回。</span></span><br><span class="line">    <span class="keyword">if</span> (node == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  	<span class="comment">// 既然当前节点取消排队，那么node.thread不再指向线程，转而执行null，用于GC</span></span><br><span class="line">    node.thread = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Skip cancelled predecessors</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">    ① </span></span><br><span class="line"><span class="comment">    例如按上面的链表节点结构假设，node4被设为CANCELLED状态前，必须先找到一个非CANCELLED的前驱节点，以便node4取消排队后，其后驱节点node5才可以挂靠在一个“可靠的前驱节点”pred，显然这个pred就是node1节点：</span></span><br><span class="line"><span class="comment">    也即：head&lt;-&gt;node1(非取消)&lt;-&gt;node4(非取消)&lt;-&gt;node5</span></span><br><span class="line"><span class="comment">    而node2和node3线程节点都是取消排队状态，因此需要跳过它们：Skip cancelled predecessors</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">    Node pred = node.prev;</span><br><span class="line">    <span class="keyword">while</span> (pred.waitStatus &gt; <span class="number">0</span>)</span><br><span class="line">        node.prev = pred = pred.prev;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// predNext is the apparent node to unsplice. CASes below will</span></span><br><span class="line">    <span class="comment">// fail if not, in which case, we lost race vs another cancel</span></span><br><span class="line">    <span class="comment">// or signal, so no further action is necessary.</span></span><br><span class="line">   <span class="comment">// pred.next=node ,pred=node.prev</span></span><br><span class="line">    Node predNext = pred.next;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Can use unconditional write instead of CAS here.</span></span><br><span class="line">    <span class="comment">// After this atomic step, other Nodes can skip past us.</span></span><br><span class="line">    <span class="comment">// Before, we are free of interference from other threads.</span></span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">   ②</span></span><br><span class="line"><span class="comment">   继续上面已经跳过cancelled predecessors，有：</span></span><br><span class="line"><span class="comment">   		head&lt;-&gt;node1(非取消)&lt;-&gt;node4(非取消)&lt;-&gt;node5，此时可以把node4设为取消状态</span></span><br><span class="line"><span class="comment">   也即：</span></span><br><span class="line"><span class="comment">   		head&lt;-&gt;node1(非取消)&lt;-&gt;node4(CANCELLED)&lt;-&gt;node5</span></span><br><span class="line"><span class="comment">   		做这个操作有个收益： other Nodes can skip past us，也就是说其他当外面有新线程入队需要做shouldParkAfterFailedAcquire等操作时，都会忽略这个已经是CANCELLED状态的node4节点，这样cancelAcquire操作就不会被外界线程节点干扰到。</span></span><br><span class="line"><span class="comment">   	  acquireQueued和shouldParkAfterFailedAcquire回答了准备设置“CANCELLED”线程节点时机，而cancelAcquire的设计回答在什么条件下将“准备取消节点”设为“CANCELLED”</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">     node.waitStatus = Node.CANCELLED;</span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">   ③</span></span><br><span class="line"><span class="comment">   对于②链表结构：head&lt;-&gt;node1(非取消)&lt;-&gt;node4(CANCELLED)&lt;-&gt;node5，如果node4本身就是tail节点（此时node5其实是null），那么将node4进行CAS设为null后，链表结构变为：</span></span><br><span class="line"><span class="comment">   head&lt;-&gt;node1(非取消)-&gt;null，也即注释里面说到的：If we are the tail, remove ourselves.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="keyword">if</span> (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123;</span><br><span class="line">        compareAndSetNext(pred, predNext, <span class="keyword">null</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// If successor needs signal, try to set pred&#x27;s next-link</span></span><br><span class="line">        <span class="comment">// so it will get one. Otherwise wake it up to propagate.</span></span><br><span class="line">        <span class="keyword">int</span> ws;</span><br><span class="line">      	<span class="comment">/* </span></span><br><span class="line"><span class="comment">         ④ </span></span><br><span class="line"><span class="comment">         根据③得到的链表结构且node4不是tail节点（此时node5是线程节点），</span></span><br><span class="line"><span class="comment">         有：head&lt;-&gt;node1(非取消)&lt;-&gt;node4(CANCELLED)&lt;-&gt;node5</span></span><br><span class="line"><span class="comment">         if条件1：如果pred不是头节点而且指向具体线程（也即node1不是头节点）且pred节点也即node1.ws是SIGNAL，那么则进入if</span></span><br><span class="line"><span class="comment">         if条件2：如果pred不是头节点而且指向具体线程（也即node1不是头节点）且pred节点也即node1.ws&lt;=0且经过CAS将node1.ws设为SIGNAL，那么则进入if。</span></span><br><span class="line"><span class="comment">         条件1意思是说如果node1.ws已经是SIGNAL，那么直接将node5（不是null且未取消）设为node1的后驱节点，这样就能正确的删除node4节点，而且能正确的将后面的线程节点挂在node1后面</span></span><br><span class="line"><span class="comment">         head&lt;-&gt;node1(ws=SIGNAL)&lt;-&gt;node5(ws非取消)</span></span><br><span class="line"><span class="comment">         条件2的意思是说如果node1.ws是未取消状态值那么就用CAS将node1.ws设为SIGNAL，然后再把node5（要求不是null且未取消）挂在node1后面</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">if</span> (pred != head &amp;&amp;</span><br><span class="line">            ((ws = pred.waitStatus) == Node.SIGNAL ||</span><br><span class="line">             (ws &lt;= <span class="number">0</span> &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp;</span><br><span class="line">            pred.thread != <span class="keyword">null</span>) &#123;</span><br><span class="line">            Node next = node.next;</span><br><span class="line">          	<span class="comment">// 要求即将被删除的node4节点的后驱节点node5既不是null且未取消状态才能将node5挂在node4前驱节点pred的后面</span></span><br><span class="line">            <span class="keyword">if</span> (next != <span class="keyword">null</span> &amp;&amp; next.waitStatus &lt;= <span class="number">0</span>)</span><br><span class="line">                compareAndSetNext(pred, predNext, next);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      	<span class="comment">/* </span></span><br><span class="line"><span class="comment">      	 ⑤ </span></span><br><span class="line"><span class="comment">      	 若不满足④说明，node4的前驱节点就是head节点，也即：</span></span><br><span class="line"><span class="comment">      	 head&lt;-&gt;node4(CANCELLED)&lt;-&gt;node5</span></span><br><span class="line"><span class="comment">      	 那么，node4很快会被删除，那么node5（如果node5不是null也不是非取消节点）就作为阻塞队列的第一个线程节点当然可以优先出队去获外面其他线程释放的锁资源</span></span><br><span class="line"><span class="comment">      	 因此ndoe4删除前，需要先唤醒后驱节点node5，接下来就来到4.6下面的章节逻辑</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">            unparkSuccessor(node);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        node.next = node; <span class="comment">// help GC</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="4-6unparkSuccessor"><a href="#4-6unparkSuccessor" class="headerlink" title="4.6unparkSuccessor"></a>4.6<code>unparkSuccessor</code></h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Wakes up node&#x27;s successor, if one exists.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> node the node</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 接上面4.5的⑤内容，如果node4有后驱节点，那么就node4需要唤醒这个后驱节点（假设后驱节点不是null也不是取消状态）</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">unparkSuccessor</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">       * If status is negative (i.e., possibly needing signal) try</span></span><br><span class="line"><span class="comment">       * to clear in anticipation of signalling.  It is OK if this</span></span><br><span class="line"><span class="comment">       * fails or if status is changed by waiting thread.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">    	<span class="comment">// 由于接上面4.5流程，node4已经设为CANCELLED，那么以下if逻辑会被跳过</span></span><br><span class="line">      <span class="keyword">int</span> ws = node.waitStatus;</span><br><span class="line">      <span class="keyword">if</span> (ws &lt; <span class="number">0</span>)</span><br><span class="line">          compareAndSetWaitStatus(node, ws, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">       * Thread to unpark is held in successor, which is normally</span></span><br><span class="line"><span class="comment">       * just the next node.  But if cancelled or apparently null,</span></span><br><span class="line"><span class="comment">       * traverse backwards from tail to find the actual</span></span><br><span class="line"><span class="comment">       * non-cancelled successor.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">    	<span class="comment">/*</span></span><br><span class="line"><span class="comment">    	node4的后驱节点一定是个正常的线程节点吗？ node4.next可能是以下几种情况：</span></span><br><span class="line"><span class="comment">    	① node4-&gt;null</span></span><br><span class="line"><span class="comment">      ② head&lt;-&gt;node4(CANCELLED)&lt;-&gt;node5(CANCELLED)&lt;-&gt;node6(CANCELLED)&lt;-&gt;node7(非取消状态)....</span></span><br><span class="line"><span class="comment">      ③ head&lt;-&gt;node4(CANCELLED)&lt;-&gt;node5(非取消)....</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      Node s = node.next; <span class="comment">// 先取出node4的后驱节点</span></span><br><span class="line">    	<span class="comment">// 针对后驱节点为null或者后驱节点是取消状态的情况，如①、②链表结构所示</span></span><br><span class="line">      <span class="keyword">if</span> (s == <span class="keyword">null</span> || s.waitStatus &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          s = <span class="keyword">null</span>;</span><br><span class="line">        	<span class="comment">// 从阻塞链表（阻塞队列）尾部向前遍历，找出一个waitStatus是不取消状态的的后驱节点，这个节点就是node4要唤醒的正常状态节点（总不能让node4唤醒一个已经取消排队或者为null的后驱节点吧）</span></span><br><span class="line">          <span class="keyword">for</span> (Node t = tail; t != <span class="keyword">null</span> &amp;&amp; t != node; t = t.prev)</span><br><span class="line">              <span class="keyword">if</span> (t.waitStatus &lt;= <span class="number">0</span>)</span><br><span class="line">                  s = t;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 针对情况③ node4的直接后驱节点就是正常状态节点或者经过&quot;traverse backwards&quot;找到的the actual non-cancelled successor，唤醒之</span></span><br><span class="line">      <span class="keyword">if</span> (s != <span class="keyword">null</span>)</span><br><span class="line">          LockSupport.unpark(s.thread);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>以上完成<code>reentrantlock</code>的加锁lock.lcok() 过程</p>
<p>在<code>unparkSuccessor</code>为何采用从链表尾部向前遍历，而不是正序遍历去找一个<code>the actual non-cancelled successor</code> ?</p>
<p>首先在AQS的开头源代码注释里面有提到：</p>
<blockquote>
<p>The “prev” links (not used in original CLH locks), are mainly needed to handle cancellation. If a node is cancelled, its successor is (normally) relinked to a non-cancelled predecessor.</p>
<p>prev前驱指针主要用于解决线程节点取消排队的情况（注意CLH locks并没有使用prev前驱指针），如果一个线程节点已经取消，那么它的后驱节点就会重新链接到一个非取消状态的前驱节点</p>
<p>例如  现有阻塞队列结构：node1(非取消)&lt;—&gt;node2(取消)&lt;—&gt;node3(非取消)&lt;—&gt;node4(非取消)，假设现在要将node3设为取消节点，最终队列肯定要变成：node1(非取消)&lt;—&gt;node4(非取消)，过程就是按照cancelAcquire的流程：</p>
<p>①  跳过node3“已经是取消状态”的节点，也即node2</p>
<p>② 此时node1就是node3的后驱节点node4需要挂靠的前驱节点</p>
<p>③ 根据以下逻辑<code>compareAndSetNext(pred, predNext, next)</code>,也即<code>compareAndSetNext(pred=node1, predNext=node3, next=node4);</code>，可以推出：在产生CANCELLED状态节点的时候，先断开的是Next指针，Prev指针并未断开，因此只能利用prev指针从后往前遍历才能够遍历完全部的Node。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (pred != head &amp;&amp;</span><br><span class="line">     ((ws = pred.waitStatus) == Node.SIGNAL ||</span><br><span class="line">         (ws &lt;= <span class="number">0</span> &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp;</span><br><span class="line">        pred.thread != <span class="keyword">null</span>) &#123;</span><br><span class="line">        Node next = node.next;</span><br><span class="line">      	<span class="comment">// 要求即将被删除的node4节点的后驱节点node5既不是null且未取消状态才能将node5挂在node4前驱节点pred的后面</span></span><br><span class="line">        <span class="keyword">if</span> (next != <span class="keyword">null</span> &amp;&amp; next.waitStatus &lt;= <span class="number">0</span>)</span><br><span class="line">            compareAndSetNext(pred, predNext, next);</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="4-7关于阻塞队列本身的其他方法：-Queue-inspection-methods"><a href="#4-7关于阻塞队列本身的其他方法：-Queue-inspection-methods" class="headerlink" title="4.7关于阻塞队列本身的其他方法： Queue inspection methods"></a>4.7关于阻塞队列本身的其他方法： Queue inspection methods</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Queries whether any threads are waiting to acquire. Note that</span></span><br><span class="line"><span class="comment">    * because cancellations due to interrupts and timeouts may occur</span></span><br><span class="line"><span class="comment">    * at any time, a &#123;<span class="doctag">@code</span> true&#125; return does not guarantee that any</span></span><br><span class="line"><span class="comment">    * other thread will ever acquire.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &lt;p&gt;In this implementation, this operation returns in</span></span><br><span class="line"><span class="comment">    * constant time.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if there may be other threads waiting to acquire</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">	<span class="comment">// 返回队列里面是否有正在等待获取锁资源的线程节点，返回true也不代表存在“正在等待获取锁资源的线程节点”，因为“线程取消排队或者timeout”的情况随时可以发生，这会导致head != tail有二义性。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">hasQueuedThreads</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> head != tail;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Queries whether any threads have ever contended to acquire this</span></span><br><span class="line"><span class="comment">    * synchronizer; that is if an acquire method has ever blocked.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &lt;p&gt;In this implementation, this operation returns in</span></span><br><span class="line"><span class="comment">    * constant time.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if there has ever been contention</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">	<span class="comment">// 返回是否曾经有线程出现过竞争同步器以获取锁资源，或者说acquire方法被阻塞过。简单的说：若head不为null，说明有线程节点执行enq方法里面的`compareAndSetHead(new Node())`，也即有竞争</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">hasContended</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> head != <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Returns the first (longest-waiting) thread in the queue, or</span></span><br><span class="line"><span class="comment">    * &#123;<span class="doctag">@code</span> null&#125; if no threads are currently queued.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &lt;p&gt;In this implementation, this operation normally returns in</span></span><br><span class="line"><span class="comment">    * constant time, but may iterate upon contention if other threads are</span></span><br><span class="line"><span class="comment">    * concurrently modifying the queue.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> the first (longest-waiting) thread in the queue, or</span></span><br><span class="line"><span class="comment">    *         &#123;<span class="doctag">@code</span> null&#125; if no threads are currently queued</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"> <span class="comment">// 返回阻塞队列中的第一个（等待时间最长的）线程节点，注意这可不是返回head这个头结点（辅助节点）。如果当前没有线程节点排队，则为null</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Thread <span class="title">getFirstQueuedThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">// handle only fast path, else relay</span></span><br><span class="line">       <span class="comment">// 先快速判断。head==tail说明阻塞队列还未创建，直接返回null即可，否则使用fullGetFirstQueuedThread去获取第一个线程节点。这种“投机性先实施A失败再实施B”的设计其实在ConcurrentHashMap里面的addCount：先尝试对baseCount累加失败则使用fullAddCount确保能够累加计数成功。</span></span><br><span class="line">       <span class="keyword">return</span> (head == tail) ? <span class="keyword">null</span> : fullGetFirstQueuedThread();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * Version of getFirstQueuedThread called when fastpath fails</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> Thread <span class="title">fullGetFirstQueuedThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * The first node is normally head.next. Try to get its</span></span><br><span class="line"><span class="comment">        * thread field, ensuring consistent reads: If thread</span></span><br><span class="line"><span class="comment">        * field is nulled out or s.prev is no longer head, then</span></span><br><span class="line"><span class="comment">        * some other thread(s) concurrently performed setHead in</span></span><br><span class="line"><span class="comment">        * between some of our reads. We try this twice before</span></span><br><span class="line"><span class="comment">        * resorting to traversal.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="comment">// h:head节点的临时变量，s：head节点的后驱节点(successor)临时变量</span></span><br><span class="line">       Node h, s;</span><br><span class="line">       Thread st;<span class="comment">// 后驱节点指向的线程:successorthread</span></span><br><span class="line">     </span><br><span class="line">       <span class="comment">/*</span></span><br><span class="line"><span class="comment">       此设计非常巧妙：</span></span><br><span class="line"><span class="comment">       正常来说，阻塞队列的第一个线程节点就是我们要找的“first queued thread”节点,那么为了确保获取该节点刚开始读的线程节点，必须要保证前后一致性读：不一致性读是如何出现的呢？ 考察这种情况，第一次读node.thread不为空但第二读node.thread已经是null，或者第一次读s.prev是原来的head节点，第二次读s.prev已经不是原来的head节点，这是因为在这两次读的过程中，有其他线程节点正在并发执行setHead操作，那么如何快速实现一致性读呢？ 直接实施两次尝试即可！</span></span><br><span class="line"><span class="comment">       if ( (第一次尝试)|| (第二次尝试))</span></span><br><span class="line"><span class="comment">       第一次尝试：要求head节点不是null且后驱节点s不为null且s.prev还是指向原head节点且后驱节点的thread引用不为null</span></span><br><span class="line"><span class="comment">       第二次尝试： 要求head节点不是null且后驱节点s不为null且s.prev还是指向原head节点且后驱节点的thread引用不为null</span></span><br><span class="line"><span class="comment">       第一次尝试成功说明没有其他线程干扰确实是一致性读，那么head的后驱节点s就是要返回的first queued thread</span></span><br><span class="line"><span class="comment">       第一次尝试失败说明有其他线程正在进行setHead操作，那么需要再投机性的进行第二次尝试。</span></span><br><span class="line"><span class="comment">       如果第一次尝试失败、第二次尝试也失败，那么只能通过遍历阻塞队列的方式去找`first queued thread`</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">       <span class="keyword">if</span> (((h = head) != <span class="keyword">null</span> &amp;&amp; (s = h.next) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">            s.prev == head &amp;&amp; (st = s.thread) != <span class="keyword">null</span>) ||</span><br><span class="line">           ((h = head) != <span class="keyword">null</span> &amp;&amp; (s = h.next) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">            s.prev == head &amp;&amp; (st = s.thread) != <span class="keyword">null</span>))</span><br><span class="line">           <span class="keyword">return</span> st;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * Head&#x27;s next field might not have been set yet, or may have</span></span><br><span class="line"><span class="comment">        * been unset after setHead. So we must check to see if tail</span></span><br><span class="line"><span class="comment">        * is actually first node. If not, we continue on, safely</span></span><br><span class="line"><span class="comment">        * traversing from tail back to head to find first,</span></span><br><span class="line"><span class="comment">        * guaranteeing termination.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">			</span><br><span class="line">     	<span class="comment">/*</span></span><br><span class="line"><span class="comment">			因为head节点next指针可能还未开始设置，又或者有线程在执行setHead过程中head的next还未设置好，因此直接无法利用next指针从链表头部往后遍历。</span></span><br><span class="line"><span class="comment">			所以需要这么做：先检查tail节点不为空时且tail节点不是head节点，说明此时有阻塞队列且有线程节点队列里面，这样就可以safely从链表尾部开始遍历，直到t指针指向head节点时说明t的前驱节点已经来到head位置即可结束遍历，</span></span><br><span class="line"><span class="comment">       */</span> </span><br><span class="line">       Node t = tail;</span><br><span class="line">       Thread firstThread = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">while</span> (t != <span class="keyword">null</span> &amp;&amp; t != head) &#123;</span><br><span class="line">           Thread tt = t.thread;</span><br><span class="line">         	<span class="comment">// firstThread不断更新指向途中经过的“非取消”的线程节点的线程引用</span></span><br><span class="line">         	<span class="comment">// 或者说跳过“取消排队”的线程节点</span></span><br><span class="line">           <span class="keyword">if</span> (tt != <span class="keyword">null</span>)</span><br><span class="line">               firstThread = tt;</span><br><span class="line">           t = t.prev;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> firstThread;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">    * Returns true if the given thread is currently queued.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * &lt;p&gt;This implementation traverses the queue to determine</span></span><br><span class="line"><span class="comment">    * presence of the given thread.</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> thread the thread</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if the given thread is on the queue</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@throws</span> NullPointerException if the thread is null</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">	<span class="comment">// 判断给定线程是否在阻塞队列里面</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">isQueued</span><span class="params">(Thread thread)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (thread == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">     	<span class="comment">// 从链表尾部开始遍历（safely traversing from tail back to head），只要遍历节点的thread==给定的thread引用，返回true。</span></span><br><span class="line">       <span class="keyword">for</span> (Node p = tail; p != <span class="keyword">null</span>; p = p.prev)</span><br><span class="line">           <span class="keyword">if</span> (p.thread == thread)</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">// 还有其他共享模式下的队列方法，这里不在讨论范围。</span></span><br></pre></td></tr></table></figure>
<h4 id="5、解锁过程"><a href="#5、解锁过程" class="headerlink" title="5、解锁过程"></a>5、解锁过程</h4><p>其实只要掌握了加锁lock.lcok() 过程，也即<code>renntrantlock</code>和AQS之间的交互，那么解锁过程则相对节点</p>
<p>调用者：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">lock.unlock() </span><br></pre></td></tr></table></figure>
<p><code>reentrantlock内部的</code>unlock`：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    sync.release(<span class="number">1</span>); <span class="comment">// 加锁每次对state加1，释放锁则对state减1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AQS内部的<code>release</code>方法：这里说明真正释放锁的所有操作都是在AQS内部完成</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Releases in exclusive mode.  Implemented by unblocking one or</span></span><br><span class="line"><span class="comment"> * more threads if &#123;<span class="doctag">@link</span> #tryRelease&#125; returns true.</span></span><br><span class="line"><span class="comment"> * This method can be used to implement method &#123;<span class="doctag">@link</span> Lock#unlock&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arg the release argument.  This value is conveyed to</span></span><br><span class="line"><span class="comment"> *        &#123;<span class="doctag">@link</span> #tryRelease&#125; but is otherwise uninterpreted and</span></span><br><span class="line"><span class="comment"> *        can represent anything you like.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the value returned from &#123;<span class="doctag">@link</span> #tryRelease&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">release</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 这里的tryRelease就是子类Sync具体要实现的方法：将state值进行CAS减1操作</span></span><br><span class="line">    <span class="comment">// (1)如果子类的tryRelease更新state同步状态且state值为0，就会返回true，那么说明外面有线程能够释放锁资源，此时就需要将阻塞队列里面的第一个线程节点唤醒</span></span><br><span class="line">   <span class="comment">// (2) 如果子类的tryRelease更新state同步状态成功且state值不为0，则会返回false，说明当前有同一线程多次重入的锁并准备多次释放锁</span></span><br><span class="line">    <span class="keyword">if</span> (tryRelease(arg)) &#123;</span><br><span class="line">        Node h = head;</span><br><span class="line">      	<span class="comment">/* </span></span><br><span class="line"><span class="comment">      	取出头节点（辅助节点），如果阻塞队列存在或者头节点的后驱节点非取消非初始化，则该后驱节点需要被h唤醒</span></span><br><span class="line"><span class="comment">        ①对于h=null，说明阻塞队列已经不存在或者还未创建，或者刚创建Head=null，但是还未有线程节点入队，显然这些情况都不需要再做“唤醒操作”</span></span><br><span class="line"><span class="comment">        ②对于h不是null，说明头节点已经是一个new Node()辅助节点，此时如果h.waitStatus == 0，说明阻塞队列刚完成初始化，仅有一个head辅助节点，单还未有其他线程节点入队，因此也不需要再做“唤醒操作”</span></span><br><span class="line"><span class="comment">        ③对于h不是null，h.waitStatus != 0取值有哪些呢，因为reentrant是独占模式，waitStatus的取值分别是1，-1，而-2，-3是共享模式不在讨论范围，因此当h.waitStatus=-1，那么恰好head的后驱节点就是需要唤醒的节点当然使用unparkSuccessor逻辑，而h.waitStatus=1，说明head节点的后驱节点是一个“取消排队”的节点，那么需要借助unparkSuccessor方面里面找到一个真正的非取消状态节点（the actual non-cancelled successor）作为要唤醒的节点</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h.waitStatus != <span class="number">0</span>)</span><br><span class="line">            unparkSuccessor(h);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>AQS内部的<code>release</code>方法中的<code>tryRelease</code>调用子类的Sysc的<code>tryRelease</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>&#123;</span><br><span class="line">     <span class="comment">// 读取最新的state值并扣减1</span></span><br><span class="line">    <span class="keyword">int</span> c = getState() - releases;</span><br><span class="line">  	<span class="comment">// 因为是独占模式，要求必须持有锁资源的线程才能去释放锁资源</span></span><br><span class="line">    <span class="keyword">if</span> (Thread.currentThread() != getExclusiveOwnerThread())</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();</span><br><span class="line"><span class="comment">// 表示是否成功释放资源</span></span><br><span class="line">    <span class="keyword">boolean</span> free = <span class="keyword">false</span>;</span><br><span class="line">  	<span class="comment">// 如果c=0，说明当前state值已经是0，说明没有其他线程占用锁资源，将独占线程引用设为null即可，并将释放资源标志设为true</span></span><br><span class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">        free = <span class="keyword">true</span>;</span><br><span class="line">        setExclusiveOwnerThread(<span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  	<span class="comment">// 将state更新为扣减后的c值：state=getState() - releases</span></span><br><span class="line">    setState(c);</span><br><span class="line">    <span class="keyword">return</span> free; <span class="comment">// true或者false将影响AQS的release流程</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="6、唤醒线程节点后的执行流"><a href="#6、唤醒线程节点后的执行流" class="headerlink" title="6、唤醒线程节点后的执行流"></a>6、唤醒线程节点后的执行流</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Convenience method to park and then check if interrupted</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> true&#125; if interrupted</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">parkAndCheckInterrupt</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        LockSupport.park(<span class="keyword">this</span>); <span class="comment">// 这里是线程节点在acquireQueued里面被阻塞自己的地方，</span></span><br><span class="line">      	<span class="comment">// 唤醒后，将会执行下面这一句：返回线程是否中断状态</span></span><br><span class="line">        <span class="keyword">return</span> Thread.interrupted();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 关于Thread.interrupted();</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Tests whether the current thread has been interrupted.  The</span></span><br><span class="line"><span class="comment">     * &lt;i&gt;interrupted status&lt;/i&gt; of the thread is cleared by this method.  In</span></span><br><span class="line"><span class="comment">     * other words, if this method were to be called twice in succession, the</span></span><br><span class="line"><span class="comment">     * second call would return false (unless the current thread were</span></span><br><span class="line"><span class="comment">     * interrupted again, after the first call had cleared its interrupted</span></span><br><span class="line"><span class="comment">     * status and before the second call had examined it).</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;A thread interruption ignored because a thread was not alive</span></span><br><span class="line"><span class="comment">     * at the time of the interrupt will be reflected by this method</span></span><br><span class="line"><span class="comment">     * returning false.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  &lt;code&gt;true&lt;/code&gt; if the current thread has been interrupted;</span></span><br><span class="line"><span class="comment">     *          &lt;code&gt;false&lt;/code&gt; otherwise.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> #isInterrupted()</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@revised</span> 6.0</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">		<span class="comment">//  返回当前线程是否被中断过,线程第一次调用将返回true，表示被中断，若线程两次调用，则第二次返回没被中断过，所以要注意其源码注释说明：after the first call had cleared its interrupted</span></span><br><span class="line">     * status and before the second call had examined it</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">interrupted</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> currentThread().isInterrupted(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里因为线程被唤醒，显然之前在阻塞队列等待的过程就等于“被中断过”，故<code>parkAndCheckInterrupt</code>返回为true，将继续<code>acquireQueued</code>以下的for流程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">acquireQueued</span><span class="params">(<span class="keyword">final</span> Node node, <span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="keyword">final</span> Node p = node.predecessor();</span><br><span class="line">            <span class="keyword">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</span><br><span class="line">                setHead(node);</span><br><span class="line">                p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></span><br><span class="line">                failed = <span class="keyword">false</span>;</span><br><span class="line">                <span class="keyword">return</span> interrupted;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp;</span><br><span class="line">                parkAndCheckInterrupt())</span><br><span class="line">                interrupted = <span class="keyword">true</span>; <span class="comment">// 线程唤醒后parkAndCheckInterrupt返回true，回到for循环，当线程能够获取锁资源成功后，这个线程中断状态interrupted = true就会acquireQueued返回外部的acquire调用者。</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (failed)</span><br><span class="line">            cancelAcquire(node);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过以上流程，<code>acquireQueued</code>返回true，回到调用<code>acquireQueued</code>的流程 <code>lock.lock()--&gt;acquire(1)--&gt;acquireQueued--&gt;selfInterrupt</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Performs lock.  Try immediate barge, backing up to normal</span></span><br><span class="line"><span class="comment">     * acquire on failure.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">            setExclusiveOwnerThread(Thread.currentThread());</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            acquire(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Acquires in exclusive mode, ignoring interrupts.  Implemented</span></span><br><span class="line"><span class="comment"> * by invoking at least once &#123;<span class="doctag">@link</span> #tryAcquire&#125;,</span></span><br><span class="line"><span class="comment"> * returning on success.  Otherwise the thread is queued, possibly</span></span><br><span class="line"><span class="comment"> * repeatedly blocking and unblocking, invoking &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * #tryAcquire&#125; until success.  This method can be used</span></span><br><span class="line"><span class="comment"> * to implement method &#123;<span class="doctag">@link</span> Lock#lock&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arg the acquire argument.  This value is conveyed to</span></span><br><span class="line"><span class="comment"> *        &#123;<span class="doctag">@link</span> #tryAcquire&#125; but is otherwise uninterpreted and</span></span><br><span class="line"><span class="comment"> *        can represent anything you like.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class="line">        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class="line">        selfInterrupt();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么将继续执行<code>selfInterrupt</code>逻辑：也即线程进入阻塞队列，被阻塞然后唤醒后（自己被中断过）拿到锁，最后线程自己中断一下，为何拿到锁后还要设计线程中断自己？这是什么逻辑？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Convenience method to interrupt current thread.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">selfInterrupt</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Thread.currentThread().interrupt();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>解释：</p>
<p>（1）先考察一个线程使用<code>lock.lock</code>且在<code>compareAndSetState</code>就直接拿到锁资源的情况（意味着它没有进入阻塞队列），显然整个过程线程自己没有被<code>LockSupport.park(this)</code>阻塞，因此这种线程无需执行<code>selfInterrupt</code> 操作，这样外界有其他逻辑检查这个线程的中断状态时就会知道它未中断过。</p>
<p>（2）由于Doug Lea采用这样的设计：Acquires in exclusive uninterruptible mode for thread already in queue（已在阻塞队列中的线程节点以独占且不可中断（不响应中断）的模式去等待获取锁资源）。只要有一个线程进入阻塞队列然后阻塞自己最后被唤醒且拿到锁的过程显然线程自己是有“中断过”的情况，因此拿到锁后，需要给自己补充一个“中断过”的标记，这样外界有其他逻辑检查这个线程的中断状态时就会知道它曾经中断过。</p>
<h4 id="7、reentrantlock的公平锁模式"><a href="#7、reentrantlock的公平锁模式" class="headerlink" title="7、reentrantlock的公平锁模式"></a>7、reentrantlock的公平锁模式</h4><p>有了以上基础，那么分析公平锁则显得简单一些。内部使用的是 FairSync内部同步器实现相关逻辑</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> ReentrantLock lock=<span class="keyword">new</span> ReentrantLock(<span class="keyword">true</span>);  <span class="comment">// 构造器指定公平锁方式</span></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates an instance of &#123;<span class="doctag">@code</span> ReentrantLock&#125; with the</span></span><br><span class="line"><span class="comment">     * given fairness policy.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> fair &#123;<span class="doctag">@code</span> true&#125; if this lock should use a fair ordering policy</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">(<span class="keyword">boolean</span> fair)</span> </span>&#123;</span><br><span class="line">        sync = fair ? <span class="keyword">new</span> FairSync() : <span class="keyword">new</span> NonfairSync();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p><code>FairSync</code>的实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sync object for fair locks</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">FairSync</span> <span class="keyword">extends</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">3000897897090466540L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        acquire(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Fair version of tryAcquire.  Don&#x27;t grant access unless</span></span><br><span class="line"><span class="comment">     * recursive call or no waiters or is first.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">  	<span class="comment">// 以下是体现公平锁的设计思想：</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Thread current = Thread.currentThread();</span><br><span class="line">        <span class="keyword">int</span> c = getState();</span><br><span class="line">      	<span class="comment">//若state同步状态变量此时为0，那么说明线程可以直接去竞争锁资源</span></span><br><span class="line">        <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">         <span class="comment">/* ① 请求独占锁前先问问阻塞队列有无阻塞线程正在等待锁（这就体现公平的原则，后者当然要等先到者）</span></span><br><span class="line"><span class="comment">          如果阻塞队列有正在排队的线程节点，那么当前新来的线程就无法设置state值，只能去到③（假设非重入线程） 返回false，最终可能是需要进入acquireQueued逻辑把自己入队处理，因此这里的逻辑正是体现了获取锁资源的“公平性”</span></span><br><span class="line"><span class="comment">         */</span> </span><br><span class="line">            <span class="keyword">if</span> (!hasQueuedPredecessors() &amp;&amp;</span><br><span class="line">                compareAndSetState(<span class="number">0</span>, acquires)) &#123;</span><br><span class="line">                setExclusiveOwnerThread(current);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ② 如果当前线程就是持有锁的线程，当然可以让它多次重入，只要不超过重入次数最大值即可。</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextc = c + acquires;</span><br><span class="line">            <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">&quot;Maximum lock count exceeded&quot;</span>);</span><br><span class="line">            setState(nextc);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ③</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>加锁请求锁资源为<code>acquire(1)</code>是AQS的内部的<code>acquire</code>方法，再调用子类的<code>tryAcquire</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!tryAcquire(arg) &amp;&amp;</span><br><span class="line">        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</span><br><span class="line">        selfInterrupt();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此完整过程是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lock.lock() --&gt; lock内部的acquire(1) --&gt; FairSync内部的tryAcquire--&gt; 查询阻塞队列是否有线程节点正在排队 --&gt; 如果tryAcquire失败则进入AQS的acquireQueued内部逻辑</span><br></pre></td></tr></table></figure>
<p>hasQueuedPredecessors的设计逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     Queries whether any threads have been waiting to acquire longer than the current thread.</span></span><br><span class="line"><span class="comment">     查询是否有线程节点的等待获取锁资源时间长于当前线程。</span></span><br><span class="line"><span class="comment">     或者用以下表达式也可以达到相同目的，但是hasQueuedPredecessors比它更高效</span></span><br><span class="line"><span class="comment"> 		 An invocation of this method is equivalent to (but may be more efficient than):</span></span><br><span class="line"><span class="comment">     获取第一个线程节点 != 当前节点且阻塞队列不为空，则返回true，说明当前节点排还不能马上去抢锁资源需要排在阻塞队列的线程节点之后</span></span><br><span class="line"><span class="comment">     如果阻塞队列为空，则返回false，当前节点可以马上去获取锁资源</span></span><br><span class="line"><span class="comment">     getFirstQueuedThread() != Thread.currentThread() &amp;&amp; hasQueuedThreads()</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">     由于中断或者timeout导致线程节点的“取消排队”的情况在任何时候都可以发生，因此返回true也不能保证其他线程就是比当前线程更早申请锁资源</span></span><br><span class="line"><span class="comment">     同理，返回false也不能说明现在阻塞队列是空，因为return false的瞬间可能会有其他线程成功竞争到进入排队</span></span><br><span class="line"><span class="comment">     Note that because cancellations due to interrupts and</span></span><br><span class="line"><span class="comment">     timeouts may occur at any time, a &#123;<span class="doctag">@code</span> true&#125; return does not</span></span><br><span class="line"><span class="comment">     guarantee that some other thread will acquire before the current</span></span><br><span class="line"><span class="comment">     thread.  Likewise, it is possible for another thread to win a</span></span><br><span class="line"><span class="comment">     race to enqueue after this method has returned &#123;<span class="doctag">@code</span> false&#125;,</span></span><br><span class="line"><span class="comment">     due to the queue being empty.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">hasQueuedPredecessors方法用于公平同步器使用以避免线程之间的碰撞竞争。</span></span><br><span class="line"><span class="comment">This method is designed to be used by a fair synchronizer to avoid barging. Such a synchronizer&#x27;s tryAcquire method should return false, and its tryAcquireShared method should return a negative value, if this method returns true (unless this is a reentrant acquire). For example, the tryAcquire method for a fair, reentrant, exclusive mode synchronizer might look like this:</span></span><br><span class="line"><span class="comment">公平的、可重入的、独占模式的同步器内部定义的tryAcquire应该是以下流程：</span></span><br><span class="line"><span class="comment"> protected boolean tryAcquire(int arg) &#123;</span></span><br><span class="line"><span class="comment">   if (isHeldExclusively()) &#123;</span></span><br><span class="line"><span class="comment">   		// 表示同一线程重入获取锁资源，只需要增加state值即可</span></span><br><span class="line"><span class="comment">     // A reentrant acquire; increment hold count</span></span><br><span class="line"><span class="comment">     return true;</span></span><br><span class="line"><span class="comment">   &#125; else if (hasQueuedPredecessors()) &#123;</span></span><br><span class="line"><span class="comment">   	//如果阻塞队列里面有线程节点那么当前线程不能马上acquire到锁资源，要等其他线程节点，因此返回false</span></span><br><span class="line"><span class="comment">     return false;</span></span><br><span class="line"><span class="comment">   &#125; else &#123;</span></span><br><span class="line"><span class="comment">   	 // 其他情况说明锁资源没有其他线程竞争，当前线程节点可以直接去acquire</span></span><br><span class="line"><span class="comment">     // try to acquire normally</span></span><br><span class="line"><span class="comment">   &#125;</span></span><br><span class="line"><span class="comment"> &#125;</span></span><br><span class="line"><span class="comment"> 以上的设计流程就是Reentrantlock里面公平锁FairSync的tryAcquire实现流程</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">     （1）如果有一个排队线程在当前线程之前则返回true，例如公平锁的FairSync，外面新来的当前线程使用tryAcquire则需要判断阻塞队列是否有阻塞线程，如果有，那么当前线程就没办法马上拿到锁，只能等，这就体现公平原则：先来先得，等得越久的那个线程节点优先获取锁资源</span></span><br><span class="line"><span class="comment">     （2）如果当前线程恰好位于第一个线程节点或者阻塞队列没有线程节点，则返回false</span></span><br><span class="line"><span class="comment">true if there is a queued thread preceding the current thread, and false if the current thread is at the head of the queue or the queue is empty</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">hasQueuedPredecessors</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// The correctness of this depends on head being initialized</span></span><br><span class="line">        <span class="comment">// before tail and on head.next being accurate if the current</span></span><br><span class="line">        <span class="comment">// thread is first in queue.</span></span><br><span class="line">      </span><br><span class="line">        Node t = tail; <span class="comment">// Read fields in reverse initialization order</span></span><br><span class="line">        Node h = head;</span><br><span class="line">        Node s;</span><br><span class="line">      	<span class="comment">// 返回true条件： </span></span><br><span class="line">      	<span class="comment">// 情况1：head辅助节点已经完成初始化也即：compareAndSetHead(new Node())  且 head的后驱节点第一个线程节点是null</span></span><br><span class="line">        <span class="comment">// 情况2：head辅助节点已经完成初始化也即：compareAndSetHead(new Node())  且 head的后驱节点第一个线程节点不是当前节点</span></span><br><span class="line">        <span class="keyword">return</span> h != t &amp;&amp;((s = h.next) == <span class="keyword">null</span> || s.thread != Thread.currentThread());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="8、到此可以总结公平和非公平的锁的区别"><a href="#8、到此可以总结公平和非公平的锁的区别" class="headerlink" title="8、到此可以总结公平和非公平的锁的区别"></a>8、到此可以总结公平和非公平的锁的区别</h4><p>非公平：直接CAS竞争state这个锁资源，不需要询问阻塞队列是否有线程节点正在等待获取锁资源</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        setExclusiveOwnerThread(Thread.currentThread());</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        acquire(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>公平：当state为0时，需要询问阻塞队列是否有线程节点正在等待获取锁资源，如果有则无法马上获取锁资源，需要等待</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Thread current = Thread.currentThread();</span><br><span class="line">    <span class="keyword">int</span> c = getState();</span><br><span class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!hasQueuedPredecessors() &amp;&amp;</span><br><span class="line">            compareAndSetState(<span class="number">0</span>, acquires)) &#123;</span><br><span class="line">            setExclusiveOwnerThread(current);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>非公平的优点：线程并发性能高，因为所有新来的线程先通过<code>compareAndSetState(0, 1)</code>竞争锁资源，如果成功就能马上返回，无需经过排队。</p>
<p>非公平的缺点：因为新来的线程很快通过<code>compareAndSetState(0, 1)</code>提前获得锁资源，导致阻塞队列的线程节点可能一直没有机会出队获取锁资源，也即出现“线程饥饿”</p>
<p>线程饿死的解释：</p>
<blockquote>
<p>If a thread is not granted CPU time because other threads grab it all, it is called “starvation”. The thread is “starved to death” because other threads are allowed the CPU time instead of it. The solution to starvation is called “fairness” - that all threads are fairly granted a chance to execute.</p>
</blockquote>
<p>公平的优点：不会出现线程饿死，保证每个新来的线程都有机会获得竞争锁资源</p>
<p>公共的缺点：线程并发性能可能比非公平锁低一些。</p>
]]></content>
      <categories>
        <category>Java高级主题</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
</search>
