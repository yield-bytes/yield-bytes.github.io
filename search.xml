<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>VMware虚拟化——使用 vCenter Server Appliance6.7（VCSA6.7）实现虚拟机不停机迁移</title>
    <url>/blog/2019/06/07/%20VMware%E8%99%9A%E6%8B%9F%E5%8C%96%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%20vCenter%20Server%20Appliance6.7%EF%BC%88VCSA6.7%EF%BC%89%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8D%E5%81%9C%E6%9C%BA%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文给出基于最新版的VCSA6.7实现虚拟机业务从当前运行的ESXI主机不停机无缝迁移至另外一台ESXI主机的相关内容。该功能是VMware公司相当惊艳的一个技术，在某几种业务场景都非常需要此类需求。</p>
<p>（1）例如多台服务器ESXI主机分别位于局域网的不同物理位置，例如不同楼层，不同数据中心，那么按以往物理机业务迁移，显然需要把当前物理机所有业务代码和存储数据拷贝在另外一台物理服务器上，把新物理服务器跑起来，两者做lvs负载均衡后，把原物理机停掉，以此达到“业务不中断，且把服务器从一个物理位置换到另外一个物理位置”</p>
<p>（2）再或者采用简单粗暴的迁移方式，直接停掉当前物理机，当然业务会中断，把该物理机搬运至到其他物理位置，再跑起来</p>
<a id="more"></a>

<p>&#8195;&#8195;显然以上两种方式在当下IT技术看来，显得不够smart。接下来要介绍的是使用“不中断、不用搬运物理机”方式，将原业务服务“迁移”到新物理位置的服务器上（原业务非虚拟化环境，可先处理为虚拟化，解决方案自行处理，并不复杂。）。此类功能的配置，在很多文章也有提到，但个人看来还是比较浅薄，大多文章只给出配置成功的过程，却不分享配置过程出错的原因分析。</p>
<p>在VCSA管理中心（IP地址简单标示为：43），在其数据中心分组的一个主机集群，已经添加两台ESXI主机，一台为跑着业务的ESXI主机39（取IP地址最后两位作为标示），一台为接纳迁移虚拟机的ESXI主机40</p>
<p>（1）第一次迁移，对话框初步校验失败，提示目标主机40 Vmotion接口未配置，无法进行下一步操作</p>
<p><img src="https://img-blog.csdnimg.cn/20190616114139330.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这是因为ESXI39主机的VMkernel 虚拟网卡和 ESXI40主机的VMkernel虚拟网卡设置中，没有容许“V motion”报文通过，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616151332884.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p> 只有两台主机的VMkernel端口设置一致且打开Vmotion服务，可通过“迁移”的第一步的测试条件。这里不再贴图。</p>
<p>（2）解决第一个问题后，重写再操作迁移，走了几步后，发现又出现迁移条件测试不通过的提示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616153053311.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>“当前已连接的网络接口”“Network adapter 1”无法使用网络“VM Network”，因为 “在目标主机上为目标网络配置的卸载或安全策略不同于在源主机上为源网络配置的卸载或安全策略”</p>
<p>这个提示，是指目标ESXi主机40的“VM Network”模块配置的内容与ESXi主机39的不同，导致无法传输Vmotion数据包</p>
<p>这里的VM Network是指什么？对应下图内容：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616154049585.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>原理是ESXi主机内的网络模块：虚拟交换机下的某个端口组，这里有两个端口组，一个是管理network，一个是虚拟机的network。</p>
<p>查看40主机的VMnetwork属性（所有属性），如下图，在上面网络接口出错的关键字“安全策略”，就是对应VMnetwork的设置里的安全选项和策略选项，对比主机39，发现40的安全项目里：含杂模式设为拒绝，而39的设置为接受。所以两台主机的VM network 配置参数不一致，无法通过vmotion的校验。</p>
<p><img src="https://img-blog.csdnimg.cn/20190616162231176.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>综上，但这些问题或者设置都搞清楚后，在千兆局域网内，迁移一个几十G内存+1TB的虚拟机到另外一台ESXI主机上，仅需10分钟左右，全过程不停机，仅出现几次丢包。</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>迁移虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos7配置docker和docker-compose环境</title>
    <url>/blog/2019/09/04/Centos7%E9%85%8D%E7%BD%AEdocker%E5%92%8Cdocker-compose%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>&#8195;&#8195;在测试机上搭建docker以及docker-compose环境目的还是为了快速构成开发环境，无需在裸机上为项目配置各种繁琐部署。</p>
<h4 id="1、安装docker"><a href="#1、安装docker" class="headerlink" title="1、安装docker"></a>1、安装docker</h4><p>查看centos版本以及内核版本，docker仅支持3.10以上的内核</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.6.1810 (Core) </span><br><span class="line">[root@dn2 ~]# uname -r</span><br><span class="line">3.10.0-957.27.2.el7.x86_64</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p><strong>更新yum包</strong></p>
<p><code>yum update</code></p>
<p><strong>安装必要的功能包</strong></p>
<p>yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动的依赖包</p>
<p><code>yum install -y yum-utils device-mapper-persistent-data lvm2</code></p>
<p><strong>新增阿里的docker镜像源</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在yum的repos.d目录下新增了一个docker-ce.repo</span></span><br><span class="line">[root@dn2 yum.repos.d]# ls</span><br><span class="line">CentOS-Base.repo      CentOS-Debuginfo.repo  CentOS-Sources.repo</span><br><span class="line">CentOS-Base.repo.bak  CentOS-fasttrack.repo  CentOS-Vault.repo</span><br><span class="line">CentOS-CR.repo        CentOS-Media.repo      docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看其镜像源地址，可以看到阿里镜像提供的stable版本</span></span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-stable-debuginfo]</span><br><span class="line">name=Docker CE Stable - Debuginfo $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/debug-$basearch/stable</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span><br></pre></td></tr></table></figure>

<p><strong>更新yum缓存</strong></p>
<p><code>yum makecache fast</code></p>
<p><strong>安装docker-ce 社区包</strong></p>
<p><code>yum -y install docker-ce</code></p>
<p><strong>启动docker以及开机自启</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line">systemctl enbale docker</span><br></pre></td></tr></table></figure>

<p><strong>查看docker版本</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# docker version         </span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           19.03.1</span><br><span class="line"> API version:       1.40</span><br><span class="line"> Go version:        go1.12.5</span><br><span class="line"> Git commit:        74b1e89</span><br><span class="line"> Built:             Thu Jul 25 21:21:07 2019</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Experimental:      false</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          19.03.1</span><br><span class="line">  API version:      1.40 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.12.5</span><br><span class="line">  Git commit:       74b1e89</span><br><span class="line">  Built:            Thu Jul 25 21:19:36 2019</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.2.6</span><br><span class="line">  GitCommit:        894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.0.0-rc8</span><br><span class="line">  GitCommit:        425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.18.0</span><br><span class="line">  GitCommit:        fec3683     </span><br></pre></td></tr></table></figure>

<p><strong>docker镜像加速</strong></p>
<p>这里的镜像是dockerhub的镜像，如果不设置为国内的docker镜像源，那么当使用docker pull 有些容量大镜像时，因走的官网下载链路，下载速度异常慢</p>
<p>通过新建/etc/docker/daemon.json文件设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># vi &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;http:&#x2F;&#x2F;hub-mirror.c.163.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line"># 重启docker服务</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<h4 id="2、安装docker-compose"><a href="#2、安装docker-compose" class="headerlink" title="2、安装docker compose"></a>2、安装docker compose</h4><p>用于编排容器以及docker自动化部署，非常出色的容器编排工具</p>
<p>官网版本发布地址：<a href="https://github.com/docker/compose/releases">release</a></p>
<p>官网：<a href="https://docs.docker.com/compose/install/">install</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接安装二进制文件</span></span><br><span class="line">curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br><span class="line"><span class="meta">#</span><span class="bash"> 给docker-compose 加入可执行权限</span></span><br><span class="line">chmod +x /usr/local/bin/docker-compose </span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">[root@dn2 opt]# docker-compose --version</span><br><span class="line">docker-compose version 1.24.0, build 0aa59064</span><br></pre></td></tr></table></figure>
<p>以上完成docker和docker compose的环境部署，有关更多docker以及项目部署的文章会放在“docker”专栏里面。</p>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title>Python开发常用的虚拟环境管理配置</title>
    <url>/blog/2019/11/18/Python%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E7%9A%84%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>&#8195;&#8195;在某些python的开发项目中，或跑一些demo，例如tensorflow的demo，要求python3.5以上的版本，若原系统环境只有python2.7.5，显然无法满足测试环境。若为系统安装python3.5+，有些库又会造成版本冲突，因此需要使用python的虚拟环境工具来解决这些矛盾。当然也可采用python的docker镜像，使用一个镜像独立环境运行项目，但相比于python虚拟化工具来说，这种docker镜像显得有点重。</p>
<a id="more"></a>

<h4 id="1、python虚拟工具介绍"><a href="#1、python虚拟工具介绍" class="headerlink" title="1、python虚拟工具介绍"></a>1、python虚拟工具介绍</h4><p>目前有几种方式创建python的虚拟环境，在python3中有标准库venv，而第三方库例如virtualenv、virtualenvwrapper、pyenv，那么在项目或者说在实际开发里面，选哪种工具更为适合？</p>
<h5 id="1-1-virtualenv"><a href="#1-1-virtualenv" class="headerlink" title="1.1 virtualenv"></a>1.1 virtualenv</h5><p>virtualenv 是目前较为常用的 python 虚拟环境配置工具。它不仅同时支持 python2 和 python3，而且可以为每个虚拟环境指定 python 解释器（要求系统已经安装了不同版本的python），并选择不继承基础版本的site-packages。</p>
<h5 id="1-2-virtualenvwrapper"><a href="#1-2-virtualenvwrapper" class="headerlink" title="1.2 virtualenvwrapper"></a>1.2 virtualenvwrapper</h5><p>virtualenvwrapper是virtualenv的一个封装，目的是使后者更好用。virtualenv在使用中，每次得去虚拟环境所在目录下的 bin 目录下 source  activate，也即当有多个虚拟环境时，得每次都去找对应的目录，virtualenvwrapper将所有的虚拟环境目录全都集中起来统一管理，避免每次开启虚拟环境时候的source 项目目录操作。</p>
<h5 id="1-3-venv"><a href="#1-3-venv" class="headerlink" title="1.3 venv"></a>1.3 venv</h5><p>Python 从3.3 版本开始，自带了一个虚拟环境 <a href="https://docs.python.org/3/library/venv.html">venv</a>，在 <a href="http://legacy.python.org/dev/peps/pep-0405/">PEP-405</a> 中可以看到它的详细介绍。它的很多操作都和 virtualenv 类似。也支持linux和win。venv也有局限性，例如当前系统python版本为3.5，那么venv只能在当前安装的python3.5版本，不能创建其它Python 3.x的版本以及Python 2的环境。</p>
<h5 id="1-4-pyenv"><a href="#1-4-pyenv" class="headerlink" title="1.4 pyenv"></a>1.4 pyenv</h5><p><code>pyenv</code>主要用来安装、管理Python的版本及其虚拟环境，比如一个项目需要Python2.x，一个项目需要Python3.x。而virtualenv主要用来管理Python包的依赖。不同项目需要依赖的包版本不同，则需要使用虚拟环境。<code>pyenv</code>通过系统修改环境变量来实现Python不同版本的切换。前面的三个工具都是用于虚拟环境切换，pyenv是 Python 版本环境切换工具，将这两套工具结合使用，可以完美解决 python 多版本环境的问题。具体实例在第4节给出。</p>
<h4 id="2、使用virtualenv创建和管理虚拟环境"><a href="#2、使用virtualenv创建和管理虚拟环境" class="headerlink" title="2、使用virtualenv创建和管理虚拟环境"></a>2、使用virtualenv创建和管理虚拟环境</h4><h5 id="2-1-为多个python版本安装相应的pip"><a href="#2-1-为多个python版本安装相应的pip" class="headerlink" title="2.1 为多个python版本安装相应的pip"></a>2.1 为多个python版本安装相应的pip</h5><p>centos7.5默认没有pip包，因此需求手动安装，本文系统已经安装python2.7和python3.6。这里给出python2.7的pip安装和python3.6的pip3安装。<br>pip安装依赖setuptools，首先为python2和python3安装相应setuptools</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost local]# pwd</span><br><span class="line">/usr/local</span><br><span class="line">[root@localhost local]# wget https://files.pythonhosted.org/packages/ab/41/ab6ae1937191de0c9cbc115d0e91e335f268aa1cd85524c86e5970fdb68a/setuptools-42.0.0.zip</span><br><span class="line">[root@localhost local]# unzip setuptools-42.0.0.zip</span><br><span class="line">[root@localhost local] cd setuptools-42.0.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里的python命令是连接到python2.7，所以setuptools库只在python2.7环境生效</span></span><br><span class="line">[root@localhost setuptools-42.0.0]# python2.7 setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给python3.6安装setuptools</span></span><br><span class="line">[root@localhost setuptools-42.0.0]# python3.6 setup.py install</span><br></pre></td></tr></table></figure>

<p>这里为何使用python2.7或者python3.6，因为如果想要构建更多python版本，其shell执行命令例如python3.5，python3.7则会显得清晰而不混乱。</p>
<p>为python2和python3安装相应pip，跟setuptools安装流程一致。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@localhost local]# pwd</span><br><span class="line">/usr/local</span><br><span class="line">[root@localhost local] wget https://files.pythonhosted.org/packages/ce/ea/9b445176a65ae4ba22dce1d93e4b5fe182f953df71a145f557cffaffc1bf/pip-19.3.1.tar.gz</span><br><span class="line">[root@localhost local]# unzip pip-19.3.1.tar.gz</span><br><span class="line">[root@localhost local] cd pip-19.3.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 为python2.7 安装pip，最终命令执行路径在：/usr/<span class="built_in">local</span>/bin/pip</span></span><br><span class="line">[root@localhost pip-19.3.1] python2.7 setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 为pytho3.6 安装pip，最终命令执行路径：/usr/<span class="built_in">local</span>/bin/pip3</span></span><br><span class="line">[root@localhost pip-19.3.1] python3.6 setup.py install</span><br></pre></td></tr></table></figure>


<h5 id="2-2-安装virtualenv"><a href="#2-2-安装virtualenv" class="headerlink" title="2.2  安装virtualenv"></a>2.2  安装virtualenv</h5><p>上面已经配置了python2.7环境和python3.6环境，virtualenv库无需在两种环境安装，这里安装到python2.7库下即可。</p>
<p>这里要注意：如果系统已经安装python3版本，且shell已经设定python命令是软链接到python3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# ls -al /usr/bin/python</span><br><span class="line">**** /usr/bin/python -&gt; /usr/bin/python3</span><br></pre></td></tr></table></figure>

<p>那么需要使用pip3安装 virtualenv，这样virtualenv库才会安装到python3的site-packages目录下，</p>
<p>如果shell已经设定python命令是软链接到python2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# ls -al /usr/bin/python</span><br><span class="line">**** /usr/bin/python -&gt; /usr/bin/python2</span><br></pre></td></tr></table></figure>

<p>那么需要使用pip安装 virtualenv，这样virtualenv库才会安装到python2的site-packages目录下。</p>
<p>若不按照上述的环境情况，安装virtualenv，会出现以下情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# virtualenv -v</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/virtualenv&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    import virtualenv</span><br><span class="line">ModuleNotFoundError: No module named &#x27;virtualenv&#x27;</span><br></pre></td></tr></table></figure>

<p>当前python是软链到python3，而pip安装的virtualenv是在python2.7路径下，python3并没有virtualenv这个库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# python3</span><br><span class="line">Python 3.6.8 (default, Apr 25 2019, 21:02:35) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import virtualenv</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">ModuleNotFoundError: No module named &#x27;virtualenv</span><br></pre></td></tr></table></figure>

<p>virtualenv在python2.7路径下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# python</span><br><span class="line">Python 2.7.5 (default, Aug  7 2019, 00:51:29) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import virtualenv</span></span><br></pre></td></tr></table></figure>

<p>解决办法：将当前python命令软链到python2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]# ln -s /usr/bin/python2 /usr/bin/python</span><br></pre></td></tr></table></figure>


<h5 id="2-3、virtualenv创建虚拟环境"><a href="#2-3、virtualenv创建虚拟环境" class="headerlink" title="2.3、virtualenv创建虚拟环境"></a>2.3、virtualenv创建虚拟环境</h5><p>在/opt/pvenv_test目录下，创建一个使用python2.7解释器、且独立安装第三方库的运行环境、名字为crmapp的python2.7项目环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost pvenv_test]# virtualenv  --python=python2.7    --no-site-packages  crmapp</span><br><span class="line">Running virtualenv with interpreter /usr/bin/python2.7</span><br><span class="line">Already using interpreter /usr/bin/python2.7</span><br><span class="line">  No LICENSE.txt / LICENSE found in source</span><br><span class="line">New python executable in /opt/pvenv_test/crmapp/bin/python2.7</span><br><span class="line">Also creating executable in /opt/pvenv_test/crmapp/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...</span><br><span class="line"></span><br><span class="line">[root@localhost pvenv_test]# ls</span><br><span class="line">crmapp</span><br><span class="line">[root@localhost crmapp]# ls</span><br><span class="line">bin  include  lib  lib64</span><br><span class="line"><span class="meta">#</span><span class="bash"> 相关执行命令在bin目录下</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 激活当前环境</span></span><br><span class="line">[root@localhost crmapp]# source bin/activate</span><br><span class="line">(crmapp) [root@localhost crmapp]# </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前虚拟环境pip包没有引入系统的pip包，从而实现包不冲突</span></span><br><span class="line">(crmapp) [root@localhost crmapp]# pip list</span><br><span class="line">pip (9.0.1)</span><br><span class="line">setuptools (28.8.0)</span><br><span class="line">wheel (0.29.0)</span><br></pre></td></tr></table></figure>

<p>在/opt/pvenv_test目录下，创建一个使用python3.6解释器、且独立安装第三方库的运行环境、名字为djwebsocket的python3.6项目环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost pvenv_test]# virtualenv  --python=python3.6    --no-site-packages  djwebsocket</span><br><span class="line">Running virtualenv with interpreter /usr/bin/python3.6</span><br><span class="line">Already using interpreter /usr/bin/python3.6</span><br><span class="line">Using base prefix &#x27;/usr&#x27;</span><br><span class="line">  No LICENSE.txt / LICENSE found in source</span><br><span class="line">New python executable in /opt/pvenv_test/djwebsocket/bin/python3.6</span><br><span class="line">Also creating executable in /opt/pvenv_test/djwebsocket/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...</span><br><span class="line">done.</span><br><span class="line"></span><br><span class="line">[root@localhost pvenv_test]# source djwebsocket/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到当前pip包为干净的</span></span><br><span class="line">(djwebsocket) [root@localhost pvenv_test]# pip list</span><br><span class="line">Package    Version</span><br><span class="line">---------- -------</span><br><span class="line">pip        19.3.1 </span><br><span class="line">setuptools 42.0.0 </span><br><span class="line">wheel      0.33.6 </span><br></pre></td></tr></table></figure>
<p>这里的 –python=python3  其实就是 /usr/bin/python3对于的python3.6解释器</p>
<h4 id="3、pyenv终极python版本和虚拟化环境管理工具"><a href="#3、pyenv终极python版本和虚拟化环境管理工具" class="headerlink" title="3、pyenv终极python版本和虚拟化环境管理工具"></a>3、pyenv终极python版本和虚拟化环境管理工具</h4><p>有关virtualenvwrapper或者venv的用法，因内容较为简单，这里不再讨论，本文推荐使用pyenv管理任意基于python项目的虚拟环境。</p>
<h5 id="3-1-pyenv的设计原理"><a href="#3-1-pyenv的设计原理" class="headerlink" title="3.1 pyenv的设计原理"></a>3.1 pyenv的设计原理</h5><p><code>pyenv</code>主要用来管理Python的版本，比如一个项目需要Python2.x，一个项目需要Python3.x。而virtualenv主要用来管理Python包的依赖。不同项目需要依赖的包版本不同，则需要使用虚拟环境。</p>
<p><code>pyenv</code>通过系统修改环境变量来实现Python不同版本的切换。而vitualenv通过Python包安装到一个目录来作为Python虚拟包环境，通过切换目录来实现不同包环境间的切换。</p>
<p><code>pyenv</code>的设计巧妙的地方在于，在PATH 的最前面插入了一个垫片路径（shims）：~/.pyenv/shims:/usr/local/bin:/usr/bin:/bin。所有对 Python 可执行文件的查找都会首先被这个 shims 路径截获，从而使后方的系统路径失效。</p>
<p>对于系统环境变量 PATH ，里面包含了一串由冒号分隔的路径，例如 /usr/local/bin:/usr/bin:/bin。每当在系统中执行一个命令时，例如 python 或 pip，操作系统就会在 PATH 的所有路径中从左至右依次寻找对应的命令。因为是依次寻找，因此排在左边的路径具有更高的优先级。在PATH 最前面插入一个 <code>$(pyenv root)/shims </code>目录，<code>$(pyenv root)/shims</code>目录里包含名称为python以及pip等可执行脚本文件；当用户执行python或pip命令时，根据查找优先级，系统会优先执行shims目录中的同名脚本。pyenv 正是通过这些脚本，来灵活地切换至我们所需的Python版本。</p>
<p>需要手工去查找python版本的所在路径，如果有多个版本，这种手工管理显得有点繁琐。</p>
<h5 id="3-2-安装pyenv"><a href="#3-2-安装pyenv" class="headerlink" title="3.2 安装pyenv"></a>3.2 安装pyenv</h5><p>pyenv安装python需要依赖底层的系统库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost bin] yum install -y gcc make patch gdbm-devel openssl-devel sqlite-devel readline-devel zlib-devel bzip2-devel ncurses-devel libffi-devel</span><br></pre></td></tr></table></figure>

<p>若系统库不全，pyenv安装python后，会有如下提示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# pyenv install 3.7.5</span><br><span class="line">Installing Python-3.7.5...</span><br><span class="line">WARNING: The Python bz2 extension was not compiled. Missing the bzip2 lib?</span><br><span class="line">WARNING: The Python readline extension was not compiled. Missing the GNU readline lib?</span><br><span class="line">WARNING: The Python sqlite3 extension was not compiled. Missing the SQLite3 lib?</span><br><span class="line">Installed Python-3.7.5 to /root/.pyenv/versions/3.7.5</span><br></pre></td></tr></table></figure>

<p>有三个warning提示，系统环境缺少 bzip2、readline、SQLite3。</p>
<p>下载pyenv</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget git <span class="built_in">clone</span> https://github.com/pyenv/pyenv.git ~/.pyenv</span><br></pre></td></tr></table></figure>

<p>将<code>PYENV_ROOT</code>和<code>pyenv init</code>加入bash的<code>~/.bashrc</code>（或zsh的<code>~/.zshrc</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &#x27;export PATH=~/.pyenv/bin:$PATH&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &#x27;export PYENV_ROOT=~/.pyenv&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &#x27;eval &quot;$(pyenv init -)&quot;&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>激活<code>pyenv</code>（zsh为<code>~/.zshrc</code>）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>指定python版本在线安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost bin]# pyenv install 3.7.5</span><br></pre></td></tr></table></figure>

<p>常用命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyenv install --list # 列出可安装版本</span><br><span class="line">pyenv install &lt;version&gt; # 安装对应版本</span><br><span class="line">pyenv uninstall &lt;version&gt; # 卸载对应版本的python</span><br><span class="line">pyenv uninstall &lt;venv name&gt; # 删除指定虚拟环境</span><br><span class="line">pyenv install -v &lt;version&gt; # 安装对应版本，若发生错误，可以显示详细的错误信息</span><br><span class="line">pyenv versions # 显示当前使用的python版本</span><br><span class="line">pyenv which python # 显示当前python安装路径</span><br><span class="line">pyenv global &lt;version&gt; # 设置默认Python版本</span><br></pre></td></tr></table></figure>


<h5 id="3-3pyenv离线安装python各版本"><a href="#3-3pyenv离线安装python各版本" class="headerlink" title="3.3pyenv离线安装python各版本"></a>3.3pyenv离线安装python各版本</h5><p>pyenv自动去官网拉取python安装包，如果要为离线服务器安装，则只需在<code>.pyenv</code>创建cache目录，将指定版本的python安装包放在该目录。</p>
<p>python安装包下载地址：<a href="https://www.python.org/ftp/python/">https://www.python.org/ftp/python/</a></p>
<p>只需下载Python-***.tar.xz 即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost cache]# pwd</span><br><span class="line">/root/.pyenv/cache</span><br><span class="line">[root@localhost cache]# ls</span><br><span class="line">Python-3.7.5.tar.xz </span><br><span class="line"></span><br><span class="line">[root@localhost .pyenv]# pyenv install 3.7.5</span><br><span class="line">Installing Python-3.7.5...</span><br><span class="line">Installed Python-3.7.5 to /root/.pyenv/versions/3.7.5</span><br></pre></td></tr></table></figure>

<h5 id="3-4-使用多个python版本"><a href="#3-4-使用多个python版本" class="headerlink" title="3.4 使用多个python版本"></a>3.4 使用多个python版本</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pyenv设为系统原有python版本：</span><br><span class="line">[root@localhost opt]# pyenv global system </span><br><span class="line">[root@localhost opt]# python -V</span><br><span class="line">Python 2.7.5</span><br><span class="line"></span><br><span class="line">pyenv更换系统python3.7.5版本后：</span><br><span class="line">[root@localhost opt]# pyenv global 3.7.5</span><br><span class="line">[root@localhost opt]# python -V</span><br><span class="line">Python 3.7.5</span><br><span class="line">[root@localhost opt]# pyenv versions</span><br><span class="line">  system</span><br><span class="line">* 3.7.5 (set by /root/.pyenv/version)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python3.7.5所在目录，所有的安装python都会集中放置在.pyenv/versions目录下</span></span><br><span class="line">[root@localhost bin]# pwd</span><br><span class="line">/root/.pyenv/versions/3.7.5/bin</span><br><span class="line">[root@localhost bin]# ls</span><br><span class="line">2to3              idle     pip3    pydoc3.7   python3.7-config   python3-config</span><br><span class="line">2to3-3.7          idle3    pip3.7  python     python3.7-gdb.py   python-config</span><br><span class="line">easy_install      idle3.7  pydoc   python3    python3.7m         pyvenv</span><br><span class="line">easy_install-3.7  pip      pydoc3  python3.7  python3.7m-config  pyvenv-3.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>pyenv在python版本方面得心应手。</p>
<h5 id="3-5-pyenv结合-pyenv-virtualenv实现灵活的虚拟环境管理"><a href="#3-5-pyenv结合-pyenv-virtualenv实现灵活的虚拟环境管理" class="headerlink" title="3.5 pyenv结合/pyenv-virtualenv实现灵活的虚拟环境管理"></a>3.5 pyenv结合/pyenv-virtualenv实现灵活的虚拟环境管理</h5><p>在第3.2章节介绍了virtualenv用于管理pip包的虚拟环境，virtualenv有个不足地方是：系统需已安装多个python版本，一般会通过手工安装，也即编译时指定不同路径，避免冲突。pyenv作者同时也开发pyenv-virtualenv的工具，跟virtualenv功能一致，也是用于管理pip包以及虚拟环境，结合pyenv，可以实现任意python版本以及pip包环境的切换以及使用，高效提高个人开发效率。</p>
<p>pyenv-virtualenv放在.pyenv/plugins 这个插件目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost plugins]# pwd</span><br><span class="line">/root/.pyenv/plugins</span><br><span class="line">[root@localhost plugins]# git clone https://github.com/yyuu/pyenv-virtualenv.git</span><br><span class="line">[root@localhost plugins]# ls</span><br><span class="line">pyenv-virtualenv  python-build</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新系统环境变量</span></span><br><span class="line">[root@localhost plugins]# exec &quot;$SHELL&quot;</span><br></pre></td></tr></table></figure>
<p>创建python3.7.5版本、虚拟环境名为tf375</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv virtualenv 3.7.5 tf375</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有使用python3.7.5版本创建的虚拟环境都在此目录：/root/.pyenv/versions/3.7.5/envs</span></span><br><span class="line">[root@localhost envs]<span class="comment"># ls</span></span><br><span class="line">pyspark375  tf375</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf375虚拟环境的目录在这里：</span></span><br><span class="line">[root@localhost opt]<span class="comment"># ls ~/.pyenv/versions/3.7.5/envs/tf375/</span></span><br><span class="line"><span class="built_in">bin</span>/        include/    lib/        lib64/      pyvenv.cfg  </span><br><span class="line"></span><br><span class="line"><span class="comment">#pyenv uninstall tf375 # 删除虚拟环境,同时也会删除其目录</span></span><br></pre></td></tr></table></figure>

<p>激活tf375环境，pyenv支持对环境名的自动补全，非常方便，pip包默认不继承系统包。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv activate tf375 </span></span><br><span class="line"></span><br><span class="line">(tf375) [root@localhost envs]<span class="comment"># pip list</span></span><br><span class="line">Package    Version</span><br><span class="line">---------- -------</span><br><span class="line">pip        <span class="number">19.2</span><span class="number">.3</span> </span><br><span class="line">setuptools <span class="number">41.2</span><span class="number">.0</span> </span><br></pre></td></tr></table></figure>

<p>创建python3.6.5版本、虚拟环境名为spk的pyspark开发环境</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv virtualenv 3.6.5 pyspark_proj</span></span><br><span class="line">[root@localhost opt]<span class="comment"># pyenv activate pyspark_proj</span></span><br><span class="line">(pyspark_proj) [root@localhost opt]<span class="comment"># pip list</span></span><br><span class="line">pip (<span class="number">9.0</span><span class="number">.3</span>)</span><br><span class="line">setuptools (<span class="number">39.0</span><span class="number">.1</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>python虚拟环境</tag>
      </tags>
  </entry>
  <entry>
    <title>Python对象的属性增删改查的实质</title>
    <url>/blog/2019/06/04/Python%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%B1%9E%E6%80%A7%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E7%9A%84%E5%AE%9E%E8%B4%A8/</url>
    <content><![CDATA[<p>&#8195;&#8195;在python对象实际使用过程中，会对其对象的属性进行增删改查，那么这背后是对哪种类型的”数据结构“进行增删改查呢？</p>
<a id="more"></a>

<p>下面以Blog类进行说明</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Blog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    title = <span class="string">&#x27;Python对象深度理解&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行实例化也即创建一个python对象</span></span><br><span class="line">myFirstBlog = Blog()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的增</span></span><br><span class="line">myFirstBlog.author=<span class="string">&#x27;Pysenen&#x27;</span></span><br><span class="line">print(myFirstBlog.author)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果：Pysenen</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的查</span></span><br><span class="line">print(myFirstBlog.title)</span><br><span class="line"><span class="comment"># 打印结果：Python对象深度理解</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的写（改）</span></span><br><span class="line">myFirstBlog.title=<span class="string">&#x27;新修改&#x27;</span></span><br><span class="line">print(myFirstBlog.title)</span><br><span class="line"><span class="comment"># 打印结果：新修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对象属性的删</span></span><br><span class="line"><span class="keyword">del</span> myFirstBlog.author</span><br><span class="line">print(myFirstBlog.author)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果：&#x27;Blog&#x27; object has no attribute &#x27;author&#x27;</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;以上操作其实就是对 myFirstBlog这个对象里的__dict__数据结构进行操作</p>
<p>打印myFirstBlog.<strong>dict</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;author&#x27;</span>: <span class="string">&#x27;Pysenen&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;新修改&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195; 因此对对象的操作，其实就是对对象的__dict__属性操作</p>
<p>也即：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 增</span></span><br><span class="line">myFirstBlog.__dict__[<span class="string">&#x27;author&#x27;</span>]=<span class="string">&#x27;Python&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查</span></span><br><span class="line">myFirstBlog.__dict__[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改</span></span><br><span class="line">myFirstBlog.__dict__[<span class="string">&#x27;title&#x27;</span>]=<span class="string">&#x27;新的title&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删</span></span><br><span class="line">myFirstBlog.__dict__.pop(<span class="string">&#x27;author&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;因此容易扩展，对对象的操作，可以转为使用字典的方式进行灵活操作，可回到大家熟悉的python字典数据结构的概念</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_dict=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;26&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>:<span class="string">&#x27;test@gmail.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;company&#x27;</span>:<span class="string">&#x27;nonono&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">myFirstBlog.__dict__=batch_dict</span><br><span class="line"></span><br><span class="line">print(myFirstBlog.__dict__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> myFirstBlog.__dict__</span><br><span class="line">print(myFirstBlog.__dict__)</span><br><span class="line"><span class="comment"># 打印结果：&#123;&#125;</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>————分割线————</p>
<p>&#8195;&#8195;由于这篇文章写得时间比较早，其实还有涉及更深的内容：类对象或者实例的所有属性都放在一个字典里，这里背后其实是由元类type掌控着。</p>
<p>&#8195;&#8195;所有的类都是type元类创建，它创建类的方式 normal_class=type(class_name,class_bases,class_dict)，这里的class_dict就是上面提到普通类的字典</p>
<p>&#8195;&#8195;例如上面定义的Blog类，用元类方式创建如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_dict=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;26&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>:<span class="string">&#x27;test@gmail.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;company&#x27;</span>:<span class="string">&#x27;nonono&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Blog=<span class="built_in">type</span>(<span class="string">&#x27;Blog&#x27;</span>,(<span class="built_in">object</span>,),batch_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">1</span>]: batch_dict=&#123;</span><br><span class="line">   ...:     <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">   ...:     <span class="string">&#x27;age&#x27;</span>:<span class="string">&#x27;26&#x27;</span>,</span><br><span class="line">   ...:     <span class="string">&#x27;email&#x27;</span>:<span class="string">&#x27;test@gmail.com&#x27;</span>,</span><br><span class="line">   ...:     <span class="string">&#x27;company&#x27;</span>:<span class="string">&#x27;nonono&#x27;</span></span><br><span class="line">   ...: &#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: Blog=<span class="built_in">type</span>(<span class="string">&#x27;Blog&#x27;</span>,(<span class="built_in">object</span>,),batch_dict)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: Blog</span><br><span class="line">Out[<span class="number">3</span>]: __main__.Blog</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: b=Blog()</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: b.name</span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">&#x27;foo&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: b.age</span><br><span class="line">Out[<span class="number">6</span>]: <span class="string">&#x27;26&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: b.email</span><br><span class="line">Out[<span class="number">7</span>]: <span class="string">&#x27;test@gmail.com&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">type</span>(b)</span><br><span class="line">Out[<span class="number">8</span>]: __main__.Blog</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;本文内容相对容易理解，如果要深入挖掘元类，可以查阅本博客文章<a href="https://blog.csdn.net/pysense/article/details/103516859">《 Python进阶——type与元类》</a></p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
  </entry>
  <entry>
    <title>VMware虚拟化——基于VCSA6.7搭建生产可用小型服务器集群（非共享存储方式）</title>
    <url>/blog/2019/06/10/VMware%E8%99%9A%E6%8B%9F%E5%8C%96%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8EVCSA6.7%E6%90%AD%E5%BB%BA%E7%94%9F%E4%BA%A7%E5%8F%AF%E7%94%A8%E5%B0%8F%E5%9E%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%9B%86%E7%BE%A4%EF%BC%88%E9%9D%9E%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F%EF%BC%89/</url>
    <content><![CDATA[<h5 id="项目背景："><a href="#项目背景：" class="headerlink" title="项目背景："></a>项目背景：</h5><p>&#8195;&#8195;使用VMware做高可用集群的前提是服务器连接共享存储（当然VMware高level许可证以及网络都要保证），这种方式可以开启VM最惊艳的高可用功能：HA、DRS、FT，VM直接在PAAS层提供高可用，无需应用虚拟机配置。但也有一些特殊场景，不需要用HA、DRS、FT，服务器不连接共享存储，该业务仅是借助了VMware虚拟化技术，将多台独立带存储的物理机虚拟化后用VCSA6.7统一管理，这种业务中的虚拟机里跑的应用自带主备配置，分别在主ESXi开启主虚拟机，在备ESXi开启备虚拟机，在应用里面配置好master-slave，通过应用层实现的“应用层高可用”，物理层网络和物理服务器互相独立。</p>
<p>架构图如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20190711221651975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img">非共享存储服务器集群虚拟化后的大致架构图</p>
<a id="more"></a>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="1、服务器物理位置"><a href="#1、服务器物理位置" class="headerlink" title="1、服务器物理位置"></a>1、服务器物理位置</h5><p>&#8195;&#8195;机框1放置主服务器，机框2放置备服务器，两台构成主备虚拟化物理华三交换机可放置与机框1底下，或者放置到旁边位置（位置放太远不方便服务器网线连接交换机），参考第二点图：</p>
<h5 id="2、物理链路连接"><a href="#2、物理链路连接" class="headerlink" title="2、物理链路连接"></a>2、物理链路连接</h5><p><img src="https://img-blog.csdnimg.cn/20190711214645479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="拓扑图">拓扑图</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>（1）以机框1的物理服务器1为例，服务器一般配置4个物理网卡，eth0、eth1为业务网口，eth2作为管理ESXi网络网口，将eth0连至主交换机端口，将eth1连至备交换机端口，服务器1与上联接入层两台交换机构成链路冗余，也即两台交换机其中一台停机，或者服务器两个网口其中一个网口配置出错等，业务口都可冗余物理连接。</p>
<p>（2）eth2作为ESXi的管理口，连接至主接入层交互机，管理口冗余，避免过多占用网口和交互机端口（例如以华三某较为高端二层48端口，减去主备虚拟化IRF2个端口+上联连至核心交互机1个端口，还剩45个端口，若一台物理服务器使用两个管理口，再加两个业务口，可提供11台服务器连接（一个机框可放置服务器的数量也就是12台这样），若服务器仅用三个网口，可连15台服务器），按实际工作上配置的三个网口，两台48端口接入层交换机，可连接三个机框的服务器，数量大致为30台高度为2个U的服务器。</p>
<p>（3）接入层主交换机上联连至上联主核心交换机trunk端口，接入备交换机上联连至上联备核心交换机trunk端口，并配置两个trunk端口为聚合端口，实现接入层与核心层交换机构成冗余连接</p>
<p>（4）所有的物理连接都统一采用千兆电口，六类网线连接</p>
<h5 id="3、交换机IRF配置（又称堆叠，交换机虚拟化）"><a href="#3、交换机IRF配置（又称堆叠，交换机虚拟化）" class="headerlink" title="3、交换机IRF配置（又称堆叠，交换机虚拟化）"></a>3、交换机IRF配置（又称堆叠，交换机虚拟化）</h5><p>&#8195;&#8195;服务器上联交换机（业务接入层交换机）做了虚拟化后，相当于主备模式，当将主业务端口绑定到对应备交换机端口，那么交换机侧不仅实现主备模式，还实现负载均衡，流量叠加（跨交换机链路聚合），实现下联服务器集群在二层物理连接的高可用，生产已使用。此外，通过将业务接入层交换机接入到上联的核心交换机，实现连接更多的物理服务器</p>
<p>&#8195;&#8195;以两台华三某系列交换机为例，配置前提条件：支持irf，相同型号交换机，相同os版本</p>
<p>主备交换机物理链路连接。这里以23、24为千兆光口为连接说明，使用多模纤850交叉连接</p>
<p><img src="https://img-blog.csdnimg.cn/20190714112224579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>配置过程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 设置主交换机成员号1</span></span><br><span class="line">（1）irf member 1 renumber 1 </span><br><span class="line"></span><br><span class="line"> <span class="comment"># 设置本交换机优先级，值大优先级越高，越有可能成为master</span></span><br><span class="line"> (2) irf member 1 priority 20</span><br><span class="line"> (3) shutdown需要irf的物理端口23/24口</span><br><span class="line"></span><br><span class="line"> <span class="comment">#创建虚拟化逻辑端口 1/1表示 member=1/portID=1 并把23和24口加入（绑定）该逻辑端口</span></span><br><span class="line"> (4) irf-port 1/1</span><br><span class="line">    port group interface 23</span><br><span class="line">    port group interface 24</span><br><span class="line">“”“</span><br><span class="line">这里需要注意：主sw使用irf-port 1/1，那么在备sw必须使用irf-port 2/2，也即portID必须互为不     同，否则irf报错，提示portID冲突，无法建立irf环境，</span><br><span class="line">主sw的irf-port 1/1对应备sw的irf-port 2/2 </span><br><span class="line">或主sw的irf-port 1/2 对应备sw的irf-port 2/1,</span><br><span class="line">不能Irf-port 1/1 搭配Irf-port 2/1 或者Irf-port 1/2 搭配Irf-port 2/2</span><br><span class="line">”“”</span><br><span class="line"> (5)开启irf的23、24物理端口，并save保存配置</span><br><span class="line"></span><br><span class="line"> <span class="comment">#激活irf配置，并重启</span></span><br><span class="line"> (6) irf-port-configuration active </span><br><span class="line"></span><br><span class="line"><span class="comment"># 备sw的配置过程同上，不同的地方</span></span><br><span class="line"> (1) 成员号配置为2，备sw交换机的端口前缀从默认的1/0/~变为2/0/~（若有第三台，配置为3，端口前缀变为3/0/~，这个成员号配置是sw堆叠后，方便区别端口是属于哪台sw）</span><br><span class="line"> (2) sw的优先级配为10，比主sw的低</span><br><span class="line"> (2) 因主sw的逻辑端口配为irf-port 1/1，故备sw需配置为irf-port 2/2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后优化irf配置</span></span><br><span class="line"> undo irf mac-address persistent</span><br><span class="line"> irf auto-update <span class="built_in">enable</span></span><br><span class="line"> undo irf link-delay</span><br><span class="line"></span><br><span class="line">查看irf状态 dis irf</span><br><span class="line">MemberID    Role    Priority  CPU-Mac         Description</span><br><span class="line"> *+1        Master  20        00f0-a31p-6t02  ---</span><br><span class="line">   2        Standby 10        00f0-a31p-6t03  ---</span><br><span class="line"></span><br><span class="line">查看irf端口状态</span><br><span class="line">Member 1</span><br><span class="line"> IRF Port  Interface                             Status</span><br><span class="line"> 1         GigabitEthernet1/0/23                 UP    </span><br><span class="line">           GigabitEthernet1/0/24                 UP    </span><br><span class="line"> 2         <span class="built_in">disable</span>                               --    </span><br><span class="line">Member 2</span><br><span class="line"> IRF Port  Interface                             Status</span><br><span class="line"> 1         <span class="built_in">disable</span>                               --    </span><br><span class="line"> 2         GigabitEthernet2/0/23                 UP    </span><br><span class="line">           GigabitEthernet2/0/24                 UP  </span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="4、每台物理服务器存储配置"><a href="#4、每台物理服务器存储配置" class="headerlink" title="4、每台物理服务器存储配置"></a>4、每台物理服务器存储配置</h5><p>&#8195;&#8195;生产数据安全需优先，对每台物理服务器存储（SAS硬盘）配置RAID1，牺牲了一部分读写速度，这里RAID1的配置有个注意的地方，如果server提供的是非多媒体图片、视频等业务，读写都是“小块数据类”，那么RAID1 initial化stride设为最小256kb，若server提供的为图片、视频等业务，读写都是“大块数据类”，那么RAID1 initial化stride设为最大1024kb，以优化RAID的读写速度。</p>
<h5 id="5、业务网段和管理网段分开"><a href="#5、业务网段和管理网段分开" class="headerlink" title="5、业务网段和管理网段分开"></a>5、业务网段和管理网段分开</h5><p>&#8195;&#8195;在第二部分的物理链路拓扑图可以看到，数据链路和管理链路是分开的物理连接，实际使用场景中，一般会规划两个不同的网段使用，例如业务数据网段182.21.0.0/16，这个网段都是给vm虚拟机使用，网段大小可以自行根据vm数量规划，这里的掩码为16位，可以提供（<img src="https://private.codecogs.com/gif.latex?2%5E%7B16%7D-2" alt="2^{16}-2">）个VM使用，当然实际场景，很多企业不可能达到这么大的主机数，掩码可以设/24；另外一网段则用于ESXI主机管理口，例如182.22.10.0/24。一般企业来说，ESXI主机数量不会太多，可以设一个掩码8位的小网段，可以提供254个ESXI服务器使用。</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
  </entry>
  <entry>
    <title>Python类私有变量覆盖特性</title>
    <url>/blog/2019/06/03/Python%E7%B1%BB%E7%A7%81%E6%9C%89%E5%8F%98%E9%87%8F%E8%A6%86%E7%9B%96%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<p>&#8195;&#8195;此内容相对简单，仅在使用子类过程中，注意私有变量在子类继承过程的特性</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    __name=<span class="string">&#x27;Foo&#x27;</span> <span class="comment"># 命名空间变为：_Person__name</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__age=<span class="number">10</span>  <span class="comment"># 命名空间变为：_Person__age</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__height</span>(<span class="params">self</span>):</span> <span class="comment"># 命名空间变为：_Person__height</span></span><br><span class="line">        print(<span class="string">&#x27;form&#x27;</span>,self.__class__.__name__)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;180cm&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">outside_access</span>(<span class="params">self</span>):</span> <span class="comment"># 类内部访问类的私有方法或者私有数据属性</span></span><br><span class="line">        print(<span class="string">&#x27;form&#x27;</span>,self.__class__.__name__)</span><br><span class="line">        <span class="keyword">return</span> self.__height()</span><br><span class="line"></span><br><span class="line">print(Person.__dict__)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;_Person__name&#x27;: &#x27;Foo&#x27;, &#x27;__doc__&#x27;: None, &#x27;__module__&#x27;: &#x27;__main__&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;__dict__&#x27;: &lt;attribute &#x27;__dict__&#x27; of &#x27;Person&#x27; objects&gt;, &#x27;__weakref__&#x27;: &lt;attribute &#x27;__weakref__&#x27; of &#x27;Person&#x27; objects&gt;, </span></span><br><span class="line"><span class="string"> &#x27;__init__&#x27;: &lt;function Person.__init__ at 0x1143171e0&gt;, </span></span><br><span class="line"><span class="string"> &#x27;_Person__height&#x27;: &lt;function Person.__height at 0x114317268&gt;, </span></span><br><span class="line"><span class="string"> &#x27;outside_access&#x27;: &lt;function Person.outside_access at 0x1143172f0&gt;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">p=Person()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(p.__name) # Person&#x27; object has no attribute &#x27;__name&#x27;</span></span><br><span class="line"><span class="comment"># print(p.__height) # Person&#x27; object has no attribute &#x27;__height&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Python 为何要在类私有变量前加上类名作为变量名称</span></span><br><span class="line"><span class="string">目的：当子类覆盖父类时，保证了子类的同名私有变量_subclass__Foo不会覆盖同名的_parentclass__Foo</span></span><br><span class="line"><span class="string">或者说：父类不想让子类覆盖自己的方法，可通过将方法私有化达到目的,这个子类指：同一模块内的方法，或被引入到其他模块当中的子类</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SubPerson</span>(<span class="params">Person</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__height</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;170cm&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">outside_access</span>(<span class="params">self</span>):</span> <span class="comment"># 类内部访问类的私有方法或者私有数据属性</span></span><br><span class="line">        print(<span class="string">&#x27;form&#x27;</span>,self.__class__.__name__)</span><br><span class="line">        <span class="keyword">return</span> self.__height()</span><br><span class="line"></span><br><span class="line">s=SubPerson()</span><br><span class="line">print(s._Person__height())</span><br><span class="line"><span class="comment"># 打印结果：180cm</span></span><br><span class="line">print(s._SubPerson__height())</span><br><span class="line"><span class="comment"># 打印结果：170cm</span></span><br><span class="line"><span class="comment"># 说明父类__height方法与子类的_height完全独立</span></span><br><span class="line"></span><br><span class="line">print(s.outside_access())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">打印结果：</span></span><br><span class="line"><span class="string">form SubPerson</span></span><br><span class="line"><span class="string">170cm</span></span><br><span class="line"><span class="string">显然父类非私有方法已被子类覆盖</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
  </entry>
  <entry>
    <title>VMware虚拟化——部署vCenter Server Appliance(VCSA6.7)完整全过程</title>
    <url>/blog/2019/06/07/VMware%E8%99%9A%E6%8B%9F%E5%8C%96%E2%80%94%E2%80%94%E9%83%A8%E7%BD%B2vCenter%20Server%20Appliance(VCSA%206.7)%E5%AE%8C%E6%95%B4%E5%85%A8%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;采用vCenter+ESXi主机集群的部署方式可统一管理中小型数据中心，虚拟化使用vCenter最新的Linux版本，非网上大多数提及的Windows server版本，也即VCSA6.7，构建过程出现一些棘手问题，在全网都未找到相关问题的分析，在本文中得以解决，为此将解决方案以文章形式发布。</p>
<h4 id="1、VCSA6-7安装失败提示"><a href="#1、VCSA6-7安装失败提示" class="headerlink" title="1、VCSA6.7安装失败提示"></a>1、VCSA6.7安装失败提示</h4><p>VCSA6.7安装多次，卡在第二阶段52%或者60%后就无法安装，多次部署都出现该出错提示。</p>
<p><img src="https://img-blog.csdnimg.cn/20190616172734717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<a id="more"></a>

<p><img src="https://img-blog.csdnimg.cn/20190616172951334.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;在error的日志文件夹内，你会看到action.log、error.log、errors-ignored.log这三个文本（当然部署过程产生的还有其他多个文件）。其中在error.log文件里仅有一句：No file found matching /etc/vmware-vpx/vc-extn-cisreg.prop。很好，这些提示，都不可能给你明显的设置方向，你只能去检索+深度分析，在网上竟然检索不到该解决方案，以下为相关检索结果：</p>
<hr>
<p>这里有人提出同样的出错，但没有详细的解决方案：参考链接</p>
<p><code>https://bbs.51cto.com/thread-1569772-1-1.html</code></p>
<p>有人提出是VCSA版本低，说部署新的可成功安装，参考链接，这不是解决方案：</p>
<p><code>https://bbs.51cto.com/thread-1564358-1-1.html</code></p>
<p>这个链接是卡在安装过程，依然没人提过详细的解决方案：</p>
<p><code>https://bbs.csdn.net/topics/392349298</code></p>
<p>去<a href="http://www.baidu.com/link?url=vuGzAs8an5IJ6HdwZ94Sp3vWVs-SvvmFk-FiVW6dro4M2AUTwaASzEZ_q3VeC1n5"><em>Stack</em> <em>Overflow检索相关出错关键字，也无特别干货。</em></a></p>
<p><em>在VMware</em> troubleshoot 官方论坛，也没找到明显的提示，它有提示说到网络连接正常、DNS具备Vcenter域名的解析记录等，很含糊的提示。</p>
<p>而VCSA6.7部署成功的文章，却没进一步探讨，哪个环节是影响部署关键部分。</p>
<hr>
<h4 id="2、出现以上部署出错对应的配置过程（还原部署流程）："><a href="#2、出现以上部署出错对应的配置过程（还原部署流程）：" class="headerlink" title="2、出现以上部署出错对应的配置过程（还原部署流程）："></a>2、出现以上部署出错对应的配置过程（还原部署流程）：</h4><h5 id="2-1准备ESXI6-7环境"><a href="#2-1准备ESXI6-7环境" class="headerlink" title="2.1准备ESXI6.7环境"></a>2.1准备ESXI6.7环境</h5><p>&#8195;&#8195;首先需在一台物理机上安装好ESXI6.7环境，参考镜像：VMware-ESXi-6.7.0-8169922-LNV-20180404.iso ，参考安装方法：使用UltraISO将该镜像制作U盘启动器，注意，这里若用大白菜等工具，会出现无法在物理机安装ESXI环境</p>
<h5 id="2-2-准备VCSA6-7镜像"><a href="#2-2-准备VCSA6-7镜像" class="headerlink" title="2.2 准备VCSA6.7镜像"></a>2.2 准备VCSA6.7镜像</h5><p>&#8195;&#8195;首先使用虚拟光驱将VMware-VCSA-all-6.7.0-8217866.iso 镜像文件加载后在H:\vcsa-ui-installer\win32上，有exe界面安装器：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616174711487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;若不理解VCSA的概念，有人以为是这样的思路：该部署电脑是windows，那么在这里打开VCSA镜像的安装器，岂不是要安装在windows系统上？</p>
<p>&#8195;&#8195;实际情况：打开安装器后，会提示选择一台外部ESXI主机用来安装VSCA6.7（要求输入远程ESXI的IP账号密码），这个过程安装器会在选中的ESXI先创建一个linux环境（phonton os）的虚拟机，然后再把真正的VCSA6.7的OVA包注入到该虚拟机上进行安装。</p>
<h5 id="2-3-开始部署安装"><a href="#2-3-开始部署安装" class="headerlink" title="2.3 开始部署安装"></a>2.3 开始部署安装</h5><p>在配置网络这里，前几次部署失败都按这种网络配置</p>
<p><img src="https://img-blog.csdnimg.cn/20190616175934878.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>配置的大致逻辑：</p>
<p>FQDN：留空</p>
<p>IP地址：192.168.10.10</p>
<p>掩码：255.255.255.0</p>
<p>网关：192.168.10.1</p>
<p>DNS：20.101.191.1</p>
<p>ok，第一阶段都会正确安装，不会出现出错。</p>
<p>此时，登录部署vcenter的远程ESXi web 界面，发现该ESXi主机已经创建并运行一个venter的虚拟机，其shell提示可进入<br><code>https://photon-machine:5480</code> 进一步配置VCSA</p>
<p><img src="https://img-blog.csdnimg.cn/20190616214105649.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这里的域名值得质疑，</p>
<p><strong>“photon-machine”哪里设置的？</strong></p>
<p><code>https://photon-machine:5480</code> ，本地电脑DNS服务器肯定没有这条解析记录，部署电脑如何能打开该域名url？**</p>
<p><strong>如果必须让本地电脑可访问该域名url，那么本地电脑必须设置一个含有photon-machine&lt;&gt;192.168.10.10记录的DNS IP？</strong></p>
<p>如果<code>https://photon-machine:5480</code>可改为<code>https://192.168.10.10:5480</code> ，那么本地电脑无需设置DNS，即可访问，在哪里改？**</p>
<h5 id="2-4-带上以上疑问开始第二阶段安装"><a href="#2-4-带上以上疑问开始第二阶段安装" class="headerlink" title="2.4 带上以上疑问开始第二阶段安装"></a>2.4 带上以上疑问开始第二阶段安装</h5><p><img src="https://img-blog.csdnimg.cn/2019061621314249.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这里的single sign-on域名用了默认域名，也可以改成自己任意取定的域名（此设置不会引起第二阶段部署出错）</p>
<p><img src="https://img-blog.csdnimg.cn/20190616213544732.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;这里的IP地址DNS与2.3所填写的一致，到了这一步要注意了，这里的主机名称（指vcenter名称）：photo-machine，原来这是在第二阶段设完所有参数后，安装程序使用的默认的英文主机名称作为vcenter的主机名称，这样的配置设好后，开始第二阶段安装，不出意外，卡在60%后报错，安装失败。</p>
<p>&#8195;&#8195;如果你思维敏捷，你会去Vcenter shell看下什么情况，并输入<code>https://192.168.10.10</code>看看能有什么反馈？结果如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190616220755366.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><img src="https://img-blog.csdnimg.cn/20190616220818565.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;用这两者里面的相关关键字去检索solution，然而似乎没什么收获。对于那些刚接触VCSA6.7且想使用其为搭建数据中心的工程师（这里指全栈工程师），2.1-2.4的配置过程几乎就是他们的配置过程，遇到这种bug应该很崩溃，因为所有的ESXi集群都搭建起来，就差Vcenter服务搭不起来，多台ESXI如何管理？如何构建高可用的数据中心？</p>
<h4 id="3、正确部署VCSA6-7的过程"><a href="#3、正确部署VCSA6-7的过程" class="headerlink" title="3、正确部署VCSA6.7的过程"></a>3、正确部署VCSA6.7的过程</h4><p>&#8195;&#8195;经过本人深度测试，目前两种部署方式可完整正确部署。</p>
<p>第一种：采用上面展示的UI安装方法，使用installer部署第一阶段&gt;&gt;进入VCenter shell设置&gt;&gt;登录Web界面进行第二阶段部署</p>
<p>第二种：采用将VCSA6.7的OVA文件导入到远程ESXi主机，使用ESXi导入ova创建&gt;&gt;进入VCenter shell设置&gt;&gt;登录Web界面进行第二阶段部署（理解了第一种方式后，第二种部署方式无需在此给出指引，可自行探索，ova文件一般位于iso文件夹里的vcsa目录下）</p>
<p>第一种方式</p>
<h5 id="3-1-第一阶段的installer，在此界面的设置参数中："><a href="#3-1-第一阶段的installer，在此界面的设置参数中：" class="headerlink" title="3.1 第一阶段的installer，在此界面的设置参数中："></a>3.1 第一阶段的installer，在此界面的设置参数中：</h5><p><img src="https://img-blog.csdnimg.cn/20190616223113370.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><strong>这里的FQDN可以留空，若要填入内容：建议务必填入V center的IP地址</strong></p>
<p><strong>这个案例中vcenter的IP地址：192.168.10.10，因为很多时候，内网并有没搭建专门的DNS服务器，直接填写为Vcenter的IP，客户机无需经过DNS解析即可访问</strong></p>
<p><strong>假设这里的FQDN填写的是域名：wow.mydatacenter，那么在第二阶段VCSA安装多个服务将采用域名去解析，若无法从所给的DNS解析出Vcenter的IP地址，导致相关服务继续安装出现错误。除非你设置的DNS服务器里，包含该域名和IP的映射记录。</strong></p>
<p><strong>所以：当FQDN填写vcenter为IP地址时，那么下方设置的DNS只需设为一个本网络段的可ping网关IP即可，因为vcenter第二阶段的服务安装直接通过IP查找，无需解析。</strong></p>
<p><strong>这里的DNS设为网关的IP：192.168.10.1</strong></p>
<h5 id="3-2-进入VCenter-shell设置"><a href="#3-2-进入VCenter-shell设置" class="headerlink" title="3.2 进入VCenter shell设置"></a>3.2 进入VCenter shell设置</h5><p>在3.1第一阶段完成后，退出installer（后面可通过web后台继续第二阶段部署），进入VCenter shell设置，在DNS里面，Hostname的值为photon-machine，这就是2.3节和2.4节内容提到<code>https://photon-machine:5480</code>等疑问的解决入口。</p>
<p><strong><img src="https://img-blog.csdnimg.cn/20190617230737363.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></strong></p>
<p>故将Hostname设为Vcenter的IP地址，shell restart 网络服务后，可看到其提示已经不再是域名的url，这意味着，</p>
<p><code>https://192.168.10.10:5480</code> 已经无需DNS解析即可访问，那么可以预测，在第二阶段部署中，某些服务的启动过程不再通过给定的DNS服务器去解析主机名称：photon-machine</p>
<p><img src="https://img-blog.csdnimg.cn/20190617230824719.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="3-3-登录web后台进行第二阶段部署：https-192-168-10-10-443"><a href="#3-3-登录web后台进行第二阶段部署：https-192-168-10-10-443" class="headerlink" title="3.3 登录web后台进行第二阶段部署：https://192.168.10.10:443"></a>3.3 登录web后台进行第二阶段部署：<code>https://192.168.10.10:443</code></h5><p>web的部署UI跟本地客户端UI一样，这里的网络配置，主机名称不再是photon-machine，而是vcenter的IP地址</p>
<p><img src="https://img-blog.csdnimg.cn/20190617231409508.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>ok，如果你的第二阶段安装最终配置如上，那么恭喜部署成功！</p>
<p><img src="https://img-blog.csdnimg.cn/20190617232019860.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<h5 id="3-4-进入vCenter进程服务管理后台"><a href="#3-4-进入vCenter进程服务管理后台" class="headerlink" title="3.4  进入vCenter进程服务管理后台"></a>3.4  进入vCenter进程服务管理后台</h5><p><code>https://192.168.10.10:5480 </code>又是啥管理后台？注意，该后台不是用于创建虚拟机和管理虚拟机的VCenter后台，是用于管理有关vCenter进程服务相关的后台。</p>
<p><img src="https://img-blog.csdnimg.cn/20190617232601668.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><img src="https://img-blog.csdnimg.cn/20190617232639689.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>基本是有关VCSA多个服务进程管理后台，以及一些基本配置，在这里我们再次可看到主机名为IP地址，在服务进程列表，可尝试停止VMware vCenter Server 进程，你会发现shell里面的Firstboot Error，访问<code>https://192.168.10.10:443</code>，出现熟悉的503</p>
<p><img src="https://img-blog.csdnimg.cn/20190616220818565.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>综上：</p>
<p>（1）如果你对DNS服务不知如何配置vCenter主机名正向和反向查询，请直接将vCenter hostname改为IP</p>
<p>（2）局域网DNS配置好相关域名记录后，也必须清楚hostname的默认值为photon-machine，需改为你自行设计的vCenter域名。</p>
<h4 id="4、VCSA6-7-web管理正确授权使用"><a href="#4、VCSA6-7-web管理正确授权使用" class="headerlink" title="4、VCSA6.7 web管理正确授权使用"></a>4、VCSA6.7 web管理正确授权使用</h4><p>&#8195;&#8195;如果两个产品的许可证不匹配，那么vcenter web面板会提示“ESXI主机的许可证与vcenter不匹配，将断开连接”，相当于无法添加ESXi主机</p>
<p>解决方案：</p>
<p>（1）Vcenter的许可证必须是：VMware vCenter Server 6 Standard</p>
<p>（2）ESXI6.7主机的许可证必须是：VMware vSphere 6 Enterprise Plus</p>
]]></content>
      <categories>
        <category>VMware</category>
      </categories>
      <tags>
        <tag>vCenter - VCSA6.7</tag>
      </tags>
  </entry>
  <entry>
    <title>ZooKeeper特性适用的应用场景</title>
    <url>/blog/2019/09/10/ZooKeeper%E7%89%B9%E6%80%A7%E9%80%82%E7%94%A8%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</url>
    <content><![CDATA[<p>&#8195;&#8195;随着对zk使用和了解更深入，不得不佩服Apache基金出品的技术，一直拥有着改变世界的能量！zookeeper结合大数据技术栈，实现无以伦比的高可用分布式大数据架构，单单这一点就非常让人兴奋，从zk的设计来看，传统的数据结构和算法以及底层网络知识和技术，仍然可以通过结合现代业务模型进行创造和创新，所有继续保持沉淀传统基础，以助力更高效吸收新技术！</p>
<p>&#8195;&#8195;以下引用了网上一些对zk的总结，内容比较简单，毕竟不是原理探讨，但对于zk的特性使用或者说发挥zk的特长，需要开发者深度理解数据模型或应用场景才能实现基于zk特性的相应逻辑。一些常用的zk特性和场合说明整理放在个人blog上，以便查阅</p>
<a id="more"></a>


<h3 id="数据发布与订阅（配置中心）"><a href="#数据发布与订阅（配置中心）" class="headerlink" title="数据发布与订阅（配置中心）"></a>数据发布与订阅（配置中心）</h3><p>&#8195;&#8195;数据发布与订阅，即所谓的配置中心，顾名思义就是发布者将数据发布到ZooKeeper节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和动态更新。<br>&#8195;&#8195;类似这样的需求：系统中需要使用一些通用的配置信息，例如机器列表信息、数据库配置信息等。这些全局配置信息通常具备以下3个特性。</p>
<ul>
<li>数据量通常比较小</li>
<li>数据内容在运行时动态变化。</li>
<li>集群中各机器共享，配置一致。</li>
</ul>
<p>对于这样的全局配置信息就可以发布到ZooKeeper上，让客户端（集群的机器）去订阅该消息。</p>
<p>发布/订阅系统一般有两种设计模式，分别是推（Push）和拉（Pull）模式。</p>
<ul>
<li>推：服务端主动将数据更新发送给所有订阅的客户端。</li>
<li>拉：客户端主动发起请求来获取最新数据，通常客户端都采用定时轮询拉取的方式。</li>
</ul>
<p>&#8195;&#8195;ZooKeeper采用的是推拉相结合的方式。如下：<br>客户端想服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据（推拉结合）。</p>
<h3 id="Naming-Service"><a href="#Naming-Service" class="headerlink" title="Naming Service"></a>Naming Service</h3><p>&#8195;&#8195;命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务，远程对象等等——都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表。通过在ZooKeepr里创建顺序节点，能够很容易创建一个全局唯一的路径，这个路径就可以作为一个名字。<br>==ZooKeeper的命名服务即生成全局唯一的ID==</p>
<h3 id="分布式协调-通知"><a href="#分布式协调-通知" class="headerlink" title="分布式协调/通知"></a>分布式协调/通知</h3><p>&#8195;&#8195;ZooKeeper中特有Watcher注册与异步通知机制，能够很好的实现分布式环境下不同机器，甚至不同系统之间的通知与协调，从而实现对数据变更的实时处理。使用方法通常是不同的客户端都对ZK上同一个ZNode进行注册，监听ZNode的变化（包括ZNode本身内容及子节点的），如果ZNode发生了变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并做出相应的处理。<br>==ZK的分布式协调/通知，是一种通用的分布式系统机器间的通信方式。==</p>
<h4 id="心跳检测"><a href="#心跳检测" class="headerlink" title="心跳检测"></a>心跳检测</h4><p>&#8195;&#8195;基于ZK的临时节点的特性，可以让不同的进程都在ZK的一个指定节点下创建临时子节点，不同的进程直接可以根据这个临时子节点来判断对应的进程是否存活。通过这种方式，检测和被检测系统直接并不需要直接相关联，而是通过ZK上的某个节点进行关联，大大减少了系统耦合。</p>
<h4 id="工作进度汇报"><a href="#工作进度汇报" class="headerlink" title="工作进度汇报"></a>工作进度汇报</h4><p>&#8195;&#8195;在一个常见的任务分发系统中，通常任务被分发到不同的机器上执行后，需要实时地将自己的任务执行进度汇报给分发系统。这个时候就可以通过ZK来实现。在ZK上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样便可以实现两个功能：</p>
<ul>
<li>通过判断临时节点是否存在来确定任务机器是否存活。</li>
<li>各个任务机器会实时地将自己的任务执行进度写到这个临时节点上去，以便中心系统能够实时地获取到任务的执行进度。</li>
</ul>
<h3 id="Master选举"><a href="#Master选举" class="headerlink" title="Master选举"></a>Master选举</h3><p>&#8195;&#8195;Master选举可以说是ZooKeeper最典型的应用场景了。比如HDFS中Active NameNode的选举、YARN中Active ResourceManager的选举和HBase中Active HMaster的选举等。</p>
<p>&#8195;&#8195;利用ZooKeepr的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即ZooKeeper将会保证客户端无法创建一个已经存在的ZNode。也就是说，如果同时有多个客户端请求创建同一个临时节点，那么最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很容易地在分布式环境中进行Master选举了。</p>
<p>&#8195;&#8195;成功创建该节点的客户端所在的机器就成为了Master。同时，其他没有成功创建该节点的客户端，都会在该节点上注册一个子节点变更的Watcher，用于监控当前Master机器是否存活，一旦发现当前的Master挂了，那么其他客户端将会重新进行Master选举。</p>
<h3 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h3><p>&#8195;&#8195;分布式锁是控制分布式系统之间同步访问共享资源的一种方式。分布式锁又分为排他锁和共享锁两种。</p>
<h4 id="排他锁"><a href="#排他锁" class="headerlink" title="排他锁"></a>排他锁</h4><p>排他锁（Exclusive Locks，简称X锁），又称为写锁或独占锁。</p>
<blockquote>
<p>如果事务T1对数据对象O1加上了排他锁，那么在整个加锁期间，只允许事务T1对O1进行读取和更新操作，其他任何事务都不能在对这个数据对象进行任何类型的操作（不能再对该对象加锁），直到T1释放了排他锁。</p>
</blockquote>
<p>可以看出，排他锁的核心是如何保证当前只有一个事务获得锁，并且锁被释放后，所有正在等待获取锁的事务都能够被通知到。</p>
<p>排他锁流程</p>
<h5 id="定义锁"><a href="#定义锁" class="headerlink" title="定义锁"></a>定义锁</h5><p>ZooKeeper上的一个znode可以表示一个锁。例如/exclusive_lock/lock节点就可以被定义为一个锁。</p>
<h5 id="获得锁"><a href="#获得锁" class="headerlink" title="获得锁"></a>获得锁</h5><p>&#8195;&#8195;如上所说，把ZooKeeper上的一个ZNode看作是一个锁，获得锁就通过创建znode的方式来实现。所有客户端都去/exclusive_lock节点下创建临时子节点/exclusive_lock/lock。ZooKeeper会保证在所有客户端中，最终只有一个客户端能够创建成功，那么就可以认为该客户端获得了锁。同时，所有没有获取到锁的客户端就需要到/exclusive_lock节点上注册一个子节点变更的Watcher监听，以便实时监听到lock节点的变更情况。</p>
<h4 id="释放锁"><a href="#释放锁" class="headerlink" title="释放锁"></a>释放锁</h4><p>&#8195;&#8195;因为/exclusive_lock/lock是一个临时节点，因此在以下两种情况下，都有可能释放锁。</p>
<ul>
<li>当前获得锁的客户端机器发生宕机或重启，那么该临时节点就会被删除，释放锁。</li>
<li>正常执行完业务逻辑后，客户端就会主动将自己创建的临时节点删除，释放锁。</li>
</ul>
<p>&#8195;&#8195;无论在什么情况下移除了lock节点，ZooKeeper都会通知所有在/exclusive_lock节点上注册了节点变更Watcher监听的客户端。这些客户端在接收到通知后，再次重新发起分布式锁获取，即重复『获取锁』过程。</p>
<h4 id="共享锁（也就是zk上实现的分布式锁）"><a href="#共享锁（也就是zk上实现的分布式锁）" class="headerlink" title="共享锁（也就是zk上实现的分布式锁）"></a>共享锁（也就是zk上实现的分布式锁）</h4><p>&#8195;&#8195;可以多个事务同时获得一个对象的共享锁（同时读），有共享锁就不能再加排他锁（因为排他锁是写锁），前面已经在文章给出基于分布式锁的非常详细的实现过程，这里不再重复。</p>
]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
  </entry>
  <entry>
    <title>centos存储扩容配置（基于LVM）</title>
    <url>/blog/2019/06/30/centos%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E9%85%8D%E7%BD%AE%EF%BC%88%E5%9F%BA%E4%BA%8ELVM%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;在全栈开发项目中，当项目已经部署到服务器（虚拟机、物理机或者云主机）后，有些分区若空间分配不合理，数据库以及相关日志文件将很快占满磁盘空间，因此需在后期手动为项目所在服务器空间进行扩容。</p>
<a id="more"></a>

<h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li>查看</li>
<li>分区</li>
<li>格式化</li>
<li>挂载到相应目录上</li>
</ol>
<ul>
<li><h4 id="查看fdisk-l"><a href="#查看fdisk-l" class="headerlink" title="查看fdisk -l"></a>查看fdisk -l</h4><p>确认容量不足，可以使用dd命令创建一个1G文件看看</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# dd if=/dev/zero of=1.0G.img bs=1M count=1000</span><br><span class="line">dd: error writing ‘1.0G.img’: No space left on device</span><br><span class="line">134+0 records in</span><br><span class="line">133+0 records out</span><br><span class="line">139460608 bytes (139 MB) copied, 0.471449 s, 296 MB/s</span><br></pre></td></tr></table></figure>
<p><code>dd: error writing ‘1.0G.img’: No space left on device</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> [root@nn ~]# fdisk -l</span><br><span class="line">Disk /dev/sda: 4294 MB, 4294967296 bytes, 8388608 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line">Disk label type: dos</span><br><span class="line">Disk identifier: 0x000cef21</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sda1   *        2048     2099199     1048576   83  Linux</span><br><span class="line">/dev/sda2         2099200     8388607     3144704   8e  Linux LVM</span><br><span class="line"></span><br><span class="line">Disk /dev/sdb: 5368 MB, 5368709120 bytes, 10485760 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line"></span><br><span class="line">Disk /dev/mapper/centos-root: 2785 MB, 2785017856 bytes, 5439488 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br><span class="line"></span><br><span class="line">Disk /dev/mapper/centos-swap: 432 MB, 432013312 bytes, 843776 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br></pre></td></tr></table></figure>
<p>以上指：<br>两个存储（硬盘）设备分别sda和sdb<br>Disk /dev/sda: 4294 MB，该硬盘已分区：sda1,sda2，原系统使用4G的存储空间<br>Disk /dev/sdb: 5368 MB，==该硬盘未分区，5G硬盘为新增==<br>==以下信息，扇区有分逻辑扇区和物理扇区，对应512字节和4096字节，为何如此规划？==<br>Disk /dev/sda: 4294 MB, 4294967296 bytes, 8388608 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 4096 bytes<br><a href="https://www.ibm.com/developerworks/cn/linux/l-4kb-sector-disks/">参考IBM官网开发者论坛的解释：</a><br>解释得挺通俗易懂</p>
</li>
</ul>
<blockquote>
<ul>
<li>大致意思：原计算机界存储底层设计一个物理扇区对应512字节，里面包括数据和纠错冗余码，但现在需要存储大量数据，需要 ==<strong>提高每个物理扇区的字节容量，才能从整体上扩大硬盘容量，也让每个扇区放入更强大的纠错算法</strong>==，从而达到增加磁盘容量+提高可靠性两方面提升</li>
<li>在以往计算机的软件产业链里面，都是基于512 字节扇区来设计和实现，例如在基本输入/输出系统（BIOS）、引导装载程序、操作系统内核、文件系统代码和磁盘工具等工具中，也就是“软件层”只认512标准，物理扇区却提供4096，如何处理这种矛盾？</li>
<li>采用“中间件（内核驱动？）”软件层， ==<strong>将底层物理扇区划分为8个512字节的扇区，对于“软件层”来说，它们还是基于在512字节的扇区去操作数据</strong>==，西部数据是第一家生产这种磁盘的制造商，它用“ Advanced Format ”来代表带 4096 字节物理扇区且向 512 字节逻辑扇区转换的磁盘<br>既然涉及到数据写入4096字节物理扇区的的磁盘，中间需要分8个逻辑扇区去写入，那么这个过程是性能损耗的，具体说明如下：<br>最新的文件系统使用 4096 字节或更大尺寸的数据结构，因此，大部分磁盘 I/O 操作占用4k倍数的大小扇区，假设现在Linux系统要将一个文件写入到磁盘，<br>（1）当文件系统数据结构正好与底层物理分区4k大小一致，对 4k字节数据结构的读，就是直接对单一扇区的读写，因此硬盘的固件不需要做其他操作，不影响性能；<br>（2） 但当文件系统数据结构与底层物理扇区4k不完全一致时，例如文件为7k大小，读写操作必须使用两个物理扇区，对于读操作，因为读写头总是以大概率扫过连续两个扇区，不会消耗而外时间，==*<em>而对于写操作，磁盘的固件首先读取两个物理扇区，修改两个扇区的分区（分成2</em>8=16个逻辑扇区），然后入两个物理扇区。该操作所需时间比 4k占用一个扇区时所需时间多。因此，性能下降。**==<br>（3）如何判断文件数据结构是否得到合理对齐（放置到扇区上）？ 大多数文件系统将其数据结构与包含其本身的分区开头对齐。因此，如果一个分区起始于以一个 4096 字节（或8 个扇区的分界）边界，例如下表所示，则表示它得到合理对齐</li>
</ul>
</blockquote>
<table>
<thead>
<tr>
<th>分区</th>
<th>start</th>
<th>end</th>
</tr>
</thead>
<tbody><tr>
<td>sda1</td>
<td>2048</td>
<td>4096</td>
</tr>
<tr>
<td>sda2</td>
<td>4096</td>
<td>8192</td>
</tr>
<tr>
<td>sda3</td>
<td>8192</td>
<td>(8192+4096*10000)</td>
</tr>
</tbody></table>
<ul>
<li><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4>在使用fdisk /dev/sdb 进行分区时，由于centos已经默认第一个分区的边界时从2048开始，而是不是从0，128等小值开始，以下是以4096作为分区的起始值，为何要以一个大值作为第一个扇区边界值：==<strong>为 MBR 与第一个分区之间的未分配空间中的装载引导程序代码留出空间</strong>==<br>Partition number (1-4, default 1):<br>First sector  ==(2048-10485759, default 2048)== 4096<br>Last sector, +sectors or +size{K,M,G} (4096-10485759, default 10485759):<br>Using default value 10485759<br>Partition 1 of type Linux and of size 5 GiB is set<br>==要设为LVM分区，t命令更改==<br>Command (m for help): t<br>Selected partition 1<br>Hex code (type L to list all codes): 8e<br>Changed type of partition ‘Linux’ to ‘Linux LVM’</li>
</ul>
<p>Command (m for help): w<br>The partition table has been altered!<br>查看最新分区</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sdb1            2048    10485759     5241856   8e  Linux LVM</span><br></pre></td></tr></table></figure>
<p>5G硬盘已经被分区sdb1，LVM类型</p>
<ul>
<li><h3 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h3>格式化为一定格式的文件系统，与目标挂载目录的文件系统一致<br>通过命令查看<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs       2G  1.9G  1  80% /</span><br><span class="line">devtmpfs                devtmpfs  482M     0  482M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     494M  7.0M  487M   2% /run</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  162M  853M  16% /boot</span><br><span class="line">tmpfs                   tmpfs      99M     0   99M   0% /run/user/0  </span><br><span class="line">....</span><br></pre></td></tr></table></figure>
以上可以看到根目录/ 文件系统格式为 xfs<br>故将sdb1格式为xfs<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可以看到mkfs有常见的文件系统格式</span></span><br><span class="line">[root@nn ~]# mkfs</span><br><span class="line">mkfs         mkfs.btrfs   mkfs.cramfs  mkfs.ext2    mkfs.ext3    mkfs.ext4    mkfs.minix   mkfs.xfs </span><br><span class="line"><span class="meta">#</span><span class="bash"> 将新分区格式为xfs</span></span><br><span class="line">[root@nn ~]# mkfs.xfs /dev/sdb1</span><br></pre></td></tr></table></figure></li>
<li>调整目标LVM大小<br>（1）查看现有Volume Group名称</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               centos</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  .....</span><br></pre></td></tr></table></figure>
<p>这里vg名称为默认值：centos<br>（2）对新分区sdb1创建对应的物理卷</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pvcreate /dev/sdb1</span><br></pre></td></tr></table></figure>
<p> (3) 用以上物理卷扩展VG：centos</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vgextend centos /dev/sdb1</span><br><span class="line"><span class="meta">#</span><span class="bash"> No physical volume label <span class="built_in">read</span> from /dev/sdb1 Writing physical volume</span> </span><br><span class="line"><span class="meta">#</span><span class="bash">data to disk <span class="string">&quot;/dev/sdb1&quot;</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash">Physical volume <span class="string">&quot;/dev/sdb1&quot;</span> successfully created</span> </span><br><span class="line"><span class="meta">#</span><span class="bash">Volume group <span class="string">&quot;centos&quot;</span> successfully extended</span></span><br></pre></td></tr></table></figure>
<p> (4) 扩展 LVM 的逻辑卷 centos-root<br> 查看有哪些LVM逻辑卷<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# lvdisplay </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/centos/swap</span><br><span class="line">  LV Name                swap</span><br><span class="line">  VG Name                centos</span><br><span class="line">  LV Size                412.00 MiB</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/centos/root</span><br><span class="line">  LV Name                root</span><br><span class="line">  VG Name                centos</span><br><span class="line">  LV Size                2.59 GiB</span><br></pre></td></tr></table></figure><br> 截取关键行，以上显示有两个逻辑卷，swap和root，都归属于VG：centos<br> 现在要用sdb1增加的容量来扩展的root 逻辑卷，对于的LV path：/dev/centos/root<br> ==扩展==<br> ==<code>lvextend  /dev/centos/root    /dev/sdb1</code>==<br> ==调整大小==<br>==<code>xfs_growfs  /dev/centos/root</code>==</p>
<p>(5) 查看扩展效果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# lvscan </span><br><span class="line">  ACTIVE            &#x27;/dev/centos/swap&#x27; [412.00 MiB] inherit</span><br><span class="line">  ACTIVE            &#x27;/dev/centos/root&#x27; [&lt;7.59 GiB] inherit</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs       7.6G  2.9G  4.8G  38% /</span><br><span class="line">devtmpfs                devtmpfs  482M     0  482M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     494M  7.0M  487M   2% /run</span><br><span class="line">tmpfs                   tmpfs     494M     0  494M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  162M  853M  16% /boot</span><br><span class="line">tmpfs                   tmpfs      99M     0   99M   0% /run/user/0</span><br></pre></td></tr></table></figure>
<p>以上都说明逻辑卷root对于根目录，容量从2.6G，扩展到7.6G</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>LVM扩容</tag>
      </tags>
  </entry>
  <entry>
    <title>flume集群高可用连接kafka集群</title>
    <url>/blog/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面blog文章中：<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>和<a href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;flume NG集群前向的source是各类实时的log数据，通过flume sink将这些日志实时sink到后向kafka集群，所有flume sink其实是本架构里kafka的producer角色，kafka集群后向连接spark streaming，用于消费kafka的实时消息（log日志数据）流。</p>
<a id="more"></a>

<p>组件版本：<br>flume-1.9.0、kafka-2.12 </p>
<h4 id="1-在kafka集群上创建相应的topic"><a href="#1-在kafka集群上创建相应的topic" class="headerlink" title="1.在kafka集群上创建相应的topic"></a>1.在kafka集群上创建相应的topic</h4><p>&#8195;&#8195;在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是直接sink到hdfs上。<br>创建topic需要做一定规划，考虑到目前有三个broker节点，分别为nn、dn1以及dn2节点，所以创建了3个分区，每个分区有三个replica，topic名为：sparkapp</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181&#x2F;kafka-zk --replication-factor 3 --partitions 3 --topic sparkapp </span><br><span class="line">Created topic sparkapp.</span><br><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181&#x2F;kafka-zk --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>-zookeeper nn:2181/kafka-zk：因为kafka在zk的所有znode都统一放置在/kafka-zk路径下，所以启动时需要注意加上该路径。</p>
<h4 id="2-单节点配置flume的agent-sink"><a href="#2-单节点配置flume的agent-sink" class="headerlink" title="2.单节点配置flume的agent sink"></a>2.单节点配置flume的agent sink</h4><h5 id="2-1-配置flume-文件"><a href="#2-1-配置flume-文件" class="headerlink" title="2.1 配置flume 文件"></a>2.1 配置flume 文件</h5><p>&#8195;&#8195;这里首先给出单节点的flume是如何连接到kafka集群，在nn节点上启动flume进程。在第3章节，将给出flume集群连接kafka集群，实现两组件之间的高可用实时数据流。<br>flume source的数据源为<code>/opt/flume_log/web_log/access.log</code><br>这里拷贝一份新的配置文件，配置过程简单，相关组件的参数说明可以参考flume官网最新文档：<br><a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-sink">kafka-sink章节</a><br><code>[root@nn conf]# cp flume-conf.properties flume-kafka.properties</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">vi flume-kafka.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">设置<span class="built_in">source</span>组件<span class="built_in">exec</span>模式用 tail -F实时读取文本新的数据行</span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel组件，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定sinktype 为kafka sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的broker列表，用逗号（英文）隔开</span></span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> 前面创建的topic名称</span></span><br><span class="line">a1.sinks.k1.kafka.topic = sparkapp</span><br><span class="line"><span class="meta">#</span><span class="bash"> How many messages to process <span class="keyword">in</span> one batch. flume一次写入kafka的消息数</span></span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line"><span class="meta">#</span><span class="bash"> ack=1 说明只要求producer写入leader主分区即完成（<span class="built_in">wait</span> <span class="keyword">for</span> leader only)）</span></span><br><span class="line"><span class="meta">#</span><span class="bash">How many replicas must acknowledge a message before its considered successfully written.</span> </span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以设置分区ID，这里使用默认，也即kafka自己分区器</span></span><br><span class="line"><span class="meta">#</span><span class="bash">a1.sinks.k1.kafka.defaultPartitionId</span></span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 5</span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息的压缩类型</span></span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>组件和sinks组件绑定channel组件</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里sink的channel是单数</span></span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<p>这里的linger.ms=5主要是处理下情况：<br>producer是按照batch进行发送，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。<br>这里设置producer请求延时5ms才会被发送。</p>
<h5 id="2-2-测试数据消费情况"><a href="#2-2-测试数据消费情况" class="headerlink" title="2.2 测试数据消费情况"></a>2.2 测试数据消费情况</h5><p>在dn1节点上启动kafka consumer进程，等待flume sink，因为kafka已经集群，所以–bootstrap-server 选任意一个节点都可以，前提所选节点需在线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  saprkapp</span><br></pre></td></tr></table></figure>

<p>在nn节点上启动flume agent，这里不是后台启动，目的是为了实时观测flume agent 实时数据处理情况。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-kafka.properties --name a1</span><br></pre></td></tr></table></figure>
<p>手动在source文件最加新的文本行<br>[root@nn web_log]# echo “testing flume to kafka” &gt;&gt;access.log<br>在dn1 上可以看到实时输出nn节点上flume sink过来的消息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  sparkapp</span><br><span class="line">testing flume to kafka</span><br></pre></td></tr></table></figure>
<p>以上完成单节点flume实时数据到kafka的配置和测试，下面将使用flume集群模式sink到kafka集群</p>
<h4 id="3-flume-NG集群连接kafka集群"><a href="#3-flume-NG集群连接kafka集群" class="headerlink" title="3.flume NG集群连接kafka集群"></a>3.flume NG集群连接kafka集群</h4><p><img src="https://img-blog.csdnimg.cn/20191201225119507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">图3 为flume NG集群连接kafka集群的示意图<br>配置也相对简单，只需要把<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>第4.2章节的collector配置的sinks部分改为kafkasink，agent1、agent2和agent3用第4.1章节所提的配置文件内容即可，本文不再给出。<br>给个flume节点角色分布表<br>| 节点 |  flume 角色|kafka角色|<br>|–|–|–|<br>| nn | agent1，collector 1|kafka broker<br>| dn1 | agen2 |kafka broker<br>| dn2 | agent3，collector2 |kafka broker</p>
<h5 id="3-1-配置collector"><a href="#3-1-配置collector" class="headerlink" title="3.1 配置collector"></a>3.1 配置collector</h5><p>因为测试环境计算资源有限，每个flume进程和kafka进程都在同一服务器上运行，实际生产环境flume和kafka分别在不同服务器上。<br>配置collector：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume-1.9.0&#x2F;conf</span><br><span class="line">[root@nn conf]# vi avro-collector.properties</span><br><span class="line"># 定义三个组件</span><br><span class="line">collector1.sources &#x3D; r1</span><br><span class="line">collector1.sinks &#x3D; k1</span><br><span class="line">collector1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 定义source：这里的source配成avro，连接flume agent端sink avro</span><br><span class="line">collector1.sources.r1.type &#x3D; avro</span><br><span class="line"># bind的属性：dn2节点对应改为dn2</span><br><span class="line">collector1.sources.r1.bind &#x3D; nn</span><br><span class="line">collector1.sources.r1.port &#x3D; 52020</span><br><span class="line"></span><br><span class="line">#定义channel</span><br><span class="line">collector1.channels.c1.type &#x3D; memory</span><br><span class="line">collector1.channels.c1.capacity &#x3D; 500</span><br><span class="line">collector1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># 指定sinktype 为kafka sink，从而使得flume collector成为kafka的producer</span><br><span class="line">collector1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">collector1.sinks.k1.kafka.bootstrap.servers &#x3D; nn:9092,dn1:9092,dn2:9092</span><br><span class="line">collector1.sinks.k1.kafka.topic &#x3D; sparkapp</span><br><span class="line">collector1.sinks.k1.kafka.flumeBatchSize &#x3D; 20</span><br><span class="line">collector1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line">collector1.sinks.k1.kafka.producer.linger.ms &#x3D; 5</span><br><span class="line">collector1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"></span><br><span class="line"># source组件和sinks组件绑定channel组件</span><br><span class="line">collector1.sources.r1.channels &#x3D; c1</span><br><span class="line">collector1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure>

<h5 id="3-2-启动flume-ng集群服务"><a href="#3-2-启动flume-ng集群服务" class="headerlink" title="3.2 启动flume-ng集群服务"></a>3.2 启动flume-ng集群服务</h5><p>在nn和dn2节点上使用nohup &amp; 后台启动flume collector进程<br>nn节点上collector进程，jps可以看到Application进程<br> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[1] 26895</span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">27168 Jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">23589 Kafka</span><br><span class="line">26895 Application</span><br></pre></td></tr></table></figure><br> dn2同样操作，这里不再给出。</p>
<p>分别在nn、dn1和dn2节点上使用nohup &amp; 后台启动启动flume agent进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-agent.properties --name  agent1 -Dflume.root.logger=INFO,console &amp; </span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">324 Jps</span><br><span class="line">23589 Kafka</span><br><span class="line">30837 Application</span><br><span class="line">32462 Application</span><br></pre></td></tr></table></figure>
<p>可以在nn节点看到两个 Application进程，一个是flume collector进程，另外一个是flume agent进程。<br>dn1和dn2取同样的后台启动方式，这里不再给出。</p>
<h5 id="3-3-测试flume与kafka高可用"><a href="#3-3-测试flume与kafka高可用" class="headerlink" title="3.3 测试flume与kafka高可用"></a>3.3 测试flume与kafka高可用</h5><p>任找一个节点，启动kafka consumer 进程，这里在dn1节点上启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# .&#x2F;bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092,dn1:9092,dn2:9092 --topic sparkapp</span><br></pre></td></tr></table></figure>
<p>因为nn、dn1、dn2三个节点上都有flume agent进程，分别在每个节点下的access.log追加新数据行，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># nn节点追加新数据行</span><br><span class="line">[root@nn web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@nn web_log]# echo &quot;flume&amp;kafka HA--msg from nn&quot; &gt;&gt;access.log</span><br><span class="line"># dn1节点追加新数据行</span><br><span class="line">[root@dn1 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn1 web_log]#echo &quot;flume&amp;kafka HA--msg from dn1&quot; &gt;&gt;access.log</span><br><span class="line"># dn2节点追加新数据行</span><br><span class="line">[root@dn2 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn2 web_log]# echo &quot;flume&amp;kafka HA--msg from dn2&quot; &gt;&gt;access.log</span><br></pre></td></tr></table></figure>
<p>在kafka consumer shell可以看到实时接收三条记录：<br><img src="https://img-blog.csdnimg.cn/20191203235843219.png" alt="在这里插入图片描述"><br>能否关闭其中collector所在服务器中的一台用于同时测试flume和kafka的高可用，例如关闭nn服务器？<br>不能，因为这里只有三个节点，zookeeper集群必须要三个及以上才能保证高可用，因此这里只需要kill nn节点上的collector，此时flume集群只有dn2的collector在工作，按上面的步骤，在三个节点上都给access.log新增数据行，同样可以正常观测到dn1的kafka consumer拿到3条message</p>
<h4 id="4-小结"><a href="#4-小结" class="headerlink" title="4.小结"></a>4.小结</h4><p>本文给出了flume与kafka连接的高可用部署过程，设置相对简单，考虑到测试环境资源限制，这里把flume集群和kafka集群放在同一服务器，生产环境中，flume集群有独立的服务器提供，kafka集群也由独立的服务器提供。后面几篇文章将给出有关spark的深度理解和kafka与spark streaming整合内容，目的为用于实现实时处理批数据的环节。</p>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume连接kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop集群平台网络配置bond模式实现高可用</title>
    <url>/blog/2019/11/19/hadoop%E9%9B%86%E7%BE%A4%E5%B9%B3%E5%8F%B0%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AEbond%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面文章关于hadoop大数据项目开发中，每台服务器之间其实存在大量的IO，例如NameNode服务器和DataNode服务器同步fimag文件件，DataNode之间的数据IO，为压榨服务器对于超大数据的吞吐量，在服务器的网络层使用bond模式配置Linux网卡，可以将该服务器上多个物理网卡虚拟成一张网卡，由该虚拟网卡对外提供统一网络服务，实现多个网卡自适应负载均衡以及提高数据的吞吐量，同时也实现链路双备份功能，保证服务器底层网络高可用性。bond虚拟网卡可以绑定多个网络，例如绑定2两个网卡，一般物理服务器有4个千兆网卡，所以也可以绑定4个网卡，绑定的网卡类型需要一致，否则无法进行bond模式配置。<br>&#8195;&#8195;以centos7.5 的两个网卡配成bond模式作为示例，其中在之前的hadoop的相关文章里，nn节点与dn2节点构成HA，dn节点作为DataNode，当这三个节点的底层网络都配成bond模式，将进一步提高本blog之前搭建大数据开发平台的高可用性。作为全栈开发者，这些涉及简单网络的配置，应该需要掌握。</p>
<a id="more"></a>

<h4 id="1、为测试服务器添加多个网卡"><a href="#1、为测试服务器添加多个网卡" class="headerlink" title="1、为测试服务器添加多个网卡"></a>1、为测试服务器添加多个网卡</h4><p>在VMware workstations中，给相应服务器新增一个网卡，选择NAT模式，这样这是基于虚拟机操作，实际生成环境中，我的开发项目会部署在真实服务器上，真实服务器不需要NAT模式。</p>
<h4 id="2、查看测试服务器的网卡信息"><a href="#2、查看测试服务器的网卡信息" class="headerlink" title="2、查看测试服务器的网卡信息"></a>2、查看测试服务器的网卡信息</h4><p>原网卡ens33是配有IP的，新增的网卡ens37无IP绑定。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ip a</span><br><span class="line"></span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.188.0.5/24 brd 192.188.0.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: ens37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 ***/64 scope link noprefixroute </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>可以看到两个网卡ens33和ens37，都是千兆网速，都为up状态</p>
<h4 id="3、配置两个网卡"><a href="#3、配置两个网卡" class="headerlink" title="3、配置两个网卡"></a>3、配置两个网卡</h4><h5 id="3-1-这里需要先把原网卡配置拷贝一份作为备份。"><a href="#3-1-这里需要先把原网卡配置拷贝一份作为备份。" class="headerlink" title="3.1 这里需要先把原网卡配置拷贝一份作为备份。"></a>3.1 这里需要先把原网卡配置拷贝一份作为备份。</h5><p>备份网卡ens33的配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# pwd</span><br><span class="line">/etc/sysconfig/network-scripts</span><br><span class="line">[root@nn network-scripts]# cp ifcfg-ens33 ifcfg-ens33.bak</span><br></pre></td></tr></table></figure>

<p>新增网卡ens37是没有对应配置文件，只需要从ens33拷贝一份即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# cp ifcfg-ens33 ifcfg-ens37</span><br></pre></td></tr></table></figure>

<h5 id="3-2-将ens33和ens37配成slave模式"><a href="#3-2-将ens33和ens37配成slave模式" class="headerlink" title="3.2 将ens33和ens37配成slave模式"></a>3.2 将ens33和ens37配成slave模式</h5><p>这里不再需要配置ip和dns，mac地址也不需要配置，linux自动识别。</p>
<p>配置ens33，</p>
<p>注意该网卡已配置有IP，在改完该网卡配置后，请勿直接重启网卡，否则ssh无法远程连接，因为slave模式下，该网卡配置文件里面是无IP地址的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=ens33</span><br><span class="line">NAME=ens33</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line"><span class="meta">#</span><span class="bash"> bond网卡的名字</span></span><br><span class="line">MASTER=bond0</span><br><span class="line"><span class="meta">#</span><span class="bash"> ens33网卡启用slave模式</span></span><br><span class="line">SLAVE=yes</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置ens37 ，同上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=ens37</span><br><span class="line">NAME=ens37</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line"><span class="meta">#</span><span class="bash"> bond网卡的名字</span></span><br><span class="line">MASTER=bond0</span><br><span class="line"><span class="meta">#</span><span class="bash"> ens33网卡启用slave模式</span></span><br><span class="line">SLAVE=yes</span><br></pre></td></tr></table></figure>

<p>若有更多的网卡需要绑定为bond0模式，则按以上配置改即可。</p>
<h4 id="4、配置bond0虚拟网卡"><a href="#4、配置bond0虚拟网卡" class="headerlink" title="4、配置bond0虚拟网卡"></a>4、配置bond0虚拟网卡</h4><p>在 <code>/etc/sysconfig/network-scripts</code>下，需要创建一个<code>ifcfg-bond0</code>文件，用于配置名字为bond0的虚拟网卡，注意这里bond0只是一个名字，可以按需要命名，例如mybond。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# vi ifcfg-bond0</span><br></pre></td></tr></table></figure>

<p>配置文件如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=bond0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">NM_CONTROLLER=no</span><br><span class="line">BONDING_OPTS=&quot;miimon=100 mode=6&quot;</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.188.0.5</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.188.0.1</span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line">DNS2=114.114.114.115</span><br></pre></td></tr></table></figure>

<p>若不想在bond0配置文件写入<code>BONDING_OPTS=&quot;miimon=100 mode=6&quot;</code></p>
<p>也可以在/etc/modprobe.d/dist.conf</p>
<p>新增如下两行：</p>
<p><code>alias bond0 bonding options bond0 miimon=100 mode=6</code></p>
<p>但个人建议直接在bond0网卡配置mod模式，方便后期查看和更改。</p>
<h4 id="5、bond0的配置说明和工作原理"><a href="#5、bond0的配置说明和工作原理" class="headerlink" title="5、bond0的配置说明和工作原理"></a>5、bond0的配置说明和工作原理</h4><p>重点配置项为：BONDING_OPTS=”miimon=100 mode=6”</p>
<p>该配置文件:miimon=100，意思是linux每100ms监测一次本服务器网络链路连接状态（这里说的网络连接，是指该服务器网卡与接入层交换机端口连接的状态），如果有其中网卡例如ens33中断，那么bond0会将网络连接切到可用ens37；</p>
<p>mode的值表示工作模式，共有0，1，2，3，4，5，6六种模式，常用为0，6，1三种</p>
<ul>
<li><p>mode=0，表示load balancing (round-robin)为负载均衡方式，两块网卡都工作，但是与网卡相连的交换必须做特殊配置（ 这两个端口需要配置成聚合方式），因为做bonding的这两块网卡是使用同一个MAC地址</p>
</li>
<li><p>mode=1，表示fault-tolerance (active-backup)提供冗余功能，工作方式是主备的工作方式，也就是说默认情况下只有一块网卡工作，另一块做备份 。</p>
</li>
<li><p>mode=6，表示load balancing (round-robin)为负载均衡方式，两块网卡都工作，该模式下无需配置交换机，因为做bonding的这两块网卡是使用不同的MAC地址。</p>
<p>mode6因为可以做负载均衡，因此实际场景使用效果好，mode6的工作原理：bond0虚拟网卡的mac地址为两张网卡mac地址其中的一个，bond0通过更改自身与某个网卡一样的Mac地址，以达到随时切换转发数据包到相应正常工作的网卡中。从外部交换机来看，交换机只看到bond0这个网卡，bond0就像Nginx，都是代理角色，代理后面有多个实体。</p>
<p>mode6负载均衡的是这么实现的：</p>
<p>假设测试服务器的网卡ens33连接交换机33号端口，ens37连接交换机37端口。</p>
<p>当bond0检测到ens33流量超过阈值时，则bond0会将自己Mac地址切换到en37的mac地址，所以交换机通过发arp广播包，找到37端口就是连接ens37网卡，所以网络流量就从交换机37端口转发到ens37网卡。</p>
</li>
</ul>
<h4 id="6、-加载内核bond模块-modprobe-bonding"><a href="#6、-加载内核bond模块-modprobe-bonding" class="headerlink" title="6、 加载内核bond模块 modprobe bonding"></a>6、 加载内核bond模块 modprobe bonding</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# modprobe bonding</span><br><span class="line">[root@nn network-scripts]# lsmod |grep bond </span><br><span class="line">bonding               152656  0 </span><br></pre></td></tr></table></figure>

<p>重启网络</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# service network restart</span><br><span class="line">Restarting network (via systemctl):                        [  OK  ]</span><br></pre></td></tr></table></figure>



<h4 id="7、查看bond0虚拟网卡状态并测试主备网卡切换"><a href="#7、查看bond0虚拟网卡状态并测试主备网卡切换" class="headerlink" title="7、查看bond0虚拟网卡状态并测试主备网卡切换"></a>7、查看bond0虚拟网卡状态并测试主备网卡切换</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# cat /proc/net/bonding/bond0 </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 负载均衡模式</span></span><br><span class="line">Bonding Mode: adaptive load balancing</span><br><span class="line">Primary Slave: None</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前对外连接网络额度是网卡ens33</span></span><br><span class="line">Currently Active Slave: ens33</span><br><span class="line"><span class="meta">#</span><span class="bash"> 状态up</span></span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens33</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f1</span><br><span class="line">Slave queue ID: 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f2</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>



<p>查看bond0的mac地址，跟ens33的mac地址一样：12:1d:21:fg:43:f1 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ip a</span><br><span class="line"></span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: ens37: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line"></span><br><span class="line">191: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.142.4/24 brd 192.168.142.255 scope global noprefixroute bond0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>



<p>关闭ens33网卡，看看bond0是否会切换到ens37</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn network-scripts]# ifdown ens33</span><br><span class="line">Device &#x27;ens33&#x27; successfully disconnected.</span><br><span class="line"></span><br><span class="line">[root@localhost network-scripts]# cat /proc/net/bonding/bond0 </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line">Bonding Mode: adaptive load balancing</span><br><span class="line">Primary Slave: None</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到当ens33不工作后，网络连接已经切换到ens37</span></span><br><span class="line">Currently Active Slave: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f2</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop集群网络配置</tag>
      </tags>
  </entry>
  <entry>
    <title>supervisor管理web服务进程</title>
    <url>/blog/2019/11/06/supervisor%E7%AE%A1%E7%90%86web%E6%9C%8D%E5%8A%A1%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;部分较小的项目例如flask，对内部使用，可无需使用web server，直接用flask自带服务即可完成需求，但考虑到直接使用<code>python app.py </code>启动flask后，运行过程进程可能会退出，因此有必要对其进行监控并自动重启，supervisor可满足需求，加上其用python写成，可以独自进行二次开发（supervisor的代码值得学习）。本文是对以往部分项目进行一个整理，作为参考资料归档。</p>
<a id="more"></a>

<h4 id="1、离线安装supervisor"><a href="#1、离线安装supervisor" class="headerlink" title="1、离线安装supervisor"></a>1、离线安装supervisor</h4><p>supervisor下载地址：</p>
<p><code>https://files.pythonhosted.org/packages/ca/1f/07713b0e1e34c312450878801d496bce8b9eff5ea9e70d41ff4e299b2df5/supervisor-4.1.0-py2.py3-none-any.whl</code></p>
<p>setuptools的下载地址:</p>
<p><code>https://files.pythonhosted.org/packages/d9/de/554b6310ac87c5b921bc45634b07b11394fe63bc4cb5176f5240addf18ab/setuptools-41.6.0-py2.py3-none-any.whl</code></p>
<p>先安装setuptools</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip  setuptools-41.6.0.zip &amp;&amp; cd  setuptools-41.6.0</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<p>再安装supervisor</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip  supervisor-4.1.0.tar.gz &amp;&amp; cd supervisor-4.1.0</span><br><span class="line">python setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试是否安装成功</span></span><br><span class="line"><span class="meta">[root@~d]#</span><span class="bash"> python</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import supervisor</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;</span> </span><br></pre></td></tr></table></figure>



<h4 id="2、配置supervisor环境变量"><a href="#2、配置supervisor环境变量" class="headerlink" title="2、配置supervisor环境变量"></a>2、配置supervisor环境变量</h4><p>很多网上教程在安装完supervisor后，都会给出如下提示：</p>
<p>在/etc/supervisor目录下生成配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">echo_supervisord_conf&gt;</span><span class="bash">/etc/supervisord.conf</span></span><br></pre></td></tr></table></figure>

<p>但如果echo_supervisord_conf或者supervisord 二进制程序所在目录没有配到PATH环境变量的话，那么提示未找到对应的命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# echo_supervisord_conf&gt;/etc/supervisord.conf</span><br><span class="line">-bash: echo_supervisord_conf: command not found</span><br></pre></td></tr></table></figure>

<p>查找echo_supervisord_conf  在哪个目录下</p>
<h5 id="2-1-PATH的问题"><a href="#2-1-PATH的问题" class="headerlink" title="2.1 PATH的问题"></a>2.1 PATH的问题</h5><p>一般会在python的安装目录下，例如/usr/local/python27/bin 或者在编译安装python所设定的路径</p>
<p><code>[root@wap~]# which python3.5 /usr/local/bin/python3.5</code></p>
<p>但会有例外：</p>
<p>很多人安装python后，并没有把python的bin路径设到PATH里，而是直接把新安装的python 路径通过软链接的方式覆盖旧python，例如下面所示：</p>
<p><code>ln -s /usr/bin/python3.5 /usr/bin/python</code></p>
<p>虽然这种方式在可以启动python3.5，但一旦安装新的库后，这些库所在路径不是位于python3.5目录下，而是位于系统默认的bin目录下，如下面所示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# ls /usr/bin/</span><br><span class="line">flask      python3.5      pyvenv-3.5   pip3    supervisorctl  supervisord  echo_supervisord_conf  </span><br></pre></td></tr></table></figure>
<p>这里flask库和supervisor库都在<code>/usrl/bin/</code>目录下<br>而此时查看系统系统的path变量，centos最初始的path配置如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .bash_profile</span></span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash"> User specific environment and startup programs</span></span><br><span class="line">PATH=$PATH:$HOME/bin</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure>
<p>显然没有加入python的环境变量，修改后如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Get the aliases and <span class="built_in">functions</span></span></span><br><span class="line">if [ -f ~/.bashrc ]; then</span><br><span class="line">        . ~/.bashrc</span><br><span class="line">fi</span><br><span class="line">PATH=$PATH:$HOME/bin:/usr/bin/</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure>

<p>到此，supervisor的相关命令可在shell直接使用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# echo_supervisord_conf </span><br><span class="line">; Sample supervisor config file.</span><br><span class="line">;</span><br><span class="line">; For more information on the config file, please see:</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<h4 id="3、-将supervisor配置成service，并加入开机自启"><a href="#3、-将supervisor配置成service，并加入开机自启" class="headerlink" title="3、 将supervisor配置成service，并加入开机自启"></a>3、 将supervisor配置成service，并加入开机自启</h4><h5 id="3-1-加入service"><a href="#3-1-加入service" class="headerlink" title="3.1 加入service"></a>3.1 加入service</h5><p>离线安装supervisor后，还需配置开机启动service，这里要注意：</p>
<p>需要将 <code>daemon &quot;supervisord -c /etc/supervisord.conf &quot;</code></p>
<p>改为<code>daemon &quot;/usr/bin/supervisord -c /etc/supervisord.conf &quot;</code></p>
<p>完整的service配置如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# vi /etc/rc.d/init.d/supervisord</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> <span class="keyword">function</span> library</span></span><br><span class="line">. /etc/rc.d/init.d/functions</span><br><span class="line">RETVAL=0</span><br><span class="line">start() &#123;</span><br><span class="line">  echo -n $&quot;Starting supervisord: &quot;</span><br><span class="line">  daemon &quot;/usr/bin/supervisord -c /etc/supervisord.conf &quot;</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  echo</span><br><span class="line">  [ $RETVAL -eq 0 ] &amp;&amp; touch /var/lock/subsys/supervisord</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop() &#123;</span><br><span class="line">  echo -n $&quot;Stopping supervisord: &quot;</span><br><span class="line">  killproc supervisord</span><br><span class="line">  echo</span><br><span class="line">  [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/supervisord</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">restart() &#123;</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line"> start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line"> stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line"> restart|force-reload|reload)</span><br><span class="line">  restart</span><br><span class="line">  ;;</span><br><span class="line"> condrestart)</span><br><span class="line">  [ -f /var/lock/subsys/supervisord ] &amp;&amp; restart</span><br><span class="line">  ;;</span><br><span class="line"> status)</span><br><span class="line">  status supervisord</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  ;;</span><br><span class="line"> *)</span><br><span class="line">  echo $&quot;Usage: $0 &#123;start|stop|status|restart|reload|force-reload|condrestart&#125;&quot;</span><br><span class="line">  exit 1</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">exit $RETVAL</span><br></pre></td></tr></table></figure>

<p>修改文件权限为755，并设置开机启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod 755 /etc/rc.d/init.d/supervisord</span><br><span class="line">chkconfig supervisor on</span><br></pre></td></tr></table></figure>



<h5 id="3-2-修改默认的-etc-supervisord-conf"><a href="#3-2-修改默认的-etc-supervisord-conf" class="headerlink" title="3.2 修改默认的/etc/supervisord.conf"></a>3.2 修改默认的/etc/supervisord.conf</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[unix_http_server]</span><br><span class="line">file=/var/run/supervisor.sock  </span><br><span class="line"></span><br><span class="line">[supervisord]</span><br><span class="line">logfile=/var/log/supervisor/supervisord.log ; 修改为 /var/log 目录，避免被系统删除</span><br><span class="line">pidfile=/var/run/supervisord.pid </span><br><span class="line">...</span><br><span class="line">[supervisorctl]</span><br><span class="line">; 和&#x27;unix_http_server&#x27;里面的设定匹配</span><br><span class="line">serverurl=unix:///var/run/supervisor.sock </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：<code>/var/log/supervisor/supervisord.log</code>需要手动创建</p>
<h5 id="3-4、配置supervisord的web界面"><a href="#3-4、配置supervisord的web界面" class="headerlink" title="3.4、配置supervisord的web界面"></a>3.4、配置supervisord的web界面</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[inet_http_server]      </span><br><span class="line">port=*:36700      </span><br><span class="line">username=***           </span><br><span class="line">password=***         </span><br></pre></td></tr></table></figure>

<h5 id="3-5、配置管理进程"><a href="#3-5、配置管理进程" class="headerlink" title="3.5、配置管理进程"></a>3.5、配置管理进程</h5><p>配置管理进程有两种方式，一种是直接在supervisord.conf文件写入相关进程配置，另外一种是在某个目录下，为每个进程单独创建一份配置文件，然后把这些配置文件include到supervisord.conf，已防止supervisord.conf的全局配置被错误修改。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> /etc/supervisor/config，用于存放全部进程管理的配置文件</span></span><br><span class="line">[root@wap ~]# mkdir -p /etc/supervisor/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在/etc/supervisord.conf全局配置文件中的include参数，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将/etc/supervisor/config目录添加到include中</span></span><br><span class="line"></span><br><span class="line">[include]</span><br><span class="line">files =/etc/supervisor/config/*.ini</span><br></pre></td></tr></table></figure>

<p>这里以配置flask两个小项目作为示例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]#  vi /etc/supervisor/config/flask_app_1.init</span><br><span class="line">[program:flask_app_1]</span><br><span class="line">command=python /opt/flask_app_1/app.py </span><br><span class="line"><span class="meta">#</span><span class="bash"> 日志目录需要手动创建，文件名由supervisor创建</span></span><br><span class="line">stdout_logfile=/etc/supervisor/logs/flask_app_1.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> stdout日志文件大小，默认50MB，这里设为10M</span></span><br><span class="line">stdout_logfile_maxbytes=10MB  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 限制保存最近10个日志</span></span><br><span class="line">stdout_logfile_backups=10   </span><br><span class="line"><span class="meta">#</span><span class="bash"> 在 supervisord 启动的时候也自动启动</span></span><br><span class="line">autostart=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 把stderr重定向到stdout，默认<span class="literal">false</span></span></span><br><span class="line">redirect_stderr = true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 程序异常退出后自动重启</span></span><br><span class="line">autorestart=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动5秒后若没有异常退出，则认为该服务已经正常启动</span></span><br><span class="line">startsecs=5</span><br><span class="line">user=root ;默认使用root用户</span><br><span class="line">priority=10  ;该进程的优先级</span><br><span class="line">stopasgroup=true   ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup=true   ;默认为false，向进程组发送kill信号，包括子进程</span><br></pre></td></tr></table></figure>
<p>flask_app_2同上配置，注意实际使用中，应与相关需求或功能命名。</p>
<h4 id="4、启动supervisor并测试"><a href="#4、启动supervisor并测试" class="headerlink" title="4、启动supervisor并测试"></a>4、启动supervisor并测试</h4><p>这里因为已经将supervisor配置成service，故不需要再使用</p>
<p><code>supervisord -c /etc/supervisord.conf </code> 这样的命令启动supervisor</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@wap ~]# service supervisor start</span><br><span class="line">Starting supervisord:                                      [  OK  ]</span><br><span class="line"></span><br><span class="line">[root@wap ~]# service supervisor status</span><br><span class="line">supervisord (pid  18921) is running...</span><br><span class="line"></span><br><span class="line">[root@wap ~]# service supervisor stop</span><br><span class="line">Stopping supervisord:                                      [  OK  ]</span><br><span class="line">[root@wap ~]# service supervisor restart</span><br><span class="line">Stopping supervisord:                                      [  OK  ]</span><br><span class="line">Starting supervisord:                                      [  OK  ]</span><br></pre></td></tr></table></figure>

<p>以上说明supervisor的service配置是成功的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看已经被管理进程，这里有两种方式，一种在web 页面查看与操作，另外一种则用cli方式查看</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cli查看相关被管理进程服务</span></span><br><span class="line">[root@portal py_apps]# supervisorctl </span><br><span class="line">flask_app_1                     RUNNING   pid 191047, uptime 0:11:04</span><br><span class="line">flask_app_2                     RUNNING   pid 191047, uptime 0:11:04</span><br><span class="line"><span class="meta">supervisor&gt;</span><span class="bash"> </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启几次flask，查看其日志</span></span><br><span class="line">[root@wap ~]# cat  /etc/supervisor/logs/flask_app_1.log   </span><br><span class="line"> * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line"> * Environment: production</span><br><span class="line">   WARNING: Do not use the development server in a production environment.</span><br><span class="line">   Use a production WSGI server instead.</span><br><span class="line"> * Debug mode: off</span><br><span class="line"> * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line"> * Environment: production</span><br><span class="line">   WARNING: Do not use the development server in a production environment.</span><br><span class="line">   Use a production WSGI server instead.</span><br><span class="line"> * Debug mode: off</span><br></pre></td></tr></table></figure>

<p>通过web查看和管理进程<br><img src="https://img-blog.csdnimg.cn/20191106145102844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="5、supervisor分布式管理服务"><a href="#5、supervisor分布式管理服务" class="headerlink" title="5、supervisor分布式管理服务"></a>5、supervisor分布式管理服务</h4><p>&#8195;&#8195;例如在serverA放了app1，serverB放了app2，serverC放了app3，而且每个server还有自行开发的其他采集模块等，因supervisor只能管理单机上的进程，对于不同服务器上的进程管理，则需要借助其他工具：例如cesi，地址：<code>https://github.com/Gamegos/cesi</code>。它可以实现跨服务器管理进程。当然我们完全可以基于supervisor进行开发，利用它提供的rpc接口，也可以开发出集群管理工具。但不建议这么做，考虑到还需要主力攻项目，这些通用工具能用第三方可优先使用。本文不再对cesi进行测试。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title>使用连接池方式和多线程方式连接mysql的测试说明</title>
    <url>/blog/2019/08/29/%E4%BD%BF%E7%94%A8%E8%BF%9E%E6%8E%A5%E6%B1%A0%E6%96%B9%E5%BC%8F%E5%92%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%96%B9%E5%BC%8F%E8%BF%9E%E6%8E%A5mysql%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面文章讨论了mysql做高可用的配置，参考<a href="https://blog.csdn.net/pysense/article/details/99892680">文章链接</a>，而本文则是开发项目过程需要用的部分，从配置数据库到实用数据库，以及再用SQL做BI分析再到SQL优化，这些都是全栈工程师的基本功。</p>
<a id="more"></a>

<h4 id="1、连接池测试mysql默认连接配置"><a href="#1、连接池测试mysql默认连接配置" class="headerlink" title="1、连接池测试mysql默认连接配置"></a>1、连接池测试mysql默认连接配置</h4><p>&#8195;&#8195;先出简单的测试连接池或多线程并发的脚本，这里先借用DBUtils创建连接池，文章后面会给出无须借用第三方库也可以实现实用的连接池。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> DBUtils.PooledDB <span class="keyword">import</span> PooledDB</span><br><span class="line"></span><br><span class="line">db_info=&#123;</span><br><span class="line">            <span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;port&#x27;</span>:<span class="number">34312</span>, <span class="comment"># 改掉默认端口号，安全考虑</span></span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;***&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;***&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;db&#x27;</span>:<span class="string">&#x27;erp_app&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;charset&#x27;</span>:<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">db_pool = PooledDB(</span><br><span class="line">    creator=pymysql,</span><br><span class="line">    maxconnections=<span class="number">3000</span>,  <span class="comment"># 连接池允许的最大连接数，0和None表示不限制连接数</span></span><br><span class="line">    mincached=<span class="number">0</span>,  <span class="comment"># 初始化时，连接池中至少创建的空闲的链接，0表示不创建</span></span><br><span class="line">    maxcached=<span class="number">0</span>,  <span class="comment"># 连接池中最多闲置的连接，0和None不限制</span></span><br><span class="line">    maxshared=<span class="number">0</span>, <span class="comment"># 连接池中最多共享的连接数量，0和None表示全部共享。</span></span><br><span class="line">    blocking=<span class="literal">True</span>,  <span class="comment"># 连接池中如果没有可用连接后，是否阻塞等待。True</span></span><br><span class="line">    maxusage=<span class="literal">None</span>,  <span class="comment"># 一个连接最多被重复使用的次数，None表示无限制</span></span><br><span class="line">    **db_info <span class="comment"># 数据库账户等信息</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试插入数据</span></span><br><span class="line">insert_sql = (<span class="string">&quot;insert into apps_name &quot;</span></span><br><span class="line">              <span class="string">&quot;(id,app_log_name,log_path,log_date) &quot;</span></span><br><span class="line">              <span class="string">&quot;values(null,%(app_log_name)s,%(log_path)s,null)&quot;</span>)</span><br><span class="line">insert_data=&#123;</span><br><span class="line">    <span class="string">&#x27;app_log_name&#x27;</span>:<span class="string">&#x27;BI-Access-Log&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;log_path&#x27;</span>:<span class="string">&#x27;/opt/data/apps_log/&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">mode,inst_sql,inst_data</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    m:多线程模式，c:连接池模式</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;m&#x27;</span>:</span><br><span class="line">        conn=pymysql.connect(**db_info)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        conn = db_pool.connection()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> conn.cursor() <span class="keyword">as</span> cur:</span><br><span class="line">            resl=cur.execute(inst_sql,inst_data)</span><br><span class="line">            conn.commit()</span><br><span class="line">    <span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> err:</span><br><span class="line">        conn.rollback()</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="comment"># PooledDB连接池关闭方法其实不是真的把该连接关闭，而是将该连接由放入池的队列里，在后文会看到该逻辑的实现</span></span><br><span class="line">        conn.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_insert</span>(<span class="params">mode,nums=<span class="number">151</span></span>):</span></span><br><span class="line">    treads=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nums):</span><br><span class="line">        t=threading.Thread(target=save_data,args=(mode,insert_sql,insert_data))</span><br><span class="line">        treads.append(t)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> treads:</span><br><span class="line">        t.start()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> treads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">mode,request_nums</span>):</span></span><br><span class="line">    start=time.time()</span><br><span class="line">    multi_insert(mode,request_nums)</span><br><span class="line">    end=time.time()</span><br><span class="line">    cost=end-start</span><br><span class="line">    print(<span class="string">&#x27;cost:&#123;0:.3&#125; s&#x27;</span>.<span class="built_in">format</span>(cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run(<span class="string">&#x27;c&#x27;</span>,<span class="number">100000</span>)</span><br></pre></td></tr></table></figure>

<p>查看mariadb默认设置的最大连接数为151</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; show variables like &#x27;%max_con%&#x27;;</span><br><span class="line">+---------------------------------------+-------+</span><br><span class="line">| Variable_name                         | Value |</span><br><span class="line">+---------------------------------------+-------+</span><br><span class="line">| extra_max_connections                 | 1     |</span><br><span class="line">| max_connect_errors                    | 100   |</span><br><span class="line">| max_connections                       | 151   |</span><br><span class="line">| performance_schema_max_cond_classes   | 80    |</span><br><span class="line">| performance_schema_max_cond_instances | -1    |</span><br><span class="line">+---------------------------------------+-------+</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;运行测试脚本，注意这里是连接池10万个连接的并发，对于server端的mysql来说，也就是同时有200个并发connections，就已经出错了，这就是模拟了客户端多线程大量并发消耗完mysql 最大连接资源引起error</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#x27;t connect to MySQL server on &#x27;192.168.100.5&#x27; ([Errno 24] Too many open files)&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="2、高并发连接数据库出错分析与测试"><a href="#2、高并发连接数据库出错分析与测试" class="headerlink" title="2、高并发连接数据库出错分析与测试"></a>2、高并发连接数据库出错分析与测试</h4><p>&#8195;&#8195;出现以上情况，可通过设置mysql max_connections最大值，来保证并发量,测试mysql在有限物理资源条件下可达到的最大连接数，随便设一个大值例如10000000，最后可mysql单机可设定的最大连接数为10万</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; set GLOBAL max_connections=10000000;</span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [erp_app]&gt; show variables like &#x27;%max_con%&#x27;;</span><br><span class="line">+---------------------------------------+--------+</span><br><span class="line">| Variable_name                         | Value  |</span><br><span class="line">+---------------------------------------+--------+</span><br><span class="line">| extra_max_connections                 | 1      |</span><br><span class="line">| max_connect_errors                    | 100    |</span><br><span class="line">| max_connections                       | 100000 |</span><br><span class="line">| performance_schema_max_cond_classes   | 80     |</span><br><span class="line">| performance_schema_max_cond_instances | -1     |</span><br><span class="line">+---------------------------------------+--------+</span><br><span class="line">5 rows in set (0.001 sec)</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;在这里，==使用多线程方式==，测试脚本发起10万个线程并发请求，运行后程序很快出现socket请求打满客户端系统缓冲区，导致系统级别出错，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#39;t connect to MySQL server on &#39;192.168.</span><br><span class="line">100.5&#39; ([WinError 10055] 由于系统缓冲区空间不足或队列已满，不能执行套接字上的操</span><br><span class="line">作。)&quot;)</span><br></pre></td></tr></table></figure>
<p>再查看服务器响应的最大连接数，成功连接仅有745个</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; show global status like &#x27;Max_used_connections&#x27;;</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Variable_name        | Value |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Max_used_connections | 745   |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">1 row in set (0.001 sec)</span><br></pre></td></tr></table></figure>
<h4 id="3、为何选用连接池优化连接？"><a href="#3、为何选用连接池优化连接？" class="headerlink" title="3、为何选用连接池优化连接？"></a>3、为何选用连接池优化连接？</h4><p>&#8195;&#8195;从第二部分的测试可知，因为每个线程创建单独的连接，当并发量大时，会造成client和msyql server之间的频繁“线程创建tcp连接-登录-线程退出关闭tcp连接”，<br>若采用连接池方式连接mysql，则是重用数据库服务端的tcp通道，以达到client和MySQL之间只需维持较少的连接，单个客户端可以提高其并发量，且消耗较低的物理资源。</p>
<p>打个不一定恰当的通俗比喻：</p>
<p>有10000辆车同时要从A点到达B点，出发前，A、B之间没有路，需要先搭建</p>
<p>1）多线程方式：需要1000个路面施工队同时搭建完1000条“高速路”后，才能同时出发，可见需要消耗非常多资源（10000个施工队以及10000条高速路资源），等10000辆车到达B点后，10000个施工队又得去拆除10000条高速路，非常耗资源</p>
<p>2）连接池方式（假设连接池大小为1000）：需要1000个路面施工队同时搭建完1000条“高速路”后，前面1000辆车到达B点，后面9000辆车出发时，施工队不需要再新建高速路，继续重用前面搭建的1000条“高速路”，极大降低的物理资源浪费。</p>
<p>连接池重要两个逻辑：</p>
<ul>
<li><p>在程序创建连接的时候，可以从连接池队列中取出一个空闲的连接，不需要重新初始化连接，提升获取连接的速度</p>
</li>
<li><p>关闭连接的时候，把连接放回存放连接池的队列中，而不是真正的关闭，所以可以减少频繁地打开和关闭tcp通道</p>
</li>
</ul>
<h4 id="4、继续测试两种连接效果"><a href="#4、继续测试两种连接效果" class="headerlink" title="4、继续测试两种连接效果"></a>4、继续测试两种连接效果</h4><p>&#8195;&#8195;mysql的默认最大连接数已设置为1万个<br>1）开启2000个线程，重复1次，使用多线程并发，不出意外，mysql接收到已用连接数为1022个，之后的请求连接全部error 中断</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@localhost opt]# python insert_test.py</span><br><span class="line">cost:1.99 s</span><br><span class="line"></span><br><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#x27;t connect to MySQL server on &#x27;192.168.100.4&#x27; ([Errno 24] Too many open files)&quot;)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show global status like &#x27;Max_used_connections&#x27;;</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Variable_name        | Value |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Max_used_connections | 1022  |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">1 row in set (0.000 sec)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>开启多线程6000个并发，运行出错，6000个线程直接把客户端的系统缓冲区打满，无法继续运行</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pymysql.err.OperationalError: (2003, &quot;Can&#39;t connect to MySQL server on &#39;192.168.100.4&#39; ([WinError 10055] 由于系统缓冲区空间不足或队列已满，不能执行套接字上的操作。)&quot;)</span><br></pre></td></tr></table></figure>

<p>3）连接池开启6000个并发，连接池最大连接限制3000个，运行没有问题，cost:24.3 s</p>
<p>在mysql也可以看到最大已用连接数为3000个</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show global status like &#x27;Max_used_connections&#x27;;</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Variable_name        | Value |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">| Max_used_connections | 3000  |</span><br><span class="line">+----------------------+-------+</span><br><span class="line">1 row in set (0.001 sec)</span><br></pre></td></tr></table></figure>
<h4 id="5-、自行实现简易使用的mysql连接池"><a href="#5-、自行实现简易使用的mysql连接池" class="headerlink" title="5 、自行实现简易使用的mysql连接池"></a>5 、自行实现简易使用的mysql连接池</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> pymysql.cursors <span class="keyword">import</span> DictCursor</span><br><span class="line"></span><br><span class="line">db_info=&#123;</span><br><span class="line">            <span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.4&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;port&#x27;</span>:<span class="number">3306</span>,</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;***&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;****&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;db&#x27;</span>:<span class="string">&#x27;erp_app&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;charset&#x27;</span>:<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConnPoolException</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;连接池出错 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MariaDBPool</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    _inst_lock=threading.RLock()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, connections, **db_conf</span>):</span></span><br><span class="line">        self.__connections = connections</span><br><span class="line">        self.__pool = Queue(connections)</span><br><span class="line">        <span class="comment"># 在init阶段，就已经创建好指定的连接，全部put到共享队列</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.__connections):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                conn = pymysql.connect(**db_conf)</span><br><span class="line">                self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">except</span> ConnPoolException <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">raise</span> IOError</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 单例模式创建连接池，个人喜欢用__new__方法创建，简洁，且使用了递归锁，保证在多线程方式创建单例模式的对象都是同一对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">with</span> cls._inst_lock:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(cls,<span class="string">&#x27;_inst&#x27;</span>):</span><br><span class="line">                cls._inst=<span class="built_in">object</span>.__new__(cls)</span><br><span class="line">        <span class="keyword">return</span> cls._inst</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute_insert</span>(<span class="params">self,sql,data_dict=<span class="literal">None</span></span>):</span></span><br><span class="line">        conn = self.__pool.get()</span><br><span class="line">        cursor = conn.cursor(DictCursor)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result=cursor.execute(sql,data_dict) <span class="keyword">if</span> data_dict <span class="keyword">else</span> cursor.execute(sql)</span><br><span class="line">            conn.commit()</span><br><span class="line">        <span class="keyword">except</span> ConnPoolException <span class="keyword">as</span> e:</span><br><span class="line">           <span class="comment"># 这里就是重点，只是关闭了游标，连接对像又返回池里   </span></span><br><span class="line">            conn.rollback()</span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 这里就是重点，只是关闭了游标，连接对像又返回池里</span></span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">executemany_insert</span>(<span class="params">self,sql,data_dict_list=<span class="literal">None</span></span>):</span></span><br><span class="line">        conn = self.__pool.get()</span><br><span class="line">        cursor = conn.cursor(DictCursor)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result=cursor.execute(sql,data_dict) <span class="keyword">if</span> data_dict_list <span class="keyword">else</span> cursor.executemany(sql)</span><br><span class="line">        <span class="keyword">except</span> ConnPoolException <span class="keyword">as</span> e:</span><br><span class="line">            conn.rollback()</span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cursor.close()</span><br><span class="line">            self.__pool.put(conn)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 这里才是真正的关闭所有连接池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.__connections):</span><br><span class="line">            self.__pool.get().close()</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;就本文测试的数据库以及简单表结构而言，使用该连接池模块，注意测试之前，需重启mysql，保证数据库最大已连接数为1，也即清空历史残留连接，以便做新测试对比。12000个并发，mysql设定最大可用连接数：3000，单例模式，cost:22.2 s，程序不会出现任何异常，当然因表结构和服务器性能情况而不同。</p>
<p>&#8195;&#8195;这里顺便提下oracle线程池，部分项目使用oracle数据库，cx_Oracle也支持使用连接池方式连接，大致流程如下，也可根据使用习惯封装更适合自身业务需要的模块。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cx_Oracle</span><br><span class="line">db_user=<span class="string">&#x27;foo&#x27;</span></span><br><span class="line">db_pwd=<span class="string">&#x27;barbar&#x27;</span></span><br><span class="line">db_host=<span class="string">&#x27;192.168.100.7&#x27;</span></span><br><span class="line">db_name=<span class="string">&#x27;erp_app&#x27;</span></span><br><span class="line">dsn = cx_Oracle.makedsn(db_host, <span class="string">&quot;1521&quot;</span>, db_name)</span><br><span class="line">db_config = &#123;</span><br><span class="line">    <span class="string">&#x27;user&#x27;</span>: db_user,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>: db_pwd,</span><br><span class="line">    <span class="string">&#x27;dsn&#x27;</span>: dsn,</span><br><span class="line">    <span class="string">&#x27;min&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;max&#x27;</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">&#x27;increment&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;threaded&#x27;</span>: <span class="literal">True</span></span><br><span class="line">&#125;</span><br><span class="line">orc_pool = cx_Oracle.SessionPool(**dbConfig)</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;若使用多线程方式，尤其数量大的情况下（500以上），很容易把底层bug爆出来，cx_Oracle会引发python解释器崩溃。其实建议，只要是连接数据库，所引入的第三方库支持线程池方法的话，都建议用线程池，哪怕你的插入数据不是太频繁或并发量不大，减少程序自身出错，也降低数据库连接压力。</p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql连接池</tag>
      </tags>
  </entry>
  <entry>
    <title>使用kazoo连接zookeeper并监听节点数量以及值变化</title>
    <url>/blog/2019/09/10/%E4%BD%BF%E7%94%A8kazoo%E8%BF%9E%E6%8E%A5zookeeper%E5%B9%B6%E7%9B%91%E5%90%AC%E8%8A%82%E7%82%B9%E6%95%B0%E9%87%8F%E4%BB%A5%E5%8F%8A%E5%80%BC%E5%8F%98%E5%8C%96/</url>
    <content><![CDATA[<p>&#8195;&#8195;目前kazoo是连接zk的最新第三方库，最新更新时间为2019年1月，其他第三方连接zk的库都长时间未更新，所以推荐使用kazoo。前面有几篇文章都已经详细给出了zk的部署，接下来是zk最核心的地方，将zk的数据结构特性跟业务场景相结合，实现复杂需求，本文给出基本demo用法介绍。</p>
<a id="more"></a>

<h5 id="1、监控节点数量的变化"><a href="#1、监控节点数量的变化" class="headerlink" title="1、监控节点数量的变化"></a>1、监控节点数量的变化</h5><p>&#8195;&#8195;基本操作，创建、更新、删除，kazoo接口已经足够简单，入参类型如果不懂，可以直接看源码，同时也有助于深入了解别人是如何构思python“中间件”</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> kazoo <span class="keyword">import</span> exceptions</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_test</span>(<span class="params">zk_path,host,port,node_list</span>):</span></span><br><span class="line">    zk=KazooClient(hosts=host+<span class="string">&#x27;:&#x27;</span>+port,timeout=<span class="number">5</span>)</span><br><span class="line">    zk.start(timeout=<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> zk.exists(zk_path):</span><br><span class="line">        print(<span class="string">&quot;node:&#123;&#125; does&#x27;t exists&quot;</span>.<span class="built_in">format</span>(zk_path))</span><br><span class="line">        <span class="comment"># 创建当前节点,持久性节点,值需要设为byte类型</span></span><br><span class="line">        zk.create(path=zk_path,value=<span class="string">b&#x27;bar&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里是获取当前节点的子节点列表，可以设定watch以及是否返回节点数据</span></span><br><span class="line">    child_node_list=zk.get_children(zk_path,watch=<span class="literal">None</span>,include_data=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 创建多个子节点，值可以设为一样，因为这里关注子节点是否存在，不关心其值</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> child_node_list:</span><br><span class="line">        <span class="keyword">for</span> sub_node <span class="keyword">in</span> node_list:</span><br><span class="line">            zk.create(zk_path + <span class="string">&#x27;/&#x27;</span> + sub_node,<span class="string">b&#x27;1&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;subnode list:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(child_node_list))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取当前节点的znode对象：含data和ZnodeStat对象</span></span><br><span class="line">    data,stat=zk.get(zk_path)</span><br><span class="line">    print(<span class="string">&#x27;current node data:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data))</span><br><span class="line">    print(<span class="string">&#x27;data version:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(stat.version))</span><br><span class="line">    print(<span class="string">&#x27;data length:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(stat.data_length))</span><br><span class="line">    print(<span class="string">&#x27;children node numbers:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(stat.numChildren))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新节点数据,可以指定值和版本，成功更新则ZnodeStat 对象</span></span><br><span class="line">    stat_new=zk.<span class="built_in">set</span>(zk_path,value=<span class="string">b&#x27;foo&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;node &#123;0&#125; is updated:&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(zk_path,stat_new))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除当前节点，若当前节点有子节点，则提示无法删除，需要使用递归删除</span></span><br><span class="line">    zk.delete(zk_path,recursive=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        last=zk.get_children(zk_path)</span><br><span class="line">        print(<span class="string">&#x27;children nodes :&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(last))</span><br><span class="line">    <span class="keyword">except</span> exceptions.NoNodeError:</span><br><span class="line">        print(<span class="string">&#x27;no children nodes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    zk.stop()</span><br><span class="line">    zk.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    normal_test(zk_path=<span class="string">&#x27;/app_conf&#x27;</span>,host=<span class="string">&#x27;192.168.100.5&#x27;</span>,port=<span class="string">&#x27;2181&#x27;</span>,node_list=[<span class="string">&#x27;foo1&#x27;</span>,<span class="string">&#x27;foo2&#x27;</span>,<span class="string">&#x27;foo3&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;==监控节点数量的变化，可以应用到相关的场景：<br>1）把节点名称设为服务器IP，可以实现服务器集群管理，服务（服务接口）上、下线通知等，又称服务发现、服务监控等<br>2）主备切换，把最小临时节点设为master角色，其他临时节点为salve角色<br>3）独占锁，若只监听一个固定临时节点，当该临时节点创建，则获得锁，否则释放锁<br>4）分布式锁，不同客户端创建不同临时顺序节点，链式监听节点是否删除事件==</p>
<h5 id="2、简单的wacher"><a href="#2、简单的wacher" class="headerlink" title="2、简单的wacher"></a>2、简单的wacher</h5><p>&#8195;&#8195;kazoo支持使用装饰器实现一个简单的wacher，kazoo有两种wacher，一个是监听子节点变化，另外一个是监听节点值的变化。监听子节点变化示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_child_node</span>(<span class="params">zk_path</span>):</span></span><br><span class="line">    zkc=KazooClient(hosts=<span class="string">&#x27;192.168.100.5:2181&#x27;</span>,timeout=<span class="number">5</span>)</span><br><span class="line">    zkc.start(timeout=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># 直接用装饰器完成监听</span></span><br><span class="line"><span class="meta">    @ChildrenWatch(<span class="params">client=zkc,path=zk_path,send_event=<span class="literal">True</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_changes_with_event</span>(<span class="params">children,event</span>):</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;Children nodes are %s&quot;</span> % children)</span><br><span class="line">         <span class="keyword">if</span> event:</span><br><span class="line">            print(<span class="string">&quot;catched nodes a children nodes event &quot;</span>,event)</span><br><span class="line">            </span><br><span class="line"><span class="meta">    @ChildrenWatch(<span class="params">client=zkc,path=zk_path</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_changes_without_event</span>(<span class="params">children</span>):</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;Children are %s&quot;</span> % children)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    	time.sleep(<span class="number">5</span>)</span><br><span class="line">    	print(<span class="string">&#x27;watching children node changes.....&#x27;</span>)</span><br><span class="line">    	</span><br><span class="line">watch_child_node(/app_conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在zk上连续创建几个子节点，可以看到监听到变化</span></span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">2</span>] create /app_conf/foo1 <span class="number">1</span></span><br><span class="line">Created /app_conf/foo1</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">3</span>] create /app_conf/foo2 <span class="number">1</span></span><br><span class="line">Created /app_conf/foo2</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">4</span>] create /app_conf/foo3 <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出，捕捉到节点变化的事件，但zk不会给出这个事件的具体发生情况：子节点被删除的事件、子节点新增的事件</span></span><br><span class="line">需要客户端根据事件的发生写一段逻辑去获取zk的节点到底是增加了还是减少了。</span><br><span class="line">Children nodes are []</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>, <span class="string">&#x27;foo2&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line">watching children nodes changes.....</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>, <span class="string">&#x27;foo2&#x27;</span>, <span class="string">&#x27;foo3&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line">Children nodes are [<span class="string">&#x27;foo1&#x27;</span>, <span class="string">&#x27;foo2&#x27;</span>]</span><br><span class="line">catching a children nodes event  WatchedEvent(<span class="built_in">type</span>=<span class="string">&#x27;CHILD&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：在注册监听环节，可以监听当前节点本身是否删除的事件，以及子节点的增、删事件，若需要zk返回event，那么需要将send_event设为True,才可以在watch函数传入event位置参数,这个逻辑可以在kazoo的源码看到<br>if self._send_event<br>    result = self._func(children, event)<br>else:<br>    result = self._func(children)<br>装饰器有两种写法，一种是从引用kazoo import的ChildrenWatch，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ChildrenWatch(<span class="params">client=zkc,path=zk_path,send_event=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_changes_with_event</span>(<span class="params">children,event</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>另外一种是从已创建的zk实例中调用ChildrenWatch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@zkc.ChildrenWatch(<span class="params">path=zk_path,send_event=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_changes_with_event</span>(<span class="params">children,event</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>



<h6 id="2-1-监听节点自身的数据变化"><a href="#2-1-监听节点自身的数据变化" class="headerlink" title="2.1 监听节点自身的数据变化"></a>2.1 监听节点自身的数据变化</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">watch_data</span>(<span class="params">zk_path</span>):</span></span><br><span class="line">    zkc = KazooClient(hosts=<span class="string">&#x27;192.168.100.5:2181&#x27;</span>, timeout=<span class="number">5</span>)</span><br><span class="line">    zkc.start(timeout=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接用装饰器完成监听,节点值的监听还可以拿到zk的事件</span></span><br><span class="line">    <span class="comment">#使用@DataWatch(client=zkc,path=zk_path)或者两种写法都可以</span></span><br><span class="line"><span class="meta">    @zkc.DataWatch(<span class="params">path=zk_path</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_watch</span>(<span class="params">data, stat,event</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">&quot;Data is &#123;0&#125; and data type is &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(data, <span class="built_in">type</span>(data)))</span><br><span class="line">        print(<span class="string">&quot;Version is %s&quot;</span> % stat.version)</span><br><span class="line">        <span class="keyword">if</span> event:</span><br><span class="line">            print(<span class="string">&quot;catching a data event &quot;</span>,event)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        print(<span class="string">&#x27;watching current node data changes.....&#x27;</span>)</span><br><span class="line"></span><br><span class="line">watch_data(<span class="string">&#x27;/app_conf&#x27;</span>)</span><br><span class="line"><span class="comment"># 在zk 将/app_conf set不同的值，可以看到监听到数据变化</span></span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">20</span>] <span class="built_in">set</span> /app_conf foo</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">19</span>] <span class="built_in">set</span> /app_conf <span class="number">2</span></span><br><span class="line"><span class="comment"># 输出，注意kazoo返回的是bytes类型的数据,data变化的事件已经捕捉到</span></span><br><span class="line">Data <span class="keyword">is</span> <span class="string">b&#x27;foo&#x27;</span> <span class="keyword">and</span> data <span class="built_in">type</span> <span class="keyword">is</span> &lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">bytes</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"><span class="title">Version</span> <span class="title">is</span> 22</span></span><br><span class="line"><span class="class"><span class="title">watching</span> <span class="title">current</span> <span class="title">node</span> <span class="title">data</span> <span class="title">changes</span>.....</span></span><br><span class="line"><span class="class"><span class="title">Data</span> <span class="title">is</span> <span class="title">b</span>&#x27;<span class="title">bar</span>&#x27; <span class="title">and</span> <span class="title">data</span> <span class="title">type</span> <span class="title">is</span> &lt;<span class="title">class</span> &#x27;<span class="title">bytes</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"><span class="title">Version</span> <span class="title">is</span> 23</span></span><br><span class="line"><span class="class"><span class="title">catching</span> <span class="title">a</span> <span class="title">data</span> <span class="title">event</span>  <span class="title">WatchedEvent</span>(<span class="params"><span class="built_in">type</span>=<span class="string">&#x27;CHANGED&#x27;</span>, state=<span class="string">&#x27;CONNECTED&#x27;</span>, path=<span class="string">&#x27;/app_conf&#x27;</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;节点数据的变化，很容易联想相关应用场景：<br>1）集中配置管理，各个客户端监听放置配置文件内容的节点，若配置有变化，则可以各个客户端拉取配置更新<br>2）消息队列：<br>&#8195;&#8195;在特定节点下创建持久顺序节点，创建成功时Watcher通知等待的队列，队列删除序列号最小的节点用以消费。此场景下znode存储的数据就是消息队列中的消息内容，持久顺序节点就是消息的编号，按排序后取出最小编号节点（先get后delete）。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题</p>
<h6 id="2-2-封装一个同时监听子节点变化和当前节点数据变化的watcher类"><a href="#2-2-封装一个同时监听子节点变化和当前节点数据变化的watcher类" class="headerlink" title="2.2 封装一个同时监听子节点变化和当前节点数据变化的watcher类"></a>2.2 封装一个同时监听子节点变化和当前节点数据变化的watcher类</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> ChildrenWatch</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> DataWatch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZKWatcher</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host,port,timeout=<span class="number">5</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self._old_node_list=[]</span><br><span class="line">        self._node_name=<span class="string">&#x27;&#x27;</span></span><br><span class="line">        self._host=host</span><br><span class="line">        self._port=port</span><br><span class="line">        self._time_out=timeout</span><br><span class="line">        self._ip_port=self._host+<span class="string">&#x27;:&#x27;</span>+self._port</span><br><span class="line">        self._zkc=KazooClient(hosts=self._ip_port,timeout=self._time_out)</span><br><span class="line">        self._zkc.start(self._time_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">watcher</span>(<span class="params">self,zk_path</span>):</span></span><br><span class="line">        <span class="comment"># 获取原子节点列表</span></span><br><span class="line">        self._old_node_list=self._zkc.get_children(zk_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 为所要监听的节点开启一个子节点监听器</span></span><br><span class="line">            ChildrenWatch(client=self._zkc,path=zk_path,func=self._node_change,send_event=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为所要监听的节点开启一个该节点值变化的监听器</span></span><br><span class="line">            DataWatch(client=self._zkc,path=zk_path,func=self._data_change)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_node_change</span>(<span class="params">self,new_node_list,event</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里的new_node_list是指当前最新的子节点列表</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> event:</span><br><span class="line">            print(<span class="string">&#x27;未有事件发生&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">		<span class="comment"># 当前节点列表与上次拿到的节点列表相等，注意不是长度相等，是列表值和长度都要相等</span></span><br><span class="line">        <span class="keyword">if</span> new_node_list == self._old_node_list:</span><br><span class="line">            print(<span class="string">&#x27;子节点列表未发生变化&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(new_node_list)&gt;<span class="built_in">len</span>(self._old_node_list):</span><br><span class="line">            <span class="keyword">for</span> new_node <span class="keyword">in</span> new_node_list:</span><br><span class="line">                <span class="keyword">if</span> new_node <span class="keyword">not</span> <span class="keyword">in</span> self._old_node_list:</span><br><span class="line">                    print(<span class="string">&#x27;监听到一个新的节点：%s&#x27;</span>%<span class="built_in">str</span>(new_node))</span><br><span class="line">                    self._old_node_list=new_node_list</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> old_node <span class="keyword">in</span> self._old_node_list:</span><br><span class="line">                <span class="keyword">if</span> old_node <span class="keyword">not</span> <span class="keyword">in</span> new_node_list:</span><br><span class="line">                    print(<span class="string">&#x27;监听到一个删除的节点：%s&#x27;</span>%<span class="built_in">str</span>(old_node))</span><br><span class="line">                    self._old_node_list=new_node_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data_change</span>(<span class="params">self,data,stat,event</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">            print(<span class="string">&#x27;节点已删除，无法获取数据&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> event:</span><br><span class="line">            print(<span class="string">&#x27;未有事件发生&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;监听到数据变化&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;数据为&#x27;</span>,data)</span><br><span class="line">        print(<span class="string">&#x27;数据长度&#x27;</span>,stat.dataLength)</span><br><span class="line">        print(<span class="string">&#x27;数据版本号：&#x27;</span>,stat.version)</span><br><span class="line">        print(<span class="string">&#x27;子节点数据版本号&#x27;</span>,stat.cversion)</span><br><span class="line">        print(<span class="string">&#x27;子节点数量&#x27;</span>,stat.numChildren)</span><br><span class="line">        print(<span class="string">&#x27;事件&#x27;</span>,event)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        zk = ZKWatcher(host=<span class="string">&#x27;192.168.100.5&#x27;</span>,port=<span class="string">&#x27;2181&#x27;</span>)</span><br><span class="line">        zk.watcher(<span class="string">&#x27;/app_locker&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            time.sleep(<span class="number">5</span>)</span><br><span class="line">            print(<span class="string">&#x27;watching......&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>kazoo</tag>
      </tags>
  </entry>
  <entry>
    <title>在HadoopHA节点上部署Kafka集群组件</title>
    <url>/blog/2019/11/28/%E5%9C%A8HadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2kafka%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章中<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件 》</a>已经介绍了flume实时收集acces.log，同时给出flume是如何实现数据流向的高可用环境测试。在后面的文章中会给出实时大数据项目的开发，实时数据源由flume sink到kafka的topic里，而不是前面提到的hdfs，目的是利用kafka强大的分布式消息组件用于分发来自flume的实时数据流。<br>kafka集群在Hadoop实时大数据项目的位置，如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191124162957995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p>
<a id="more"></a>

<h4 id="1、Kafka的基本介绍"><a href="#1、Kafka的基本介绍" class="headerlink" title="1、Kafka的基本介绍"></a>1、Kafka的基本介绍</h4><h5 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h5><p>Kafka 是一种分布式的，基于发布/订阅的消息系统（redis也可以实现该功能），主要设计目标如下：<br>以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。<br>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。<br>支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。<br>同时支持离线数据处理和实时数据处理。<br>Scale out：支持在线水平扩展。</p>
<h5 id="1-2-kafka-应用场景"><a href="#1-2-kafka-应用场景" class="headerlink" title="1.2 kafka 应用场景"></a>1.2 kafka 应用场景</h5><ul>
<li>日志收集：可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer。不过在本文中，flume用于收集数据日志，kafka组件用于接受来自flume的event</li>
<li>流式处理：spark streaming，在上面的架构图也可以清楚看到kafka组件的下游为spark streaming，它消费来自kafka topic的实时数据消息。</li>
<li>消息系统：解耦生产者和消费者、缓存消息等。</li>
<li>用户活动跟踪：kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后消费者通过订阅这些topic来做实时的监控分析，亦可保存到hbase、mangodb等数据库。</li>
<li>运营指标：kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，</li>
<li>生产各种操作的集中反馈，比如报警和报告。<br>可以看出kafka在大数据实时处理以及互联网产品方面应用最为突出。</li>
</ul>
<h5 id="1-3-kafka相关术语"><a href="#1-3-kafka相关术语" class="headerlink" title="1.3 kafka相关术语"></a>1.3 kafka相关术语</h5><ul>
<li><p>producer : 生产者，生产message发送到topic，例如flume sink就是生产者</p>
</li>
<li><p>consumer : 消费者，订阅topic并消费message, consumer作为一个线程来消费，例如实时处理的spark streaming。</p>
</li>
<li><p>Broker：Kafka节点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群，在大数据项目中，直接利用已有的hadoop节点服务器配置成kafka集群。整个 Kafka 集群架构会有一个 zookeeper集群，通过 zookeeper 管理集群配置，选举 kafka Leader，以及在 Consumer Group 发生变化时进行 Rebalance。</p>
</li>
<li><p>topic：一类消息，消息存放的目录即主题，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发</p>
</li>
<li><p>massage： Kafka中最基本的传递对象。</p>
</li>
<li><p>partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的segment以及index：partition在物理上已一个文件夹的形式存在，由多个segment文件组成和多个index文件，它们是很对出现，每个Segment存着message信息，每个index存放着message的offset</p>
</li>
<li><p>replica：partition 的副本，保障 partition 的高可用。个人建议写成replica partition–副本分区</p>
</li>
<li><p>leader：这里的leader要理解为某个partition 作为主分区，也即称为leader partition，要注意该partition所在的服务器不能称为leader，否认会被误认为是kafka集群的master服务器（Kafka把master服务器称为controller）。 producer 和 consumer 只跟 leader petition交互。</p>
</li>
<li><p>replicas：leader 角色的partition加上replica角色的partition，一起成为replicas，也就是该topic总共有多少个副本数，副本数包含一个主分区副本和其余的副本分区。</p>
</li>
<li><p>controller：为了避免更leader这个词混淆，开发者将kafka 集群中的其中一台服务器称为controller，用于对每个topic的partition leader选举以及实现对partition的failover。</p>
</li>
<li><p>consumer Group：消费者组，一个Consumer Group包含多个consumer</p>
</li>
<li><p>offset：偏移量，理解为消息partition中的索引</p>
</li>
</ul>
<h4 id="2、kafka-单点部署与测试"><a href="#2、kafka-单点部署与测试" class="headerlink" title="2、kafka 单点部署与测试"></a>2、kafka 单点部署与测试</h4><h5 id="2-1-配置文件"><a href="#2-1-配置文件" class="headerlink" title="2.1 配置文件"></a>2.1 配置文件</h5><p>目前官方kafka最新稳定版本为2.3.1<br>按官方建议以下建议，项目用到scala2.1.3，kafka用了官方的建议版本2.1.2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">We build for multiple versions of Scala. This only matters if you are using Scala and you want a version built for the same Scala version you use. Otherwise any version should work (2.12 is recommended). </span><br></pre></td></tr></table></figure>
<p>kafka组件同样被放置在/opt目录下，该目录放置所有Hadoop及其组件，便于统一管理</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hbase-2.1.7   kafka-2.12      scala-2.13.1             </span><br><span class="line">flume_log     hive-3.1.2    mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    xcall.sh          </span><br></pre></td></tr></table></figure>

<p>配置server.properties。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn config]# vi server.properties </span><br><span class="line">[root@nn config]# pwd</span><br><span class="line">/opt/kafka-2.12/config</span><br><span class="line"><span class="meta">#</span><span class="bash"> The id of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果是kafka集群，需配置全局id</span></span><br><span class="line">broker.id=10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Socket Server Settings #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以不设置，kafka自动获取hostname</span></span><br><span class="line">listeners=PLAINTEXT://nn:9092</span><br><span class="line">advertised.listeners=PLAINTEXT://nn:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Basics #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终存放消息的路径,建议放在kafka组件目录下，方便管理</span></span><br><span class="line">log.dirs=/opt/kafka-2.12/kafka-logs</span><br><span class="line">num.partitions=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Zookeeper #############################</span></span></span><br><span class="line">zookeeper.connect=nn:2181,dn1:2181,dn2:2181/kafka-zk</span><br><span class="line"><span class="meta">#</span><span class="bash"> Timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br></pre></td></tr></table></figure>
<p>已更新配置：<del>zookeeper.connect=nn:2181,dn1:2181,dn2:2181</del><br>考虑到后面项目中，对kafka在zk上方便更为管理，用了新的配置：zookeeper.connect=nn:2181,dn1:2181,dn2:2181/kafka-zk<br>因为kafka默认在zk的根路径下创建多个节点路径，当需要去zk查看kafka相关的元数据时显得有点混乱，因此这里要求kafka将它要创建的所有znode都统一放在/kafka-zk这个路径下，方便集中查看和管理kafka的元数据，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0]  ls /kafka-zk</span><br><span class="line">[cluster, controller_epoch, controller, brokers, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config]</span><br></pre></td></tr></table></figure>
<p>本文后面内容所有kafka命令中，若有<code>--zookeeper nn:2181</code> 这样启动参数，都需要改为<code>--zookeeper nn:2181/kafka-zk</code></p>
<h5 id="2-2-启动kafka进程"><a href="#2-2-启动kafka进程" class="headerlink" title="2.2 启动kafka进程"></a>2.2 启动kafka进程</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh config/server.properties </span><br></pre></td></tr></table></figure>
<p>启动后提示内存不足<br>“There is insufficient memory ”<br>因为kafka的启动脚本为最大堆申请1G内存，由于使用虚拟机跑项目，资源有限，将 kafka-server-start.sh的export KAFKA_HEAP_OPTS=”-Xmx1G -Xms1G”修改为export KAFKA_HEAP_OPTS=”-Xmx256M -Xms128M”，最大堆空间为256M，初始堆空间为128M。<br>使用后台进程方式启动kafka服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过jps也可以看到kafka进程</span></span><br><span class="line">[root@nn kafka-2.12]# jps</span><br><span class="line">4609 QuorumPeerMain</span><br><span class="line">14436 JournalNode</span><br><span class="line">2454 HMaster</span><br><span class="line">2552 Jps</span><br><span class="line">14185 DataNode</span><br><span class="line">15017 NodeManager</span><br><span class="line">2365 Kafka</span><br><span class="line">13983 NameNode</span><br><span class="line">14879 ResourceManager</span><br></pre></td></tr></table></figure>
<h5 id="2-3-测试topic"><a href="#2-3-测试topic" class="headerlink" title="2.3 测试topic"></a>2.3 测试topic</h5><p>创建无备份的topic,名称为hadoop，分区数1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]#bin/kafka-topics.sh --create --zookeeper nn:2181 --replication-factor 1  --partitions 1 --topic hadoop</span><br><span class="line">Created topic hadoop.</span><br></pre></td></tr></table></figure>
<p>查看新建的topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn bin]# kafka-topics.sh --list --zookeeper nn:2181</span><br><span class="line">hadoop</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看kafka在zookeeper上创建的topic znode上可以看到 hadoop这个topic</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls /brokers</span><br><span class="line">[ids, topics, seqid]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /brokers/topics</span><br><span class="line">[hadoop]</span><br></pre></td></tr></table></figure>
<p>启动producer进程，这是一个console，可以命令式发送message</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list nn:9092 --topic hadoop</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">spark</span></span><br></pre></td></tr></table></figure>

<p>新打开一个shell用于启动consumer进程，订阅hadoop这个topic，该进程会持续监听9092端口，一旦上面producer的console写入信息，这边consumer就会立刻打印同样信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic hadoop</span><br><span class="line">hello kafka</span><br><span class="line">spark</span><br></pre></td></tr></table></figure>

<p>查看hadoop这个topic的所在的物理文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里的hadoop-0就是hadoop topic的parition</span></span><br><span class="line">[root@nn hadoop-0]# pwd</span><br><span class="line">/opt/kafka-2.12/kafka-logs/hadoop-0</span><br><span class="line"></span><br><span class="line">[root@nn hadoop-0]# ls</span><br><span class="line">00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>
<p>有index、log文件，新版本的kafka还多了timeindex时间索引。至此完成kafka单节点的配置和测试。</p>
<h4 id="3、kafka集群部署与测试"><a href="#3、kafka集群部署与测试" class="headerlink" title="3、kafka集群部署与测试"></a>3、kafka集群部署与测试</h4><h5 id="3-1-配置server-properties"><a href="#3-1-配置server-properties" class="headerlink" title="3.1 配置server.properties"></a>3.1 配置server.properties</h5><p>kafka集群部署要求所在节点上已经运行zookeeper集群。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn config]# vi server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个节点id需唯一nn设10，dn1设11，dn2设12</span></span><br><span class="line">broker.id=10</span><br><span class="line">ip和端口这里可以不配置，kafka自动读取，也方便把整个kafka目录分发到其他节点上</span><br><span class="line"><span class="meta">#</span><span class="bash">listeners=PLAINTEXT://:9092</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放的日志，kafka自动创建</span></span><br><span class="line">log.dirs=/opt/kafka-2.12/kafka-logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置zk集群</span></span><br><span class="line">zookeeper.connect=nn:2181,dn1:2181,dn2:2181</span><br></pre></td></tr></table></figure>
<p>其他属性项基本是调优项目，这里不再一一给出，后面用单独一篇文章给出讨论。<br>将kafka-2.12目录拷贝到dn1和dn2节点上，修改对应的broker.id即可</p>
<h5 id="3-2-集群测试"><a href="#3-2-集群测试" class="headerlink" title="3.2 集群测试"></a>3.2 集群测试</h5><p>分布在三个节点上启动kafka服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@dn1 kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@dn2 kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line"><span class="meta">#</span><span class="bash"> jps可以看到每个节点上都已经有kafka进程</span></span><br><span class="line">[root@nn opt]# sh xcall.sh jps |grep ka</span><br><span class="line">10569 Kafka</span><br><span class="line">12836 Kafka</span><br><span class="line">28243 Kafka</span><br></pre></td></tr></table></figure>

<p>创建一个新的topic：sparkapp，3份拷贝，3分区</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-topics.sh --create --zookeeper nn:2181,dn1:2181,dn2:2181 --replication-factor 3 --partitions 3 --topic sparkapp</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看sparkapp主分区及其副本分区的情况</span></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-topics.sh --describe --zookeeper nn:2181 --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br></pre></td></tr></table></figure>
<p>该命令其实就是读取/brokers/topics/sparkapp/partitions/**/state 所有分区的state节点值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] get  &#x2F;brokers&#x2F;topics&#x2F;sparkapp&#x2F;partitions&#x2F;0&#x2F;state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:22,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[10,11,12]&#125;</span><br></pre></td></tr></table></figure>


<p>在nn节点启动producer进程，连接broker分别为nn自己、dn1节点和dn2节点，都能正常连接，同理，dn1、dn2的producer进程使用dn1、dn2、nn节点都能正常连接</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list nn:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br><span class="line"></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list dn1:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br><span class="line"></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list dn2:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br></pre></td></tr></table></figure>

<p>在nn节点启动producer进程，然后在dn1节点、dn2节点以及nn新shell分别启动consumer，看看一个producer生产msg，其他三个节点能否同时收到</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br><span class="line">[root@dn2 kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br></pre></td></tr></table></figure>

<p>查看kafka-cluster这个topic的partition在物理文件上的分布</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-logs]# ls sparkapp-</span><br><span class="line">sparkapp-0/ sparkapp-1/ sparkapp-2/ </span><br><span class="line">[root@nn kafka-logs]# ls sparkapp-0/</span><br><span class="line">00000000000000000000.index  00000000000000000000.timeindex</span><br><span class="line">00000000000000000000.log    leader-epoch-checkpoint</span><br></pre></td></tr></table></figure>
<p>可以看到三个分区对于三个文件目录，每个目录有索引文件和数据文件</p>
<h5 id="3-3-在zk上查看集群情况"><a href="#3-3-在zk上查看集群情况" class="headerlink" title="3.3 在zk上查看集群情况"></a>3.3 在zk上查看集群情况</h5><p>kafka在zk上的数据存储结构：<br>brokers列表：ls /brokers/ids<br>某个broker信息：get /brokers/ids/10<br>topic信息：get /brokers/topics/sparkapp<br>partition信息：get /brokers/topics/sparkapp/partitions/0/state<br>controller中心节点变更次数：get /controller_epoch<br>conrtoller信息：get /controller<br>[zk: localhost:2181(CONNECTED) 2] get /controller<br>{“version”:1,”brokerid”:10,”timestamp”:”***”}，可以看到当前kafka集群的controller节点为nn服务器brokerid为10.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 集群的brokers信息在/brokers持久节点下，ids节点用于存放上线的brokers id号，topics：集群上所有的topces都放在在节点下</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 20] ls /brokers</span><br><span class="line">[ids, topics, seqid]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> brokers在持久ids节点下注册临时节点，节点名称就是broker自己的id号，这里说明为何在server.properties里面的broker.id要设为唯一，因为利用zookeeper的临时节点以及保证节点命名唯一。</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 19] ls /brokers/ids</span><br><span class="line">[10,11,12]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取其中一个broker id节点的信息，例如dn2这个broker</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 24] get /brokers/ids/12</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://dn2:9092&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;dn2&quot;,&quot;timestamp&quot;:&quot;*****&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看sparkapp这个topics的分区数量</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /brokers/topics/sparkapp/partitions</span><br><span class="line">[0, 1, 2]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前kafka集群的leader状态，通过在topic的分区的state节点可以看到当前leader是节点dn1，对应的broker id为1</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 36] get /brokers/topics/sparkapp/partitions/1/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:6,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:2,&quot;isr&quot;:[11,12,10]&#125;</span><br></pre></td></tr></table></figure>
<p>至此，完成Kafka的集群配置和测试</p>
<h4 id="4、-小结"><a href="#4、-小结" class="headerlink" title="4、 小结"></a>4、 小结</h4><p>为hadoop环境配置kafka组件的过程相对简单，鉴于Kafka这个中间件具有非常不错应用价值，本blog继续用1到2篇文章深入探讨有关Kafka核心内容。此外还用另外一篇文章用于给出flume和kafka两者的整合——<a href="https://blog.csdn.net/pysense/article/details/103335495">《flume集群高可用连接kafka集群》</a>。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka集群</tag>
        <tag>hadoop HA</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce设计原理</title>
    <url>/blog/2019/10/27/MapReduce%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h4 id="1、MR基本定义"><a href="#1、MR基本定义" class="headerlink" title="1、MR基本定义"></a>1、MR基本定义</h4><p>&#8195;&#8195;参考百度百科定义，简要概括如下：<br>MapReduce是分布式的计算框架或者解决方案，大致有基本内容：</p>
<ul>
<li>1）首先MapReduce重点是工作在集群的节点上，而非在单台服务器上做计算、做统计等</li>
<li>2）MapReduce把用户提交的任务以分布式放在多个节点上执行，自动划分计算数据和计算任务等，这些并行计算涉及到的系统底层的复杂细节交由MR框架负责处理。换句话：数据开发人员只需要定义“我要统计词频”的“任务”后，提交给MR框架即可，坐等计算结果，至于MR是如何从多台服务上找数据文件、计算统计、缓存中间计算结果、存储最终技术结果、CPU、内存、IO网络资源使用等，数据开发人员都无需关注，MR自己会处理。</li>
</ul>
<a id="more"></a>

<h4 id="2、HDFS和MR的关系"><a href="#2、HDFS和MR的关系" class="headerlink" title="2、HDFS和MR的关系"></a>2、HDFS和MR的关系</h4><p>&#8195;&#8195;HDFS和MR共同组成Hadoop分布式系统体系结构的核心。HDFS在集群上实现了分布式文件系统，MR在集群上实现了分布式计算和任务处理。HDFS在MR任务处理过程中提供了文件操作和存储等支持，MR在HDFS的基础上实现了任务的分发、计算、跟踪等工作，并收集结果，二者相互作用，完成分布式集群的主要任务。</p>
<h4 id="3、MR组件架构"><a href="#3、MR组件架构" class="headerlink" title="3、MR组件架构"></a>3、MR组件架构</h4><p><img src="https://img-blog.csdnimg.cn/20191016230840663.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;MapReduce框架设计为4部分：Client、JobTracker、TaskTracker以及Map Task &amp; Reduce Task。具体内容如下：</p>
<p>1）Client 客户端<br>Hadoop中，把用户提交的MapReduce程序成为“Job”（作业），每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 jar文件存储在 HDFS，也就是这个MR程序 jar包在集群中每个节点都存放在一份，而且是存放在hdfs文件系统上，并把jar包路径告诉 JobTracker ，由它创建每一个 Task（即 Map Task 和 Reduce Task） 将它们分发到各个 TaskTracker 服务中去执行。</p>
<p>2）JobTracker</p>
<p>JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，当然TaskTracker也会主动上报自己的情况，若JobTracker一旦发现某个TaskTracker失败，就将相应的任务转移到其他健康TaskTracker节点继续干活；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器Task Scheduler，而调度器会在TaskTracker所在节点资源出现空闲时，选择合适的任务工作交由这些空闲的TaskTracker来干活。这里说的调度器，就是我们后面要讨论的YARN，在hadoop 1，MR框架还需要运行资源调度和管理服务，在hadoop 2中，这个资源调度功能在MR框架中剥离出来，交由更专业的YARN框架统一处理。</p>
<p>3）TaskTracker</p>
<p>TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。</p>
<p>4）Map Task &amp; Reduce Task<br>Map Task、Reduce Task由Datanode节点上的TaskTracker 启动，HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce的处理单位是：split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。正因为屏蔽了物理文件层，在逻辑层，数据文件划分方法完全由数据开发者自行决定，这就是为何MR通过再次封装使用split来组织数据块的原因。这就像Linux LVM逻辑，多个物理卷PV组成一个大的卷组VG（或者物理卷池），逻辑卷LV在VG的上面，因为物理卷被逻辑化，因此可以由用户自行划分区或或者调整分区大小。<br>split 的多少决定了Map Task 的数目，每个split 只会交给一个Map Task 处理。<br>Split 和 Block的关系如下图所示：<br><img src="https://img-blog.csdnimg.cn/2019101721123387.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="4、MR运行流程"><a href="#4、MR运行流程" class="headerlink" title="4、MR运行流程"></a>4、MR运行流程</h4><p>4.1 分区原理</p>
<ul>
<li><p>1）、首先我们需要了解MR的分区partition原理，它是参考了一致性hash算法的概念：<br>Hash算法是为了保证数据均匀的分布，例如：有3个目录，编号分别为：0、1、2；现在有12个文件，如何把12个文件平均存放到3个目录下呢？按Hash算法的做法是，将12个文件从0开始编号，得到这样的一个数组：<br>[0，1，2，3，4，5，6，7，8，9，10，11]，每个文件所存放目录号=文件序号 mod 3，任何数字对3取模，最终得到的结果都是0，1，2。<br>最后将取模结果为0的文件放入0号目录下，结果为1的文件放入1号目录下，结果为2的文件放入2号目录，12个文件最终均匀的分布到3个目录下：<br>0、3、6、9、12号的文件存放在0号目录下<br>1、4、7、10号的文件存放在1号目录下<br>2、5、8、11号的文件存放在2号目录下<br>MR的分区partition就是这样的过程，partition_num=hash（key）mod N</p>
</li>
<li><p>2）MR为何做partition？<br>在进行MapReduce计算时，经常的需求是把最终的输出数据分到不同的文件中，例如：<br>按照年度（季度、月等）划分的话，需要把同一年份的数据放到一个文件中，因此不同年份对应不同的文件；<br>按性别业务划分，需要把同一性别的数据放到一个文件中，因此不同性别的数据放在不同的文件上。<br>从逆向来看：这些最终的输出是我们需要的数据，是由Reducer任务产生，而如果要得到最终输出的多个数据文件，意味着有同样数量的Reducer任务在跑；Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的Map阶段的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition，负责实现划分数据的类称作Partitioner。</p>
</li>
</ul>
<p>4.2 MapReduce统计词频的MapReduce流程<br><img src="https://img-blog.csdnimg.cn/20191027104923213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">该图统计词频流程主要分为以下几步。<br>第1步：假设一个文件有三行英文句子作为 MapReduce 的Input（输入），这里经过 Splitting 过程把文件分割为3块。分割后的3块数据就可以并行处理，每一块交给一个 map 线程处理，对应3个map task。</p>
<p>第2步：每个 map 线程中，以每个单词为key，以1作为词频数value，并建结果写到磁盘上，而非内存中（spark计算框架则会把此计算结果放在内存中，减少IO加速计算）。</p>
<p>第3步：每个 map 的输出要经过 shuffling（混洗），将相同的单词key放在同一分区，然后交给 reduce task处理。</p>
<p>第4步：reduce 接受到 shuffling 后的数据（reduce去读相应的map计算结果文件）， 会将相同的单词进行合并，得到每个单词的词频数，最后将统计好的每个单词的词频数作为输出结果。</p>
<p><strong>Map Reduce具体实现</strong><br>==Map阶段==<br>Map 阶段是由一定数量的 Map Task 组成。这些 Map Task 可以同时运行，每个Task所使用的计算资源由slot决定，每个 Map Task又是由以下三个部分组成。</p>
<ul>
<li><p>1）、对输入数据格式进行解析的一个组件：InputFormat。因为不同的数据可能存储的数据格式不一样，这就需要有一个 InputFormat 组件来解析这些数据的存放格式。默认情况下，它提供了一个 TextInputFormat 来读取输入数据格式（就像在Pandas中，使用read_csv()方法读取csv格式数据，用read_excel()读取Excel格式的数据）。TextInputFormat 会将文件的每一行解释成(key,value)，key代表每行偏移量，value代表每行数据内容。 通常不需要自定义 InputFormat，因为 MapReduce 提供了很多种InputFormat的实现，可直接引用。</p>
</li>
<li><p>2）、对输入数据进一步处理：Mapper——这个 Mapper 计算指具体业务逻辑，因为不同的业务对数据有不同的处理。</p>
</li>
<li><p>3)、数据分组：Partitioner。Mapper 数据处理之后输出导reduce之前，输出key会经过 Partitioner 分组选择不同的reduce。默认的情况下，Partitioner 会对 map 输出的key进行hash取模，比如有3个Reduce Task，它就是模（mod）3，如果key的hash值为0对应第0个 Reduce Task，hash值1对应第1个Reduce Task，hash值2对应第2个Reduce Task。如上图第一组map的bear和第二组map的bear hash值一样，因此被shuffling到同一组，然后交给第一个 reduce 来处理。</p>
</li>
</ul>
<p>==Reduce 阶段==<br>Reduce 阶段由一定数量的 Reduce Task 组成。这些 Reduce Task 可以同时运行，每个 Reduce Task又是由以下四个部分组成。</p>
<ul>
<li><p>1）、数据远程（http方式）或本地拷贝。Reduce Task 通过http拷贝每个 map 处理的结果，从每个 map 中读取一部分结果。每个 Reduce Task 拷贝哪些数据，是由上面 Partitioner 决定的(具体由reducer向ApplicationMaster请求拷贝数据的安排信息)。（这里要求服务器直接的网卡连接至少是Gb级别以上的光纤链路，提供拷贝数据的传输吞吐量）</p>
</li>
<li><p>2）、数据按照key排序。Reduce Task 读取完数据后，要按照key进行排序。按照key排序后，相同的key被分到一组，交给同一个 Reduce Task 处理。</p>
</li>
<li><p>3）、数据处理：Reducer。以WordCount为例，相同的单词key分到一组，交个同一个Reducer处理，这样就实现了对每个单词的词频统计。</p>
</li>
<li><p>4）、数据输出格式：OutputFormat。Reducer 统计的结果，将按照 OutputFormat 格式输出。默认情况下的输出格式为 TextOutputFormat，以WordCount为例，这里的key为单词，value为词频数。</p>
</li>
</ul>
<p>InputFormat、Mapper、Partitioner、Reducer和OutputFormat 都可以数据开发者自行实现，通常情况下，只需要实现 Mapper和Reducer。（数据开发者主要任务：集合数据业务需求，专注编写mapper和reducer二十年，该工作性质类似在关系型数据库中，数据分析岗根据市场运营提的数据报表需求写sql）</p>
<p>4.3  Shuffle（混洗）与reducer拉取数据细节<br>　　从4.2节内容可知，在map到reduce的过程，有一个环节为shuffling，那么shuffle是如何运作的呢？reducer又是如何从mapper节点去拷贝中间数据呢？按以下图示说明：<br>　<img src="https://img-blog.csdnimg.cn/20191017231015820.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>当map任务将数据output时，不仅仅是将结果输出到磁盘，它是将其写入内存缓冲区域，并进行一些预分类。</p>
<p>1）、Map阶段</p>
<p>首先map任务的output过程是一个环状的内存缓冲区，缓冲区的大小默认为100MB（可通过修改配置项mpareduce.task.io.sort.mb进行修改），当写入内存的大小到达一定比例，默认为80%（可通过mapreduce.map.sort.spill.percent配置项修改），便开始将这些缓存数据spill溢出写入磁盘。</p>
<p>在写入磁盘之前，线程将会指定数据写入与reduce相应的patitions中，最终传送给reduce。在每个partition中，后台线程将会在内存中进行Key的排序，（如果代码中有combiner方法，则会在output时就进行sort排序，这里，如果只有少于3个写入磁盘的文件，combiner将会在outputfile前启动，如果只有一个或两个，那么将不会调用）</p>
<p>这里将map输出的结果进行压缩会大大减少磁盘IO与网络传输的开销（配置参数mapreduce.map .output.compress 设置为true，如果使用第三方压缩jar，可通过mapreduce.map.output.compress.codec进行设置)</p>
<p>随后这些paritions输出文件将会通过HTTP发送至reducers，传送的最大启动线程通过mapreduce.shuffle.max.threads进行配置，如果mapper跟reducer同在一台服务器上，则reducer无需通过网络，直接读取本地文件，效率更高。</p>
<p>2）、The Reduce Side</p>
<p>首先上面每个节点的map都将结果写入了本地磁盘中，现在reduce需要将map的结果通过集群拉取过来，这里要注意的是，需要等到所有map任务结束后，reduce才会对map的结果进行拷贝，由于reduce函数有少数几个复制线程，以至于它可以同时拉取多个map的输出结果。默认的为5个线程（可通过修改配置mapreduce.reduce.shuffle.parallelcopies来修改其个数）</p>
<p>这里有个问题，那么reducers怎么知道从哪些机器拉取数据呢？ 当所有map的任务结束后，applicationMaster通过心跳机制（heartbeat mechanism)，由它负责mapping的输出结果与机器host的信息。所以reducer会定时的通过一个线程访问ApplicationMaster请求map的输出结果。</p>
<p>Map的结果将会被拷贝到reduce task的JVM的内存中（内存大小可在mapreduce.reduce.shuffle.input.buffer.percent中设置）如果不够用，则会写入磁盘。当内存缓冲区的大小到达一定比例时（可通过mapreduce.reduce.shuffle.merge.percent设置)或map的输出结果文件过多时（可通过配置mapreduce.reduce.merge.inmen.threshold)，将会触发合并(merged)随之写入磁盘。</p>
<p>这时要注意，所有的map结果这时都是被压缩过的，需要先在内存中进行解压缩，以便后续合并它们。（合并最终文件的数量可通过mapreduce.task.io.sort.factor进行配置） 最终reduce进行运算进行输出。<br>以上内容参考博文：<a href="https://blog.csdn.net/qq_36864672/article/details/78561375">shuffle和reducer拷贝mapper数据的细节</a></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Keepalived实现双机主备</title>
    <url>/blog/2019/06/30/%E5%9F%BA%E4%BA%8EKeepalived%E5%AE%9E%E7%8E%B0%E5%8F%8C%E6%9C%BA%E4%B8%BB%E5%A4%87/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文使用keepalived快速配置实现双机主备模式，该模式为keepalived入门使用，生产使用需要谨慎，当然可用于帮助理解keepalived</p>
<p>步骤：</p>
<p>1）主备server安装keepalived</p>
<p>2）主备server配置keepalived.conf</p>
<p>3）主备server安装httpd web服务（用于测试）</p>
<p>4）主备启动keepalived，并测试master、backup各自中断服务后，访问情况</p>
<a id="more"></a>

<h5 id="1、yum-y-install-keepalived-httpd"><a href="#1、yum-y-install-keepalived-httpd" class="headerlink" title="1、yum -y install keepalived httpd"></a>1、yum -y install keepalived httpd</h5><h5 id="2、主server的配置文件："><a href="#2、主server的配置文件：" class="headerlink" title="2、主server的配置文件："></a>2、主server的配置文件：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id s0  ！ 集群作用域的全局路由id，每台serverID要求唯一</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER ! 主server为master，备server为BACKUP</span><br><span class="line">    interface eth0 ！该server的网卡</span><br><span class="line">    virtual_router_id 51  ！集群作用域所有server 相同id</span><br><span class="line">    priority 100 ！优先级，同一个vrrp_instance里，主server必须要高于备server</span><br><span class="line">    advert_int 1 </span><br><span class="line">    authentication &#123;  ！主与备之间的认证机制</span><br><span class="line">        auth_type PASS  </span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123; ！ 虚拟IP</span><br><span class="line">        192.168.1.10</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>备机的配置同上，只需将state和priority改下 。从该配置文件可知，kl提供的功能极为简单，因此可实现一主多备的模式，而且非常容易配置，例如通过ansible 批量配置备机。</p>
<h5 id="3、安装apache-httpd-web-server"><a href="#3、安装apache-httpd-web-server" class="headerlink" title="3、安装apache httpd web server"></a>3、安装apache httpd web server</h5><p>修改web主页内容，以便显示访问的是来自哪台server：</p>
<p>cd /var/www/html</p>
<p>vi index.html</p>
<p>内容：from keepalived master server</p>
<p>同理备机：from keepalived slave server</p>
<p>（聪明的你，可用docker去启动一个web服务，甚至keepalived也被docker化）</p>
<h5 id="4、主备分别启动keepalived"><a href="#4、主备分别启动keepalived" class="headerlink" title="4、主备分别启动keepalived"></a>4、主备分别启动keepalived</h5><p>server keepalived start</p>
<p>在主server的linux日志可看到相关运行info</p>
<p>vi /var/log/messages</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn systemd: Started LVS <span class="keyword">and</span> VRRP High Availability Monitor.</span><br><span class="line"><span class="comment"># 启动LVS和VRRP HA监测服务</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_healthcheckers[<span class="number">11285</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line"><span class="comment"># 读取ka配置文件</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Registering Kernel netlink reflector</span><br><span class="line"><span class="comment"># ka使用Linux内核netlink创建与ka进程的通信</span></span><br><span class="line"><span class="comment"># netlink是用户进程与内核的IP网络配置之间的通信接口，同时它也可以作为内核内部与多个用户空间进程之</span></span><br><span class="line"><span class="comment"># 间的消息传输系统.基于netlink，用户进程和内核之间的通信支持全双工、组播、异步，像一台纯软件实现的“虚拟机三层交换机”，从这一点可感受到Linux内核有多强大！</span></span><br><span class="line"></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Registering Kernel netlink command channel</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Registering gratuitous ARP shared channel</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) removing protocol VIPs.</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Using LinkWatch kernel netlink reflector...</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">10</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP sockpool: [ifindex(<span class="number">2</span>), proto(<span class="number">112</span>), unicast(<span class="number">0</span>), fd(<span class="number">10</span>,<span class="number">11</span>)]</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">11</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) Transition to MASTER STATE</span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) Entering MASTER STATE</span><br><span class="line"><span class="comment"># 通过VRRP协议和ka的配置文件，将这台server选为master角色</span></span><br><span class="line"></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) setting protocol VIPs.</span><br><span class="line"><span class="comment"># eth0网口上设置VIP</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line"><span class="comment"># 在eth0发送ARP广播，对外问下192.168.1.10有没有机器在使用</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: VRRP_Instance(VI_1) Sending/queueing gratuitous ARPs on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.88</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br><span class="line">Jun <span class="number">27</span> 02:<span class="number">58</span>:<span class="number">12</span> nn Keepalived_vrrp[<span class="number">11286</span>]: Sending gratuitous ARP on eth0 <span class="keyword">for</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.10</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>备机的message：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 systemd: Starting LVS <span class="keyword">and</span> VRRP High Availability Monitor...</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11248</span>]: Starting Keepalived v1<span class="number">.3</span><span class="number">.5</span> (03/<span class="number">19</span>,<span class="number">2017</span>), git commit v1<span class="number">.3</span><span class="number">.5</span>-<span class="number">6</span>-g6fa32f2</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11248</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun 27 03:25:23 dn2 systemd: PID file /var/run/keepalived.pid not readable (yet?) after start.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11249</span>]: Starting Healthcheck child process, pid=<span class="number">11250</span></span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived[<span class="number">11249</span>]: Starting VRRP child process, pid=<span class="number">11251</span></span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 systemd: Started LVS <span class="keyword">and</span> VRRP High Availability Monitor.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_healthcheckers[<span class="number">11250</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Registering Kernel netlink reflector</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Registering Kernel netlink command channel</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Registering gratuitous ARP shared channel</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Opening file <span class="string">&#x27;/etc/keepalived/keepalived.conf&#x27;</span>.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: VRRP_Instance(VI_1) removing protocol VIPs.</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: Using LinkWatch kernel netlink reflector...</span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: VRRP_Instance(VI_1) Entering BACKUP STATE</span><br><span class="line"><span class="comment"># 备机进入BACKUP状态</span></span><br><span class="line">Jun <span class="number">27</span> 03:<span class="number">25</span>:<span class="number">23</span> dn2 Keepalived_vrrp[<span class="number">11251</span>]: VRRP sockpool: [ifindex(<span class="number">2</span>), proto(<span class="number">112</span>), unicast(<span class="number">0</span>), fd(<span class="number">10</span>,<span class="number">11</span>)]</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>对比主备ka的运行日志发现：</p>
<p>主server比被server多了在eth0网口设置VIP：192.168.1.10的过程，使用ip a命令查看网口配置可看到：</p>
<p>主server网口多了VIP，而备机网口还是单IP，这里涉及到keepalived的设计原理，以下为原理解析部分</p>
<h5 id="5、keepalived工作原理"><a href="#5、keepalived工作原理" class="headerlink" title="5、keepalived工作原理"></a>5、keepalived工作原理</h5><h6 id="5-1-模块设计"><a href="#5-1-模块设计" class="headerlink" title="5.1 模块设计"></a>5.1 模块设计</h6><p>keepalived采用是模块化设计，不同模块实现不同的功能，keepalived主要有三个模块，分别是core、check和vrrp。</p>
<p>core：是keepalived的核心，负责主进程的启动和维护，全局配置文件的加载解析等</p>
<p>check： 负责healthchecker(健康检查)，包括了各种健康检查方式，以及对应的配置的解析包括LVS的配置解析；</p>
<p>可基于脚本检查对IPVS后端服务器健康状况进行检查。</p>
<p>vrrp：VRRPD子进程，VRRPD子进程就是来实现VRRP协议，虚拟冗余路由协议。</p>
<p>下面是其他库： libipfwc：iptables/ipchains库，配置LVS会用到 libipvs*：配置LVS会用到，注意，keepalived和LVS完全不同的技术方向，各司其职相互配合</p>
<p>架构图：</p>
<p><img src="https://img-blog.csdnimg.cn/20190630203253526.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;keepalived启动后会有三个进程:</p>
<p>父进程：内存管理，IO调度、子进程管理等等</p>
<p>子进程：vrrpd子进程</p>
<p>子进程：healthchecker子进程</p>
<p>&#8195;&#8195;由上图可知，两个子进程都被系统WatchDog看管，两个子进程各自实现自己的事，healthchecker子进程实现检查各自服务器的健康程度，例如HTTP，LVS等等，如果healthchecker子进程检查到MASTER上服务不可用，就会通知本机上的兄弟VRRP子进程，让他删除通告，并且去掉虚拟IP，转换为BACKUP状态</p>
<h6 id="5-2-工作原理"><a href="#5-2-工作原理" class="headerlink" title="5.2 工作原理"></a>5.2 工作原理</h6><p>&#8195;&#8195;kpa是一个类似于layer3，4 &amp; 5交换机制的软件，类似第3层、第4层和第5层物理交换机，分别工作在IP/TCP协议栈的IP层、TCP层、应用层。原理分别如下：</p>
<p>&#8195;&#8195;在layer3，kpa使用layer3的方式工作式时，kpa会定期向服务器群中的服务器发送一个ICMP的数据包（既Ping），如果发现某台服务的IP地址没有激活，kpa便报告这台服务器失效，并将它从服务器群中剔除(这种情况的典型例子是某台服务器被非法关机)。Layer3方式是以服务器的IP地址是否有效作为服务器工作正常与否的标准。</p>
<p>&#8195;&#8195;在layer4: layer4主要以TCP端口的状态来决定服务器工作正常与否。如web server的服务端口一般是80，8080或者443，如果kpa检测到80端口没有启动，则kpa将把这台服务器从服务器群中剔除。</p>
<p>&#8195;&#8195;Layer5： Layer5就是工作在具体的应用层了，比Layer3,Layer4要复杂，在网络上占用的带宽也要大一些。kpa将根据用户的设定规则检查服务器相应服务是否运行正常，如果没有正常运行，则Keepalived将把服务器从服务器群中剔除。</p>
<h6 id="5-3-VRRP虚拟冗余路由协议"><a href="#5-3-VRRP虚拟冗余路由协议" class="headerlink" title="5.3 VRRP虚拟冗余路由协议"></a>5.3 VRRP虚拟冗余路由协议</h6><p>&#8195;&#8195;在现实的网络环境中，两台需要通信的主机大多数情况下并没有直接的物理连接。对于这样的情况，它们之间路由怎样选择？主机如何选定到达目的主机的下一跳路由，这个问题通常的解决方法有二种：在主机上使用动态路由协议(RIP、OSPF等)或者在主机上配置静态路由。显然，在主机上配置动态路由是不切实际的，因为管理、维护成本以及是否支持等诸多问题。配置静态路由则变得十分流行，但路由器(或者说默认网关default gateway)却经常成为单点。</p>
<p>VRRP的目的就是为了解决静态路由单点故障问题。VRRP通过一竞选(election)协议来动态的将路由任务交给LAN中虚拟路由器中的某台VRRP路由器。</p>
<p>​      VRRP工作机制</p>
<p>​      在一个VRRP虚拟路由器中，有多台物理的VRRP路由器，但是这多台的物理的机器并不能同时工作，而是由一台称为MASTER的负责路由工作，其它的都是BACKUP，MASTER并非一成不变，VRRP让每个VRRP路由器参与竞选，最终获胜的就是MASTER。MASTER拥有一些特权，比如 拥有虚拟路由器的IP地址，我们的主机就是用这个IP地址作为静态路由的。拥有特权的MASTER要负责转发发送给网关地址的包和响应ARP请求。VRRP通过竞选协议来实现虚拟路由器的功能，所有的协议报文都是通过IP多播(multicast)包形式发送的。虚拟路由器由VRID(范围0-255)和一组IP地址组成，对外表现为一个周知的MAC地址。所以，在一个虚拟路由 器中，不管谁是MASTER，对外都是相同的MAC和IP(称之为VIP)。客户端主机并不需要因为MASTER的改变而修改自己的路由配置，对他们来说，这种主从的切换是透明的。</p>
<p> &#8195;&#8195;在一个虚拟路由器中，只有作为MASTER的VRRP路由器会一直发送VRRP广告包(VRRPAdvertisement message)，BACKUP不会抢占MASTER，除非它的优先级(priority)更高。当MASTER不可用时(BACKUP收不到广告包)， 多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(&lt;1s)，以保证服务的连续性</p>
<h5 id="6、入门级kpa优缺点"><a href="#6、入门级kpa优缺点" class="headerlink" title="6、入门级kpa优缺点"></a>6、入门级kpa优缺点</h5><p>&#8195;&#8195;优点当然是配置简单，缺点：浪费服务器资源，因为配置模式为主备，正常对外服务时，仅有一台主机对外提供服务，而多台备机只能处于未使用状态。在之后的几篇文章中，将开始使用keepalived+nginx，keepalived+lvs实现负载均衡高可用的主从、双主模式。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Zookeeper的临时顺序节点实现分布式锁</title>
    <url>/blog/2019/09/11/%E5%9F%BA%E4%BA%8EZookeeper%E7%9A%84%E4%B8%B4%E6%97%B6%E9%A1%BA%E5%BA%8F%E8%8A%82%E7%82%B9%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章中，已经给出基于kazoo操作zk的逻辑，接下来利用zk的临时序列节点机制实现分布式锁，分布式锁场景使用非常广，在读写方面都非常适合，个人认为比基于redis实现的分布式锁更具可靠性（但性能方面，redis应该更强？）。</p>
<a id="more"></a>

<h5 id="1、zk的临时顺序节点"><a href="#1、zk的临时顺序节点" class="headerlink" title="1、zk的临时顺序节点"></a>1、zk的临时顺序节点</h5><p>&#8195;&#8195;临时顺序节点其实是结合临时节点和顺序节点的特点：在某个固定的持久节点(例如/locker)下创建子节点时，zk通过将10位的序列号附加到原始名称来设置znode的路径。例如，使用路径 /locker/foo创建为临时顺序节点，zk会将路径设为为 /locker/foo0000000001 ，并将下一个临时迅速节点设为/locker/foo0000000002，以此类推。如果两个顺序节点是同时创建的，那么zk不会对每个znode使用相同的数字。当创建节点的客户端与zk断开连接时（socket断开，更深一层应该是收到客户端发来的TCP挥手FIN 报文），服务端zk底层收到客户端的FIN报文后将由该客户端创建的临时节点删除掉。<br>&#8195;&#8195;临时顺序节点结构大致如下，/locker节点为持久节点，该节点下有多个子节点，这些子节点由不同客户端创建。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">locker&#x2F;</span><br><span class="line">├── foo0000000001</span><br><span class="line">├── foo0000000002</span><br><span class="line">├── foo0000000003</span><br><span class="line">├── foo0000000004</span><br><span class="line">└── foo0000000005</span><br></pre></td></tr></table></figure>
<h5 id="2、分布式锁的实现流程"><a href="#2、分布式锁的实现流程" class="headerlink" title="2、分布式锁的实现流程"></a>2、分布式锁的实现流程</h5><p>&#8195;&#8195;注意：本文提供的是基于zk的共享锁，而非排他锁（独占锁），看完本文后，实现独占锁会简单很多<br>&#8195;&#8195;==共享锁定义：又称读锁。如果事务T1对数据对象O1加上了共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放。zk实现的“共享锁”就是有多个序号的临时节点。==<br>&#8195;&#8195;共享锁与排他锁的区别在于，加了排他锁之后，数据对象只对当前事务可见，而加了共享锁之后，数据对象对所有事务都可见。</p>
<h5 id="分布式锁流程："><a href="#分布式锁流程：" class="headerlink" title="分布式锁流程："></a>分布式锁流程：</h5><pre><code>(1) 客户端发起请求，在zk指定持久节点/locker下（若不存在该locker节点则创建），创建临时顺序节点/locker/foo0000000003
(2) 获取/locker下所有子节点，例如有三个不同客户端各自创建的节点`all_nodes=[&#39;/locker/foo0000000001&#39;,&#39;/locker/foo0000000002&#39;,&#39;/locker/foo0000000003&#39;]`
(3) 对子节点按节点自增序号从小到大排序
(4) 判断本节点/locker/foo0000000003是否为节点列表中最小的子节点，若是，则获取锁，处理业务后，删除本节点/locker/foo0000000003；若不是，则监听排在该节点前面的那个节点/locker/foo0000000002“是否存在”事件
注意：这里产生这样的节点监听链，有两个监听链：
`/locker/foo0000000002监听/locker/foo0000000001是否存在的事件`
`/locker/foo0000000003监听/locker/foo0000000002是否存在的事件`
(5) 若被监听的节点路径“是否存在”的事件触发，处理业务，删除本节点；否则客户端阻塞自己，继续等待监听事件触发。</code></pre>
<h5 id="图示说明"><a href="#图示说明" class="headerlink" title="图示说明"></a>图示说明</h5><p>流程用OmniGraffle（Mac）画成，比Visio好用<br><img src="https://img-blog.csdnimg.cn/2019091000072398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="3、show-me-the-code"><a href="#3、show-me-the-code" class="headerlink" title="3、show me the code"></a>3、show me the code</h5><p>&#8195;&#8195;用python实现的zk临时顺序节点分布式锁的文章，在csdn等貌似没看到过，很多文章都是使用别人已经封装好的zklock或者直接使用kazoo提供zklock来做例子说明。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用上下文管理协议，对于使用者友好以及简单易用</span></span><br><span class="line"><span class="keyword">with</span> ZkDistributedLock(**conf):</span><br><span class="line">     <span class="comment"># 调用者的业务逻辑代码</span></span><br><span class="line">     doing_jobs(*args,**kwargs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> kazoo.client <span class="keyword">import</span> KazooClient</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZkDistributedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    基于zk的临时顺序节点实现分布式锁</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hosts, locker_path, sub_node_name, timeout, default_value=<span class="string">b&#x27;1&#x27;</span></span>):</span></span><br><span class="line">        self.hosts = hosts</span><br><span class="line">        <span class="comment"># 持久节点路径</span></span><br><span class="line">        self.locker_path = locker_path</span><br><span class="line">        self.timeout = timeout</span><br><span class="line">        <span class="comment"># 子节点路径</span></span><br><span class="line">        self.sub_node_path = os.path.join(self.locker_path, sub_node_name)</span><br><span class="line">        <span class="comment"># 创建子节点为临时顺序节点的默认值（只需要有值就行）</span></span><br><span class="line">        self.default_value = default_value</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用于客户端自己首次发起请求为获得锁后，用线程的事件阻塞自己不退出，继续等待zk的删除事件通知</span></span><br><span class="line">        <span class="comment"># 这比使用while True+time.sleep()方式更优雅</span></span><br><span class="line">        self.thread_event = threading.Event()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建zk连接，若未创建成功，直接raise Kazoo定义的连接错误，这里无需再给出try except的错误。</span></span><br><span class="line">        self.zkc = KazooClient(hosts=self.hosts, timeout=self.timeout)</span><br><span class="line">        self.zkc.start(self.timeout)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.zkc.exists(self.locker_path):</span><br><span class="line">            self.zkc.create(self.locker_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lock</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 这里是直接返回临时顺序节点的完整路径，例如返回：&#x27;/locker/foo0000000002&#x27;</span></span><br><span class="line">        self.current_node_path = self.zkc.create(path=self.sub_node_path, value=self.default_value, ephemeral=<span class="literal">True</span>,</span><br><span class="line">                                                 sequence=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取固定节点下的所有临时顺序节点列表</span></span><br><span class="line">        all_nodes = self.zkc.get_children(self.locker_path)</span><br><span class="line">        <span class="comment"># 对临时顺序节点列表进行排序，小到大，kazoo返回是节点名称，不是路径：[&#x27;foo0000000001&#x27;,&#x27;foo0000000002&#x27;,&#x27;foo0000000003&#x27;....]</span></span><br><span class="line">        all_nodes = <span class="built_in">sorted</span>(all_nodes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(all_nodes) == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果仅有zk的/locker路径下仅有一个临时顺序节点，说明没有其他客户端争抢，本客户端直接获得锁</span></span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;current node &#123;0&#125; got the locker at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            <span class="comment"># 线程阻塞事件被set为True，通知客户端无需再阻塞自己，已经获得锁。</span></span><br><span class="line">            self.thread_event.<span class="built_in">set</span>()</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取最小节点名例如&#x27;foo0000000001&#x27;</span></span><br><span class="line">        min_node = all_nodes[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 拼接最小节点路径:&#x27;/locker/foo0000000001&#x27;</span></span><br><span class="line">        min_node_path = os.path.join(self.locker_path, min_node)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果自身节点为最小节点，说明获得锁，进行操作后可以是释放锁</span></span><br><span class="line">        <span class="keyword">if</span> self.current_node_path == min_node_path:</span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;current node &#123;0&#125; got the locker at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            self.thread_event.<span class="built_in">set</span>()</span><br><span class="line">            <span class="comment"># 线程阻塞事件被set为True，通知客户端无需再阻塞自己，已经获得锁。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 当前节点不是最小节点，获取当前节点的前面的节点，并对该节点进行路径存在监听（注意这里不是对最小节点监听,避免羊群效应）</span></span><br><span class="line">            current_node = os.path.split(self.current_node_path)[<span class="number">1</span>]</span><br><span class="line">            pre_node_index = all_nodes.index(current_node) - <span class="number">1</span></span><br><span class="line">            pre_node = all_nodes[pre_node_index]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获得在当前节点前面的那个节点路径</span></span><br><span class="line">            self.pre_node_path = os.path.join(self.locker_path, pre_node)</span><br><span class="line">            print(<span class="string">&#x27;current node：&#123;0&#125; is watching the pre node：&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, self.pre_node_path))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对当前节点前面的那个节点增加&quot;exists事件&quot;监听</span></span><br><span class="line">            self.zkc.exists(path=self.pre_node_path, watch=self.watch_node_is_exist)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">watch_node_is_exist</span>(<span class="params">self, event</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;当前节点前面的那个节点被删除，触发删除事件，该函数被回调，获得锁</span></span><br><span class="line"><span class="string">        若</span></span><br><span class="line"><span class="string">        :param event:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> event:</span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;current node &#123;0&#125; got the locker at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            self.thread_event.<span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">release</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 释放锁，通过删除当前子节点路径实现</span></span><br><span class="line">        <span class="keyword">if</span> self.zkc.exists(self.current_node_path):</span><br><span class="line">            d = datetime.datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">            print(<span class="string">&#x27;deleted node &#123;0&#125; at &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(self.current_node_path, d))</span><br><span class="line">            self.zkc.delete(self.current_node_path)</span><br><span class="line">            self.zkc.stop()</span><br><span class="line">            self.zkc.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 客户端首次发起请求锁，线程事件为False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.thread_event.is_set():</span><br><span class="line">            <span class="comment"># 去zk获取锁</span></span><br><span class="line">            self.get_lock()</span><br><span class="line">            <span class="comment"># 如果本客户端首次请求锁却未能获得，那么客户端可以阻塞自己不退出，这里没限制重新获取锁的次数</span></span><br><span class="line">            <span class="comment"># （也可以设计为retry次数到达前，阻塞自己，超过retry次数后，客户端退出并提示获取锁失败）</span></span><br><span class="line">            self.thread_event.wait()</span><br><span class="line">            <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        self.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">a, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    模拟业务处理逻辑</span></span><br><span class="line"><span class="string">    :param a:</span></span><br><span class="line"><span class="string">    :param b:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    c = a + b</span><br><span class="line">    print(<span class="string">&#x27;doing jobs&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    print(<span class="string">&#x27;jobs is done!&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    conf = &#123;</span><br><span class="line">        <span class="string">&#x27;hosts&#x27;</span>: <span class="string">&#x27;192.168.100.5:2181&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;locker_path&#x27;</span>: <span class="string">&#x27;/locker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sub_node_name&#x27;</span>: <span class="string">&#x27;foo&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;timeout&#x27;</span>: <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ZkDistributedLock(**conf):</span><br><span class="line">        doing_jobs(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="4、测试分布式锁运行效果"><a href="#4、测试分布式锁运行效果" class="headerlink" title="4、测试分布式锁运行效果"></a>4、测试分布式锁运行效果</h5><p>1）单个客户端请求锁：<br>单个客户端请求模拟情况较为简单：</p>
<p>2）模拟多个客户端并发请求锁：<br>启动多个客户端程序前，先手动在zk服务器上创建一个临时顺序节点并保持shell不退出，如下所示，当前已有一个最小节点，以后创建的下一个节点要监听该节点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">5</span>] create -e -s /locker/foo <span class="number">1</span>  </span><br><span class="line">Created /locker/foo0000000605</span><br><span class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">6</span>] ls /locker</span><br><span class="line">[foo0000000602]</span><br></pre></td></tr></table></figure>

<p>接着运行多个以上程序，这里已三个为例：<br>==以上foo0000000602未删除前==<br>第一个客户端程序，可以看到第一个客户端创建603临时顺序节点，并监听着602节点并保持运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node：&#x2F;locker&#x2F;foo0000000603 is watching the pre node：&#x2F;locker&#x2F;foo0000000602</span><br></pre></td></tr></table></figure>

<p>第二个客户端程序，603节点因为602节点还未删除所以还存在，因此第二个客户端创建的604临时顺序节点要监听它前面的603节点并保持运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node：&#x2F;locker&#x2F;foo0000000604 is watching the pre node：&#x2F;locker&#x2F;foo0000000603</span><br></pre></td></tr></table></figure>

<p>==foo0000000602删除后（在zk服务器上手动删除602节点后，第一个客户端获得锁马上打印相关操作）==<br>第一个客户端打印，10:35:36获得锁，业务逻辑耗时5秒，并在10:35:41释放锁：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node &#x2F;locker&#x2F;foo0000000603 got the locker at *** 10:35:36</span><br><span class="line">doing jobs</span><br><span class="line">jobs is done!</span><br><span class="line">deleted node &#x2F;locker&#x2F;foo0000000603 at *** 10:35:41</span><br></pre></td></tr></table></figure>

<p>第二个客户端打印，在第一个客户端释放锁的时刻10:35:41，第二个客户端同时获得锁，业务逻辑耗时5秒，并在10:35:46释放锁：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current node &#x2F;locker&#x2F;foo0000000603 got the locker at *** 10:35:41</span><br><span class="line">doing jobs</span><br><span class="line">jobs is done!</span><br><span class="line">deleted node &#x2F;locker&#x2F;foo0000000603 at *** 10:35:46</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title>基于redis实现分布式锁（单实例）</title>
    <url>/blog/2019/09/19/%E5%9F%BA%E4%BA%8Eredis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%88%E5%8D%95%E5%AE%9E%E4%BE%8B%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;zookeeper的分布式方案当然最优雅也最可靠，如果redis集群服务已经搭起或者哨兵模式已经部署的条件下，那么基于多个redis实例实现的分布式锁同样高可用，而且redis性能凸显，本文给出的是在单个redis服务上使用setnx+expire实现可用的分布式锁，也可使用redis的事务MULTI+WATCH机制实现分布式锁，只不过这种方式相对简单，本文不再赘述。</p>
<a id="more"></a>

<h4 id="1、基于redis单实例实现的分布式锁"><a href="#1、基于redis单实例实现的分布式锁" class="headerlink" title="1、基于redis单实例实现的分布式锁"></a>1、基于redis单实例实现的分布式锁</h4><h5 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h5><p>加锁实际上就是在redis中，给Key键设置一个全局唯一值，为避免死锁（客户端加锁后，一直没有释放锁），并该key设一个过期时间，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET locker uuid NX PX 20000</span><br><span class="line"></span><br><span class="line"># locker 所有客户端都统一在redis设置的key，名称可以根据业务逻辑，例如app_write_locker</span><br><span class="line"># uuid是客户端自己生成的全局唯一的字符串标识，在python中可以通过UUID库生成唯一标识。</span><br><span class="line"># NX 代表只在键不存在时，才能成功设置key也即成功加锁，否则给客户端返回false</span><br><span class="line"># PX 设置键的过期时间为2000毫秒。</span><br><span class="line"># EX 若要设置秒数，则用EX </span><br><span class="line"># 如果上面的命令执行成功，则证明客户端获取到了锁。</span><br></pre></td></tr></table></figure>

<h5 id="延期锁的过期时间"><a href="#延期锁的过期时间" class="headerlink" title="延期锁的过期时间"></a>延期锁的过期时间</h5><p>假设有这样的场景：A客户端从加锁-业务执行-释放锁，这一过程需要5秒，锁的过期时间仅为2秒，显然2秒后，A的锁已经失效，B客户端加锁成功，但A还未处理完业务，也即出现两个客户端都加锁成功的情况。当然你可以将锁的失效时间设为更大值，这取决你对业务逻辑执行时长的熟悉度。</p>
<p>事实上，无需熟悉业务执行时长的情况下，也可以让客户端加锁后，再开启一个子线程不断对该客户端创建的锁延期，以保证足够的时间让业务逻辑执行完。</p>
<p>这个“延期锁的过期时间”在分布式锁当中，不是强制要实现的，正如前面所说，你非常清楚业务执行流程耗时基本不超过1秒，那么设置锁过期5秒，也完全OK。</p>
<h5 id="解锁"><a href="#解锁" class="headerlink" title="解锁"></a>解锁</h5><p>解锁就是客户端将自己设置的Key删除，而且只能限制客户端A的删除自己设置的key，而且不能删除其他客户端设置的key，通过比加锁时设置的<code>uuid</code>，以及释放锁拿到的uuid是否一致作为实现。为了保证删除key操作的原子性，这里借用redis官方建议LUA脚本key的删除操作。</p>
<p>为何客户端只能删除自己设定key？</p>
<p>例如客户端A加锁locker成功，业务执行需要5秒，而key过期时间为2秒；2秒后导致客户端B加锁成功，业务执行需要5秒，当时间线来到第5秒时，A把B的锁删除了（客户端C此时加锁成功），而此时B还在执行业务，显然不合理。</p>
<h4 id="2、python代码实现"><a href="#2、python代码实现" class="headerlink" title="2、python代码实现"></a>2、python代码实现</h4><p>基于redis单实例的分布式锁流程图<br><img src="https://img-blog.csdnimg.cn/20190915103408140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这里用with协议封装，使得使用者简单调用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> _thread <span class="keyword">import</span> start_new_thread</span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host, port, redis_timeout, retries, retry_itv, locker_key, expire, watch,extend, extend_interval</span>):</span></span><br><span class="line">        self.host = host</span><br><span class="line">        self.port = port</span><br><span class="line">        <span class="comment"># 客户端加锁的重试次数</span></span><br><span class="line">        self.retries = retries</span><br><span class="line">        <span class="comment"># 客户端加锁请求间隔时长</span></span><br><span class="line">        self.retry_itv = retry_itv</span><br><span class="line">        <span class="comment"># 客户端连接redis服务超时时长</span></span><br><span class="line">        self.r_timeout = redis_timeout</span><br><span class="line">        <span class="comment"># 客户端加锁的key</span></span><br><span class="line">        self.locker_key = locker_key</span><br><span class="line">        <span class="comment"># 锁的过期时长</span></span><br><span class="line">        self.expire = expire</span><br><span class="line">        <span class="comment"># 每次在锁原有过期时长的基础上再延长多长时间，秒或者毫秒</span></span><br><span class="line">        self.extend = extend</span><br><span class="line">        <span class="comment"># 每次延长锁的过期时间的wait间隔时长</span></span><br><span class="line">        self.extend_interval = extend_interval</span><br><span class="line">        <span class="comment"># 是否开启延长客户端锁的子线程,默认开启</span></span><br><span class="line">        self.watch_dog_thread = watch</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 客户端全局唯一ID标识</span></span><br><span class="line">        self.app_id=<span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 是否获得锁</span></span><br><span class="line">        self.is_get_lock=<span class="literal">False</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment"># 使用连接池</span></span><br><span class="line">        self.conn_pool = redis.ConnectionPool(host=self.host, port=self.port, socket_connect_timeout=self.r_timeout)</span><br><span class="line">        self.r = redis.Redis(connection_pool=self.conn_pool)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">watch_dog</span>(<span class="params">self</span>):</span></span><br><span class="line">       <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">       为客户端的锁延长过期时间</span></span><br><span class="line"><span class="string">       &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            new_app_id = self.r.get(self.locker_key)</span><br><span class="line">            <span class="comment"># 客户端还未设置key时watch_dog线程抢先get key导致查询到为空</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> new_app_id:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 只能延长由自己id加锁的失效时间</span></span><br><span class="line">            <span class="keyword">if</span> self.app_id == new_app_id.decode(<span class="string">&#x27;utf-8&#x27;</span>):</span><br><span class="line">                <span class="comment"># 对原有的过期时间延长</span></span><br><span class="line">                self.expire=self.expire+self.extend</span><br><span class="line">                self.r.<span class="built_in">set</span>(self.locker_key, self.app_id, ex=self.expire)</span><br><span class="line">                ttl=self.r.pttl(self.locker_key)</span><br><span class="line">                print(<span class="string">&#x27;watch_dog已延长该锁的过期时间至：&#123;&#125;s&#x27;</span>.<span class="built_in">format</span>(ttl/<span class="number">1000</span>))</span><br><span class="line">                time.sleep(self.extend_interval)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">               <span class="comment"># 说明主线程以及完成业务逻辑且成功释放说，该watch_dog子线程退出</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire</span>(<span class="params">self,retries</span>):</span></span><br><span class="line"></span><br><span class="line">        self.app_id = <span class="built_in">str</span>(uuid.uuid1())</span><br><span class="line">        <span class="comment"># 尝试加锁</span></span><br><span class="line">        is_set = self.r.<span class="built_in">set</span>(self.locker_key, self.app_id, ex=self.expire, nx=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> is_set:</span><br><span class="line">            <span class="comment"># 加锁成功</span></span><br><span class="line">            print(<span class="string">&#x27;已获得锁&#123;&#125;开始watch dog 线程&#x27;</span>.<span class="built_in">format</span>(self.app_id))</span><br><span class="line">            <span class="comment"># 开启延长锁的过期时间的子线程</span></span><br><span class="line">            <span class="keyword">if</span> self.watch_dog_thread:</span><br><span class="line">                self.start_new_thread(self.watch_dog, ())</span><br><span class="line">            self.is_get_lock=<span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> self.is_get_lock</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 重试加锁，超过尝试次数则加锁失败</span></span><br><span class="line">            retries=retries-<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> retries == <span class="number">0</span>:</span><br><span class="line">                self.is_get_lock=<span class="literal">False</span></span><br><span class="line">                <span class="keyword">return</span> self.is_get_lock</span><br><span class="line">            print(<span class="string">&#x27;未获得锁，重试获锁剩余次数：&#123;0&#125;,每次获取锁的间隔时长：&#123;1&#125;s&#x27;</span>.<span class="built_in">format</span>(retries,self.retry_itv))</span><br><span class="line">            time.sleep(self.retry_itv)</span><br><span class="line">            self.acquire(retries)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">atomic_delete</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在redis服务端执行原生lua脚本，保证原子删除key，也即保证一定能解锁</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># lua的用法可以redis官网查阅到，KEYS：存放键的列表，ARGV：存放值得列表</span></span><br><span class="line">        <span class="comment"># KEYS[1]取第一个键，ARGV[1]取第一个值</span></span><br><span class="line">        lua_del_script = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then</span></span><br><span class="line"><span class="string">            return redis.call(&quot;del&quot;,KEYS[1])</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            return 0</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        lua_del_func = self.r.register_script(lua_del_script)</span><br><span class="line">        result= lua_del_func(keys=[self.locker_key], args=[self.app_id])</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">release</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 解锁前，先获取该锁的值</span></span><br><span class="line">        now_app_id = self.r.get(self.locker_key)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> now_app_id:</span><br><span class="line">            <span class="comment"># 客户端已完成业务逻辑，但锁已失效，此时不影响其他客户端请求锁，直接return即可</span></span><br><span class="line">            print(<span class="string">&#x27;客户端未完成任务，但锁提前过期了，无法完成更新数据&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只能释放自己申请的锁，若别人申请的锁，自己不能删除，直接返回</span></span><br><span class="line">        <span class="keyword">if</span> self.app_id == now_app_id.decode(<span class="string">&#x27;utf-8&#x27;</span>):</span><br><span class="line">            self.atomic_delete()</span><br><span class="line">            print(<span class="string">&#x27;执行原子删除，锁释放：&#x27;</span>, self.app_id)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 加锁入口</span></span><br><span class="line">        self.acquire(self.retries)</span><br><span class="line">        <span class="keyword">return</span> self.is_get_lock</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        <span class="comment"># 解锁出口</span></span><br><span class="line">        self.release()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">n</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;doing jobs at:&#x27;</span>,datetime.datetime.now())</span><br><span class="line">    time.sleep(n)</span><br><span class="line">    print(<span class="string">&#x27;finish jobs at&#x27;</span>,datetime.datetime.now())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    db_conf = &#123;</span><br><span class="line">        <span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;port&#x27;</span>: <span class="number">6379</span>,</span><br><span class="line">        <span class="string">&#x27;retries&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;retry_itv&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;redis_timeout&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">&#x27;locker_key&#x27;</span>: <span class="string">&#x27;locker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;expire&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend_interval&#x27;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> RedLock(**db_conf) <span class="keyword">as</span> lock:</span><br><span class="line">        <span class="keyword">if</span> lock:</span><br><span class="line">            doing_jobs(<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&#x27;获取锁超时&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意：在释放锁的逻辑中，引用了lua脚本执行redis删除键，这是非常微妙且核心的细节，如果使用非原生删除，有可能会出现以下极端情况：<br>流程顺序：(1)==&gt;(2)==&gt;(3)==&gt;(4)<br><img src="https://img-blog.csdnimg.cn/20190916215733472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">（补充极限情况（2）的另外一个突发场景：key过期时间设置为毫秒单位，<br>redis设置为AOF持久化数据时，AOF同步到磁盘的方式默认每秒1次，如果在这1秒内断电，会导致内存数据丢失，立即重启服务器后，A设置的key已不在，故B加锁成功，A此时开始执行删除key的操作，导致互斥性失效）<br>==redis作者给出的key的有效期可使用毫秒精度的UNIX 时间戳，显然有较高的精度，因此场景（2）是可能发生的，因此直接用redis.del()命令删除key，不保证原子性<br>至于redis官网给出的原子删除lua脚本声称是保证原子操作，那么可认为以下两个操作：判断uuid是否为加锁者的uuid操作和删除该key的操作，它们一起消耗的时刻将足够微小，认为是原子的。（暂且相信官网，就像你相信mysql的事务原子操作）==</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if redis.call(&quot;get&quot;,KEYS[1]) &#x3D;&#x3D; ARGV[1] then</span><br><span class="line">      return redis.call(&quot;del&quot;,KEYS[1])</span><br></pre></td></tr></table></figure>

<p>单线程运行结果：</p>
<p>任务运行需要5秒，锁过期时长为1秒，那么watch_dog每隔1秒延长锁的过期时间1秒，那么在任务运行5秒这个过程中，总共将锁过期时长延长至6秒，足以保证任务完整运行且锁不失效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">已获得锁0381e3c6-d6fa-11e9-bfb9-a45e60c5a11d开始watch dog 线程</span><br><span class="line">doing jobs at: ** 10:14:50.119315</span><br><span class="line">watch_dog已延长该锁的过期时间至：1.999s</span><br><span class="line">watch_dog已延长该锁的过期时间至：2.999s</span><br><span class="line">watch_dog已延长该锁的过期时间至：4.0s</span><br><span class="line">watch_dog已延长该锁的过期时间至：5.0s</span><br><span class="line">watch_dog已延长该锁的过期时间至：6.0s</span><br><span class="line">finish jobs at ** 10:14:55.121694</span><br><span class="line">执行原子删除，锁释放： 0381e3c6-d6fa-11e9-bfb9-a45e60c5a11d</span><br></pre></td></tr></table></figure>



<p>不开启watch_dog，模拟任务运行时间过长，锁先失效的情况，程序运行1秒后，锁已经失效，该客户端任务结束后，发现自己加的锁没有了，为了数据安全，客户端只能放弃本次更新记录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">已获得锁e04f94b0-d6fa-11e9-b8cf-a45e60c5a11d开始watch dog 线程</span><br><span class="line">doing jobs at: ** 10:21:00.531881</span><br><span class="line">finish jobs at ** 10:21:05.532265</span><br><span class="line">客户端未完成任务，但锁提前过期了，无法完成更新数据</span><br></pre></td></tr></table></figure>

<h4 id="3、多线程模拟多个客户端并发争取分布式锁"><a href="#3、多线程模拟多个客户端并发争取分布式锁" class="headerlink" title="3、多线程模拟多个客户端并发争取分布式锁"></a>3、多线程模拟多个客户端并发争取分布式锁</h4><p>这里改下一部分代码：也即，每个线程共享redis连接池对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,r,redis_timeout, retries, retry_itv, locker_key, expire, extend, extend_interval</span>):</span></span><br><span class="line">        ......</span><br><span class="line">        self.r = r</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r</span>):</span></span><br><span class="line">    thread_name = threading.currentThread().name</span><br><span class="line">    bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">    total = r.get(bonus)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">        print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">redis_conn</span>):</span></span><br><span class="line">    info = &#123;</span><br><span class="line">        <span class="string">&#x27;r&#x27;</span>:redis_conn,</span><br><span class="line">        <span class="string">&#x27;retries&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;retry_itv&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;redis_timeout&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">        <span class="string">&#x27;locker_key&#x27;</span>: <span class="string">&#x27;locker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;expire&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;extend_interval&#x27;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">with</span> RedLock(**info) <span class="keyword">as</span> lock:</span><br><span class="line">        <span class="keyword">if</span> lock:</span><br><span class="line">            doing_jobs(redis_conn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&#x27;获取锁超时&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    pool_obj = redis.ConnectionPool(host=<span class="string">&#x27;192.168.100.5&#x27;</span>, port=<span class="number">6379</span>, socket_connect_timeout=<span class="number">5</span>)</span><br><span class="line">    redis_conn_obj = redis.Redis(connection_pool=pool_obj)</span><br><span class="line">    threads = []</span><br><span class="line">    <span class="comment"># 开启100个线程模拟客户端争取redis锁</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        t = threading.Thread(target=run, args=(redis_conn_obj,))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> 在redis服务端设置值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set money 10</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>测试结果，资源为10份，故对应有10个客户端可获得资源，且是有序的扣减，其他客户端要么未抢到锁，要么抢到锁后发现没资源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">客户端:Thread-14抢到奖金，还剩9</span><br><span class="line">客户端:Thread-3抢到奖金，还剩8</span><br><span class="line">客户端:Thread-78抢到奖金，还剩7</span><br><span class="line">客户端:Thread-16抢到奖金，还剩6</span><br><span class="line">客户端:Thread-11抢到奖金，还剩5</span><br><span class="line">客户端:Thread-17抢到奖金，还剩4</span><br><span class="line">客户端:Thread-5抢到奖金，还剩3</span><br><span class="line">客户端:Thread-66抢到奖金，还剩2</span><br><span class="line">客户端:Thread-38抢到奖金，还剩1</span><br><span class="line">客户端:Thread-9抢到奖金，还剩0</span><br><span class="line">获取锁超时</span><br><span class="line">获取锁超时</span><br><span class="line">奖金已被抢完</span><br><span class="line">奖金已被抢完</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>以上的实现都是基于redis单服务，若redis服务不可用或宕机之类的，所有客户端都无法加锁，显然单点故障不可靠，因此要实现高可用的redis分布式锁，还需设计如何在多个redis服务上实现，这就需要参考RedLock算法，后面的文章将进一步讨论。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title>堡垒机系统自定义Chrome代填程序开发</title>
    <url>/blog/2019/06/19/%E5%A0%A1%E5%9E%92%E6%9C%BA%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%AE%9A%E4%B9%89Chrome%E4%BB%A3%E5%A1%AB%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>&#8195;&#8195;企业内部有多个业务系统、管理系统，而这些系统相互独立，也无相关审计工具对操作人员进行审计，使用人员或者管理员在对账户等也存在一定疏忽，为加强网络安全以及保证内部信息安全，一般都要求这些管理系统接入堡垒机，堡垒机的审计原理：使用人员登录堡垒机系统，选中要登录的管理系统，堡垒机自动打开远程桌面，并代自动填入需要访问系统的账户和密码，这一过程对使用者透明，但堡垒机可通过录制使用者操作视频以及记录相关登入登出操作，达到对多个管理系统统一管理。</p>
<a id="more"></a>

<p>&#8195;&#8195;关键功能配置在此：</p>
<p>&#8195;&#8195;需在堡垒机系统上配置纳入审计的URL、账号和密码，其自带功能仅支持ie浏览器登录，而内部使用多个系统已不支持ie或者IE体验极差（包括旧管理系统和新管理系统），在应用类型选项仅有“IE代填以及自定义代填”两项选择，却没有类似“IE代填、Chrome代填、FireFox代填这些选项”， 而实际使用场景为使用Chrome浏览器代填，堡垒机厂家提供的自定义代填功能竟然无相关程序。</p>
<p><img src="https://img-blog.csdnimg.cn/20190731130451317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;如上图：厂家提供的默认选项IE代填，无需给出账户以及密码输入框的Xpath，即可实现通过IE打开相应web管理页面，但仅限IE打开！</p>
<p><img src="https://img-blog.csdnimg.cn/2019073113053794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>&#8195;&#8195;选择自定义代填，从界面选项来看，它仅提供目标地址url、登录账户、登录密码，如果要用chrome打开该url，首先要解决的是如何定位账户输入框位置和密码输入框位置，以便使用selenium打开webdriver定位到相应元素并自动填入信息。显然厂家无法提供该脚本，需自行开发！故需构造类似这样的脚本应用路径：</p>
<p><code>python autoFillingIn.py  -url ** -user ** -psw ** -wait ** -hup ** -hpp ** -lp**  -al **</code></p>
<p>其中-hup –hpp这两个入参为用户名输入框的xpath和密码输入框的xpath</p>
<p>堡垒机通过打开远程桌面后并运行该autoFillingIn.py，该程序通过入参调用chrome driver.exe从而打开chrome浏览器</p>
<p>以下为开发过程：</p>
<h4 id="（1）环境准备："><a href="#（1）环境准备：" class="headerlink" title="（1）环境准备："></a>（1）环境准备：</h4><p>由堡垒机打开的远程桌面所在服务器需安装python3.5环境、selenium库，以及chrome driver.exe</p>
<p>chromedriver下载</p>
<p>在阿里的镜像源：<code>http://npm.taobao.org/mirrors/chromedriver/2.8/</code></p>
<p>chromedriver_win32.zip （可适用32位和64位windows）</p>
<p>selenium离线安装包</p>
<p><code>git clone https://github.com/SeleniumHQ/selenium.git</code></p>
<p>里面已有setup.py，将该离线包上传到远程桌面所在服务器</p>
<p>python setup.py install</p>
<p>注：如果使用其他浏览器例如Firefox、Edge 、Safari，可在以下官网下载相应driver</p>
<p><code>https://pypi.org/project/selenium/</code></p>
<h4 id="（2）建议将chromedriver-exe和autoFillingIn-py放在同一目录下"><a href="#（2）建议将chromedriver-exe和autoFillingIn-py放在同一目录下" class="headerlink" title="（2）建议将chromedriver.exe和autoFillingIn.py放在同一目录下"></a>（2）建议将chromedriver.exe和autoFillingIn.py放在同一目录下</h4><p>这里放在c:\ chromeTools目录下</p>
<p>程序逻辑如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keep_double_quotes</span>(<span class="params">xpath</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过Chrome右键Copy Xpath获取目标url账户、密码、登录按钮输入框的Xpath有两种通用形式：</span></span><br><span class="line"><span class="string">    第一种在其html元素显示给出id值的：//*[@id=&quot;usernameInput&quot;] 以及 //*[@id=&quot;loginForm&quot;]/fieldset/div[1]/input</span></span><br><span class="line"><span class="string">    第二种在其html元素没有id索引的：/html/body/form/table[1]/tbody/tr[5]/td[3]/input 或 /html/body/form/table[1]/tbody/tr[5]/td[3]/img</span></span><br><span class="line"><span class="string">    对于第一种Xpath，当使用入参 -up //*[@id=&quot;login_input&quot;]时，会被argparse解析为 //*[@id=login_input],双引号没了！因此需要还原输入参数的格式，</span></span><br><span class="line"><span class="string">    否则find_element_by_xpath将无法定位，无法实现自动代填</span></span><br><span class="line"><span class="string">    该bug相对隐藏！</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m=re.search(<span class="string">r&#x27;=(.*?)\]&#x27;</span>,xpath)</span><br><span class="line">    <span class="keyword">if</span> m:</span><br><span class="line">        id_name=m.group(<span class="number">1</span>)</span><br><span class="line">        quotes_id_name=<span class="string">&#x27;&quot;&#x27;</span>+id_name+<span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">        double_quotes_path=re.sub(<span class="string">r&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(id_name),quotes_id_name,xpath)</span><br><span class="line">        <span class="keyword">return</span> double_quotes_path</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> xpath</span><br><span class="line"></span><br><span class="line">parser=argparse.ArgumentParser(description=<span class="string">&#x27;自动代填&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-url&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;网站url,支持http和https&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-user&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;用户名&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-psw&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;密码&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-wait&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">int</span>,default=<span class="number">5</span>,<span class="built_in">help</span>=<span class="string">&#x27;网页加载等待秒数，保证元素加载出来后才能定位成功&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-up&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,<span class="built_in">help</span>=<span class="string">&#x27;用户input提示元素的Xpath,取第一个input元素的id，style为&quot;display: block;&quot;，如没有该id，则可忽略该-up选项&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-hup&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;用户input存放用户名元素的Xpath,style为&quot;display: none;&quot;，表示隐藏用户名输入&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-pp&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,<span class="built_in">help</span>=<span class="string">&#x27;密码input提示元素的Xpath,取第一个input元素的id，style为&quot;display: block;&quot;，如没有该id，则可忽略该-pp选项&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-hpp&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,required=<span class="literal">True</span>,<span class="built_in">help</span>=<span class="string">&#x27;密码input真实密码填入元素的Xpath,style为&quot;display: none;&quot;，表示隐藏密码输入&#x27;</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-lp&#x27;</span>,<span class="built_in">type</span>=keep_double_quotes,<span class="built_in">help</span>=<span class="string">&#x27;登录按钮的Xpath，使用Chrome的Copy Xpath定位元素路径&#x27;</span>) </span><br><span class="line">parser.add_argument(<span class="string">&#x27;-al&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;指定是否自动登录,如指定自动，填入Y或y，如需要填入验证码、短信等二次认证的输入框，请填入N或n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输入参数</span></span><br><span class="line">inputs=parser.parse_args()</span><br><span class="line">target_url=inputs.url</span><br><span class="line">username=inputs.user</span><br><span class="line">password=inputs.psw</span><br><span class="line">sleep_time=inputs.wait</span><br><span class="line">u_inp=inputs.up</span><br><span class="line">hidden_u_inp=inputs.hup</span><br><span class="line">psw_inp=inputs.pp</span><br><span class="line">hidden_psw_inp=inputs.hpp</span><br><span class="line">login_input=inputs.lp</span><br><span class="line">auto_login=inputs.al</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取chrome_driver所在路径</span></span><br><span class="line">chrome_name = <span class="string">&#x27;chromedriver.exe&#x27;</span></span><br><span class="line">dir_name = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">chrome_driver_path = os.path.join(dir_name, chrome_name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加chrome的一些优化设置例如不弹出无意义的提示栏、容许不安全访问、最大窗口访问、禁止gpu加速，</span></span><br><span class="line"><span class="comment"># 如不设置，Chrome会弹出一些警告栏甚至提示安全禁止打开url，将无法自动代填或自动代填后自动登入系统进入首页</span></span><br><span class="line">options=webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">&#x27;--disable-popup-blocking&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--disable-infobars&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--allow-running-insecure-content&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--start-maximized&#x27;</span>)</span><br><span class="line">options.add_argument(<span class="string">&#x27;--disable-gpu&#x27;</span>)</span><br><span class="line">driver=webdriver.Chrome(chrome_driver_path,chrome_options=options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Chrome浏览器打开一个窗口并打开该url</span></span><br><span class="line">driver.get(target_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待</span></span><br><span class="line">time.sleep(sleep_time)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定位相关输入框或按钮位置并填入值</span></span><br><span class="line">selector=driver.find_element_by_xpath</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户名含有隐藏输入时，有两个input id，需要对第一个input进行focus</span></span><br><span class="line"><span class="keyword">if</span> u_inp:</span><br><span class="line">    u_inp_selector=selector(u_inp)</span><br><span class="line">    driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>,u_inp_selector)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户名输入框第二个input id，隐藏输入用户名的id，也要对其进行focus</span></span><br><span class="line">hidden_u_inp_selector=selector(hidden_u_inp)</span><br><span class="line">driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>, hidden_u_inp_selector)</span><br><span class="line">hidden_u_inp_selector.send_keys(username)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 密码含有隐藏输入时，有两个input id，需要对第一个input进行focus</span></span><br><span class="line"><span class="keyword">if</span> psw_inp:</span><br><span class="line">    psw_inp_selector=selector(psw_inp)</span><br><span class="line">    driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>,psw_inp_selector)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 密码输入框第二个input id，隐藏输入用户名的id，也要对其进行focus</span></span><br><span class="line">hidden_psw_inp_selector=selector(hidden_psw_inp)</span><br><span class="line">driver.execute_script(<span class="string">&quot;arguments[0].focus();&quot;</span>, hidden_psw_inp_selector)</span><br><span class="line">hidden_psw_inp_selector.send_keys(username)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">login=selector(login_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否自动登录</span></span><br><span class="line"><span class="keyword">if</span> auto_login <span class="keyword">in</span> (<span class="string">&#x27;Y&#x27;</span>,<span class="string">&#x27;y&#x27;</span>):</span><br><span class="line">    login.click()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>这里为何不设计为以下代码结构：</p>
<p>def auto_filling_in(**kargs):</p>
<p>​     pass</p>
<p>if <strong>name</strong>==”__main__”:</p>
<p>​     获取入参</p>
<p>​     调用auto_filling_in(**kargs)</p>
<p>因为使用该形式运行时，selenium会以子进程运行chromedriver.exe</p>
<p>if <strong>name</strong>==”__main__”:以下的代码作为父进程</p>
<p>而父进程在获取入参和调用auto_filling_in()后会结束，导致子进程运行的chrome也退出</p>
<h4 id="（3）程序使用说明"><a href="#（3）程序使用说明" class="headerlink" title="（3）程序使用说明"></a>（3）程序使用说明</h4><p>最后在堡垒机上的配置该脚本：</p>
<p>应用路径:c:\ chromeTools\ autoFillingIn.py</p>
<p>运行参数：-url %Target -user %Username -psw %Password -wait 5 -fup /html/body/form/table[1]/tbody/tr[5]/td[1]/input  -fpp /html/body/form/table[1]/tbody/tr[5]/td[4]/input -lp /html/body/form/table[1]/tbody/tr[5]/td[5]/img  -al Y</p>
<p>登录地址：需要审计的管理系统url</p>
<p>登录账户：账户名</p>
<p>登录密码：密码</p>
<p>以上部分参数说明：</p>
<p>-fup账户输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[1]/input，若为隐藏元素，则需要加入up选项</p>
<p>-fpp密码输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[4]/input，若为隐藏元素，则需要加入pp选项</p>
<p>-lp登录输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[5]/img</p>
<p>==重点内容==：</p>
<p>以上都是基于用户输入框和密码输入框都为输入字符可见的条件下，可以代成功，但是对于用户名、密码输入框设为输入字符隐藏时，需要加入以下两个选项才能定位到元素并正确代填，需要将鼠标先focus到用户名输入框或者密码输入框。</p>
<p>-up账户输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[2]/input</p>
<p>-pp密码输入框Xpath： /html/body/form/table[1]/tbody/tr[5]/td[3]/input</p>
<h4 id="4-运行效果："><a href="#4-运行效果：" class="headerlink" title="(4) 运行效果："></a>(4) 运行效果：</h4><p><img src="https://img-blog.csdnimg.cn/20190805092838974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p><img src="https://img-blog.csdnimg.cn/20190805092853755.png" alt="img"><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p>
<p>​    任务栏开启了两个远程桌面窗口，一个是autoFillingIn.py运行窗口，另一个为chrome打开的被审计web系统</p>
<p>​    综上，最关键的设计：入参双引号处理，以及wait等待页面加载完毕的时间，因为某些旧管理系统服务器性能较低，打开页面慢，容易导致selenium无法定位元素位置。另外对于有验证码、短信等二次认证的web管理页面，自动登录选项 -al 要设为N或n，即不自动登录到首页，由使用人员自行填入验证码（账户和密码已经后台自动填到输入框内，无需再填入，除非使用人刷新了以打开的登录窗口，那么账户和密码输入框会被清空）。</p>
<p>​    由于解决了argparse对入参Xpath字符串路径问题，本文的解决方案适用所有管理系统需要通过Chrome自动代填达到审计目的场景，之前在全网以及检索国外论坛，都未找到合适参考的方案，用了几天时间自行研究到部署可用，恰好满足实际场景（否则堡垒机系统使用受限，而且厂家无法提供该技术支持！）。在这里，也要介绍国内也有非常出色的开源堡垒机系统例如：Jumpserver，使用python+django开发（个人擅长的技术栈），而且该开源堡垒机系统还符合4A（认证Authentication、账号Account、授权Authorization、审计Audit）的专业运维审计系统，可docker化部署！</p>
<p>Jumpserver的官网：<code>http://www.jumpserver.org/</code></p>
<p>github：<code>https://github.com/jumpserver/jumpserver</code></p>
<p>相关开源堡垒机系统的对比参考文章：</p>
<p><code>https://blog.csdn.net/enweitech/article/details/88840456</code></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>深入理解RDD弹性分布式数据集</title>
    <url>/blog/2019/12/26/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3RDD%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的博客<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》 </a>深入探讨了Spark架构原理内容，该文提到Stage的划分，为什么要做Stage划分？是为了得到更小的Task计算单元，分发给Executor的线程运行，将规模庞大、多流程的计算任务划分为有序的小颗粒的计算单元，实现更高效的计算。那么Stage划分怎么实现？需依赖RDD(Resilient Distributed Datasets，弹性分布式数据集)，可以说，RDD是Spark最为核心的概念。本文内容部分参考了<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_RDD.md">《弹性式数据集RDDs》</a>以及<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">《Spark官方文档》</a></p>
<a id="more"></a>

<h4 id="1、RDD简介"><a href="#1、RDD简介" class="headerlink" title="1、RDD简介"></a>1、RDD简介</h4><p>&#8195;&#8195;RDD 是分布式的、可容错的、只读的、分区记录的弹性数据集合（在开发角度来看，它是一种数据结构，并是绑定了多个方法和属性的一种object），支持并行操作，可以由外部数据集或其他 RDD 转换而来，细读Resilient Distributed  Dataset这三个单词，更深层的含义如下：</p>
<ul>
<li>Resilient: 弹性的，RDD如何体现出弹性呢？通过使用RDD血缘关系图——DAG，在丢失节点上重新计算上，弹性容错。</li>
<li>Distributed:分布式的，RDD的数据集驻留在多个节点的内存中或者磁盘上（一分多）。</li>
<li>Dataset: 在物理文件存储的数据</li>
</ul>
<p>更具体的说明：</p>
<ul>
<li><p>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数。<br>（这一特点体现出RDD的分布式，RDD的分区是在多个节点上指定的，注意不是指把master分区拷贝到其他节点上，spark强调的是”移动数据不如移动计算“，避免跨节点拷贝分区数据。做这样假设：RDD如果不设计为多个分区，那么一个RDD就是代表一个超大数据集，而且只能在单机行运行，这跟Pandas的DataFrame区别就不大了。）</p>
</li>
<li><p>Spark一个计算程序，往往会产生多个RDD，这些RDD会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。<br>（这个特点就体现了RDD可恢复性，）</p>
</li>
<li><p>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)。<br>（其实很多中间件或者组件只要涉及到Partition，必然少不了使用Partitioner(分区器)，例如kafka的partition，Producer可通过对消息key取余将消息写入到不同副本分区上，例如redis的key在slot上分配，也是通过对key取余。）</p>
</li>
<li><p>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</p>
</li>
</ul>
<h4 id="2、创建RDD"><a href="#2、创建RDD" class="headerlink" title="2、创建RDD"></a>2、创建RDD</h4><ul>
<li><p>由一个已经存在的Scala 数组创建或者Python列表创建。<br>val rdd0 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) # Scala<br>val rdd0 = sc.parallelize([1,2,3,4,5,6,7,8]) # Python</p>
</li>
<li><p>由外部存储系统的文件创建。包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、HBase等，其他流式数据：Socket流等。<br>val rdd2 = sc.textFile(“hdfs://nn:9000/data_files”)<br>sc是spark-shell内置创建的sparkcontext</p>
</li>
<li><p>已有的RDD经过算子转换生成新的RDD<br><code>val rdd1=rdd0.flatMap(_.split(&quot; &quot;))</code><br><code>val rdd2=rdd1.map((_, 1))</code></p>
</li>
</ul>
<h4 id="3、宽依赖和窄依赖"><a href="#3、宽依赖和窄依赖" class="headerlink" title="3、宽依赖和窄依赖"></a>3、宽依赖和窄依赖</h4><p>&#8195;&#8195;在<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》 </a>的第6章节”理解Spark Stage的划分“内容，正是RDD 与它的父 RDD(s) 之间的依赖关系，才有Stage划分的基础，主要为以下两种不同的依赖关系：</p>
<ul>
<li>窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖，一对一关系。例如Map、Filter、Union等算子。（你可以这样形象理解Spark为什么要用narrow这个词：从下图中例如Map算子，一个父rdd到一个子rdd的依赖关系，一条线直连关系，用窄形容恰当）</li>
<li>宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDD 的多个分区所依赖，一对多关系。例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffling。（你可以这样形象理解Spark为什么要用wide这个词：从下图中例如groupByKey算子，一个父RDD有多条线连接到不同子RDD，用宽形容恰当）</li>
<li></li>
<li>Lineage（血统）<br>例如RDD0转换为下一个RDD1，RDD1转换为下一个RDD2，RDD2转换为下一个RDD3，…，这一系列的转换过程叫做血统，可以构成DAG图（执行计划）<br>有何作用？当RDD3丢失，只需要使用Lineage关系图和重新计算RDD2，即可恢复出RDD3数据集。而无需从头RDD0重新计算，省时省力。<br>对于下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：<br><img src="https://img-blog.csdnimg.cn/20191222202637659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">对于map、filter、union这些算子可以很直观看到它们所计算的rdd依赖关系是窄依赖<br>注意：对于join算子，如果是协同划分的话，两个父RDD之间， 父RDD与子RDD之间能形成一致的分区安排，即同一个Key保证被映射到同一个分区，这种join是窄依赖。（协同划分就是指定分区器以产生前后一致的分区安排）<br>如果不是协同划分，就会形成宽依赖。</li>
</ul>
<p>这两种依赖关系除了可以把一个Job划分多个Stage，还有以下最为重要的两点作用：<br>&#8195;&#8195;第一：窄依赖可实现在当前节点上（数据无需夸节点传输）以流水线的方式（pipeline管道方式）对父分区数据进行流水线计算，例如先执行 Map算子，接着执行Filter算子，这两个操作一气呵成。而宽依赖则需要先计算好所有父分区的数据，接着将数据通过跨节点传递后执行shuffling（不跨节点，又怎么能把不同节点但key相同的项归并到同一分区上呢？所以宽依赖必然要进行磁盘IO和Socket跨节点传数据），这一过程与 MapReduce 类似。<br>&#8195;&#8195;第二：窄依赖能够更有效地进行数据恢复，根据上面”Lineage（血统）“所提的逻辑，只需重新对丢失子rdd的父rdd进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据重新计算并再次 shuffling，效率低而且耗时。</p>
<h4 id="4、通过RDD的依赖关系构建DAG计算图"><a href="#4、通过RDD的依赖关系构建DAG计算图" class="headerlink" title="4、通过RDD的依赖关系构建DAG计算图"></a>4、通过RDD的依赖关系构建DAG计算图</h4><p>&#8195;&#8195;上面提到的多个RDD之间的两种依赖关系组成了 DAG，DAG 定义了这些 RDD之间的 Lineage链，通过血统关系（就像你手上拿了一张关系图谱），如果一个 RDD 的部分或者全部计算结果丢失了，也可以根据”这张关系图谱“重新进行计算出结果。Spark根据RDD依赖关系的不同将 DAG 划分为不同的执行阶段 (Stage)：</p>
<ul>
<li>对于窄依赖，由于分区的依赖关系是确定的，分区在当前节点上（数据无需夸节点传输）进行转换操作，也就是说可在同一个线程执行当前阶段计算任务，而且多个分区可以直接并行运行（因此分区数就决定并发计算的粒度，可用于Spark计算性能调优）。因此窄依赖的RDD可以划分到同一个执行阶段Stage；</li>
<li>对于宽依赖，由于 Shuffle 的存在，只能等多个父 RDD被 Shuffle 处理完成后（不同父分区的Shuffle导致数据需夸节点传输），才能开始对子RDD计算，因此遇到宽依赖就需要重新划分阶段。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20191224193522623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>正是有了以上对Stage的划分设计，Spark在执行作业时， 生成一个完整的、最优的执行计划，从而比MapReduce”更加聪明地利用资源地“完成计算作业。</p>
<h4 id="5、RDD-持久化"><a href="#5、RDD-持久化" class="headerlink" title="5、RDD 持久化"></a>5、RDD 持久化</h4><p>&#8195;&#8195;这一章节内容直接翻译spark：<br>&#8195;&#8195;RDD最重要的特点是可将数据集缓存在内存（或者磁盘上），这也是Spark之所以快的原因。使用RDD persist时，每个节点都会存储当前RDD计算的dataset，以便被其他RDD直接在内存加载使用，这种效果将使得之后的action算子至少提高10倍上计算速度。<br>&#8195;&#8195;通过调用persist()或者cache()方法即可触发RDD persist，而且持久化另外一个作用：Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.（可根据已有的算子重新计算丢失的RDD）<br>&#8195;&#8195;Spark 在持久化 RDDs 的时候提供了 3 种storage level：存在内存中的非序列化的 java 对象、存在内存中的序列化的数据以及存储在磁盘中。第一种选择的性能是最好的，因为 JVM 可以很快的访问 RDD 的每一个元素。第二种选择是在内存有限的情况下，使的用户可以以很低的性能代价而选择的比 java 对象图更加高效的内存存储的方式。如果内存完全不够存储的下很大的 RDDs，而且计算这个 RDD 又很费时的，那么选择第三种方式。</p>
<p>The full set of storage levels is:</p>
<table>
<thead>
<tr>
<th>Storage Level</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will    not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the    partitions that don’t fit on disk, and read them from there when they’re needed.</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER   (Java and Scala)</td>
<td>Store RDD as <em>serialized</em> Java objects (one byte array per partition).    This is generally more space-efficient than deserialized objects, especially when using a    <a href="http://spark.apache.org/docs/latest/tuning.html">fast serializer</a>, but more CPU-intensive to read.</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER   (Java and Scala)</td>
<td>Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of    recomputing them on the fly each time they’re needed.</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>Store the RDD partitions only on disk.</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
<td>Same as the levels above, but replicate each partition on two cluster nodes.</td>
</tr>
<tr>
<td>OFF_HEAP (experimental)</td>
<td>Similar to MEMORY_ONLY_SER, but store the data in    <a href="http://spark.apache.org/docs/latest/configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to be enabled.</td>
</tr>
</tbody></table>
<p>Note: In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2.<br>注意对使用Python来开发spark的话，它存储的对象都是由Pickle库实现序列化，因此对于python，除了OFF_HEAP这个level，其他level都适用。<br>Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it.<br>此外Spark在shuffle操作例如reduceByKey这类操作时会自动对RDD做持久化，以防止某些节点再做shuffle过程中挂了导致需要重新计算父RDD。Spark建议使用者在RDD需要重新使用的场景下可先持久化RDD。</p>
<h4 id="6、RDD的checkpointing机制"><a href="#6、RDD的checkpointing机制" class="headerlink" title="6、RDD的checkpointing机制"></a>6、RDD的checkpointing机制</h4><p>&#8195;&#8195;前面提到对于要恢复某些丢失的RDD，可根据父 RDDs 的血缘关系recomputed，但是如果这个血缘关系链很长的话（例如业务逻辑里面有10来个transmissions以及有多个actions），则recomputed需要耗费很长时间，因此在这种场景下，将一些 RDDs 的数据持久化到稳定存储系统中是有必要的。<br>&#8195;&#8195;checkpointing 对具有很长的血缘关系链且包含了宽依赖的 RDDs 是非常有用的，比如spark给出的PageRank 例子，在这些场景下，集群中的某个节点的失败会导致每一个父亲 RDD 的一些数据的丢失，最惨的是这些父RDD都是宽依赖以及很长的窄依赖链关系，显然需要重新所有的计算。<br>&#8195;&#8195;对于普通的窄依赖的 RDDs（spark给出例子中线性回归例子中的 points 和 PageRank 中的 link 列表数据），checkpointing 可能一点用都没有。如果一个节点失败了，spark照应可以很快在其他的节点中并行的重新计算出丢失了数据的分区，这个成本只是备份整个 RDD 的成本的一点点而已。</p>
<p>&#8195;&#8195;Spark 目前提供了一个 checkpointing 的 api（persist 中的标识为 REPLICATE，还有 checkpoint()），用户自行选择在一些最佳的 RDDs 来进行 checkpointing，以达到最小化恢复时间。（这就像你玩过关类游戏，总共20关，这个游戏本身没有自动保存检查点，必须用户自己触发，在第19关是最难的关卡耗费一周时间拿下，而你确忘记存档，能不崩溃吗）</p>
<h4 id="7、Spark分区与RDD分区（待更新）"><a href="#7、Spark分区与RDD分区（待更新）" class="headerlink" title="7、Spark分区与RDD分区（待更新）"></a>7、Spark分区与RDD分区（待更新）</h4><p>这部分内容难度较大，但确实能真正理解Spark和RDD并行计算的底层逻辑，相关知识点需回顾hadoop文件分区。这部分内容待更新</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase架构分析</title>
    <url>/blog/2019/11/02/HBase%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的<a href="https://blog.csdn.net/pysense/article/details/102635656">文章</a>中，详细给出了HBase HA高可用部署以及测试的内容，本篇文章将对HBase架构进行分析。</p>
<h3 id="1、有关HBase基本介绍"><a href="#1、有关HBase基本介绍" class="headerlink" title="1、有关HBase基本介绍"></a>1、有关HBase基本介绍</h3><h4 id="1-1-HBase解决的痛点："><a href="#1-1-HBase解决的痛点：" class="headerlink" title="1.1 HBase解决的痛点："></a>1.1 HBase解决的痛点：</h4><ul>
<li>解决随机近实时的高效的读写</li>
<li>解决非结构化的数据存储</li>
</ul>
<h4 id="1-2-HBase应用："><a href="#1-2-HBase应用：" class="headerlink" title="1.2 HBase应用："></a>1.2 HBase应用：</h4><ul>
<li>可以存储非结构化的数据</li>
<li>被用来做实时数据分析(整合flume、storm、streaming等)<blockquote>
<p>引用HBase作为业务存储，则需要注意的点（按官方指引的翻译）：<br>首先确保使用场景有足够多的数据，上亿或者十几亿行的数据，HBase将非常适合<br>第二，确保业务需求中不需要用到关系型数据库那种严格的索引、事务、高级查询等<br>第三点，确保足够多的硬件服务器，至少5台个HDFS节点以上，以便发挥HDFS性能。</p>
</blockquote>
</li>
</ul>
<h4 id="1-3-Hbase特性："><a href="#1-3-Hbase特性：" class="headerlink" title="1.3 Hbase特性："></a>1.3 Hbase特性：</h4><ul>
<li><p>Hadoop的分布式、开源的、多版本的非关系型数据库</p>
</li>
<li><p>Hbase存储Key-Value格式，面向列存储，Hbase底层为byte[]比特数组数据，不存在数据类型一说。</p>
</li>
<li><p>严格一致的读写</p>
</li>
<li><p>表的自动和可配置分片</p>
</li>
<li><p>RegionServer之间的自动故障转移支持</p>
</li>
<li><p>方便的基类，用于通过Apache HBase表备份Hadoop MapReduce作业</p>
</li>
<li><p>块缓存和布隆过滤器用于实时查询</p>
</li>
</ul>
<h3 id="2、HBase架构"><a href="#2、HBase架构" class="headerlink" title="2、HBase架构"></a>2、HBase架构</h3><p><img src="https://img-blog.csdnimg.cn/20191027170545189.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">如上图所示，架构主要有以下4个部分</p>
<a id="more"></a>

<h4 id="2-1-HMaster"><a href="#2-1-HMaster" class="headerlink" title="2.1 HMaster"></a>2.1 HMaster</h4><ul>
<li>负责管理HBase的元数据，表结构，表的Region信息</li>
<li>负责表的创建，删除和修改</li>
<li>负责为HRegionServer分配Region，分配后将元数据写入相应位置</li>
</ul>
<h4 id="2-1-HRegionServer"><a href="#2-1-HRegionServer" class="headerlink" title="2.1 HRegionServer"></a>2.1 HRegionServer</h4><ul>
<li>存储多个HRegion</li>
<li>处理Client端的读写请求（根据从HMaster返回的元数据找到对应的HRegionServer）</li>
<li>管理Region的Split分裂、StoreFile的Compaction合并。</li>
<li>一个RegionServer管理着多个Region，在HBase运行期间，可以动态添加、删除HRegionServer。</li>
</ul>
<h4 id="2-3-HRegion"><a href="#2-3-HRegion" class="headerlink" title="2.3 HRegion"></a>2.3 HRegion</h4><ul>
<li>一个HRegion里可能有1个或多个Store（参考上图）。</li>
<li>HRegionServer维护一个HLog。</li>
<li>HRegion是HBase分布式存储和负载的最小单元，但不是HBase数据存储到文件的最小单元。</li>
<li>表通常被保存在多个HRegionServer的多个Region中。</li>
</ul>
<h4 id="2-4-Store"><a href="#2-4-Store" class="headerlink" title="2.4 Store"></a>2.4 Store</h4><ul>
<li><p>Store由内存中的MemStore和磁盘中的若干StoreFile组成（参考上图）</p>
</li>
<li><p>一个Store里有1个或多个StoreFile和一个memStore。</p>
</li>
<li><p>每个Store存储一个列族。</p>
<h4 id="2-5-MemStore、StoreFile、HFile"><a href="#2-5-MemStore、StoreFile、HFile" class="headerlink" title="2.5 MemStore、StoreFile、HFile"></a>2.5 MemStore、StoreFile、HFile</h4></li>
<li><p>MemStore：<br>首先，memStore 是在内存中存在，保存修改key-value数据；当memStore的大小达到一个阀值（默认64MB）时，memStore会被flush到文件，也就是存在磁盘上。</p>
</li>
<li><p>StoreFile：<br>接上面内容，memStore的每次flush操作都会生成一个新的StoreFile，StoreFile底层是以HFile的格式保存，当有多个StoreFile后，将会触发为合并为一个大的StoreFile。</p>
</li>
<li><p>HFile：<br>HFile是HBase中KeyValue数据的存储格式，在hdfs上是二进制格式文件，一个StoreFile对应着一个HFile。HFile有自己的数据结构。<br>HFile写入的时候，分一个块一个块的写入，每个Block块64KB，这样有利于数据的随机访问，不利于连续访问，若连续访问需求大，可将Block块设为较大值。<br>HFile物理文件形式参考下面4.3内容</p>
</li>
</ul>
<h4 id="2-6-WALs——Write-Ahead-Log预写日志（HLog）"><a href="#2-6-WALs——Write-Ahead-Log预写日志（HLog）" class="headerlink" title="2.6 WALs——Write-Ahead-Log预写日志（HLog）"></a>2.6 WALs——Write-Ahead-Log预写日志（HLog）</h4><p>&#8195;&#8195;数据库在事务机制中常见的一致性的实现方式就是通过记录日志，通过日志文件的方式实现写入一致性、数据恢复或者数据备份，那么对于HBases也有同样的逻辑，因为大型分布式系统中硬件故障很常见，如果MemStore还没有及时flush到HFile，服务器宕机或者断电，那么MemStore部分的数据肯会丢失。<br>&#8195;&#8195;HBase给出的解决方案：先写入MemStore中，然后更新hlog中，只有成功更新到hlog之后，写操作才能被认为是成功完成。如果在MemStore没有写到hlog之前宕机，HBase重启后可以从hlog恢复。Hbase集群中每台服务器维护一个hlog文件。</p>
<p>==<strong>hlog过期</strong>==<br>&#8195;&#8195;当数据从memstore写入到磁盘中，Hlog就已经没有用了，会把/hbase/WALs目录下的数据移动到/hbase/oldWALs 目录下，oldWALs目录下的数据会根据 hbase.master.cleaner.interval (默认1分钟)配置的时间定期去检查，如发现有数据会清除，清除前还会检验一个参数 hbase.master.logcleaner.ttl ，也就是说数据保存1分钟以上才会删除，如果一分钟内数据直接从memstore写入到了磁盘，oldWALs下的数据也不会被删除</p>
<h4 id="2-7-zookeeper"><a href="#2-7-zookeeper" class="headerlink" title="2.7 zookeeper"></a>2.7 zookeeper</h4><ul>
<li>通过选举（抢占zk临时节点），保证任何时候，集群中只有一个master，Master与RegionServers 启动时会向ZooKeeper注册</li>
<li>RegionServer 向zookeeper的临时znode注册，提供RegionsSrver状态信息（是否在线）。</li>
<li>存放所有Region的寻址入口</li>
<li>存储HBase的schema和table元数据</li>
<li>HMaster启动时候会将hbase系统表-ROOT- 加载到 zookeeper cluster，通过zookeeper cluster可以获取当前系统表元信息的存储所对应的RegionServer信息。</li>
</ul>
<h3 id="3、数据模型"><a href="#3、数据模型" class="headerlink" title="3、数据模型"></a>3、数据模型</h3><p>&#8195;&#8195;这里以Amandeep Khurana的<a href="http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf">《Introduction to HBase Schema Design》</a>作为参考<br>&#8195;&#8195;HBase的表结构如下图3所示：<br><img src="https://img-blog.csdnimg.cn/20191027173035586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;该表有两个列簇： Personal and Office，每个列簇有两个列，例如Personal：姓名和家庭电话，每个小格子cells用来存储真正数据，例如John及其电话，但Row Key 不属于cell。</p>
<p>&#8195;&#8195;HBase行记录的数据结构，其实就是一个嵌套map（嵌套字典），HBase用这种方式组织了数据，以上图的第一行John为例，该0001行字典如下：<br><img src="https://img-blog.csdnimg.cn/20191027173150411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;因为大家更熟悉hash map或字典结构，从上图可以看到，HBase的数据单元cell，通过把timestamp作为最里层字典的key，从而数据多版本的控制。看到该map结构，大家可以联想到mangodb，这伙计大有赶超hadoop之势，同样具备分布式大数据的存储、查询能力，它也用这种结构来存储数据，只不过mangodb用的粒度更小是json结构。</p>
<h3 id="4、表结构组成（对应图3）"><a href="#4、表结构组成（对应图3）" class="headerlink" title="4、表结构组成（对应图3）"></a>4、表结构组成（对应图3）</h3><p>Table（表格）<br>一个HBase表格由多行组成。</p>
<p>Row（行）<br>&#8195;&#8195;RowKey没有特定的数据类型，都是字节数组类型，任何字符串都可以作为行键表的中行数据按照Rowkey的字节（byte order) 排序存储HBase中的行里面包含一个key和一个或者多个包含值的列。行按照行的key字母顺序存储在表格中。因为这个原因，行的key的设计就显得非常重要。数据的存储设计目的是让数据类型相近的数据存储到一起。</p>
<p>Column Family（列族）<br>&#8195;&#8195;因为性能的原因，列族物理上包含一组列和它们的值。每一个列族拥有一系列的存储属性，例如值是否缓存在内存中，数据是否要压缩或者他的行key是否要加密等等。表格中的每一行拥有相同的列族，尽管一个给定的行可能没有存储任何数据在一个给定的列族中。</p>
<p>Column Qualifier（列名）<br>&#8195;&#8195;列族必须定义表时给出，每个列族可以有一个或多个列成员（Column Qualigier)，列成员不需要在定义表时给出，新的列族成员（name、phone）可以随后按需动态加入。列族Personal在创建表格时已被确定，但是列名则可以以后业务需要时动态追加，例如给Personal增加一个列名:company字段，用于存储个人的公司信息</p>
<p>Cell（单元）</p>
<p>&#8195;&#8195;单元是由行、列族、列限定符、值和代表值版本的时间戳组成的。Cell由RowKey，列族：列名（Column Qualifier），时间戳唯一决定。Cell中的数据是没有类型的，全部以字节码形式存储。每个单元格保存着同一份数据的多个版本<br>，不同时间版本的数据按照时间顺序倒序排序。</p>
<p>Timestamp（时间戳）<br> &#8195;&#8195;每个Cell可能有多个版本，它们之间用时间戳区分。</p>
<h3 id="5、HBase物理文件存储过程"><a href="#5、HBase物理文件存储过程" class="headerlink" title="5、HBase物理文件存储过程"></a>5、HBase物理文件存储过程</h3><h4 id="5-1-HRegion在表的上位置"><a href="#5-1-HRegion在表的上位置" class="headerlink" title="5.1 HRegion在表的上位置"></a>5.1 HRegion在表的上位置</h4><p>&#8195;&#8195;HBase的一个典型Big Table 在行的方向上水平分割为多个HRegion，所有行都按照row key的字典序排列。（注意如果Table表刚建初期，数据量不多，此时Table仅有一个HRegion）<br><img src="https://img-blog.csdnimg.cn/20191028225523874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="4-2-HRegion自动拆分"><a href="#4-2-HRegion自动拆分" class="headerlink" title="4.2 HRegion自动拆分"></a>4.2 HRegion自动拆分</h4><p>&#8195;&#8195;HRegion按照大小拆分，每个Table初始只有一个HRegion，随着数据不断插入表，Rowkey越来越多，HRegion不断增大，当增大到一个阀值（默认256M）的时候，HRegion会被拆分为两个新的HRegion，如下图所示，所以当Table中的行不断增多，就会有越有更多HRegion，这些HRegion分布式存在多台HRegion Server上，只要有足够的节点服务器，那么HBase就可以继续被扩容，存下百亿行都OK。在传统关系型数据库中，这种体量的行数，很难想象！<br><img src="https://img-blog.csdnimg.cn/20191028232225435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="4-3-HRegion-分布式存储"><a href="#4-3-HRegion-分布式存储" class="headerlink" title="4.3 HRegion 分布式存储"></a>4.3 HRegion 分布式存储</h4><p>&#8195;&#8195;HRegion是Hbase中分布式存储和负载均衡的最小单元，同一个table的不同HRegion可以分布式存储在不同的HRegion Server上，这里说的“负载均衡”是指：例如下图，如果table的所有HRegion都扔到一台服务器上存储，容易出现不平衡分布（数据倾斜），因此需要考虑所有的Region平衡分布到每个节点上，如下图所示，但一个HRegion是不会拆分到多个节点上，因为HRegion是HMaster分布式存储可以管理的最小单元。（机灵的同学此时会想到：同一个HRegion需要拷贝三份或多份再存到不同节点上吗？注意：这里完全不需要，因为HRegion最终存放是在HDFS上的一个二进制文件block，hdfs本身就会将这个block自动拷贝多份，再存到其他节点。）<br><img src="https://img-blog.csdnimg.cn/2019102900132276.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="5-2-Hbase数据在HDFS的存储"><a href="#5-2-Hbase数据在HDFS的存储" class="headerlink" title="5.2 Hbase数据在HDFS的存储"></a>5.2 Hbase数据在HDFS的存储</h4><p>&#8195;&#8195;HRegion虽然是分布式存储的最小单元，但并不是存储的最小单元。HRegion由一个或者多个Store组成，每个Store保存一个Columns Family。每个Store又由一个memStore和0至多个StoreFile组成。StoreFile最终以HFile格式保存在HDFS上<br><img src="https://img-blog.csdnimg.cn/20191029213447409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;对于company表，上面提到每个Store保存一个Columns Family，那么在HDFS上是怎样的文件呢？通过查看HBase在hdfs文件系统创建的data目录如下所示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# hdfs dfs -ls /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 root supergroup         42 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/.regioninfo</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/recovered.edits</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/staff_info</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;Hbase的数据在HDFS中的路径结构如下：<br>==hdfs://dn2:9000/hbase/data/{名字空间}/{表名}/{区域名称}/{列族名称}/{文件名}==<br>具体说明：<br>hdfs://dn2:9000：hdfs分布式节点dn2为该HBase提供底层文件服务</p>
<ul>
<li>{名字空间}：在本文的示例中，因为在hbase shell建表做测试时，没有创建新的名字空间（相当于关系型数据库的database），所以hbase为company table提供默认default空间</li>
<li>{表名}:具体的HBase table名称，如本例的company table</li>
<li>{区域名称}：指HRegion的字符串名称：38445bbc84c64c82b8273edafbd19b07  由每张表切割形成，table创建开始阶段，仅有一个HRegion，当table越来越大，路径<code>hdfs://dn2:9000/hbase/data/&#123;名字空间&#125;/&#123;表名&#125;/</code>下将会有多个“区域名称”（也即分割为多个HRegion），这就是HRegion在hdfs的存在形式。</li>
<li>{列族名称}：例如本例创建的depart_info和staff_info 这两个列簇</li>
<li>{文件名}：这个文件就是Hfile，每个列簇下有自己存放key-value数据的最终物理文件，这里就是HBase存放数据的最小物理文件。</li>
</ul>
<p>以depart_info列簇查看它包含的文件有：bdccf10bf1a344baa68c4404b385da7b和dddb8e7e43134c16823db24416</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@nn ~]# hdfs dfs -ls /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup       5082 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info/bdccf10bf1a344baa68c4404b385da7b</span><br><span class="line">-rw-r--r--   3 root supergroup       4859 ** /hbase/data/default/company/38445bbc84c64c82b8273edafbd19b07/depart_info/dddb8e7e43134c16823db24416</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;继续查看depart_info路径下bdccf10bf1a344baa68c4404b385da7b文件内容，该文件为二进制文件，从内容可以大致看到该文件已经存储了R1和R2两行数据内容以及一些元信息等</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DATABLK*ˇˇˇˇˇˇˇˇ@&quot;&quot;R1depart_infoinner_telmÌµçS108R1depart_infolevelmÌµq19</span><br><span class="line">R1depart_infonamemÌµWOHR</span><br><span class="line">&quot;R2depart_infoinner_telmÌ∑£106R2depart_infolevelmÌ∑äF9R2depart_infonamemÌ∑n6Finance$©…]BLMFBLK2ˇˇˇˇˇˇˇˇ@%ÈíT@3+D4IDXROOT23&#x2F;ˇˇˇˇˇˇˇˇ@P&amp;&quot;R1depart_infoinner_telmÌµçS…“?IDXROOT2O@!ÁµãÏFILEINF2îêˇˇˇˇˇˇˇˇ@±PBUFä</span><br><span class="line"></span><br><span class="line">BLOOM_FILTER_TYPEROW</span><br><span class="line"></span><br><span class="line">DELETE_FAMILY_COUNT</span><br><span class="line"></span><br><span class="line">EARLIEST_PUT_TSmÌµWO</span><br><span class="line"></span><br><span class="line">KEY_VALUE_VERSION</span><br><span class="line"></span><br><span class="line">LAST_BLOOM_KEYR2</span><br><span class="line"></span><br><span class="line">MAJOR_COMPACTION_KEY</span><br><span class="line"></span><br><span class="line">MAX_MEMSTORE_TS_KEY</span><br><span class="line"></span><br><span class="line">MAX_SEQ_ID_KEY</span><br><span class="line"></span><br><span class="line">	TIMERANGEmÌµWOmÌ∑£</span><br><span class="line"></span><br><span class="line">hfile.AVG_KEY_LEN</span><br><span class="line"></span><br><span class="line">hfile.AVG_VALUE_LEN</span><br><span class="line"> </span><br><span class="line">hfile.CREATE_TIME_TSmÌÔ∂</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">hfile.LASTKEYR2depart_infonamemÌ∑n6˙x%ªBLMFMET2&lt;8ˇˇˇˇˇˇˇˇ@Y&amp;)R1Z8˚TRABLK&quot;$H»œ&#x2F; Ú&amp;(08@HPZ-org.apache.hadoop.hbase.KeyValue$KVComparator&#96;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;在namenode web页面：<a href="http://dn2:50070/">http://dn2:50070/</a> 可以查看HFile相关信息，如下图所示，该HFile在HDFS集群上是3份拷贝的分布式存储。<br><img src="https://img-blog.csdnimg.cn/20191029225007131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="6、HBase目录结构"><a href="#6、HBase目录结构" class="headerlink" title="6、HBase目录结构"></a>6、HBase目录结构</h3><p>&#8195;&#8195;HMaser在hdfs文件系统上创建了HBase目录，该目录用于存放所有关于HBase文件、数据、元信息等内容。</p>
<ul>
<li>/hbase/.tmp: 临时目录，当对表做创建和删除操作时，会将表move到该目录下，然后进行操作。</li>
<li> /hbase/WALs:在2.6章节已经提到，该目录为保存操作日志hlog，如下所示：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]#  hdfs dfs -ls &#x2F;hbase&#x2F;WALs&#x2F;</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   * &#x2F;hbase&#x2F;WALs&#x2F;dn1,16020,*</span><br><span class="line">drwxr-xr-x   * &#x2F;hbase&#x2F;WALs&#x2F;dn2,16020,*</span><br><span class="line">drwxr-xr-x   * &#x2F;hbase&#x2F;WALs&#x2F;nn,16020,*</span><br></pre></td></tr></table></figure>
从WALs目录下可以看到每个HRegionServer维护自己一个hlog</li>
<li>/hbase/data:核心目录，存储Hbase表的数据默认情况下该目录下有两个目录</li>
<li>/hbase/data/default:当在用户创建表的时候，没有指定namespace时，表就创建在此目录下<br>– /hbase/data/hbase：系统内部创建的表，hbase:meta，namespace</li>
<li>/hbase/hbase.id：存储集群唯一cluster id(UUID)</li>
<li> /hbase/hbase.version：集群版本号</li>
<li>/hbase/oldWALs：参考2.6章节内容——hlog过期</li>
</ul>
<h3 id="7、HBase的读写流程"><a href="#7、HBase的读写流程" class="headerlink" title="7、HBase的读写流程"></a>7、HBase的读写流程</h3><p>&#8195;&#8195;之所以把该章节安排在文章最后，是因为基于前面讨论HBase内部结构已经有一定了解后，再分析其读写流程则显得更容易理解。</p>
<h4 id="7-1-写流程"><a href="#7-1-写流程" class="headerlink" title="7.1 写流程"></a>7.1 写流程</h4><p>1）Client从Zookeeper中获取表region相关信息，根据要插入的rowkey，获取指定的Regionserver信息。<br>2）数据被写入Region的MemStore（操作也同时写入到Hlog中），若持续写入数据量超过MemStore达到预设阈值，MemStore会flush到StoreFile，当数据都同时写入到MemStore和Hlog后，那么对于client来说，本次写入即完成。<br>3）随着StoreFile文件的不断增多，当其数量增长到一定阈值后，触发Compact合并操作，将多个StoreFile合并成一个StoreFile。StoreFiles通过不断的Compact合并操作，逐步形成越来越大的StoreFile。<br>4）单个StoreFile大小超过一定阈值后，触发Split操作，把当前Region Split成2个新的Region。旧Region会下线，新Split出的2个Region会被HMaster分配到相应的RegionServer上，使得原先1个Region的压力得以分流到2个Region上。</p>
<h4 id="7-2-读流程"><a href="#7-2-读流程" class="headerlink" title="7.2 读流程"></a>7.2 读流程</h4><p>1）跟写流程一样，client先连接ZooKeeper，访问Meta Regionserver节点信息，把HBase的meta表缓存到client端。<br>2）根据rowkey找到Region，再去相应Regionserver 发起读请求。<br>3）RegionServer接收该读请求之后，既然是来询问key值是否存在，那么HBase针对检索，有自己一套方法：先查询MemStore，如果未找到，再去查询BlockCache加速读内容的缓冲区，如果还没有找到，就会到StoreFile（HFile）中查找这条数据，然后将这条数据返回给client。（注意：这里忽略了一个有关key高效检索是否存在与于HFIle的知识点：bloom-filter）<br>（从client在HBase读写请求过程可知，client其实无需与HMaster通信，只需要知道ZooKeeper地址即可）</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本文已经完成对HBase架构及其数据模型较为全面的讨论，接下来在另外一篇文章开启对Hive的部署和架构分析，Hive跟HBase、Hadoop结合使用后，形成一套可用的数据仓库。</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>MariaDB+Keepalived 搭建双主HA数据库服务</title>
    <url>/blog/2019/08/26/MariaDB+Keepalived%20%E6%90%AD%E5%BB%BA%E5%8F%8C%E4%B8%BBHA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<h5 id="1、安装mysql"><a href="#1、安装mysql" class="headerlink" title="1、安装mysql"></a>1、安装mysql</h5><p>&#8195;&#8195;在linux版本下，mysql称为mariadb，可以选择在线安装，或编译安装。MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可。开发这个分支的原因之一是：甲骨文公司收购了MySQL后，有将MySQL闭源的潜在风险，因此社区采用分支的方式来避开这个风险。MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。</p>
<a id="more"></a>

<p>&#8195;&#8195;用国内镜像源替换官方的MariaDB镜像源，目前MariaDB最新版本10.4，这里选10.3作为稳定版安装，本人项目或者测试环境，都是以MariaDB为主。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/yum.repos.d/MariaDB.repo</span><br><span class="line">[mariadb]</span><br><span class="line">name = MariaDB</span><br><span class="line">baseurl = http://mirrors.ustc.edu.cn/mariadb/yum/10.3/centos7-amd64/</span><br><span class="line">gpgkey=http://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDB</span><br><span class="line">gpgcheck=1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install mariadb mariadb-server -y</span><br><span class="line">systemctl restart mariadb</span><br><span class="line">systemctl enable mariadb</span><br><span class="line">mysql_secure_installation</span><br><span class="line"><span class="meta">#</span><span class="bash"> 该命令为重置root密码，并做一些安全配置,若不做配置，后续将无法使用sell进入mysql</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="2、配置主备server的my-cnf"><a href="#2、配置主备server的my-cnf" class="headerlink" title="2、配置主备server的my.cnf"></a>2、配置主备server的my.cnf</h5><p>&#8195;&#8195;通过my.cnf可以优化或者定制更高级的mysql使用，具体参考另外一篇文章==todo==，这里只写简单主主同步配置</p>
<p>主服务器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 数据目录可以使用默认也可以自行定义，所有的binlog以及db物理文件都在datadir里面后期可以通过挂载存储扩容</span></span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/lib/mysql/mysql.sock</span><br><span class="line">server-id=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启日志模式</span></span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">relay-log=mysql-relay-bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 双主模式下，防止两边插入插入时，自增键冲突，主服务器自增规则：1，3，5奇数自增</span></span><br><span class="line">auto_increment_offset=1</span><br><span class="line">auto_increment_increment=2</span><br><span class="line">log_slave_updates =1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略一些测试表</span></span><br><span class="line">replicate-wild-ignore-table=test.%</span><br><span class="line">replicate-wild-ignore-table=mysql.%</span><br><span class="line">replicate-wild-ignore-table=performance_schema.%</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">出现错误后忽略，若不跳过，出现任何同步错误，slave-IO进程会终止</span></span><br><span class="line">slave-skip-errors=all</span><br></pre></td></tr></table></figure>

<p>备服务器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/lib/mysql/mysql.sock</span><br><span class="line"></span><br><span class="line">server-id=2</span><br><span class="line"></span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">relay-log=mysql-relay-bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 双主模式下，防止两边插入插入时，自增键冲突，备服务器自增规则：2，4，6奇数自增</span></span><br><span class="line">auto_increment_offset=2</span><br><span class="line">auto_increment_increment=2</span><br><span class="line">log_slave_updates =1</span><br><span class="line"></span><br><span class="line">replicate-wild-ignore-table=test.%</span><br><span class="line">replicate-wild-ignore-table=mysql.%</span><br><span class="line">replicate-wild-ignore-table=performance_schema.%</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">出现错误后忽略，若不跳过，出现任何同步错误，slave-IO进程会终止</span></span><br><span class="line">slave-skip-errors=all</span><br></pre></td></tr></table></figure>

<h5 id="3、创建新账户"><a href="#3、创建新账户" class="headerlink" title="3、创建新账户"></a>3、创建新账户</h5><p>&#8195;&#8195;在主服务器，备服务器分别创建用于管理主备的msyql帐号，请勿用root账户，以免出现安全问题！</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql -u root -psd123321sd</span><br><span class="line">create user &#x27;sync_acct&#x27;@&#x27;192.168.100.%&#x27; identified by &#x27;*&amp;@jall190&#x27;;</span><br><span class="line">grant replication slave on *.* to &#x27;sync_acct&#x27;@&#x27;192.168.100.%&#x27;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下两条配置可以实现在shell中mysql -usync_acct -p*&amp;@jall190 直接登录，否则会提示如下：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ERROR 1045 (28000): Access denied <span class="keyword">for</span> user <span class="string">&#x27;sync_acct&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> (using password: YES)，登录失败</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grant select,insert,update,delete on</span> </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO sync_acc@&quot;127.0.0.1&quot; IDENTIFIED BY &quot;*&amp;@jall190&quot; WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO sync_acc@&quot;localhost&quot; IDENTIFIED BY &quot;*&amp;@jall190&quot; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br><span class="line">exit</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看可本地登录的记录</span></span><br><span class="line">MariaDB [(none)]&gt; select HOST,User from mysql.user;</span><br><span class="line">+-----------------------+-----------+</span><br><span class="line">| HOST                  | User      |</span><br><span class="line">+-----------------------+-----------+</span><br><span class="line">| 127.0.0.1             | root      |</span><br><span class="line">| 127.0.0.1             | sync_acc  |</span><br><span class="line">| 192.168.100.%         | sync_acct |</span><br><span class="line">| ::1                   | root      |</span><br><span class="line">| localhost             | root      |</span><br><span class="line">| localhost             | sync_acc  |</span><br><span class="line">| localhost.localdomain | root      |</span><br><span class="line">+-----------------------+-----------+</span><br></pre></td></tr></table></figure>

<h5 id="4、查看主服务器bin-log信息"><a href="#4、查看主服务器bin-log信息" class="headerlink" title="4、查看主服务器bin-log信息"></a>4、查看主服务器bin-log信息</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">对数据库进行只读锁定（防止查看二进制日志同时有人对数据库修改操作）</span></span><br><span class="line">MariaDB [(none)]&gt; flush tables with read lock;</span><br><span class="line">MariaDB [(none)]&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">| mysql-bin.000002 |      504 |              |                  |</span><br><span class="line">+------------------+----------+--------------+------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">MariaDB [(none)]&gt; unlock tables;</span><br></pre></td></tr></table></figure>

<p>以上的file名称：mysql-bin.000002跟position：504在一下主备需要用到</p>
<h5 id="5、分别在主备服务器上配置主-主模式"><a href="#5、分别在主备服务器上配置主-主模式" class="headerlink" title="5、分别在主备服务器上配置主-主模式"></a>5、分别在主备服务器上配置主-主模式</h5><p>（1） 在备服务器上，配置db1-192.168.100.5作为master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接在命令行上敲</span></span><br><span class="line">MariaDB [(none)]&gt; change master to </span><br><span class="line">master_host=&#x27;192.168.100.5&#x27;, </span><br><span class="line">master_port=41210,</span><br><span class="line">master_user=&#x27;sync_acct&#x27;, </span><br><span class="line">master_password=&#x27;*&amp;@jall190&#x27;, </span><br><span class="line"><span class="meta">#</span><span class="bash">指明初始复制时的mysql1中的binlog文件</span></span><br><span class="line">master_log_file=&#x27;mysql-bin.000003&#x27;,</span><br><span class="line"><span class="meta">#</span><span class="bash">指明初始复制时binlog文件的位置</span></span><br><span class="line">master_log_pos=655;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; start slave;</span><br></pre></td></tr></table></figure>

<p>​    从服务器开启slave线程后，通过show slave status\G可以看到一些信息，这里\G是表示按竖立显示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.100.5</span><br><span class="line">                  Master_User: sync_acct</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000002</span><br><span class="line">          Read_Master_Log_Pos: 4041005</span><br><span class="line">               Relay_Log_File: mysql-relay-bin.000003</span><br><span class="line">                Relay_Log_Pos: 4041030</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000002</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">  ......</span><br></pre></td></tr></table></figure>

<p>以上信息可知</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#这个是指Slave连接到Master的状态，当前IO线程的状态为等待master发送事件，该字段显示mysql不同状态的不同信息，而且信息简单易读</span><br><span class="line">Slave_IO_State: Waiting for master to send event：</span><br><span class="line"></span><br><span class="line">Slave_IO_Running: Yes，显示slave的I&#x2F;O线程是否被启动并成功地连接到主服务器上。</span><br><span class="line">Slave_SQL_Running: Yes，显示slave上用于读取Relay_Log的SQL线程是否被启动</span><br></pre></td></tr></table></figure>

<p> 另外，备份Slave_IO_State信息对于在主服务器上的状态，用<code>show processlist</code>查看主服务器显示master已经发送所有的binlog到salve，并等待主服务器更新这个binlog</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show processlist\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">      Id: 24</span><br><span class="line">    User: sync_acct</span><br><span class="line">    Host: 192.168.100.6:35474</span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Binlog Dump</span><br><span class="line">    Time: 1143</span><br><span class="line">   State: Master has sent all binlog to slave; waiting for binlog to be updated</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br></pre></td></tr></table></figure>

<p>以上说明，主-备模式成功配置，但还不是主-主模式</p>
<p>（2） 在(1)上，已经配置好主-从同步，且已经验证可正常写入同步，这里，将配置主-主模式，在主服务器db2-192.168.100.5上，配置db2-192.168.100.6作为master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 直接在命令行上敲</span></span><br><span class="line">MariaDB [(none)]&gt; change master to </span><br><span class="line">master_host=&#x27;192.168.100.6&#x27;, </span><br><span class="line">master_user=&#x27;sync_acct&#x27;, </span><br><span class="line">master_password=&#x27;*&amp;@jall190&#x27;, </span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下两个字段的值，务必在从服务器上，用show master status 查看相关值</span></span><br><span class="line">master_log_file=&#x27;mysql-bin.000003&#x27;,</span><br><span class="line">master_log_pos=587;</span><br><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.100.6</span><br><span class="line">                  Master_User: sync_acct</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000003</span><br><span class="line">          Read_Master_Log_Pos: 587</span><br><span class="line">               Relay_Log_File: mysql-relay-bin.000002</span><br><span class="line">                Relay_Log_Pos: 529</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000003</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>（3）在第（1）、（2）已经成功完成主-主模式，但也仅仅是说明msyql主主配置正常，但还未通过数据写入来测试其正确性，可通过以下简单的两个步骤测试主-主模式数据的同步</p>
<ul>
<li>主服务器上新建一个databases，创建一个简单表，观察slave是否同步新建情况</li>
<li>在刚新建的表里插入一条记录，观察slave对应的表是否也多一条记录</li>
<li>在备服务器上drop 掉刚建的数据库，然后在salve重复以上操作，观察master上数据同步情况</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里仅简单给出一些命令，不再具体展开</span></span><br><span class="line">create database erp_app;</span><br><span class="line">use erp_app;</span><br><span class="line">create table if not exists apps_name(</span><br><span class="line">id int(4) not null primary key auto_increment,</span><br><span class="line">app_log_name char(20) not null,</span><br><span class="line">log_path char(200) not null,</span><br><span class="line">log_date timestamp default current_timestamp</span><br><span class="line">);</span><br><span class="line">show tables</span><br><span class="line"><span class="meta">#</span><span class="bash">插入数据，因为id是自增，无需自行插入，使用null交给mysql自动插入相应id号</span></span><br><span class="line">insert into apps_name values(null,&#x27;BI-Access-Log&#x27;,&#x27;/opt/data/apps_log/&#x27;,null);</span><br></pre></td></tr></table></figure>
<p>使用python插入数据做测试使用，后面可作为验证主-主故障切换使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line">db_info=&#123;</span><br><span class="line">            <span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;port&#x27;</span>:<span class="number">3306</span>,</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>:<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;password&#x27;</span>:<span class="string">&#x27;****&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;db&#x27;</span>:<span class="string">&#x27;erp_app&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;charset&#x27;</span>:<span class="string">&#x27;utf8&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    conn = pymysql.connect(**db_info)</span><br><span class="line">    insert_sql = (<span class="string">&quot;insert into apps_name &quot;</span></span><br><span class="line">                  <span class="string">&quot;(id,app_log_name,log_path,log_date) &quot;</span></span><br><span class="line">                  <span class="string">&quot;values(null,%(app_log_name)s,%(log_path)s,null)&quot;</span>)</span><br><span class="line">    insert_data=&#123;</span><br><span class="line">        <span class="string">&#x27;app_log_name&#x27;</span>:<span class="string">&#x27;BI-Access-Log&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;log_path&#x27;</span>:<span class="string">&#x27;/opt/data/apps_log/&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> conn.cursor() <span class="keyword">as</span> cur:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            resl=cur.execute(insert_sql,insert_data)</span><br><span class="line">            print(resl)</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">            conn.commit()</span><br><span class="line"><span class="keyword">except</span> pymysql.MySQLError <span class="keyword">as</span> err:</span><br><span class="line">    print(err)</span><br><span class="line">    conn.rollback()</span><br><span class="line"></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>在第2点配置my.cnf，设定了自增键规则，也可以验证主、从插入数据时，其自增键的情况</strong></p>
<p>在主服务器重新插入四条，id为奇数id：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt; select * from apps_name;</span><br><span class="line">+----+---------------+</span><br><span class="line">| id | app_log_name  |</span><br><span class="line">+----+---------------+</span><br><span class="line">|  1 | BI-Access-Log |</span><br><span class="line">|  3 | BI-Access-Log |</span><br><span class="line">|  5 | BI-Access-Log |</span><br><span class="line">|  7 | BI-Access-Log |</span><br><span class="line">+----+---------------+</span><br></pre></td></tr></table></figure>

<p>清空之前测试的数据，在备服务器重新插入四条，id为偶数id：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [erp_app]&gt;  select * from apps_name;</span><br><span class="line">+----+---------------+</span><br><span class="line">| id | app_log_name  |</span><br><span class="line">+----+---------------+</span><br><span class="line">|  2 | BI-Access-Log |</span><br><span class="line">|  4 | BI-Access-Log |</span><br><span class="line">|  6 | BI-Access-Log |</span><br><span class="line">|  8 | BI-Access-Log |</span><br><span class="line">+----+---------------+</span><br></pre></td></tr></table></figure>

<p>但是这种配置自增算法有bug，只在双主模式下，假设现在要三台服务器都作为主，互相同步，那么现有的自增键策略无法扩展，其实有两种可行方案：</p>
<p>第一种：</p>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"> #</span><span class="bash"> 通过增大步长，例如有10台，步长值=主服务器数量，解决自增键冲突</span></span><br><span class="line"><span class="meta"> #</span><span class="bash"> 服务器1</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 1</span><br><span class="line"><span class="meta"> #</span><span class="bash"> 服务器2</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 2</span><br><span class="line"><span class="meta"> #</span><span class="bash"> 服务器3</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 3</span><br><span class="line"> ......</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">服务器10</span></span><br><span class="line">auto-increment-increment = 10</span><br><span class="line">auto-increment-offset   = 10</span><br></pre></td></tr></table></figure>

<p>第二种：数据库表不设置自增字段，由程序逻辑用UUID实现id唯一值，个人推荐此方式，不受限制。</p>
<p><strong>注意</strong>：</p>
<p>在主从模式下，或者主-主模式下，测试数据过程中，例如在master上进行drop erp_app时，若slave不存在erp_app数据库（主从分离连接后，从已先删除），slave将无法进行同步删除操作，后续的同步操作也无法正常进行，show slave status\G，看到有报错提示db不存在无法drop的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Last_SQL_Errno: 1008</span><br><span class="line">Last_SQL_Error: Error &#39;Can&#39;t drop database &#39;erp_app&#39;; database doesn&#39;t exist&#39; on query. Default database: &#39;erp_app&#39;. Query: &#39;drop database erp_app&#39;</span><br></pre></td></tr></table></figure>

<p>需在两台服务器上进行如下设置，==如果已经在my.cnf配置文件配好slave-skip-errors=all可跳过一下设置==</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; stop slave;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"># 将同步指针向移动下一个位置，跳过异常，以便mysql可继续执行下一个同步操作</span><br><span class="line">MariaDB [(none)]&gt; set global sql_slave_skip_counter&#x3D;1;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>

<p>（4）查看binlog,relay-log,创建数据库对应的物理存储文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost mysql]# ls</span><br><span class="line">aria_log.00000001  ibtmp1             mysql-bin.000003  mysql-bin.000010  mysql-bin.index</span><br><span class="line">aria_log_control   localhost.pid      mysql-bin.000004  mysql-bin.000011  mysql-relay-bin.000050</span><br><span class="line">erp_app            master.info        mysql-bin.000005  mysql-bin.000012  mysql-relay-bin.000051</span><br><span class="line">ib_buffer_pool     multi-master.info  mysql-bin.000006  mysql-bin.000013  mysql-relay-bin.index</span><br><span class="line">ibdata1            mysql              mysql-bin.000007  mysql-bin.000014  mysql.sock</span><br><span class="line">ib_logfile0        mysql-bin.000001   mysql-bin.000008  mysql-bin.000015  performance_schema</span><br><span class="line">ib_logfile1        mysql-bin.000002   mysql-bin.000009  mysql-bin.000016  relay-log.info</span><br></pre></td></tr></table></figure>
<p>相关文件说明：</p>
<p>master.info：主库A的账户密码等同步基本配置信息</p>
<p>mysql-bin.index：记录主库A所有的binlog日志文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./mysql-bin.000001</span><br><span class="line">./mysql-bin.000002</span><br><span class="line">./mysql-bin.000003</span><br><span class="line">./mysql-bin.000004</span><br><span class="line">./mysql-bin.000005</span><br><span class="line">./mysql-bin.000006</span><br><span class="line">./mysql-bin.000007</span><br><span class="line">./mysql-bin.000008</span><br><span class="line">./mysql-bin.000009</span><br><span class="line">./mysql-bin.000010</span><br><span class="line">./mysql-bin.000011</span><br><span class="line">./mysql-bin.000012</span><br><span class="line">./mysql-bin.000013</span><br><span class="line">./mysql-bin.000014</span><br><span class="line">./mysql-bin.000015</span><br><span class="line">./mysql-bin.000016</span><br><span class="line">~</span><br></pre></td></tr></table></figure>
<p>mysql-bin.000**：主库A的binlog日志里的sql操作命令，vi该二进制文件进去可以看到sql命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">^@^@^@^@^@^@^@&lt;84&gt;芦盲6贸bZ]^B^B^@^@^@&#125;^@^@^@茫     ^@^@^@^@        ^@^@^@^A^@^@^@^G^@^@^_^@^@^@^@^@^@^A^@^@ T^@^@^F^Cstd^C^B^@^B^@^D!^@!^@^H^@erp_app^@insert into apps_name values(null,&#x27;BI-Access-Log&#x27;)陆@莽篓贸bZ]^P^B^@^@^@^@^@^B</span><br></pre></td></tr></table></figure>
<p>relay-log.info:记录最新使用日志信息</p>
<p>mysql-relay-bin.000**：主库A在主库B同步回来的中继日志文件，记录主库B执行过的SQL命令</p>
<p>mysql-relay-bin.index：中继日志的索引文件，记录所有relay日志文件</p>
<p>==从上面的相关物理文件，也可看出msyql主从同步过程==</p>
<p>从库的IO线程 把主库mysql-bin.000**  日志append到本机的中继日志文件mysql-relay-bin.000**<br>从库SQL线程 执行本机中继日志文件里的sql命令，将数据写入进本机。</p>
<p>通过<code>show processlist</code>也可清晰看到相关线程的作用：</p>
<p>Slave_IO线程说：正在等待主库发送事件</p>
<p>Slave_SQL线程说：本从库已经读完所有中继日志，正在等待Slave_IO线程更新到中继日志文件（或新建一个中继日志文件）</p>
<p>Binlog Dump：（这个线程说明，本机也是主库角色）本机已经把所有binlog发送到了从库，正在等待本机更新binlog</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show processlist\G</span><br><span class="line">*************************** 6. row ***************************</span><br><span class="line">      Id: 10</span><br><span class="line">    User: system user</span><br><span class="line">    Host: </span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Slave_IO</span><br><span class="line">    Time: 812</span><br><span class="line">   State: Waiting for master to send event</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br><span class="line">*************************** 7. row ***************************</span><br><span class="line">      Id: 11</span><br><span class="line">    User: system user</span><br><span class="line">    Host: </span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Slave_SQL</span><br><span class="line">    Time: 812</span><br><span class="line">   State: Slave has read all relay log; waiting for the slave I/O thread to update it</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br><span class="line">*************************** 8. row ***************************</span><br><span class="line">      Id: 13</span><br><span class="line">    User: sync_acct</span><br><span class="line">    Host: 192.168.142.4:51432</span><br><span class="line">      db: NULL</span><br><span class="line"> Command: Binlog Dump</span><br><span class="line">    Time: 772</span><br><span class="line">   State: Master has sent all binlog to slave; waiting for binlog to be updated</span><br><span class="line">    Info: NULL</span><br><span class="line">Progress: 0.000</span><br></pre></td></tr></table></figure>

<p>（5）查看mariaDB 存储引擎</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show variables like &#x27;storage_engine&#x27;;</span><br><span class="line">+----------------+--------+</span><br><span class="line">| Variable_name  | Value  |</span><br><span class="line">+----------------+--------+</span><br><span class="line">| storage_engine | InnoDB |</span><br><span class="line">+----------------+--------+</span><br></pre></td></tr></table></figure>

<p><strong>可在my.cnf配置文件下设置’default-storage-engine=?’更改默认引擎，这是全局修改</strong></p>
<p><strong>若修改表的存储引擎 alter table table_name engine=engine_name;</strong></p>
<h5 id="6、keepalived配置以及VIP漂移策略"><a href="#6、keepalived配置以及VIP漂移策略" class="headerlink" title="6、keepalived配置以及VIP漂移策略"></a>6、keepalived配置以及VIP漂移策略</h5><p>（1）keepalived配置<code>/etc/keepalived/keepalived.conf</code>，权限必须为644</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 邮箱预警设置简单，不再说明</span></span><br><span class="line">   notification_email &#123;</span><br><span class="line">     acassen@firewall.loc</span><br><span class="line">     failover@firewall.loc</span><br><span class="line">     sysadmin@firewall.loc</span><br><span class="line">   &#125;</span><br><span class="line"><span class="meta">   #</span><span class="bash">notification_email_from Alexandre.Cassen@firewall.loc</span></span><br><span class="line"><span class="meta">   #</span><span class="bash">smtp_server 192.168.200.1</span></span><br><span class="line"><span class="meta">   #</span><span class="bash">smtp_connect_timeout 30</span>   </span><br><span class="line"><span class="meta">   #</span><span class="bash">以下两行处理脚本执行权限问题</span></span><br><span class="line">   script_user root</span><br><span class="line">   enable_script_security </span><br><span class="line"></span><br><span class="line">   router_id hdA # 备服务器hdB，主、备的router_id必须不同，否则VRRP无法选举</span><br><span class="line">   vrrp_skip_check_adv_addr</span><br><span class="line">   vrrp_strict</span><br><span class="line">   vrrp_garp_interval 0</span><br><span class="line">   vrrp_gna_interval 0</span><br><span class="line">   </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检测db主-主同步脚本</span></span><br><span class="line">vrrp_script chk_mariadb_sync &#123;</span><br><span class="line">    script &quot;/etc/keepalived/check_db_sync.sh&quot;</span><br><span class="line">    interval 2</span><br><span class="line">    weight 10</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 非抢占模式</span></span><br><span class="line">    state BACKUP</span><br><span class="line">    nopreempt</span><br><span class="line">    interface ens33</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 100 # 备服务器99</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 89287201</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.100.7</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">track_script &#123;</span><br><span class="line">     chk_mariadb_sync</span><br><span class="line">   &#125;  </span><br></pre></td></tr></table></figure>

<p>（2） mariaDB 同步状态检测脚本 <code>/etc/keepalived/check_db_sync.sh</code></p>
<p>注意，这里别用root账户，保证一定安全，<code>chmod 744 check_db_sync.sh</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">mysql_bin=/usr/bin/mysql</span><br><span class="line">user=sync_acc</span><br><span class="line">pw=passPass</span><br><span class="line">host=127.0.0.1</span><br><span class="line">port=3306</span><br><span class="line"><span class="meta">#</span><span class="bash"> Seconds_Behind_Master</span></span><br><span class="line">sbm=60</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">io_thread_state=`$mysql_bin -h $host -P $port -u$user -p$pw -e &#x27;show slave status\G&#x27;  2&gt;/dev/null|grep &#x27;Slave_IO_Running:&#x27;|awk &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $io_thread_state</span><br><span class="line"></span><br><span class="line">sql_thread_state=`$mysql_bin -h $host -P $port -u$user -p$pw -e &#x27;show slave status\G&#x27; 2&gt;/dev/null|grep &#x27;Slave_SQL_Running:&#x27;|awk &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $sql_thread_state</span><br><span class="line"></span><br><span class="line">SBM=`$mysql_bin -h $host -P $port -u$user -p$pw -e &#x27;show slave status\G&#x27; 2&gt;/dev/null|grep &#x27;Seconds_Behind_Master:&#x27;|awk &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $SBM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Check <span class="keyword">for</span> <span class="variable">$mysql_bin</span></span></span><br><span class="line">if [ ! -f $mysql_bin ];then</span><br><span class="line">        echo &#x27;the path of mysqlbin is incorrect,please check msyql path&#x27;</span><br><span class="line">        exit 2</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> check mysql status whether is dead</span></span><br><span class="line">service mariadb status &amp;&gt;/dev/null</span><br><span class="line">if [ $? -ne 0 ];then</span><br><span class="line">        pkill keepalived</span><br><span class="line">        echo &#x27;mysql is dead&#x27;</span><br><span class="line">        exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> -z 表示如果<span class="variable">$IOThread</span>变量为空，说明数据库服务不可用，已down</span></span><br><span class="line">if [[ -z &quot;$io_thread_state&quot; ]];then</span><br><span class="line">		pkill keepalived</span><br><span class="line">		echo &#x27;mysql is dead&#x27;</span><br><span class="line">        exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if [[ &quot;$io_thread_state&quot; == &quot;Connecting&quot; &amp;&amp; &quot;$sql_thread_state&quot; == &quot;Yes&quot; ]];then</span><br><span class="line">		echo &#x27;master is down,but slave still works&#x27;</span><br><span class="line">        exit 0</span><br><span class="line">        </span><br><span class="line">        # Seconds_Behind_Master timeout</span><br><span class="line">        elif [[ $SBM -ge $sbm ]];then</span><br><span class="line">                pkill keepalived</span><br><span class="line">                exit 1</span><br><span class="line">        else</span><br><span class="line">                exit 0</span><br><span class="line">fi</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>VIP切换到正常的主服务逻辑：</p>
<p>1）主A，主B db数据库正常，IO线程同步正常，vip被主A占用</p>
<p>2）主A数据库down后，如检测脚本所示，脚本把主A 的keepalived服务kill掉，此时VIP飘移到主B服务器，实现故障转移，对于主B，它的线程连接主A超时，无法同步，但仍可对外提供db服务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Slave_IO_State: Reconnecting after a failed master event read</span><br><span class="line">Slave_IO_Running: Connecting</span><br><span class="line">Slave_SQL_Running: Yes</span><br></pre></td></tr></table></figure>

<p>3）当主A 恢复数据库后，主A重启 keepalived服务，因为配置为非抢占模式，故此时还是由主B对外提供db服务</p>
<p>4）如果主A、主B服务都down了？ 你应该在此之前准备好相关邮件或者短信监控并人工介入</p>
<p>补充：VIP漂移策略不一定按上述情况，可自行加入数据库相关的监控指标来确定脚本，也可用python作为脚本</p>
<h5 id="7、-在防火墙写入ACL，开放相关端口"><a href="#7、-在防火墙写入ACL，开放相关端口" class="headerlink" title="7、 在防火墙写入ACL，开放相关端口"></a>7、 在防火墙写入ACL，开放相关端口</h5><blockquote>
<p>个人发现csdn上绝对大部分博客教程，尤其在前期配置环境这一类文章，一律跳过防火墙设置，直接停掉防火墙，以便快速部署，部署成功后，大部分文章会忽略最后加入防火墙设置，事实上一旦形成这种“偷懒的”习惯，在生成环境很容易出现“服务器、数据、漏洞安全”，即使服务内网使用。而且当前网络安全形势突出，在参与2019护网行动以及等保工作中，对安全有了较深刻体会。</p>
</blockquote>
<p><strong>1)</strong> 端口加入防火墙，防火墙设置分为centos7.5 firewalld和centos6或者redhat6.5的iptables，因为本人长期使用centos发行版，这里给出的是firewalld的设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">firewall-cmd --state</span><br><span class="line">systemctl start firewalld</span><br><span class="line">systemctl enable firewalld</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试端口连通性，很多人会用telnet测试端口是否打开，但centos默认没有telnet服务，其实针对tcp层连通性测试，使用http协议也一样，而且系统自带wget命令，非常方便测试</span></span><br><span class="line">在备服务器上，关闭防火墙的两种情况</span><br><span class="line">[root@localhost ~]# wget http://192.168.100.5:3306</span><br><span class="line">--2019-08-16 10:50:59--  http://192.168.100.5:3306/</span><br><span class="line">Connecting to 192.168.100.5:3306... connected.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开防火墙，默认并未放通3306端口</span></span><br><span class="line">[root@localhost ~]# wget http://192.168.100.5:3306</span><br><span class="line">--2019-08-16 10:51:10--  http://192.168.100.5:3306/</span><br><span class="line">Connecting to 192.168.100.5:3306... failed: No route to host.</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 限制仅192.168.100.0/24网段能访问端口3306</span></span><br><span class="line">firewall-cmd --permanent --zone=public --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.100.0/24&quot; port protocol=&quot;tcp&quot; port=&quot;3306&quot; accept&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新防火墙规则</span></span><br><span class="line">firewall-cmd --reload或firewall-cmd --complete-reload</span><br><span class="line">(两者的区别就是第一个无需断开连接，就是firewalld特性之一动态添加规则，第二个需要断开连接，类似重启服务)</span><br></pre></td></tr></table></figure>

<p><strong>2)</strong>  开放VRRP协议，用于keepalived 主备状态检测</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 主备都运行下面的命令，从组播地址224.0.0.18，可以看出VRRP用了组播协议</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 入方向</span></span><br><span class="line">firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --in-interface ens33 --destination 224.0.0.18 --protocol vrrp -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 出方向</span></span><br><span class="line">firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --out-interface ens33 --destination 224.0.0.18 --protocol vrrp -j ACCEPT</span><br><span class="line">firewall-cmd --reload</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql HA</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas数据预处理的常用函数</title>
    <url>/blog/2019/11/17/Pandas%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>&#8195;&#8195;以往项目中也有引入Pandas，用于有关数据处理和分析的环节，结合Python的Web开发，很容易开发出一款轻量BI系统。Pandas、Numpy、Scipy、matplotlib、scikit-learn和Jupyter Notebook结合使用，完全可以组成非常出色的数据分析与挖掘的生产环境工具，数据方面的应用，比matlab强不少，以至于本人也不断强化这方面的积累。单独拿出这方面技能，即可完成数据分析师的相关工作（又称提数工程师）。本文主要归档一些高频使用的预处理方面的函数，注意本文不涉及Pandas数据挖掘和数理统计方面的知识点（会在另外blog给出）。</p>
<a id="more"></a>

<h3 id="1、读取数据文件"><a href="#1、读取数据文件" class="headerlink" title="1、读取数据文件"></a>1、读取数据文件</h3><p>&#8195;&#8195;读取数据的相关接口使用在pandas官网的document有非常详细的说明:在<a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html">IO tools部分</a>。pandas 不仅能读取基本常用的Excel、csv、文本，还可以读取hadoop文件，或者直接读取数据库等</p>
<h4 id="1-1-读取excel数据文件"><a href="#1-1-读取excel数据文件" class="headerlink" title="1.1  读取excel数据文件"></a>1.1  读取excel数据文件</h4><ul>
<li><p>加载Excel表，使用skiprows=1跳过首行<br>并指定加载的列，注意数据文件的编码，默认utf-8，常用还有gb2312，根据自身数据而定。</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">raw_pd = pd.read_excel(data_file,,skiprows=<span class="number">1</span>,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br><span class="line"><span class="number">181</span> ms ± <span class="number">1.32</span> ms per loop (mean ± std. dev. of <span class="number">7</span> runs, <span class="number">1</span> loop each)</span><br></pre></td></tr></table></figure>

<p>  这里可以为每个执行单元之前加入<code>%%timeit</code>，观察其耗时情况。</p>
</li>
<li><p>加载Excel表，使用header=0跳过有列标题的首行<br>除了使用skiprows=1可跳过首行，header=0也可以实现同样效果</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd = pd.read_excel(data_file,header=<span class="number">0</span>,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，首行为数据，不是列标题<br>若该表第一行不是列标题行而是数据行，则需要指定header=None，否则读取后，第一行数据会被作为column name</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，读取前n行数据<br>若数据文件大小为几个G，显然若直接全量读取，内存会挤爆，因此可以先读取前n看看。使用nrows=500，表示只读取前500行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>, nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，跳过所有空白行<br>若有些表出现了不定数量的空白行，可以使用skip_blank_lines=True处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,skip_blank_lines = <span class="literal">True</span>, nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载Excel表，通过自定规则，跳过满足规则的行<br>例如跳过有值为单数的行，定义更复杂的函数，用于跳过满足复杂规则的行。不过，除非这些行很多，否则可以在读取后，直接用正则drop掉来得更有效。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.read_csv(data_file, skiprows=<span class="keyword">lambda</span> x: x % <span class="number">2</span> != <span class="number">0</span>)</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h4 id="1-2-读取csv文件"><a href="#1-2-读取csv文件" class="headerlink" title="1.2 读取csv文件"></a>1.2 读取csv文件</h4><p>&#8195;&#8195;读取csv文件跟读取Excel文件区别不大，这里简单给出示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_csv(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>读取文件，需要注意的地方一般是选择编码，数据文件的编码决定读取数据后，是否正常显示。</p>
<h4 id="1-3-读取数据时，跳过尾行"><a href="#1-3-读取数据时，跳过尾行" class="headerlink" title="1.3 读取数据时，跳过尾行"></a>1.3 读取数据时，跳过尾行</h4><p>有些报表一般会在表（例如财务系统导出）的后几行写上制表人、制表日期<br>这里要注意，若使用c engine，则无法使用从尾部跳过数据的功能：</p>
<blockquote>
<p>skipfooter : int, default <code>0</code></p>
<p>Number of lines at bottom of file to skip (unsupported with engine=’c’).</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_csv(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>, skipfooter=<span class="number">5</span>,encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="1-4-读取特定分割符的数据文件"><a href="#1-4-读取特定分割符的数据文件" class="headerlink" title="1.4 读取特定分割符的数据文件"></a>1.4 读取特定分割符的数据文件</h4><p>read_csv也可以读取任意文本文件，只需要指定列分割符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd=pd.read_csv(<span class="string">&#x27;data_file.txt&#x27;</span>,sep=<span class="string">&#x27;||&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="1-5-使用c或者python作为读取文件的引擎"><a href="#1-5-使用c或者python作为读取文件的引擎" class="headerlink" title="1.5 使用c或者python作为读取文件的引擎"></a>1.5 使用c或者python作为读取文件的引擎</h4><p>pd.read_*** 方法默认使用python解释器作为读取文件engine，若数据文件大，可选择c engine</p>
<blockquote>
<p>engine : {<code>&#39;c&#39;</code>, <code>&#39;python&#39;</code>}</p>
<p>Parser engine to use. The C engine is faster while the Python engine is currently more feature-complete.</p>
</blockquote>
<h4 id="1-6-使用迭代器读取超大文件"><a href="#1-6-使用迭代器读取超大文件" class="headerlink" title="1.6 使用迭代器读取超大文件"></a>1.6 使用迭代器读取超大文件</h4><p>参考官网文档给出的示例，使用<code>iterator=True</code>， 或者chunksize=4读取超大文件，返回的是TextFileReader，是一个文件迭代器</p>
<p>chunksize方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">187</span>]: reader = pd.read_csv(<span class="string">&#x27;tmp.sv&#x27;</span>, sep=<span class="string">&#x27;|&#x27;</span>, chunksize=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">188</span>]: reader</span><br><span class="line">Out[<span class="number">188</span>]: &lt;pandas.io.parsers.TextFileReader at <span class="number">0x7f2b428c17f0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">189</span>]: <span class="keyword">for</span> chunk <span class="keyword">in</span> reader:</span><br><span class="line">   .....:     print(chunk)</span><br></pre></td></tr></table></figure>

<p>iterator=True方式：<br>使用iterator=True方式，值读取前面5行，放回的也是df对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">190</span>]: reader = pd.read_csv(<span class="string">&#x27;tmp.sv&#x27;</span>, sep=<span class="string">&#x27;|&#x27;</span>, iterator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">191</span>]: chunk_pd=reader.get_chunk(<span class="number">5</span>)</span><br><span class="line">chunk_pd.head()</span><br></pre></td></tr></table></figure>

<p>当然最佳的方式是两者结合使用：返回迭代器方式，并指定分块读取，例如分64k读取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iter_df=pd.read_csv(large_file,iterator=<span class="literal">True</span>，chunksize=<span class="number">64</span>*<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2、查看数据的基本信息"><a href="#2、查看数据的基本信息" class="headerlink" title="2、查看数据的基本信息"></a>2、查看数据的基本信息</h3><p>读入数据后，一般需要对数据进行基本的观察</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd.head(<span class="number">5</span>) <span class="comment"># 查看数据基本信息（前5行）</span></span><br><span class="line">raw_pd.tail(<span class="number">5</span>) <span class="comment"># 查看末尾5行</span></span><br><span class="line">raw_pd.sample(<span class="number">5</span>) <span class="comment"># 随机抽取5行查看</span></span><br><span class="line">raw_pd.dtypes <span class="comment"># 查看每列数据类型</span></span><br><span class="line">raw_pd.columns    <span class="comment">#查看列名</span></span><br><span class="line">raw_pd.info()     <span class="comment">#查看各字段的信息</span></span><br><span class="line">raw_pd.shape      <span class="comment">#查看数据集行列分布，几行几列</span></span><br><span class="line">raw_pd.describe() <span class="comment"># 快速查看数据的基本统计信息</span></span><br></pre></td></tr></table></figure>

<h3 id="3、有关空值处理"><a href="#3、有关空值处理" class="headerlink" title="3、有关空值处理"></a>3、有关空值处理</h3><p>空值：在pandas中的空值是””<br>缺失值：在dataframe中为NaN或者NaT（缺失时间），在series中为none或者nan</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试数据</span></span><br><span class="line">raw_pd = pd.DataFrame(&#123;<span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;aoo&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;college&quot;</span>: [np.nan, <span class="string">&#x27;SACT&#x27;</span>, <span class="string">&#x27;AACT&#x27;</span>],</span><br><span class="line">                  <span class="string">&quot;birth_date&quot;</span>: [pd.NaT, pd.Timestamp(<span class="string">&quot;2000-10-01&quot;</span>),pd.NaT]&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="3-1-行的空值处理"><a href="#3-1-行的空值处理" class="headerlink" title="3.1 行的空值处理"></a>3.1 行的空值处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">axis：0或者&#x27;index&#x27;代表行操作（默认）  1或者&#x27;column&#x27;：列操作</span></span><br><span class="line"><span class="string">how：any-只要有空值就删除（默认），all-全部为空值才删除</span></span><br><span class="line"><span class="string">inplace：False-返回新的数据集（默认），True-在愿数据集上操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用频率高：查看name列中，有多少行为空值行,value_counts其实是一个统计方法</span></span><br><span class="line">raw_pd[<span class="string">&#x27;name&#x27;</span>].isnull().value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用频率高：any表示行的任意一列有空值，则删除该行；all表示该行全部为空，则删除</span></span><br><span class="line">raw_pd.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 行的任意一列有空值,且出现2个空值才删除这些行。例如该行有3列，其中2列都是为空，那么可以删掉该行。</span></span><br><span class="line">使用频率低：raw_pd.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>,thresh=<span class="number">2</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-2-列的空值处理"><a href="#3-2-列的空值处理" class="headerlink" title="3.2 列的空值处理"></a>3.2 列的空值处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用频率高：指定某几列，若这些列中出现了空值，则直接删除所在行</span></span><br><span class="line">raw_pd.dropna(subset=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;birth_date&#x27;</span>],inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<h4 id="3-3-空值的填充"><a href="#3-3-空值的填充" class="headerlink" title="3.3 空值的填充"></a>3.3 空值的填充</h4><p>最简单的用法，对全部数据记录里面的空值填充指定值</p>
<p>df.fillna(value=’bar’)</p>
<p>频繁使用：对指定列的空值进行填充</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd[<span class="string">&#x27;name&#x27;</span>]=raw_pd[<span class="string">&#x27;name&#x27;</span>].fillna(value=<span class="string">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>高级填充方式</strong><br>使用与空值单元相邻近的值来填充。该用法一般用在大量数据统计分析的场景或者图像的像素值填充、实验室的实验数据。相邻是指可以使用上下左右四个方向的值实现前向或者后向填充</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">DataFrame.fillna(value=<span class="literal">None</span>, method=<span class="literal">None</span>, axis=<span class="literal">None</span>, inplace=<span class="literal">False</span>, limit=<span class="literal">None</span>, downcast=<span class="literal">None</span>, **kwargs)</span><br><span class="line"></span><br><span class="line">method : &#123;‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, <span class="literal">None</span>&#125;, default <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">Method to use <span class="keyword">for</span> filling holes <span class="keyword">in</span> reindexed Series pad / ffill: </span><br><span class="line">propagate last valid observation forward to <span class="built_in">next</span> valid backfill / bfill: use NEXT valid observation to fill gap</span><br><span class="line"></span><br><span class="line">axis : &#123;<span class="number">0</span> <span class="keyword">or</span> ‘index’, <span class="number">1</span> <span class="keyword">or</span> ‘columns’&#125;</span><br><span class="line">limit:限制填充的个数</span><br></pre></td></tr></table></figure>
<p>这里使用比较频繁的是纵向填充，因为纵向代表的是列，从相邻样本的同一特征中填值，对每列的空值实施前项或者后项填充。</p>
<p><code>df.fillna(method=&#39;ffill&#39;)</code> or df.fillna(method=’bfill’)</p>
<h4 id="3-4-空值使用所在列或者所在行的均值、中位数来填补"><a href="#3-4-空值使用所在列或者所在行的均值、中位数来填补" class="headerlink" title="3.4  空值使用所在列或者所在行的均值、中位数来填补"></a>3.4  空值使用所在列或者所在行的均值、中位数来填补</h4><p>这里以均值填充为例，当然也可以用该列的预测值填充</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mean_value = df[<span class="string">&#x27;age&#x27;</span>].mean()</span><br><span class="line">df[<span class="string">&#x27;age&#x27;</span>].fillna(mean_value, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<h3 id="4、dataframe-取（定位）数据的操作"><a href="#4、dataframe-取（定位）数据的操作" class="headerlink" title="4、dataframe 取（定位）数据的操作"></a>4、dataframe 取（定位）数据的操作</h3><h4 id="4-1-按给定列名取数，类似字典操作：df-‘列名’"><a href="#4-1-按给定列名取数，类似字典操作：df-‘列名’" class="headerlink" title="4.1 按给定列名取数，类似字典操作：df[‘列名’]"></a>4.1 按给定列名取数，类似字典操作：df[‘列名’]</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd= raw_pd[<span class="string">&#x27;name&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>取出多列数据，入参为包含多个字段的list：[‘name’,’college’]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_pd[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;college&#x27;</span>]]</span><br></pre></td></tr></table></figure>

<h4 id="4-2-按行默认的行索引号选取数据：df-loc"><a href="#4-2-按行默认的行索引号选取数据：df-loc" class="headerlink" title="4.2  按行默认的行索引号选取数据：df.loc"></a>4.2  按行默认的行索引号选取数据：df.loc</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;aoo&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;college&quot;</span>: [np.nan, <span class="string">&#x27;SACT&#x27;</span>, <span class="string">&#x27;AACT&#x27;</span>],</span><br><span class="line">                  <span class="string">&quot;birth_date&quot;</span>: [pd.NaT, pd.Timestamp(<span class="string">&quot;2000-10-01&quot;</span>),pd.NaT]&#125;)</span><br><span class="line"><span class="comment"># 查看该df的行索引</span></span><br><span class="line">df.index</span><br><span class="line">RangeIndex(start=<span class="number">0</span>, stop=<span class="number">3</span>, step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印                  </span></span><br><span class="line">	birth_date 	college 	name</span><br><span class="line"><span class="number">0</span> 	NaT 	NaN 	aoo</span><br><span class="line"><span class="number">1</span> 	<span class="number">2000</span>-<span class="number">10</span>-01 	SACT 	boo</span><br><span class="line"><span class="number">2</span> 	NaT 	AACT 	coo </span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的0,1,2就是pandas默认给加载的数据提供的行索引号</span></span><br></pre></td></tr></table></figure>

<p>按索引取数据，跟列表使用slice切片获取数据的用法一致<br>df.loc[1] 获取行索引1的行数据，df.loc[1:2]获取1到2行数据</p>
<p>若行索引号不是int，例如将以上数据的默认index序列，改成字符索引序列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.index=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"> 	birth_date 	college 	name</span><br><span class="line">a 	NaT 	NaN 	aoo</span><br><span class="line">b 	<span class="number">2000</span>-<span class="number">10</span>-01 	SACT 	boo</span><br><span class="line">c 	NaT 	AACT 	coo</span><br></pre></td></tr></table></figure>
<p>获取索引为b的数据：df.loc[‘b’]<br>或者索引为b、c的数据：df.loc[[‘b’,’c’]]</p>
<h4 id="4-3-按给定列名以及行索引取出数据"><a href="#4-3-按给定列名以及行索引取出数据" class="headerlink" title="4.3 按给定列名以及行索引取出数据"></a>4.3 按给定列名以及行索引取出数据</h4><p>例如取出college列的b、c行数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.loc[[<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>],<span class="string">&#x27;college&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>例如取出college列、birth_date列的b、c行数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.loc[[<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>],[<span class="string">&#x27;college&#x27;</span>,<span class="string">&#x27;birth_date&#x27;</span>]]</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line">	college 	birth_date</span><br><span class="line">b 	SACT 	<span class="number">2000</span>-<span class="number">10</span>-01</span><br><span class="line">c 	AACT 	NaT</span><br></pre></td></tr></table></figure>


<h4 id="4-4-df-iloc利用index获取行数据或者列数据"><a href="#4-4-df-iloc利用index获取行数据或者列数据" class="headerlink" title="4.4  df.iloc利用index获取行数据或者列数据"></a>4.4  df.iloc利用index获取行数据或者列数据</h4><p>df.iloc只能使用整型切片获取数据:例如df.iloc[0:10]<br>而df.loc可以使用字符型索引等来取数</p>
<h3 id="5、通过复杂规则取数"><a href="#5、通过复杂规则取数" class="headerlink" title="5、通过复杂规则取数"></a>5、通过复杂规则取数</h3><p>在sql中经常会在where子句使用筛选条件：<br>select * from emp e  where e.age&gt;20 and e.dep_name=’dev’ and e.city&lt;&gt;’foo’<br>在pandas里面则使用方式如下：<br>单个筛选条件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">20</span>]</span><br><span class="line">或者</span><br><span class="line">df.loc[df[<span class="string">&#x27;age&#x27;</span>]&gt;<span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<p>多个筛选条件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[(df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">20</span>)&amp;(df[<span class="string">&#x27;dep_name&#x27;</span>]==<span class="string">&#x27;dev&#x27;</span>)&amp;~(df[<span class="string">&#x27;city&#x27;</span>]==<span class="string">&#x27;foo&#x27;</span>)]</span><br></pre></td></tr></table></figure>

<p>使用isin方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;city&#x27;</span>].isin([<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>])]</span><br></pre></td></tr></table></figure>

<p>根据时间范围取值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Excel加载的日期，如果格式不对，有可能是object类型，需将其转为datetime64[ns]类型，否则无法进行日期筛选比较</span></span><br><span class="line">df[<span class="string">&#x27;date_col&#x27;</span>]= df.to_datetime(df[<span class="string">&#x27;date_col&#x27;</span>])</span><br><span class="line">start_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment">#或者pd.Timestamp(&#x27;2017-02-01&#x27;)</span></span><br><span class="line">end_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">14</span>) <span class="comment">#或者pd.Timestamp(&#x27;2017-02-14&#x27;)</span></span><br><span class="line"><span class="comment"># 注意以上的实际范围其实是 2017-02-01 00:00:00 ~2017-02-14 00:00:00</span></span><br><span class="line"><span class="comment"># 或者截止到当天最后一秒</span></span><br><span class="line">end_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">14</span>,<span class="number">23</span>,<span class="number">59</span>,<span class="number">59</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找指定时间范围内的数据行</span></span><br><span class="line">filter_df=df[(df[<span class="string">&#x27;start_time&#x27;</span>]&gt;=start_time) &amp; (df[<span class="string">&#x27;end_time&#x27;</span>]&lt;=end_time)]</span><br></pre></td></tr></table></figure>
<p>还有另外这一种方式是选择一个时间列，变将其设为当前df的时间索引，</p>
<p>常用：根据某个字段，取出获取删除其值出现频率排在前n的数据行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  对name字段进行group_by</span></span><br><span class="line">groupby_df=df.groupby(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line"><span class="comment"># groupby_df:&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000010190C18&gt;</span></span><br><span class="line"></span><br><span class="line">resl_df=groupby_df[<span class="string">&#x27;name&#x27;</span>].count()</span><br><span class="line"><span class="comment"># resl_df 就像透视表的数据形式</span></span><br><span class="line">name</span><br><span class="line">foo    <span class="number">10</span></span><br><span class="line">bar    <span class="number">5</span></span><br><span class="line">coo    <span class="number">4</span></span><br><span class="line">dee    <span class="number">2</span></span><br><span class="line">Name: bar, dtype: int64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找name字段里，字符出现频率排在前2位，例如上述的例子：foo，bar。按降序返回一个python的列表</span></span><br><span class="line">top_2_list=resl_df.sort_values(ascending=<span class="literal">False</span>).head(<span class="number">2</span>)</span><br><span class="line">print(top_2_list)</span><br><span class="line">[<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将出现频率排在前2的内容拼接成用于正则匹配的字符串</span></span><br><span class="line">pattern_str=<span class="string">&#x27;|&#x27;</span>.join(top_2_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pandas提供的正则方法，剔除name字段中出现频率排在前2的数据行</span></span><br><span class="line">filtered_df= df[~df[<span class="string">&#x27;name&#x27;</span>].<span class="built_in">str</span>.contains(pattern_str, case=<span class="literal">False</span>, na=<span class="literal">False</span>,regex=<span class="literal">True</span>)]</span><br></pre></td></tr></table></figure>

<p>==使用时间索引选取数据行==<br>个人认为，这种方式是时间选取数据场景最高效的手段<br>例如有数据df，其中create_date是该df唯一的日期字段，通常做法:<br>新增一列命名为back_up_date，用于备份create_date<br><code>df[&#39;back_up_date&#39;]=df[&#39;create_date&#39;]</code><br>将crete_date置为该df的时间索引<br><code>df=df.set_index(&#39;create_date&#39;)</code><br>当时间索引设置后，那么根据时间筛选数据将变得异常简单<br>取2000年的数据行<br><code>df[&#39;2000&#39;]</code><br>取2000年到2019年的数据行<br><code>df[&#39;2000&#39;:&#39;2019&#39;]</code><br>某天具体时间到某天具体时间的数据行<br><code>df[&#39;2015-03-15 11:11:10&#39;:&#39;2015-05-15 10:30:00&#39;]</code><br>有关pandas时间的专题，官方文档给出了非常详细的用法示例，这里不再赘述，<a href="https://pandas.pydata.org/pandas-docs/version/0.25/user_guide/timeseries.html">timeseries链接</a></p>
<h3 id="6、调整列位置、列的增、删"><a href="#6、调整列位置、列的增、删" class="headerlink" title="6、调整列位置、列的增、删"></a>6、调整列位置、列的增、删</h3><p>交换birth_date和college列的位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;birth_date&#x27;</span>,<span class="string">&#x27;college&#x27;</span>]]=df[[<span class="string">&#x27;college&#x27;</span>,<span class="string">&#x27;birth_date&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<p>删除指定列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop(columns=[<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>删除指定行，使用行索引</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop([<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>删除重复行:df.drop_duplicates</p>
<p>直接删除重复行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates()</span><br></pre></td></tr></table></figure>

<p>删除name列、age列存在重复的行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates([<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>],keep=<span class="string">&#x27;first&#x27;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>请注意：以上删除行的操作，会破坏df原有的0到n的连续索引，例如原行索引为：0，1，2，3…n，其中索引1，2为空行，删除空行后，df的索引变为0，3…n，显然不连续，因此需要重置索引：df.reset_index(drop=True)，重置后，索引变为0，1，2，3…n</p>
<h3 id="7、单元格的字符相关处理"><a href="#7、单元格的字符相关处理" class="headerlink" title="7、单元格的字符相关处理"></a>7、单元格的字符相关处理</h3><p>例如有字段app_id，有部分值字符串为数字：‘12331’，需转成int64<br>有部分值为字符加数字：‘TD12331’，去掉字符TD并转成int64<br>有些值为非id例如：‘ # llsd’，需对此类值用固定数值填充。因此需要对其统一处理成整型id<br>使用replace方法去掉值里面的TD字符串</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>].replace(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>,regex=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>使用apply方法通过定义简单的lambda去掉值里面的TD字符串</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].apply(<span class="keyword">lambda</span>:item:item.replace(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure>

<p>其实apply才是终极方法，适用各种自定义的数据行或者列的处理，例如对同一列的值有多种判断需要处理，则可以在外部定义要处理的函数，再用apply广播到该列的每个cell中。例如上面的例子：如果单元格数值含有TD则去掉TD字符保留其数值部分，如果单元格出现非数值，则将其设为NaN空值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_id</span>(<span class="params">cell</span>):</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">&#x27;\d&#123;3&#125;&#x27;</span>,cell):</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    <span class="keyword">elif</span> re.match(<span class="string">&#x27;TD&#x27;</span>,cell):</span><br><span class="line">        <span class="keyword">return</span> re.sub(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>,cell)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.nan</span><br><span class="line">        </span><br><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].apply(filter_id)</span><br></pre></td></tr></table></figure>

<p>apply方法另外一种常用的方式:对数值进行分级，例如10&lt;item&lt;30为D，30&lt;=item&lt;60为C，60&lt;=item&lt;90为B。此外，货币进行转化、时间转换也是常用的场景</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">10</span>&lt;=item&lt;<span class="number">30</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;D&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">30</span>&lt;=item&lt;<span class="number">60</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;C&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">60</span>&lt;=item&lt;<span class="number">90</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;B&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;A&#x27;</span></span><br><span class="line">    </span><br><span class="line">df[<span class="string">&#x27;level&#x27;</span>]=df[<span class="string">&#x27;level&#x27;</span>].apply(level)    </span><br></pre></td></tr></table></figure>

<p>使用astype转成整型id号，具体其他数据类型不再列出。astype要求整列数据是完整的同一数据类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].astype(<span class="string">&#x27;int64&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>使用频繁：使用pandas.Series.str.contains方法处理列的值</p>
<blockquote>
<p>Series.str.contains(pat, case=True, flags=0, na=nan, regex=True)<br>pat : str<br>    Character sequence or regular expression.<br>case : bool, default True<br>    If True, case sensitive.<br>flags : int, default 0 (no flags)<br>    Flags to pass through to the re module, e.g. re.IGNORECASE.<br>na : default NaN<br>    Fill value for missing values.<br>regex : bool, default True<br>    If True, assumes the pat is a regular expression.<br>    If False, treats the pat as a literal string.</p>
</blockquote>
<p>删除name列中含有foo字符串的行，默认使用正则匹配<br>df=df[~df[‘name’].str.contains(‘foo’, case=false, flags=re.IGNORECASE, na=False)]</p>
<p>或者使用正则匹配<br>df=df[~df[‘name’].str.contains(‘^foo’, case=false, flags=re.IGNORECASE, na=False)]</p>
<h3 id="8、有关遍历行的处理"><a href="#8、有关遍历行的处理" class="headerlink" title="8、有关遍历行的处理"></a>8、有关遍历行的处理</h3><p>单表处理，请勿使用循环！！ 效率很低！有apply方法足以，底层是矩阵操作。</p>
<p>遍历行，一般用在两个表之间，表A字段’date’与表B字段’date‘的比较<br>使用iterrows遍历行<br>iterate over DataFrame rows as (index, Series) pairs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这种方式可以把索引和行数据遍历出，其中row的数据结构为nametuple</span></span><br><span class="line"><span class="keyword">for</span> index,row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    print(index,row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这种方式其实就是itertuples(index=False)的遍历</span></span><br><span class="line"><span class="keyword">for</span> _,row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    print(row.A,row.B)</span><br></pre></td></tr></table></figure>

<p>使用itertuples遍历行<br>Iterate over DataFrame rows as namedtuples of the values.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.Series(pd.date_range(<span class="string">&#x27;2012-1-1&#x27;</span>, periods=<span class="number">10</span>, freq=<span class="string">&#x27;D&#x27;</span>))</span><br><span class="line">td = pd.Series([pd.Timedelta(days=i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: s, <span class="string">&#x27;B&#x27;</span>: td&#125;)</span><br><span class="line"><span class="comment">#这种方式，取出的每行为nametuple</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">    print(row.A,row.B)</span><br></pre></td></tr></table></figure>

<p>使用iteritems遍历列<br>这种方式以横向遍历列数据，每次返回该列名和该列Series</p>
<h3 id="9、DataFrame和DataFrame合并、关联查询等"><a href="#9、DataFrame和DataFrame合并、关联查询等" class="headerlink" title="9、DataFrame和DataFrame合并、关联查询等"></a>9、DataFrame和DataFrame合并、关联查询等</h3><h4 id="9-1-DataFrame和DataFrame合并"><a href="#9-1-DataFrame和DataFrame合并" class="headerlink" title="9.1 DataFrame和DataFrame合并"></a>9.1 DataFrame和DataFrame合并</h4><p>合并具有相同结构的df<br>将多个DataFrame按垂直方向或者水平方向合并：这种场合使用批量处理具有相同字段结构的多份报表或数据源</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 默认是按垂直方向合并三个子df</span></span><br><span class="line">frames = [df1, df2, df3]</span><br><span class="line">result = pd.concat(frames)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在合并后，还可以为每个子df设定相应key</span></span><br><span class="line">result = pd.concat(frames, keys=[<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;cee&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用上面key，可以一次性取回合并前的df1</span></span><br><span class="line">df1=result.loc[<span class="string">&#x27;foo&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>合并字段不同的df</p>
<h4 id="9-2-DataFrame和DataFrame之间的关联查询"><a href="#9-2-DataFrame和DataFrame之间的关联查询" class="headerlink" title="9.2 DataFrame和DataFrame之间的关联查询"></a>9.2 DataFrame和DataFrame之间的关联查询</h4><p>因为关联查询基本是数据分析里面重要的、使用频繁的需求，例如实现报表1和报表的用vlookup关联查询、sql中多个表的关联查询（内连接、左连接、右连接、全连接）。pandas的doc官方文档在这部分的内容已经非常详细，并且有相应的关联前后的图文说明，本文不再一一赘述，仅给出简单的关联用法。<br>以内连接为例：<br>实现类似sql使用两表的多个外键关联：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select t1.*,t2.* from t1,t2 where t1.a&#x3D;t2.a</span><br><span class="line">and t1.b&#x3D;t2.b</span><br><span class="line">and t1.c&#x3D;t2.c</span><br></pre></td></tr></table></figure>
<p>pandas的方式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.merge(df2, on=[ key1 ,  key2 ,  key ])</span><br></pre></td></tr></table></figure>
<p>使用单个字段(外键)关联两表</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1.merge(df2, on=<span class="string">&#x27;dept_id&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="10、groupby基本用法"><a href="#10、groupby基本用法" class="headerlink" title="10、groupby基本用法"></a>10、groupby基本用法</h3><p>groupby可以说面对不同的数据需求，有不同用法，对sql熟悉的人应该无需多说。这里仅给出一些简单用法。</p>
<p>按季度分组，提取每个分组前n个数据行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_n</span>(<span class="params">df,n=<span class="number">3</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> df[<span class="number">0</span>:n]</span><br><span class="line"><span class="comment"># 这里的n是top_n自定义的关键字参数n，不是apply的参数    </span></span><br><span class="line">df.groupby(<span class="string">&#x27;quarter&#x27;</span>).apply(top_n,n=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>


<p>按产品种类分组，提取每个分组里最大值和最小值之差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每个产品种类的数值跨度范围，也即最大值减去最小值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_min</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">return</span> item.<span class="built_in">max</span>() - item.<span class="built_in">min</span>()</span><br><span class="line">df.groupby(<span class="string">&#x27;prod&#x27;</span>).agg(max_min)</span><br></pre></td></tr></table></figure>

<p>按产品种类分组，一次性取出每组的最值、均值、数值跨度范围，这里需要注意agg的入参为方法的列表，内置方法使用其字符名，自定义方使用其函数名</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;prod&#x27;</span>).agg([<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,max_min])</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>数据分析与挖掘</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark DataFrame、Spark SQL、Spark Streaming入门教程</title>
    <url>/blog/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark DataFrame</a>、<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a>、<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a></p>
<h4 id="1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming"><a href="#1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming" class="headerlink" title="1、RDD、Spark DataFrame、Spark SQL、Spark Streaming"></a>1、RDD、Spark DataFrame、Spark SQL、Spark Streaming</h4><p>&#8195;&#8195;RDD：大家最熟悉的数据结构，主要使用transmissions和actions 相关函数式算子完成数据处理和数理统计，例如map、reduceByKey，rdd没有定义 Schema（一般指未定义字段名及其数据类型）， 所以一般用列表索引号来指定每一个字段。 例如， 在电影数据的推荐例子中:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">move_rdd.map(lambda line:line.split(&#39;|&#39;)).map(lambda a_list:(alist[0],a_list[1],a_list[2]))</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p>每行有15个字段的数据，因此只能通过索引号获取前3个字段的数据，这要求开发者必须掌握 Map/Reduce 编程模式，不过， RDD 功能也最强， 能完成所有 Spark 数据处理与分析需求。</p>
<p>&#8195;&#8195;Spark DataFrame：创建DataFrame时，可以定义 Schema，通过定义每一个字段名与数据类型，以便之后直接用字段名进行数据索引，用法跟Pandas的DataFrame差别不大。Spark DataFrame是一种更高层的API，而且基于PySpark，用起来像Pandas的”手感“，很容易上手。</p>
<p>&#8195;&#8195;Spark SQL 底层是封装了DataFrame（DataFrame封装了底层的RDD） ，让使用者直接用sql的方式操作rdd，进一步降低Spark作为分布式计算框架的使用门槛。<br>&#8195;&#8195;Spark Streaming是本博客重点要解析的数据结构，实际项目将使用它实现流式计算，相关定义参考原文：</p>
<blockquote>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.<br>Spark Streaming具有扩展性、数据吞吐量高、容错的特点，底层基于core Spark API 实现，用于流数据处理。Spark Streaming注入的实时数据源可来自Kafka、Flume、Kinesis或者TCP流等，park Streaming可借助Map、reduce、join和window等高级函数接口搭建复杂的算法用于数据处理。Spark Streaming实时处理后数据可存储到文件系统、数据库或者实时数据展示仪表。</p>
</blockquote>
<h4 id="2、Spark-DataFrame"><a href="#2、Spark-DataFrame" class="headerlink" title="2、Spark DataFrame"></a>2、Spark DataFrame</h4><p>&#8195;&#8195;Spark DataFrame API比较多，既然用于数据处理和计算，当然会有预处理接口以及各统计函数、各种方法，详细参考<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">官网:pyspark.sql.DataFrame</a>以及<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions">pyspark.sql.functions module模块</a></p>
<p>&#8195;&#8195;目前版本中，创建Spark DataFrame的Context接口可以直接用SparkSession接口，无需像RDD创建上下文时用Spark Context。<br>SparkSession：pyspark.sql.SparkSession:Main entry point for DataFrame and SQL functionality.</p>
<h5 id="2-1-创建基本的Spark-DataFrame"><a href="#2-1-创建基本的Spark-DataFrame" class="headerlink" title="2.1 创建基本的Spark DataFrame"></a>2.1 创建基本的Spark DataFrame</h5><p>&#8195;&#8195;创建 Spark DataFrame有多种方式，先回顾Pandas的DataFrame，Pandas可从各类文件、流以及集合中创建df对象，同样 Spark DataFrame也有类似的逻辑<br>首先需加载spark的上下文：SparkSession</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession <span class="comment">#  用于Spark DataFrame的上下文</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,StructType,StructField, LongType,DateType <span class="comment"># 用于定义df字段类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row,Column</span><br><span class="line"></span><br><span class="line"><span class="comment">#本地spark单机模式</span></span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br><span class="line">print(spark)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出spark上下文信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparkSession - in-memory</span><br><span class="line">SparkContext</span><br><span class="line">Spark UI</span><br><span class="line">Version v2.4.4</span><br><span class="line">Master</span><br><span class="line">    local[*]</span><br><span class="line">AppName</span><br><span class="line">    spark_dataframe</span><br></pre></td></tr></table></figure>


<p>from pyspark.sql.types：df目前支持定义的字段类型（参考源码），用于定义schema，类似关系型数据库建表时，定义表的字段类型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">__all__ = [</span><br><span class="line">    <span class="string">&quot;DataType&quot;</span>, <span class="string">&quot;NullType&quot;</span>, <span class="string">&quot;StringType&quot;</span>, <span class="string">&quot;BinaryType&quot;</span>, <span class="string">&quot;BooleanType&quot;</span>, <span class="string">&quot;DateType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;TimestampType&quot;</span>, <span class="string">&quot;DecimalType&quot;</span>, <span class="string">&quot;DoubleType&quot;</span>, <span class="string">&quot;FloatType&quot;</span>, <span class="string">&quot;ByteType&quot;</span>, <span class="string">&quot;IntegerType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;LongType&quot;</span>, <span class="string">&quot;ShortType&quot;</span>, <span class="string">&quot;ArrayType&quot;</span>, <span class="string">&quot;MapType&quot;</span>, <span class="string">&quot;StructField&quot;</span>, <span class="string">&quot;StructType&quot;</span>]</span><br></pre></td></tr></table></figure>


<p>直接用从RDD创建dataframe对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_rdd = spark.sparkContext.parallelize([</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义schema，就像数据库建表的定义：数据模型，定义列名，类型和是否为能为空</span></span><br><span class="line">schema = StructType([StructField(<span class="string">&quot;id&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;item&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;price&quot;</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;pub_date&quot;</span>, DateType(), <span class="literal">True</span>)])</span><br><span class="line"><span class="comment"># 创建Spark DataFrame</span></span><br><span class="line">spark_df= spark.createDataFrame(spark_rdd, schema)</span><br><span class="line"><span class="comment"># 创建一个零时表，用于映射到rdd上</span></span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;iPhone&quot;</span>)</span><br><span class="line"><span class="comment"># 使用Sql语句,语法完全跟sql一致</span></span><br><span class="line">data = spark.sql(<span class="string">&quot;select a.item,a.price from iPhone a&quot;</span>)</span><br><span class="line"><span class="comment"># 查看dataframe的数据</span></span><br><span class="line">print(data.collect())</span><br><span class="line"><span class="comment"># 以表格形式展示数据</span></span><br><span class="line">data.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Row(item&#x3D;&#39;iPhoneX&#39;, price&#x3D;6000), Row(item&#x3D;&#39;iPhone7&#39;, price&#x3D;4000), Row(item&#x3D;&#39;iPhone4&#39;, price&#x3D;1000)]</span><br><span class="line">+-------+-----+</span><br><span class="line">|   item|price|</span><br><span class="line">+-------+-----+</span><br><span class="line">|iPhoneX| 6000|</span><br><span class="line">|iPhone7| 4000|</span><br><span class="line">|iPhone4| 1000|</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure>

<p>通过该例子，可了解df基本用法，只要从spark上下文加载完数据并转为dataframe类型后，之后调用df的api跟pandas的api大同小异，而且可将dataframe转为Spark SQL，直接使用sql语句操作数据。</p>
<p>上面的例子用了显示定义schema字段类型，pyspark支持自动推理创建df，也即无需原数据定义为rdd，和自动类型，直接传入Python列表的数据，以及定义字段名称即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_list = [</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line"><span class="comment"># 自动推理创建df,代码内部通过各类if 判断类型实现。</span></span><br><span class="line">spark_df= spark.createDataFrame(a_list, schema=[<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;item&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;pub_date&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h5 id="2-2-从各类数据源创建Spark-DataFrame"><a href="#2-2-从各类数据源创建Spark-DataFrame" class="headerlink" title="2.2  从各类数据源创建Spark DataFrame"></a>2.2  从各类数据源创建Spark DataFrame</h5><p>相关接口方法参考官网<a href="http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html">文档</a></p>
<p>==从csv文件创建Spark DataFrame==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file = <span class="string">&#x27;/opt/spark/data/train.csv&#x27;</span></span><br><span class="line">df = spark.read.csv(file,header=<span class="literal">True</span>,inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>==从pandas的DataFrame创建Spark DataFrame==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pandas_df = pd.DataFrame(np.random.random((<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line">spark_df = spark.createDataFrame(pandas_df, schema=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>==从json创建Spark DataFrame,json文件可以通过远程拉取，或者本地json，并设定json的字段schema==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">json_df = spark.read.json(<span class="string">&#x27;/opt/data/all-world-cup-players.json&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>==从各类数据库加载数据：==</p>
<p>pg数据库，使用option属性传入参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df = spark.read \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>) \</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure>

<p>pg数据库，用关键字参数传入连接参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:postgresql://localhost:5432/&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;db_name.table_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure>

<p>mysql数据库，用关键字参数传入连接参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:mysql://localhost:3306/db_name&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;table_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure>

<p>从hive里面读取数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果在SparkSession设置为连接hive，可以直接读取hive数据</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .enableHiveSupport() \      </span><br><span class="line">        .master(<span class="string">&quot;localhost:7077&quot;</span>) \</span><br><span class="line">        .appName(<span class="string">&quot;spark_hive&quot;</span>) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">spark_df=spark.sql(<span class="string">&quot;select * from hive_app_table&quot;</span>)</span><br><span class="line">spark_df.show()</span><br></pre></td></tr></table></figure>
<p>连接数据库需要相关的jar包，例如连接mysql，则需要将mysql-connector放在spark目录的jar目录下。</p>
<h5 id="2-3-Spark-DataFrame持久化数据"><a href="#2-3-Spark-DataFrame持久化数据" class="headerlink" title="2.3 Spark DataFrame持久化数据"></a>2.3 Spark DataFrame持久化数据</h5><p>以csv存储</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.write.csv(path=local_file_path, header=<span class="literal">True</span>, sep=<span class="string">&quot;,&quot;</span>, mode=<span class="string">&#x27;overwrite&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>注意：mode=‘overwrite’ 模式时，表示创建新表，若表名已存在则会被删除，整个表被重写。而 mode=‘append’ 模式就是普通的最加数据。</p>
<p>写入mysql：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url = <span class="string">&#x27;jdbc:mysql://localhost:3306/db_name?characterEncoding=utf-8&amp;autoReconnect=true&amp;useSSL=false&amp;serverTimezone=GMT&#x27;</span></span><br><span class="line">table = <span class="string">&#x27;table_name&#x27;</span></span><br><span class="line">properties = &#123;<span class="string">&quot;user&quot;</span>:<span class="string">&quot;test&quot;</span>,<span class="string">&quot;password&quot;</span>:<span class="string">&quot;test&quot;</span>&#125;</span><br><span class="line">spark_df.write.jdbc(url,table,mode=<span class="string">&#x27;append&#x27;</span>,properties=properties)</span><br></pre></td></tr></table></figure>



<p>写入hive</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开动态分区</span></span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition.mode = nonstrict&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition=true&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定分区写入表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).partitionBy(<span class="string">&quot;name&quot;</span>).insertInto(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用分区，直接将数据保存到Hive新表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from your_db.table_name&quot;</span>).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>默认的方式将会在hive分区表中保存大量的小文件，在保存之前对 DataFrame重新分区，从而控制保存的文件数量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.repartition(<span class="number">5</span>).write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>写入redis：<br>这里需要自行实现redis的写入方法，其实也简单，定义入参为dataframe，函数内部连接redis后，从dataframe取出数据再将其插入redis即可。对于写入其他文件或者数据库，需自行实现相应的数据转存逻辑。</p>
<h5 id="2-4-Dataframe常见的API"><a href="#2-4-Dataframe常见的API" class="headerlink" title="2.4 Dataframe常见的API"></a>2.4 Dataframe常见的API</h5><p><a href="https://github.com/Apress/machine-learning-with-pyspark/blob/master/chapter_2_Data_Processing/sample_data.csv">样例数据参考</a></p>
<p>查看字段</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.columns </span><br><span class="line"><span class="comment"># [&#x27;ratings&#x27;, &#x27;age&#x27;, &#x27;experience&#x27;, &#x27;family&#x27;, &#x27;mobile&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>spark_df.count() 统计行数<br>查看df的shape</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print((df.count(),<span class="built_in">len</span>(df.columns))) <span class="comment"># (33, 5)</span></span><br></pre></td></tr></table></figure>

<p>查看dataframe的schema字段定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.printSchema()</span><br><span class="line">输出：</span><br><span class="line">root</span><br><span class="line"> |-- ratings: integer (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- experience: double (nullable = true)</span><br><span class="line"> |-- family: integer (nullable = true)</span><br><span class="line"> |-- mobile: string (nullable = true)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>随机抽样探索数据集合：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark_df.sample(False,0.5,0).show(5)</span><br><span class="line">用法：</span><br><span class="line">Signature: spark_df.sample(withReplacement&#x3D;None, fraction&#x3D;None, seed&#x3D;None)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a sampled subset of this :class:&#96;DataFrame&#96;.</span><br><span class="line"></span><br><span class="line">:param withReplacement: Sample with replacement or not (default False).</span><br><span class="line">:param fraction: Fraction of rows to generate, range [0.0, 1.0].</span><br><span class="line">:param seed: Seed for sampling (default a random seed).</span><br></pre></td></tr></table></figure>

<p>查看行记录：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.show(<span class="number">3</span>) </span><br><span class="line">spark_df.head(<span class="number">3</span>) </span><br><span class="line">spark_df.take(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>以python列表返回记录，list中每个元素是Row类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[Row(ratings=<span class="number">3</span>, age=<span class="number">32</span>, experience=<span class="number">9.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Vivo&#x27;</span>, age_2=<span class="number">32</span>),</span><br><span class="line"> Row(ratings=<span class="number">3</span>, age=<span class="number">27</span>, experience=<span class="number">13.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Apple&#x27;</span>, age_2=<span class="number">27</span>),</span><br><span class="line"> Row(ratings=<span class="number">4</span>, age=<span class="number">22</span>, experience=<span class="number">2.5</span>, family=<span class="number">0</span>, mobile=<span class="string">&#x27;Samsung&#x27;</span>, age_2=<span class="number">22</span>)]</span><br></pre></td></tr></table></figure>

<p>查看null的行，可以传入isnull函数，也可以自定义lambda函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">spark_df = spark_df.<span class="built_in">filter</span>(isnull(<span class="string">&quot;name&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>选择列数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>).show(<span class="number">2</span>)<span class="comment"># select方法</span></span><br></pre></td></tr></table></figure>

<p>扩展dataframe的列数:withColumn可以在原df上新增列数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure>

<p>注意该方式不会更新到原df，如需替换原df，则更新spark_df即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure>

<p>新增一列数据，并将新增的数据转为double类型，需要用到cast方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,DoubleType,IntegerType</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;age_double&#x27;</span>,spark_df[<span class="string">&#x27;age&#x27;</span>].cast(DoubleType()))</span><br></pre></td></tr></table></figure>

<p>根据条件查询df，使用filter方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Apple&#x27;</span>)</span><br><span class="line"><span class="comment">#筛选记录后，再选出指定的字段记录</span></span><br><span class="line">spark_df.<span class="built_in">filter</span>(df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;ratings&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>选择mobile列值为‘Apple’的记录，多条件查询:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>)</span><br><span class="line">或者：</span><br><span class="line">spark_df.<span class="built_in">filter</span>((spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>)&amp;(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>distinct：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先用select取出要处理的字段，获取不同类型的mobile</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计不同类型mobile的数量</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct().count()</span><br></pre></td></tr></table></figure>

<p>groupBy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().show(<span class="number">5</span>,<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">+-------+-----+</span><br><span class="line">|mobile |count|</span><br><span class="line">+-------+-----+</span><br><span class="line">|MI     |<span class="number">8</span>    |</span><br><span class="line">|Oppo   |<span class="number">7</span>    |</span><br><span class="line">|Samsung|<span class="number">6</span>    |</span><br><span class="line">|Vivo   |<span class="number">5</span>    |</span><br><span class="line">|Apple  |<span class="number">7</span>    |</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure>

<p>groupBy之后，按统计字段进行降序排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().orderBy(<span class="string">&#x27;count&#x27;</span>,ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>groupBy之后，按mobile分组，求出每个字段在该分组的均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).mean().show(<span class="number">2</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|mobile|avg(ratings)     |avg(age)          |avg(experience)   |avg(family)       |avg(age_2)        |</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|MI    |3.5              |30.125            |10.1875           |1.375             |30.125            |</span><br><span class="line">|Oppo  |2.857142857142857|28.428571428571427|10.357142857142858|1.4285714285714286|28.428571428571427|</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">only showing top 2 rows</span><br></pre></td></tr></table></figure>

<p>同理还有<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code>、<code>df.groupBy(&#39;mobile&#39;).max()</code>、<code>df.groupBy(&#39;mobile&#39;).min()</code>等，或者用agg方法，然后传入相应的聚合函数</p>
<p><code>spark_df.groupBy(&#39;mobile&#39;).agg(&#123;&#39;experience&#39;:&#39;sum&#39;&#125;)</code>等同于<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code></p>
<p>用户定义函数UDF：</p>
<p>用户定义函数一般用于对列或者对行的数据进行定制化处理，就sql语句中，价格为数字的字段，根据不同判断条件，给字段加上美元符号或者指定字符等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costom_func</span>(<span class="params">brand</span>):</span></span><br><span class="line">    <span class="keyword">if</span> brand <span class="keyword">in</span> [<span class="string">&#x27;Samsung&#x27;</span>,<span class="string">&#x27;Apple&#x27;</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;High Price&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> brand ==<span class="string">&#x27;MI&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Mid Price&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Low Price&#x27;</span></span><br><span class="line">        </span><br><span class="line">brand_udf=udf(costom_func,StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;price_range&#x27;</span>,brand_udf(spark_df[<span class="string">&#x27;mobile&#x27;</span>])).show(<span class="number">5</span>,<span class="literal">False</span>) <span class="comment"># 使用spark_df[&#x27;mobile&#x27;]或者使用spark_df.mobile都可以</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|price_range|</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |Low Price  |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |High Price |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |High Price |</span><br><span class="line">|4      |37 |16.5      |4     |Apple  |37   |High Price |</span><br><span class="line">|5      |27 |9.0       |1     |MI     |27   |Mid Price  |</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用lambda定义udf</span></span><br><span class="line">age_udf = udf(<span class="keyword">lambda</span> age: <span class="string">&quot;young&quot;</span> <span class="keyword">if</span> age &lt;= <span class="number">30</span> <span class="keyword">else</span> <span class="string">&quot;senior&quot;</span>, StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&quot;age_group&quot;</span>, age_udf(df.age)).show(<span class="number">3</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|age_group|</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |senior   |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |young    |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |young    |</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>

<p>删除重复记录：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重复的行，将被删除</span></span><br><span class="line">spark_df=spark_df.dropDuplicates()</span><br></pre></td></tr></table></figure>

<p>删除一列数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_new=spark_df.drop(<span class="string">&#x27;mobile&#x27;</span>) <span class="comment"># 删除多列 spark_df.drop(&#x27;mobile&#x27;,&#x27;age&#x27;)</span></span><br></pre></td></tr></table></figure>

<h4 id="3、Spark-SQL"><a href="#3、Spark-SQL" class="headerlink" title="3、Spark SQL"></a>3、Spark SQL</h4><p>&#8195;&#8195;在第二部分内容给出了创建spark sql的方法，本章节给出更为详细的内容：这里重点介绍spark sql创建其上下文，完成相应的context设置后，剩下的就是熟悉的写SQL了。<br><strong>第一种方式：将本地文件加载为dataframe</strong><br>之后再使用createOrReplaceTempView方法转为<code>SQL模式</code>，流程如下</p>
<p>用第2节内容的数据做演示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.csv(<span class="string">&#x27;sample_data.csv&#x27;</span>,inferSchema=<span class="literal">True</span>,header=<span class="literal">True</span>)</span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;phone_sales&quot;</span>)</span><br><span class="line">df1 = spark.sql(<span class="string">&quot;select age,family,mobile from phone_sales &quot;</span>)</span><br><span class="line">df1.show(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+---+------+-------+</span><br><span class="line">|age|family| mobile|</span><br><span class="line">+---+------+-------+</span><br><span class="line">| 32|     3|   Vivo|</span><br><span class="line">| 27|     3|  Apple|</span><br><span class="line">| 22|     0|Samsung|</span><br><span class="line">+---+------+-------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>

<p>spark.sql用于传入sql语句，返回dataframe对象，故后续的数据处理将变得非常灵活，使用SQL确实能够降低数据处理门槛，再例如：</p>
<p><code>spark_df.groupBy(&#39;mobile&#39;).count().show(5,False)</code> 等同于</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">your_sql=(<span class="string">&quot;select mobile,count(mobile) as count from phone_sales group by mobile &quot;</span></span><br><span class="line">df1 = spark.sql(your_sql)</span><br></pre></td></tr></table></figure>



<p>如果df1集合较大，适合用迭代器方式输出记录（适合逐条处理的逻辑）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> each_record <span class="keyword">in</span> df1.collect(): </span><br><span class="line">	data_process(each_record)</span><br></pre></td></tr></table></figure>


<p><strong>第二种方式：直接连接诸如mysql或者hive的context，基于该context直接运行sql</strong></p>
<p>以mysql为例：<br>（1）配置mysql连接需要相关jar包和路径配置：<br> mysql-connector-java-5.1.48.jar 放入spark目录<code>/opt/spark-2.4.4-bin-hadoop2.7/jars/</code>目录下， mysql-connector包可在mysql自行下载。<br>在spark-env.sh 配置了EXTRA_SPARK_CLASSPATH=/opt/spark-2.4.4-bin-hadoop2.7/jars/（也可不配，spark按默认目录检索）</p>
<p>（2）连接mysql</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession </span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br></pre></td></tr></table></figure>
<p>连接数据库以及读取表</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apps_name_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).</span><br><span class="line">options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;erp_app.apps_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure>
<p>read方法详细使用可参考：<a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.format">spark.read.format</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pirnt(apps_name_df)</span><br><span class="line"><span class="comment"># DataFrame[id: int, app_log_name: string, log_path: string, log_date: timestamp]</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apps_name_df.show(5)</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">| id| app_log_name|           log_path|           log_date|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">|  1|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  3|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  5|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  7|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  9|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure>

<p>上述连接mysql的erp_app后，直接读取apps_name全部字段的数据，如果想在连接时，指定sql，则需按以下方式进行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/erp_app&#x27;</span>,</span><br><span class="line">                                           dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span>,</span><br><span class="line">                                           user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">                                           password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">                                          ).load()</span><br></pre></td></tr></table></figure>

<p>dbtable这个值可以为一条sql语句，而且格式必须为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span></span><br></pre></td></tr></table></figure>

<p>如果写成以下格式，则提示解析出错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;select id,app_log_name,log_path from apps_name&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="4、Spark-Streaming"><a href="#4、Spark-Streaming" class="headerlink" title="4、Spark Streaming"></a>4、Spark Streaming</h4><p>&#8195;&#8195;以上在介绍dataframe和spark sql的相关用法，都用离线数据进行测试，本章节将给出spark的核心组件之一：spark streaming：实时流式计算（微批处理），该功能将应用于本bg其他项目。有关流式计算的相关概念，可以查看相关参考资料，这里不再累赘。此外，本bg也将给出一篇关于spark streaming的深度解析文章。</p>
<h5 id="4-1-实时计算TCP端口的数据"><a href="#4-1-实时计算TCP端口的数据" class="headerlink" title="4.1 实时计算TCP端口的数据"></a>4.1 实时计算TCP端口的数据</h5><p>&#8195;&#8195;以一个简单demo介绍pyspark实现streaming的流程：<br>在数据源输入端，使用netcat打开一个7070端口，手动持续向netcat shell输入句子；<br>在实时计算端：streaming连接7070端口，并实时计算word count，并将统计结果实时打印。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建本地的streaming context，并指定4个worker线程</span></span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少spark自生成的日志打印</span></span><br><span class="line"><span class="comment"># 每批处理间隔为1秒</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 连接netcat的tcp端口，用于读取netcat持续输入的行字符串</span></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;192.100.0.10&quot;</span>, <span class="number">7070</span>)</span><br></pre></td></tr></table></figure>
<p>socketTextStream创建的对象称为：Discretized Streams（离散流） ，简称 DStream，是spark的核心概念</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计word的逻辑，这段代码再熟悉不过了。</span></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> value_1, value_2: value_1 + value_2)</span><br><span class="line">wordCounts.pprint() <span class="comment"># 这里wordCounts是&#x27;TransformedDStream&#x27; object，不再是普通的离线rdd</span></span><br></pre></td></tr></table></figure>

<p>启动流计算，并一直等到外部中断程序（相当于线程里面的jion）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)    <span class="comment"># 默认无timeout，程序会一直阻塞在这里</span></span><br></pre></td></tr></table></figure>

<p>启动后，如果你使用jupyter开发，可以看到notebook的cell每隔1秒打印一次</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:50</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:51</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>在netstat shell输入字符串</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# nc -l 7070</span><br><span class="line">this is spark streaming</span><br><span class="line">streaming wordcount is awesome</span><br></pre></td></tr></table></figure>



<p>再查看notebook的cell的的实时打印出了wordcount统计结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;spark&#39;, 1)</span><br><span class="line">(&#39;this&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:58</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;wordcount&#39;, 1)</span><br><span class="line">(&#39;awesome&#39;, 1)</span><br></pre></td></tr></table></figure>

<p>以上实现了一个完整的实时流计算，虽然该streaming的demo较为简单，但却给了大家非常直观的流计算处理设计思路，只需改造相关逻辑，即可满足符合自己业务的需求，在这里给出一个可行的设计：</p>
<p>（1）实时数据源替换为Kafka等组件：启动一个进程，用于运行streaming。streaming的实时数据源来自kafka的topic<br>（2）定制MapReduce的计算逻辑，用于实时预处理流数据<br>（3）将（2）的实时结果保存到redis的list上<br>（4）启动另外一个进程，从结果队列里面取出并存到Hbase集群或者hdfs<br>或者无需使用队列，Spark Streaming实时预处理后直接写入Hbase。</p>
<h5 id="4-2-实时计算本地文件"><a href="#4-2-实时计算本地文件" class="headerlink" title="4.2 实时计算本地文件"></a>4.2 实时计算本地文件</h5><p>&#8195;&#8195;对于python接口，<code>streamingContext.textFileStream(dataDirectory)</code>方法可以实时监听并读取本地目录下的日志文件，但有几点需要指出，参考官方文档指引：</p>
<ul>
<li><p>能实时监听<code>dataDirectory</code>目录下创建的任意类型文件</p>
</li>
<li><p><code>dataDirectory</code>主要分为两种文件系统，第一种为本地文件系统，例如监听<code>/opt/appdata/</code>目录下的所有文件，格式为<code>file:///opt/appdata/</code>，第二种为hdfs文件系统：格式为<code>hdfs://namenode:8040/logs/</code></p>
</li>
<li><p>支持文件正则匹配，例如要监听本地文件目录下，所有以<code>.log</code>作为后缀类型的文件，<code>file:///opt/appdata/*.log</code></p>
</li>
<li><p>要求监听目录下的所有文件，里面的数据格式是一致的，例如1.log和2.log,里面都相同固定格式的日志记录。（每次新增的文件如果数据格式不一样，显然streaming处理逻辑无法完成）</p>
</li>
<li><p>目录下的文件数量越多，streaming扫描耗时将越长</p>
</li>
<li><p>若移动文件到这个监控目录，则无法触发streaming读取该新增文件，必须用流的形式写入到这个目录的文件才能被监听到</p>
</li>
<li><p>最后也是也是最重要的：streaming只处理在时间窗口内创建的新的数据文件，这里如何理解<code>新的数据文件</code>？</p>
<p>例如streaming流计算设为5秒，这个5秒就是时间窗口，若在5秒内目录产生了一个1.log，这个1.log会被读取，当5秒时间窗口已经过了，那么即使1.log有数据更新，streaming也不再reload该文件，为什么会这么设计呢？</p>
<p>流式处理的逻辑：一批一批的实时读取，已经读取过的数据文件，在下一轮时间窗口不再读取。假设在下一轮时间窗口，还读取已处理过的文件（该文件追加了新的数据行），那么该设计逻辑不再是流式处理了。例如<code>/opt/appdata/</code>目录下，有1.log,…100.log，并还会持续新增数据文件，101.log….等，如果streaming在每轮时间窗口还要对已处理过文件：<code>1.log,...100.log</code>再重新读取（读取新增加的数据行），那么spark streaming就无法完成微批的、实时的流式处理逻辑。在下面的实例会加以说明:</p>
</li>
</ul>
<p>spark streaming 监听文件夹实例：</p>
<p>时间窗口为5秒，实时监听<code>/opt/words/</code>目录下的文件，只要有新的文件创建（这里新的文件是指：每次创建的新文件，文件名必须唯一，streaming才会触发读取）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>)<span class="comment"># 时间窗口为5秒</span></span><br><span class="line">lines = ssc.textFileStream(<span class="string">&quot;file:///opt/words/&quot;</span>)</span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">wordCounts.pprint()</span><br><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)  </span><br></pre></td></tr></table></figure>



<p>模拟实时生成数据文件，每5秒生成一份数据文件，并在生成文件前，删除之前的文件（因为这个旧文件已经被spark streaming读取并流式计算过，下一轮时间窗口不再读取，所以可以删除旧文件）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">import</span> random,os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_text</span>(<span class="params">dir_path,file_name</span>):</span></span><br><span class="line">    words=[<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;streaming&#x27;</span>,<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>,<span class="string">&#x27;kafka&#x27;</span>,<span class="string">&#x27;yarn&#x27;</span>,<span class="string">&#x27;zookeeper&#x27;</span>]</span><br><span class="line">    line_num=random.randint(<span class="number">1000</span>,<span class="number">2000</span>)</span><br><span class="line">    text=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(line_num):</span><br><span class="line">        line=<span class="string">&#x27; &#x27;</span>.join([random.choice(words) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">        text=text+line+<span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    data_file_path=os.path.join(dir_path,file_name)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file_path,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(text)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">streaming_gen_data</span>(<span class="params">stream_dir_path</span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        tmp_file=os.listdir(stream_dir_path)</span><br><span class="line">        <span class="keyword">if</span>  tmp_file:<span class="comment"># 如果监听的目录下有旧文件，则直接删除</span></span><br><span class="line">            file_path=os.path.join(stream_dir_path,tmp_file[<span class="number">0</span>])</span><br><span class="line">            os.remove(file_path)</span><br><span class="line">        file_name=<span class="built_in">str</span>(uuid.uuid1())+<span class="string">&#x27;.log&#x27;</span> <span class="comment"># 保证每次新增的文件名唯一</span></span><br><span class="line">        save_text(stream_dir_path,file_name)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    streaming_gen_data(<span class="string">&#x27;/opt/words&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 933)</span><br><span class="line">(&#39;foo&#39;, 970)</span><br><span class="line">(&#39;yarn&#39;, 951)</span><br><span class="line">(&#39;zookeeper&#39;, 938)</span><br><span class="line">(&#39;bar&#39;, 1020)</span><br><span class="line">(&#39;spark&#39;, 990)</span><br><span class="line">(&#39;streaming&#39;, 1021)</span><br><span class="line">(&#39;kafka&#39;, 949)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:35</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 651)</span><br><span class="line">(&#39;zookeeper&#39;, 623)</span><br><span class="line">(&#39;bar&#39;, 593)</span><br><span class="line">(&#39;yarn&#39;, 584)</span><br><span class="line">(&#39;foo&#39;, 659)</span><br><span class="line">(&#39;spark&#39;, 623)</span><br><span class="line">(&#39;streaming&#39;, 592)</span><br><span class="line">(&#39;kafka&#39;, 571)</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>每隔5秒，spark streaming 完成微批计算：实时统计新创建文件的单词数</p>
<p>在这里重点说明 ：<code>file_name=str(uuid.uuid1())+&#39;.log&#39; </code>,这句保证了每次生成的文件使用了唯一文件名称，这样spark streaming才会瞬间监听到变化，及时读取到该文件。</p>
<p>有人会问：在生成文件前，已经删除了旧文件，那么每次创建文件使用固定文件名，对于spark streaming来说应该是唯一的、未加载过的文件才对吧？</p>
<p>解释：当使用os.remove一个文件后，如果等待间隔时长不长（例如几秒钟）又再创建同名文件，linux底层文件系统使用了缓存，用<code>ls -al</code>  查看该文件，会发现新创建文件的创建时间没有及时改变，导致spark streaming认为该文件还是原的旧文件，也就不再读取。</p>
<p>具体说明如下：<br>第一次创建文件的时间为<code>16 16:10</code>，接着下轮生成新文件，删除旧文件，等待5秒后，再创建同名新文件时，会发现创建时间没有改变还是<code>16 16:10</code>，而ssc.textFileStream读取时用的是创建时间去判断是否为新文件，所以才导致<code>明明已经创建新文件，但是</code>ssc.textFileStream却不读取的情况，这是pyspark textFileStream的bug，这个接口不应该只用创建时间判断。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 33847  16 16:10 streaming_data.log # 首次创建文件</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory #目录下的文件被删除</span><br><span class="line">[root@nn words]# ls -al streaming_data.log</span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">-rw-r--r-- 1 root root 31166  16 16:10 streaming_data.log # 5秒后，第二次创建同名的文件，创建时间未改变（如果等待时间去到十来秒，此时创建同名的文件的创建时间会发生变化）</span><br></pre></td></tr></table></figure>

<p>鉴于<code>textFileStream</code>接口使用场景受限，所以spark streaming实时数据源最适合的场景：接收kafka或者flume推来的流数据，这保证spark streaming能够立刻监听流数据的到来时间是已经发生变化，能触发streaming计算。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark DataFrame</tag>
        <tag>Spark SQL</tag>
        <tag>Spark Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title>redis-cluster原理及其部署测试</title>
    <url>/blog/2019/09/14/redis-cluster%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E9%83%A8%E7%BD%B2%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h3 id="1、Part-I"><a href="#1、Part-I" class="headerlink" title="1、Part I"></a>1、Part I</h3><p>这里主要讨论redis集群数据分片的内容</p>
<h4 id="1-1-为何使用redis-cluster模式？"><a href="#1-1-为何使用redis-cluster模式？" class="headerlink" title="1.1 为何使用redis-cluster模式？"></a>1.1 为何使用redis-cluster模式？</h4><p>1）首先避免单点故障，本人项目中用了主从模式，但因并发量不高，而且在redis不可用条件下，可以直接去数据库拿数据，所以还未部署集群模式。</p>
<p>2）redis官方给出单服务最高可以达到每秒执行10万条命令的性能，其实这对于绝大部分项目都够用了，这里给出集群模式的讨论，一为了深入了解redis，二是如果有一种需求需要百万/s的操作，显然redis单服务无法满足需求</p>
<p>3）内存吃满，redis首先将数据写在内存中，同时不定时异步持久化存在硬盘，一般服务器32G、64G、128G内存，若redis接收的数据量高达1T，128G内存的高性能服务器也会崩溃，故需要做</p>
<p>数据分片（sharding），分别存储在多个redis服务器中，这就需要redis集群模式支持了，这不就是经典的分布式数据库思想吗！</p>
<a id="more"></a>

<h4 id="1-2-客户端分片"><a href="#1-2-客户端分片" class="headerlink" title="1.2 客户端分片"></a>1.2 客户端分片</h4><p>redis集群采用P2P模式，完全去中心化，将redis所有的key分成了16384个槽位，每个redis实例负责一部分slot，集群中的所有信息通过节点数据交换而更新。redis实例集群主要思想是将redis数据的key进行散列，通过hash函数特定的key会映射到指定的redis节点上</p>
<h4 id="1-3-数据分布基础"><a href="#1-3-数据分布基础" class="headerlink" title="1.3 数据分布基础"></a>1.3 数据分布基础</h4><p>分布式数据库首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集。常见的分区规则有哈希分区和顺序分区。<code>Redis Cluster</code>采用哈希分区规则，因此接下来会讨论哈希分区规则。</p>
<ul>
<li>节点取余分区</li>
<li>一致性哈希分区</li>
<li><strong>虚拟槽分区(redis-cluster采用的方式)</strong></li>
</ul>
<p>1)顺序分片</p>
<p>顺序分片按数据大小比较后进行分片</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">id1((数据1-100)) --&gt; id2((数据1-33))</span><br><span class="line">id1((数据1-100)) --&gt; id3((数据34-66))</span><br><span class="line">id1((数据1-100)) --&gt; id4((数据67-100))</span><br></pre></td></tr></table></figure>

<p>2）节点取余分区算法</p>
<p>以三个节点为例：1~100对3取余后有三种情况：余数为0，余数为1，余数2；（除数n，余数情况为n-1种）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">id1((数据1-100)) --&gt;|hashkey后取余%3|id2((3,6,..99))</span><br><span class="line">id1((数据1-100)) --&gt;|hashkey后取余%3| id3((1,4,..100))</span><br><span class="line">id1((数据1-100)) --&gt;|hashkey后取余%3| id4((2,5,..98))</span><br></pre></td></tr></table></figure>

<p>对于N个redis节点，那么数据分片为：hash(key)%N，节点取余分区算法显然是简单高效的</p>
<h4 id="1-4-虚拟槽分区"><a href="#1-4-虚拟槽分区" class="headerlink" title="1.4 虚拟槽分区"></a>1.4 虚拟槽分区</h4><p>虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有的数据映射(通过计算<code>CRC16(key)%16383</code>)到一个固定范围内的整数集合，redis将这种整数定义为槽（slot），Redis Cluster槽的范围是0～16383，总共有16384个哈希槽，槽是集群内数据管理和迁移的基本单位，每个节点负责一定数量的槽，例如有三个redis节点，那么槽位范围分配为：</p>
<p>redis node 1==&gt;slots:[0-5460]</p>
<p>redis node 2==&gt;slots:[5461-10922] </p>
<p>redis node 2==&gt;slots:[10923-16383]</p>
<p>文章后面在部署集群创建槽位分区会看到这个配置</p>
<p>当有某个key被映射到某个Master节点负责的槽，那么这个Master节点负责为这个key提供服务，只有Master节点才拥有槽的所有权，如果是某个Master的slave，这个slave节点只负责槽的使用，但是没有所有权。</p>
<h4 id="1-5-redis的槽位数量为什么是16384（2-14）个？"><a href="#1-5-redis的槽位数量为什么是16384（2-14）个？" class="headerlink" title="1.5 redis的槽位数量为什么是16384（2^14）个？"></a>1.5 redis的槽位数量为什么是16384（2^14）个？</h4><blockquote>
<p>参考redis的作者：在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用<code>char</code>进行bitmap压缩后是2k（<code>2 * 8 (8 bit) * 1024(1k) = 2K</code>），也就是说使用2k的空间创建了16k的槽数。</p>
<p>虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k（<code>8 * 8 (8 bit) * 1024(1k) = 8K</code>），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。</p>
</blockquote>
<h4 id="1-6-redis-cluster小结"><a href="#1-6-redis-cluster小结" class="headerlink" title="1.6 redis-cluster小结:"></a>1.6 redis-cluster小结:</h4><p>1）所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.</p>
<p>2）节点的fail是通过集群中超过半数的节点检测失效时才生效.通过投票机制</p>
<p>3）客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可</p>
<p>4）redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node&lt;-&gt;slot&lt;-&gt;value</p>
<p>5）redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis客户端先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点</p>
<h3 id="2、Part-2"><a href="#2、Part-2" class="headerlink" title="2、Part 2"></a>2、Part 2</h3><h4 id="2-1-redis单服务安装和配置"><a href="#2-1-redis单服务安装和配置" class="headerlink" title="2.1 redis单服务安装和配置"></a>2.1 redis单服务安装和配置</h4><p>这里先给出单个redis服务安装和部署过程，之后会基于多个实例集合redis自身的集群工具搭建redis集群</p>
<p>安装stable版本：目前未redis5.05版本，下载<a href="https://redis.io/download">地址</a></p>
<p>这里以安装在/usr/local目录下为例啊，安装和编译，当然需要服务器已经具备c环境yum install gcc-c++</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost local]# tar -xzf redis-5.0.5.tar.gz</span><br><span class="line">[root@localhost local]# cd redis-5.0.5  </span><br><span class="line">[root@localhost redis-5.0.5]# make    </span><br><span class="line">[root@localhostredis-5.0.5]# cd ./src   #进入到redis-5.0.5/src 文件目录下</span><br><span class="line">[root@localhost src]# make test #</span><br><span class="line">Hint: It&#x27;s a good idea to run &#x27;make test&#x27; ;)</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line"><span class="meta">#</span><span class="bash">redis 可执行脚本都在 utils目录下</span>    </span><br><span class="line">[root@localhost redis-5.0.5]# ls utils/</span><br><span class="line">build-static-symbols.tcl  hashtable          redis_init_script.tpl</span><br><span class="line">cluster_fail_time.tcl     hyperloglog        redis-sha1.rb</span><br><span class="line">corrupt_rdb.c             install_server.sh  releasetools</span><br><span class="line">create-cluster            lru                speed-regression.tcl</span><br><span class="line">generate-command-help.rb  redis-copy.rb      whatisdoing.sh</span><br><span class="line">graphs                    redis_init_script</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 为方便将集群中多个配置文件放在一块管理，可以新建一个redis-conf目录，放置多个redis实例的启动配置文件</span></span><br><span class="line">[root@localhost redis-conf]# ls</span><br><span class="line">redis.conf</span><br><span class="line">[root@localhost redis-conf]# vi redis.conf # 将redis改为后台运行</span><br><span class="line"><span class="meta">#</span><span class="bash"> Note that Redis will write a pid file <span class="keyword">in</span> /var/run/redis.pid when daemonized.</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash">容许远程访问（宿主机访问）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">bind</span> 127.0.0.1</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置密码</span></span><br><span class="line">requirepass foofoo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动一个实例</span></span><br><span class="line">[root@localhost redis-5.0.5]# redis-server redis-conf/redis.conf </span><br><span class="line">12797:C 12 Sep 2019 15:20:41.243 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">12797:C 12 Sep 2019 15:20:41.243 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=12797, just started</span><br><span class="line">12797:C 12 Sep 2019 15:20:41.243 # Configuration loaded</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 本地客户端登录测试</span></span><br><span class="line">[root@localhost redis-5.0.5]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt;</span><br><span class="line">127.0.0.1:6379&gt; set test foo</span><br><span class="line">(error) NOAUTH Authentication required # 提示需要密码认证</span><br><span class="line">127.0.0.1:6379&gt; AUTH foofoo #输入密码</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set test foo</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get test</span><br><span class="line">&quot;foo&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>设置redis开机启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-conf]# vi /etc/rc.d/rc.local </span><br><span class="line">redis-server /usr/local/redis-5.0.5/redis-conf</span><br></pre></td></tr></table></figure>

<p>以上完成redis单服务部署，显然部署过程相当简单，用redis-server 启动一个配置文件，则相应启动一个实例，==若需要在单服务器启动多个redis实例，则只需把整个redis5.0.5目录拷贝多份，修改配置文件redis.conf相应监听端口号等某几个设置项即可==，考虑到后面文章会实现高可用的redis分布式锁，这里会启动6个redis实例集群，以模拟真实集群服务。</p>
<p>为什么是6个实例？</p>
<p>因Redis的容错投票机制是集群中过半数的节点认为某个节点检测失效时才生效，因此最小集群模式至少需要三个节点，而为了高可用，还需要为这三个节点各增加一个slave节点，因此这里就需要6个。而在redis分布式锁算法中，需要在超过半数的redis实例中设置锁成功才能认为获得锁。</p>
<h4 id="2-2-启动多个redis实例的配置"><a href="#2-2-启动多个redis实例的配置" class="headerlink" title="2.2 启动多个redis实例的配置"></a>2.2 启动多个redis实例的配置</h4><p>创建放置集群redis目录，方便管理<code>/usr/local/redis-cluster</code></p>
<p>首先设置redis1，这里给出每个redis服务默认配置文件需要改的地方：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# vi redis1/redis-conf/redis.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 放通全网访问（当然是这内网访问）</span></span><br><span class="line">bind 0.0.0.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开远程访问</span></span><br><span class="line">protected-mode no</span><br><span class="line"><span class="meta">#</span><span class="bash">设置监听端口</span></span><br><span class="line">port 23451</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置redis进程pid路径</span></span><br><span class="line">pidfile /var/run/redis_23451.pid</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置集群密码</span></span><br><span class="line">masterauth foofoo</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置本服务密码</span></span><br><span class="line">requirepass foofoo</span><br><span class="line"><span class="meta">#</span><span class="bash">设置开启AOF模式</span>  </span><br><span class="line">appendonly yes  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定redis日志文件，方便查看启动日志或者出错日志，需自行创建,每个实例使用不同日志文件</span></span><br><span class="line">logfile &quot;/var/log/redis/redis1.log&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用集群模式</span></span><br><span class="line">cluster-enabled yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定集群节点配置文件路径，redis实例会创建该文件，每个实例使用不同nodes.conf</span></span><br><span class="line">cluster-config-file /usr/local/redis-cluster/nodes-conf/nodes-1.conf</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将redis1拷贝多份到 /usr/local/redis-cluster目录下，并命名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@localhost redis-cluster]# pwd</span><br><span class="line">/usr/local/redis-cluster</span><br><span class="line">[root@localhost redis-cluster]# mv redis-5.0.5/ redis1</span><br><span class="line">[root@localhost redis-cluster]# ls</span><br><span class="line">[root@localhost redis-cluster]# ls</span><br><span class="line">redis1  redis2  redis3  redis4  redis5  redis6</span><br></pre></td></tr></table></figure>

<p>在redis2-redis6目录，修改redis.conf文件，只需修改端口号、pid文件、logfile、cluster-config-file，其他设置项一致。端口也可以按业务规定划分，这里设为五位数端口23452~23456。</p>
<p>手工一个个去启动实例未免麻烦，可以在写个start_all_redis.sh，批量启动，作为测试环境，这里不再将其随机启动。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# ls</span><br><span class="line">appendonly.aof  nodes-6379.conf  redis1  redis2  redis3  redis4  redis5  redis6  start_all_redis.sh</span><br><span class="line"></span><br><span class="line">[root@localhost redis-cluster]# vi start_all_redis.sh </span><br><span class="line">redis-server /usr/local/redis-cluster/redis1/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis2/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis3/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis4/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis5/redis-conf/redis.conf</span><br><span class="line">redis-server /usr/local/redis-cluster/redis6/redis-conf/redis.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最加执行权限</span></span><br><span class="line">[root@localhost redis-cluster]# chmod +x start_all_redis.sh </span><br></pre></td></tr></table></figure>

<p>启动六个实例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# sh start_all_redis.sh </span><br><span class="line">[root@localhost redis-cluster]# ps -ef|grep redis</span><br><span class="line">root       7130      1  0 16:55 ?        00:00:00 redis-server *:23451 [cluster]</span><br><span class="line">root       7132      1  0 16:55 ?        00:00:00 redis-server *:23452 [cluster]</span><br><span class="line">root       7137      1  0 16:55 ?        00:00:00 redis-server *:23453 [cluster]</span><br><span class="line">root       7142      1  0 16:55 ?        00:00:00 redis-server *:23454 [cluster]</span><br><span class="line">root       7147      1  0 16:55 ?        00:00:00 redis-server *:23455 [cluster]</span><br><span class="line">root       7152      1  0 16:55 ?        00:00:00 redis-server *:23456 [cluster]</span><br><span class="line">root       7160   7091  0 16:55 pts/2    00:00:00 grep --color=auto redis</span><br></pre></td></tr></table></figure>

<p>去日志路径查看启动日志，这里截取了一部分日志内容，可以看到redis是集群模式，而且后面还有redis给出3项优化建议</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis]# pwd</span><br><span class="line">/var/log/redis</span><br><span class="line">[root@localhost redis]# cat redis4.log </span><br><span class="line">Redis 5.0.5 (00000000/0) 64 bit</span><br><span class="line">Running in cluster mode</span><br><span class="line">Port: 23454</span><br><span class="line">PID: 7142</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is <span class="built_in">set</span> to the lower value of 128.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING overcommit_memory is <span class="built_in">set</span> to 0! Background save may fail under low memory condition. To fix this issue add <span class="string">&#x27;vm.overcommit_memory = 1&#x27;</span> to /etc/sysctl.conf and <span class="keyword">then</span> reboot or run the <span class="built_in">command</span> <span class="string">&#x27;sysctl vm.overcommit_memory=1&#x27;</span> <span class="keyword">for</span> this to take effect.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING you have Transparent Huge Pages (THP) support enabled <span class="keyword">in</span> your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the <span class="built_in">command</span> <span class="string">&#x27;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&#x27;</span> as root, and add it to your /etc/rc.local <span class="keyword">in</span> order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span></span><br><span class="line">* Ready to accept connections</span><br></pre></td></tr></table></figure>



<h4 id="2-3-测试redis集群情况"><a href="#2-3-测试redis集群情况" class="headerlink" title="2.3 测试redis集群情况"></a>2.3 测试redis集群情况</h4><p>登录其中一个redis实例并测试，提示没有slot</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli -p 23451 -c</span><br><span class="line">127.0.0.1:23451&gt; AUTH foofoo</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23451&gt; set test foo</span><br><span class="line">(error) CLUSTERDOWN Hash slot not served</span><br></pre></td></tr></table></figure>

<p>==以上只是启动六个redis，但redis集群的slot还未分配到每个redis上，还需最后一步分配slot！==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli  --cluster create 127.0.0.1:23451 127.0.0.1:23452 127.0.0.1:23453 127.0.0.1:23454 127.0.0.1:23455 127.0.0.1:23456  --cluster-replicas 1 -a foofoo</span><br><span class="line">Warning: Using a password with &#x27;-a&#x27; or &#x27;-u&#x27; option on the command line interface may not be safe.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Performing <span class="built_in">hash</span> slots allocation on 6 nodes...</span></span><br><span class="line">Master[0] -&gt; Slots 0 - 5460</span><br><span class="line">Master[1] -&gt; Slots 5461 - 10922</span><br><span class="line">Master[2] -&gt; Slots 10923 - 16383</span><br><span class="line">Adding replica 127.0.0.1:23455 to 127.0.0.1:23451</span><br><span class="line">Adding replica 127.0.0.1:23456 to 127.0.0.1:23452</span><br><span class="line">Adding replica 127.0.0.1:23454 to 127.0.0.1:23453</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Trying to optimize slaves allocation <span class="keyword">for</span> anti-affinity</span></span><br><span class="line">[WARNING] Some slaves are in the same host as their master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis1实例作为master角色</span></span><br><span class="line">M: 920db62a3c29e3cb28dcbb575d4a438761563feb 127.0.0.1:23451</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis2实例作为master角色</span></span><br><span class="line">M: 3c54734e178ca585477c8fb8b78d87d0d7a1e038 127.0.0.1:23452</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis3实例作为master角色</span></span><br><span class="line">M: beb60be98bdc8ce50825b9d42e5efd819b98dc34 127.0.0.1:23453</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span><span class="bash"> redis4实例作为slave角色</span></span><br><span class="line">S: 4d451a885c1974679fe7c193ecac0373bd5cd808 127.0.0.1:23454</span><br><span class="line">   replicates 3c54734e178ca585477c8fb8b78d87d0d7a1e038</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span><span class="bash"> redis5实例作为slave角色</span></span><br><span class="line">S: 6ea365783619da802cd917b10fafd6096b4166a5 127.0.0.1:23455</span><br><span class="line">   replicates beb60be98bdc8ce50825b9d42e5efd819b98dc34</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span><span class="bash"> redis6实例作为slave角色</span></span><br><span class="line">S: 120026cbf95be79f4a3b2ed32c6867e238b7f8ec 127.0.0.1:23456</span><br><span class="line">   replicates 920db62a3c29e3cb28dcbb575d4a438761563feb</span><br><span class="line">Can I set the above configuration? (type &#x27;yes&#x27; to accept): yes</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Nodes configuration updated</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Assign a different config epoch to each node</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span></span><br><span class="line">Waiting for the cluster to join</span><br><span class="line">.....</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 127.0.0.1:23451)</span></span><br><span class="line">M: 920db62a3c29e3cb28dcbb575d4a438761563feb 127.0.0.1:23451</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 4d451a885c1974679fe7c193ecac0373bd5cd808 127.0.0.1:23454</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 3c54734e178ca585477c8fb8b78d87d0d7a1e038</span><br><span class="line">M: 3c54734e178ca585477c8fb8b78d87d0d7a1e038 127.0.0.1:23452</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 120026cbf95be79f4a3b2ed32c6867e238b7f8ec 127.0.0.1:23456</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 920db62a3c29e3cb28dcbb575d4a438761563feb</span><br><span class="line">M: beb60be98bdc8ce50825b9d42e5efd819b98dc34 127.0.0.1:23453</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 6ea365783619da802cd917b10fafd6096b4166a5 127.0.0.1:23455</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates beb60be98bdc8ce50825b9d42e5efd819b98dc34</span><br><span class="line">[OK] All nodes agree about slots configuration.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="keyword">for</span> open slots...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 提示16384个槽位已经分区好</span></span><br><span class="line">[OK] All 16384 slots covered.</span><br></pre></td></tr></table></figure>



<p>登录其中一台，注意这是集群模式登录，如果不带密码-a，则在set值时，分配到其它节点槽号将提示认证失败</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli -p 23451 -c</span><br><span class="line">127.0.0.1:23451&gt; AUTH foofoo</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23451&gt; set test foo</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [6918] located at 127.0.0.1:23452</span></span><br><span class="line">(error) NOAUTH Authentication required</span><br></pre></td></tr></table></figure>

<p>启动客户端需要带上密码选项，可以看到成功set值，通过设置不同的key，可以看到数据被hash后重定向到不同节点上的槽号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-cluster]# redis-cli -p 23451 a foofoo -c</span><br><span class="line">127.0.0.1:23451&gt; set test foo</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [6918] located at 127.0.0.1:23452</span></span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23452&gt; set cluster-test 11</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [14783] located at 127.0.0.1:23453</span></span><br><span class="line">OK</span><br><span class="line">127.0.0.1:23453&gt; get test</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Redirected to slot [6918] located at 127.0.0.1:23452</span></span><br><span class="line">&quot;foo&quot;</span><br></pre></td></tr></table></figure>

<p>以上完成了cluster模式的部署和测试，接下将给出redis主从模式，以及使用docker部署redis服务，之后给出了基于redis集群实现的高可用分布式锁Redlock的过程，在此之前已经给出了zookeeper的分布式锁实现。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis-cluster</tag>
      </tags>
  </entry>
  <entry>
    <title>利用pandas将groupby分组后将组内多行为一行</title>
    <url>/blog/2020/11/22/%E5%88%A9%E7%94%A8pandas%E5%B0%86groupby%E5%88%86%E7%BB%84%E5%90%8E%E5%B0%86%E7%BB%84%E5%86%85%E5%A4%9A%E8%A1%8C%E4%B8%BA%E4%B8%80%E8%A1%8C/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文的使用场景相对常见，尤其来了一份含几千上万行Excel表（或者长报告里需要插入表格数据），需要将某列的单元格长字符串展开展开为多行，或者将groupby后将同组的多行记录拼接为单个字符串，使用pandas可以快速完成。（文章由jupyter notebook导出的Markdown文件生成）</p>
<h3 id="groupby后将同组的多行值连接为一个长字符串"><a href="#groupby后将同组的多行值连接为一个长字符串" class="headerlink" title="groupby后将同组的多行值连接为一个长字符串"></a>groupby后将同组的多行值连接为一个长字符串</h3><h4 id="使用df-groupby、apply、pd-merge"><a href="#使用df-groupby、apply、pd-merge" class="headerlink" title="使用df.groupby、apply、pd.merge"></a>使用df.groupby、apply、pd.merge</h4><p>（在mysql中，可以直接使用GROUP_CONCAT(字段 SEPARATOR ‘、’)方法实现）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;省份&#x27;</span>:[<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;上海&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;大学名称&#x27;</span>:[<span class="string">&#x27;复旦大学&#x27;</span>,<span class="string">&#x27;中山大学&#x27;</span>,<span class="string">&#x27;同济大学&#x27;</span>,<span class="string">&#x27;华南理工大学&#x27;</span>,<span class="string">&#x27;上海交通大学&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df=pd.DataFrame(data)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>3</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>4</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flatten_rows=<span class="keyword">lambda</span> s:<span class="string">&#x27;、&#x27;</span>.join(s) <span class="comment"># 这就是分组后，将组内多行合并为一行的处理逻辑，这里用顿号连接行字符串</span></span><br><span class="line"><span class="comment"># 使用apply方法,as_index=False,表示不将省份作为groupby的行索引</span></span><br><span class="line">df1=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>)[<span class="string">&#x27;大学名称&#x27;</span>].apply(flatten_rows)</span><br><span class="line">df1</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>)[<span class="string">&#x27;大学名称&#x27;</span>].agg(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">df2</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将数量统计一列合并到df1，用省份关联即可,指定合并后，非key列的后缀，以便区分列来源哪个dataframe</span></span><br><span class="line">df1.merge(df2,on=<span class="string">&#x27;省份&#x27;</span>,suffixes=[<span class="string">&#x27;_df1&#x27;</span>,<span class="string">&#x27;_df2&#x27;</span>]) </span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称_df1</th>
      <th>大学名称_df2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">merged_df=pd.merge(df1,df2,on=<span class="string">&#x27;省份&#x27;</span>,suffixes=[<span class="string">&#x27;_df1&#x27;</span>,<span class="string">&#x27;_df2&#x27;</span>]) <span class="comment"># 也可直接用pd.merge方法来做</span></span><br><span class="line">merged_df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学名称_df1</th>
      <th>大学名称_df2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">merged_df.columns=[<span class="string">&#x27;省份&#x27;</span>,<span class="string">&#x27;大学列表&#x27;</span>,<span class="string">&#x27;数量&#x27;</span>]</span><br><span class="line">merged_df</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学列表</th>
      <th>数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



<h4 id="df-groupby和agg方法"><a href="#df-groupby和agg方法" class="headerlink" title="df.groupby和agg方法"></a>df.groupby和agg方法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">groupby_df=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>).agg(&#123;<span class="string">&#x27;大学名称&#x27;</span>: [flatten_rows,<span class="string">&#x27;count&#x27;</span>]&#125;)</span><br><span class="line">groupby_df</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead tr th &#123;
    text-align: left;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th>省份</th>
      <th colspan="2" halign="left">大学名称</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>&lt;lambda_0&gt;</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">groupby_df.columns=[<span class="string">&#x27;省份&#x27;</span>,<span class="string">&#x27;大学列表&#x27;</span>,<span class="string">&#x27;数量&#x27;</span>]</span><br><span class="line">groupby_df</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省份</th>
      <th>大学列表</th>
      <th>数量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="同组一行长字符串展开为多行"><a href="#同组一行长字符串展开为多行" class="headerlink" title="同组一行长字符串展开为多行"></a>同组一行长字符串展开为多行</h3><p>此内容为前面的逆向处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data2=&#123;</span><br><span class="line">    <span class="string">&#x27;省&#x27;</span>:pd.Series([<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;广西&#x27;</span>]),</span><br><span class="line">    <span class="string">&#x27;大学列表&#x27;</span>:pd.Series([<span class="string">&#x27;复旦大学、同济大学、上海交通大学&#x27;</span>,<span class="string">&#x27;中山大学、华南理工大学&#x27;</span>,<span class="literal">None</span>])</span><br><span class="line">&#125;</span><br><span class="line">df=pd.DataFrame(data2)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df21=df.copy()</span><br><span class="line"><span class="comment"># 创建数据集要小心，用pd.Series创建，则每行数据为字符串类型</span></span><br><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="使用pd-explode方法快速将一行展开为多行"><a href="#使用pd-explode方法快速将一行展开为多行" class="headerlink" title="使用pd.explode方法快速将一行展开为多行"></a>使用pd.explode方法快速将一行展开为多行</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pd.explode要求单元格的值为一个列表，如[&#x27;复旦大学&#x27;, &#x27;同济大学&#x27;, &#x27;上海交通大学&#x27;]，因此需原单元格长字符串做处理</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_func</span>(<span class="params">cell</span>):</span> <span class="comment"># 每个cell的长字符串分割，注意对于空值的处理</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> cell:</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    <span class="keyword">return</span> cell.split(<span class="string">&#x27;、&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>]=df21[<span class="string">&#x27;大学列表&#x27;</span>].apply(split_func) <span class="comment"># 大学列表这一列的每行值都是字符串类型</span></span><br><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<pre><code>[&#39;复旦大学&#39;, &#39;同济大学&#39;, &#39;上海交通大学&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df21.explode(<span class="string">&#x27;大学列表&#x27;</span>,ignore_index=<span class="literal">False</span>) <span class="comment">#ignore_index=False使得DataFrame对象保持原一行对应的索引号</span></span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>

<p>以上重点解析：对cell单元格的长字符串进行分割为一个列表，需自行设计一个分割函数<br>例如字符串：’复旦大学、同济大学、上海交通大学’<br>同理其他更为复杂的字符串:”黄小明201、李小明202|黄小明203%…”,采用正则分割即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pandas.DataFrame.explode()内部调用pandas.Series.explode()：</span></span><br><span class="line"><span class="string">        result = df[column].explode()</span></span><br><span class="line"><span class="string">        # 删除原df指定展开列后，再与该列单独使用pandas.Series.explode得到的结果进行索引关联合并</span></span><br><span class="line"><span class="string">        result = df.drop([column], axis=1).join(result) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pandas.Series.explode()内部使用reshape.explode</span></span><br><span class="line"><span class="string">values, counts = reshape.explode(np.asarray(self.array))</span></span><br><span class="line"><span class="string"># 该方法来pandas._libs基础库目录的reshape.cpython-37m-darwin.so*，这里已经被编译为动态链接库</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># pandas.Series.explode()方法表示将一列中单元格为列表值展开为多行</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series([[1, 2, 3], &#x27;foo&#x27;, [], [3, 4]])</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s</span></span><br><span class="line"><span class="string">0    [1, 2, 3]</span></span><br><span class="line"><span class="string">1          foo</span></span><br><span class="line"><span class="string">2           []</span></span><br><span class="line"><span class="string">3       [3, 4]</span></span><br><span class="line"><span class="string">dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s.explode()</span></span><br><span class="line"><span class="string">0      1</span></span><br><span class="line"><span class="string">0      2</span></span><br><span class="line"><span class="string">0      3</span></span><br><span class="line"><span class="string">1    foo</span></span><br><span class="line"><span class="string">2    NaN</span></span><br><span class="line"><span class="string">3      3</span></span><br><span class="line"><span class="string">3      4</span></span><br><span class="line"><span class="string">dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 可以同一单元格展开多行后，索引号跟原单元格索引号相同，因此可用原df通过join展开多行的Series，就能快速完成全表展开。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="使用stack-join方法快速将一行展开为多行"><a href="#使用stack-join方法快速将一行展开为多行" class="headerlink" title="使用stack+join方法快速将一行展开为多行"></a>使用stack+join方法快速将一行展开为多行</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22=df.copy()</span><br><span class="line">df22</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_1=df22[<span class="string">&#x27;大学列表&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;、&#x27;</span>,expand=<span class="literal">True</span>) <span class="comment"># 将一个cell值扩展为多列的cell值，该方法能自动处理cell空值</span></span><br><span class="line">df22_1</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>复旦大学</td>
      <td>同济大学</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>中山大学</td>
      <td>华南理工大学</td>
      <td>None</td>
    </tr>
    <tr>
      <th>2</th>
      <td>None</td>
      <td>None</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#三、将扩展的列转成多行，有些列的单元格值为None，转为行记录需要去掉</span></span><br><span class="line">df22_2=df22_1.stack(dropna=<span class="literal">True</span>)</span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure>


<pre><code>0  0      复旦大学
   1      同济大学
   2    上海交通大学
1  0      中山大学
   1    华南理工大学
dtype: object</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_2.index</span><br></pre></td></tr></table></figure>


<pre><code>MultiIndex([(0, 0),
            (0, 1),
            (0, 2),
            (1, 0),
            (1, 1)],
           )</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 观察以上索引，若能把MultiIndex的第1列索引留下，则可关联到原表的0，1索引号</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_2.reset_index(level=<span class="number">1</span>,drop=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure>


<pre><code>0      复旦大学
0      同济大学
0    上海交通大学
1      中山大学
1    华南理工大学
dtype: object</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_2.name=<span class="string">&#x27;new_col&#x27;</span></span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure>


<pre><code>0      复旦大学
0      同济大学
0    上海交通大学
1      中山大学
1    华南理工大学
Name: new_col, dtype: object</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span>(df22[<span class="string">&#x27;大学列表&#x27;</span>]) <span class="comment"># 删除原数据集大学列表这一列，用于后面合并</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_3=df22.join(df22_2) <span class="comment"># 默认使用两个df的索引号进行关联合并。最终完成将一行.注意df22是dataframe才有join方法，df22_2是Series，没有join方法</span></span><br><span class="line">df22_3</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>new_col</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 主要思路：将需要展开为多行的单元格值展开为多列，通过旋转操作将其变为一列Series，重置索引号，最终使用两个df索引号进行join关联合并,除了用pd.join，还可使用pd.concat两个省列与大学名称列关联合并为一张dataframe</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># df22[&#x27;省&#x27;]属于Series类型，df22_2也是Series类型，两者通过索引号进行关联后，按列方向合并</span></span><br><span class="line">df22_3=pd.concat([df22[<span class="string">&#x27;省&#x27;</span>],df22_2],join=<span class="string">&#x27;inner&#x27;</span>,axis=<span class="number">1</span>)</span><br><span class="line">df22_3</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>new_col</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df22_3.columns=[<span class="string">&#x27;省&#x27;</span>,<span class="string">&#x27;大学名称&#x27;</span>]</span><br><span class="line">df22_3 <span class="comment"># 注意到广西所在行已被删除，最终可将None行垂直合并到df22_3即可</span></span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
  </tbody>
</table>
</div>

<h4 id="自行设计合并逻辑"><a href="#自行设计合并逻辑" class="headerlink" title="自行设计合并逻辑"></a>自行设计合并逻辑</h4><p>上海    复旦大学、同济大学、上海交通大学<br>广东    中山大学、华南理工大学<br>生成两个Series列数据<br>[‘上海’,’上海’,’上海’,’广东’,’广东’]<br>[‘复旦大学’,’同济大学’,’上海交通大学’,’中山大学’,’华南理工大学’]<br>再将这两个Series生成Dataframe即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df23=df.copy()</span><br><span class="line">df23</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学列表</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学、同济大学、上海交通大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>广东</td>
      <td>中山大学、华南理工大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">province_list=[] <span class="comment"># 存放省的Series的值</span></span><br><span class="line">college_list=[] <span class="comment"># 存放大学名称Series的值</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df23.itertuples():</span><br><span class="line">    prov_cell=row[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> row[<span class="number">2</span>]:</span><br><span class="line">        mult_cell=row[<span class="number">2</span>].split(<span class="string">&#x27;、&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> mult_cell:</span><br><span class="line">            province_list.append(prov_cell)</span><br><span class="line">            college_list.append(item)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 注意对cell为空值得处理</span></span><br><span class="line">        province_list.append(prov_cell)</span><br><span class="line">        college_list.append(row[<span class="number">2</span>])</span><br><span class="line">    </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">province_list</span><br></pre></td></tr></table></figure>


<pre><code>[&#39;上海&#39;, &#39;上海&#39;, &#39;上海&#39;, &#39;广东&#39;, &#39;广东&#39;, &#39;广西&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">college_list</span><br></pre></td></tr></table></figure>


<pre><code>[&#39;复旦大学&#39;, &#39;同济大学&#39;, &#39;上海交通大学&#39;, &#39;中山大学&#39;, &#39;华南理工大学&#39;, None]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data23=&#123;</span><br><span class="line">    <span class="string">&#x27;省&#x27;</span>:province_list,</span><br><span class="line">    <span class="string">&#x27;大学名称&#x27;</span>:college_list</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.DataFrame(data23)</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>省</th>
      <th>大学名称</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>上海</td>
      <td>复旦大学</td>
    </tr>
    <tr>
      <th>1</th>
      <td>上海</td>
      <td>同济大学</td>
    </tr>
    <tr>
      <th>2</th>
      <td>上海</td>
      <td>上海交通大学</td>
    </tr>
    <tr>
      <th>3</th>
      <td>广东</td>
      <td>中山大学</td>
    </tr>
    <tr>
      <th>4</th>
      <td>广东</td>
      <td>华南理工大学</td>
    </tr>
    <tr>
      <th>5</th>
      <td>广西</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>]]></content>
      <categories>
        <category>数据分析与挖掘</category>
      </categories>
  </entry>
  <entry>
    <title>基于Centos+uWSGI+Nginx部署Django项目(详细版)</title>
    <url>/blog/2019/08/24/%E5%9F%BA%E4%BA%8ECentos+uWSGI+Nginx%E9%83%A8%E7%BD%B2Django%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<p>&#8195;&#8195;部署背景：之前开发了一个本地的个人blog项目，还未部署云服务器，有些功能还未完善，先尝试在本地部署。本篇内容则是基于centos+uwsgi+nginx来部署项目，实现高性能web服务，其中还给出uwsgi和nginx分别部署在不同服务器上步骤，这一点，很多文章并未给出相关探讨。</p>
<a id="more"></a>

<h3 id="1、相关服务安装配置"><a href="#1、相关服务安装配置" class="headerlink" title="1、相关服务安装配置"></a>1、相关服务安装配置</h3><p>安装nginx。配置官方镜像源，这里baseurl里面有$basearch变量，用来检索yum命令安装所需要的包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/yum.repos.d/nginx.repo</span></span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=http://nginx.org/packages/centos/7/$basearch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"><span class="meta">$</span><span class="bash"> yum -y install nginx</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> nginx</span></span><br></pre></td></tr></table></figure>

<p>安装python3.6，采用python虚拟环境部署项目，跟系统不冲突</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> yum -y install python36 python36-devel</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置并载入 Python3 虚拟环境</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /opt</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在这里不需要将centos默认python2.7版本配置为默认python3.6</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 直接使用python3.6作为启动命令即可，可避免冲突</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python3.6 -m venv py36  <span class="comment"># py3 为虚拟环境名称, 可自定义</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出虚拟环境可以使用 deactivate 命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /opt/py36/bin/activate</span>  </span><br><span class="line">(py36) [root@nn py36]# ls</span><br><span class="line">bin  include  lib  lib64  pyvenv.cfg</span><br><span class="line"><span class="meta">#</span><span class="bash"> 载入环境后默认以下所有命令均在该虚拟环境中运行</span></span><br><span class="line">(py3) [root@localhost py3]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 Python 库依赖</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install --upgrade pip setuptools</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install -r requirements.txt</span></span><br></pre></td></tr></table></figure>
<p>安装uwsgi</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> uwsgi需要使用gcc环境编译否则无法安装成功</span></span><br><span class="line">yum install gcc -y</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 激活py36环境</span></span><br><span class="line">(py36) [root@nn mywebapp]# pip install uwsgi</span><br><span class="line">Successfully installed uwsgi-2.0.18</span><br></pre></td></tr></table></figure>
<p>数据库安装和配置可以参考本人<a href="https://blog.csdn.net/pysense/article/details/99892680">这篇文章</a></p>
<p>以上后台技术栈的相关版本总揽如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Centos 7.5</span><br><span class="line">MariaDB 10.3.17</span><br><span class="line">Python 3.6</span><br><span class="line">Django1.11</span><br><span class="line">uWSGI 2.0.18</span><br><span class="line">Nginx 1.12.2</span><br><span class="line">Redis 5.0.4</span><br></pre></td></tr></table></figure>
<p>这些服务都是部署同一台服务器上，适合单台的个人云服务器，毕竟一年几百元，服务器配置有限，而对于本地部署，可以通过多台linux虚拟机分别部署不同服务，并做HA，这一过程相信也会积累不少知识和经验，学有余力的同学一定要试试。</p>
<h3 id="2、配置uwsgi启动django项目"><a href="#2、配置uwsgi启动django项目" class="headerlink" title="2、配置uwsgi启动django项目"></a>2、配置uwsgi启动django项目</h3><h4 id="2-1-uwsgi-命令行启动项目"><a href="#2-1-uwsgi-命令行启动项目" class="headerlink" title="2.1 uwsgi 命令行启动项目"></a>2.1 uwsgi 命令行启动项目</h4><p>查看项目，对项目路径必须清楚那些文件在哪里路径下，否则使用uwsgi启动设置参数，容易出错，以本项目为例，项目根目录路径为：<code>/opt/mywebapp</code>，项目根目录内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mywebapp]# pwd</span><br><span class="line">/opt/mywebapp</span><br><span class="line">(py36) [root@nn mywebapp]# ls</span><br><span class="line">account  blog    db.sqlite3  __init__.py  media   script  templates</span><br><span class="line">article  course  image       manage.py    mysite  static</span><br></pre></td></tr></table></figure>

<p>==<strong>其中非常关键的wsgi入口</strong>==，在mysite目录下，也就是django项目总settings.py所在的目录，mysite目录下的wsgi.py，将在之后的uwsgi启动中使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mysite]# ls</span><br><span class="line">__init__.py  __pycache__  settings.py  urls.py  wsgi.py</span><br></pre></td></tr></table></figure>

<p>wsgi.py代码逻辑：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">WSGI config for mysite project.</span></span><br><span class="line"><span class="string">It exposes the WSGI callable as a module-level variable named ``application``.</span></span><br><span class="line"><span class="string">For more information on this file, see</span></span><br><span class="line"><span class="string">https://docs.djangoproject.com/en/1.10/howto/deployment/wsgi/</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> django.core.wsgi <span class="keyword">import</span> get_wsgi_application</span><br><span class="line">os.environ.setdefault(<span class="string">&quot;DJANGO_SETTINGS_MODULE&quot;</span>, <span class="string">&quot;mysite.settings&quot;</span>)</span><br><span class="line">application = get_wsgi_application()</span><br></pre></td></tr></table></figure>

<p>确认项目使用manage.py启动能正常运行<br><code>python manage.py runserver 0.0.0.0:9000</code></p>
<p>使用uwsgi启动项目并测试是否成功运行django项目，通过连接mysite/wsgi.py实现web server和application通信</p>
<p>这里有两种启动方式：</p>
<p>==<strong><em>A、不带静态文件启动，静态文件将无法加载，页面不正常显示</em></strong>==</p>
<p>–chdir /opt/mywebapp/     django项目根路径<br>wsgi-file mysite/wsgi.py    django项目settings.py所在目录下的 wsgi.py文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uwsgi --http 0.0.0.0:9000 --chdir /opt/mywebapp/ --wsgi-file mysite/wsgi.py --master --processes 4 --threads 2</span><br></pre></td></tr></table></figure>

<p>==<strong><em>B、带静态文件启动，也就是网页打开后，页面能正常显示</em></strong>==</p>
<p>–chdir /opt/mywebapp/     django项目根路径<br>wsgi-file mysite/wsgi.py    django项目settings.py所在目录下的 wsgi.py文件<br>–static-map=/static=static  django项目web页面静态文件，所在根目录的’static’目录<br>–static-map=/static=media  django项目内容静态文件，所在根目录的’media’目录</p>
<p>==注意：这里仅是指static、media目录，根目录下还有其他blog，account,templates目录等，可能也有人会问，是不是都需要把这些目录都要一一加入–static-map里面，答案是不需要，因为这些都不是django application对应的“static”目录(已在settins设定，并可以让其他views索引到static目录)，如果使用-static-map=/static=templates，uwsgi将无法找到相关静态文件==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uwsgi --http 0.0.0.0:9000 --chdir /opt/mywebapp/ --wsgi-file mysite/wsgi.py --static-map=/static=static --master --processes 4 --threads 2</span><br></pre></td></tr></table></figure>

<p>==注意==，对于这种启动方式，动、静态资源都可以访问</p>
<h4 id="2-2-使用uwsgi-ini配置文件启动项目"><a href="#2-2-使用uwsgi-ini配置文件启动项目" class="headerlink" title="2.2 使用uwsgi.ini配置文件启动项目"></a>2.2 使用uwsgi.ini配置文件启动项目</h4><p>以上两种启动直接在命令加入环境参数，比较繁琐，可通过在配置文件中放置这些环境参数，方便启动，在manage.py 同目录下（项目根目录），新建一个目录uwsgi_conf用来放置uwsgi.ini配置文件，目录路径：<code>/opt/mywebapp/uwsgi_conf</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mywebapp]# ls</span><br><span class="line">account  blog    db.sqlite3  __init__.py  media   uwsgi_conf  templates</span><br><span class="line">article  course  image       manage.py    mysite  static</span><br></pre></td></tr></table></figure>

<p>在uwsgi_conf目录下新建uwsgi.ini文件，配置如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> uwsig使用配置文件启动</span></span><br><span class="line">[uwsgi]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 项目所在的根目录</span></span><br><span class="line">chdir=/opt/mywebapp/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定项目的application,区别于启动命令--wsgi-filemysite/wsgi.py</span></span><br><span class="line">module=mysite.wsgi:application</span><br><span class="line"><span class="meta">#</span><span class="bash">the <span class="built_in">local</span> unix socket file than commnuincate to Nginx</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定sock的文件路径，这个sock文件会在nginx的uwsgi_pass配置，用来nginx与uwsgi通信</span>       </span><br><span class="line"><span class="meta">#</span><span class="bash"> 支持ip+port模式以及socket file模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash">socket=%(<span class="built_in">chdir</span>)/uwsgi_conf/uwsgi.sock</span></span><br><span class="line">socket=127.0.0.1:9001</span><br><span class="line"><span class="meta">#</span><span class="bash"> 进程个数</span>       </span><br><span class="line">processes = 8</span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个进程worker数</span></span><br><span class="line">workers=5</span><br><span class="line">procname-prefix-spaced=mywebapp                # uwsgi的进程名称前缀</span><br><span class="line">py-autoreload=1                              # py文件修改，自动加载</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定IP端口，web访问入口</span></span><br><span class="line">http=0.0.0.0:9000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定多个静态文件：static目录和media目录,也可以不用指定该静态文件，在nginx中配置静态文件目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> uwsgi有自己的配置语法，详细可参考官网，无需写绝对路径，可以用循环、判断等高级配置语法</span></span><br><span class="line">for =static media</span><br><span class="line">static-map=/static=%(chdir)/%(_)</span><br><span class="line">endfor =</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动uwsgi的用户名和用户组</span></span><br><span class="line">uid=root</span><br><span class="line">gid=root</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用主进程</span></span><br><span class="line">master=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自动移除unix Socket和pid文件当服务停止的时候</span></span><br><span class="line">vacuum=true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 序列化接受的内容，如果可能的话</span></span><br><span class="line">thunder-lock=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用线程</span></span><br><span class="line">enable-threads=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置一个超时，用于中断那些超过服务器请求上限的额外请求</span></span><br><span class="line">harakiri=30</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置缓冲</span></span><br><span class="line">post-buffering=4096</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置日志目录</span></span><br><span class="line">daemonize=%(chdir)/uwsgi_conf/uwsgi.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> uWSGI进程号存放</span></span><br><span class="line">pidfile=%(chdir)/uwsgi_conf/uwsgi.pid</span><br><span class="line"><span class="meta">#</span><span class="bash">monitor uwsgi status  通过该端口可以监控 uwsgi 的负载情况</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 支持ip+port模式以及socket file模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> stats=%(<span class="built_in">chdir</span>)/uwsgi_conf/uwsgi.status</span> </span><br><span class="line">stats = 127.0.0.1:9001</span><br></pre></td></tr></table></figure>
<p>之所以要新建一个uwsgi_conf目录，是为了集中放置uWSGI配置以及日志、进程等文件，方便管理，配置语法可参考<a href="https://uwsgi-docs.readthedocs.io/en/latest/ConfigLogic.html">官方配置文档说明</a></p>
<p>基于配置文件uwsgi启动django项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn uwsgi_conf]# uwsgi --ini uwsgi.ini </span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动后打印的信息，可以看到static静态文件和media媒体资源目录被uWSGI索引</span></span><br><span class="line">[uWSGI] getting INI configuration from uwsgi.ini</span><br><span class="line">[uwsgi-static] added mapping for /static =&gt; /opt/mywebapp/static</span><br><span class="line">[uwsgi-static] added mapping for /static =&gt; /opt/mywebapp/media</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行后，自动参数日志、进程,建议自行查看日志文件内容，了解更多uwsgi</span></span><br><span class="line">(py36) [root@nn uwsgi_conf]# ls</span><br><span class="line">uwsgi.ini  uwsgi.log  uwsgi.pid </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 停止uwsgi服务</span></span><br><span class="line">(py36) [root@nn uwsgi_conf]# uwsgi --stop uwsgi.pid</span><br></pre></td></tr></table></figure>
<p>以上说明使用uWSGI配置启动django项目成功运行，因uWSGI的配置文件里已加入静态文件static-map，因此在不需要nginx的配置下，也可以支撑服务（只是性能未到完美级别），此部分的流程如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190826235642727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>下面将使用nginx配置静态文件请求，uwsgi只负责动态请求部分的请求，各司其职，以进一步压榨服务性能。如果确定使用nginx代理django项目静态文件服务，那么配置之前，先把uwsgi.ini里面的–static-map=/static部分注释掉。</p>
<h3 id="3、使用nginx启动uwsgi"><a href="#3、使用nginx启动uwsgi" class="headerlink" title="3、使用nginx启动uwsgi"></a>3、使用nginx启动uwsgi</h3><h4 id="3-1-配置nginx"><a href="#3-1-配置nginx" class="headerlink" title="3.1 配置nginx"></a>3.1 配置nginx</h4><p>在/etc/nginx/conf.d/，新建一个myblog.conf文件，配置为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">upstream blog_app &#123;</span><br><span class="line"><span class="meta">   #</span><span class="bash"> nginx通过socket在环回接口地址的9001端口与本地的uWSGI进程通信</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"> 支持ip:port模式以及socket file模式</span></span><br><span class="line"><span class="meta">   #</span><span class="bash">server unix:/opt/mywebapp/uwsgi_conf/uwsgi.sock;</span></span><br><span class="line">   server 127.0.0.1:9001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">    listen 9090;</span><br><span class="line">    server_name 192.168.100.5;</span><br><span class="line">    </span><br><span class="line">    access_log /var/log/nginx/access.log;</span><br><span class="line">    charset utf-8;</span><br><span class="line">  </span><br><span class="line">    gzip_types text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;</span><br><span class="line">    error_page 404 /404.html;</span><br><span class="line">    error_page 500 502 503 504 /50x.html;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        # nginx转发动态请求到uWSGI</span><br><span class="line">        include uwsgi_params;</span><br><span class="line">        uwsgi_connect_timeout 20</span><br><span class="line">        uwsgi_pass blog_app;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 如果写成/static/,nginx无法找到项目静态文件路径</span><br><span class="line">    location /static &#123;</span><br><span class="line">        alias /opt/mywebapp/static;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 如果写成/media/,nginx无法找到项目媒体文件路径</span><br><span class="line">    location /media &#123;</span><br><span class="line">        alias /opt/mywebapp/media;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>重启nginx服务，server nginx restart，只要在第2部分uWSGI与django application正常连接，那么到这部分，nginx是能够正常代理uWSGI服务的，如有问题，请认真检查nignx的在/etc/nginx/conf.d/myblog.conf配置文件。<br>==这里要解释为何配置可以放在/etc/nginx/conf.d/目录下，网上不是有很多教程是在/etc/nginx/nginx.conf默认配置文件改动吗？==<br>其实nginx配置文件很灵活，可以从其他模块include根配置文件里面，查看主配置nginx.conf内容里面的http 模块，它可以是可以把/etc/nginx/conf.d/目录下所有的配置文件内容包含到主配置文件里面，注意如果使用这种方式，请把主配置文件server模块注释，其实就是关闭多余其他服务端口而已。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 要确保nginx用户对django项目根目录下静态文件具有读权限,否则会出现403 Forbidden</span></span><br><span class="line">user nginx;</span><br><span class="line">worker_processes auto;</span><br><span class="line">error_log /var/log/nginx/error.log;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line"><span class="meta">#</span><span class="bash"> Load dynamic modules. See /usr/share/nginx/README.dynamic.</span></span><br><span class="line">include /usr/share/nginx/modules/*.conf;</span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    ......省略</span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    ......省略</span><br><span class="line"><span class="meta">	#</span><span class="bash">server  &#123;</span></span><br><span class="line">    ......省略</span><br><span class="line">    #		&#125;    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h4 id="3-2-关于myblog-conf路径放置"><a href="#3-2-关于myblog-conf路径放置" class="headerlink" title="3.2 关于myblog.conf路径放置"></a>3.2 关于myblog.conf路径放置</h4><p>理解了nginx的include配置文件方式，那么我们可以不需要在/etc/nginx/conf.d/目录下创建myblog.conf，直接在django项目的根目录下mywebapp/，新建一个nginx_conf目录,在这里放置myblog.conf，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn nginx_conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;mywebapp&#x2F;nginx_conf</span><br><span class="line">[root@nn nginx_conf]# ls</span><br><span class="line">myblog.conf</span><br></pre></td></tr></table></figure>
<p>然后把该配置路径<code>/opt/mywebapp/nginx_conf/myblog.conf</code> 加入到nginx默认的主配置文件里面</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    ......省略</span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    # 不再是 include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    include /opt/mywebapp/nginx_conf/myblog.conf;</span><br><span class="line">    ......省略</span><br><span class="line"><span class="meta">	#</span><span class="bash">server  &#123;</span></span><br><span class="line">    ......省略</span><br><span class="line">    #		&#125;    </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>server nginx restart重启服务，nginx首先解析主配置文件/etc/nginx/nginx.conf，发现主配置里面，include了在其他位置的配置文件，于是nginx找到myblog.conf并加载，接着完成一些列其他逻辑。以上两种nginx配置都可以连接uWSGI服务，至于选哪种方式，看个人需求或项目文件管理习惯。</p>
<h4 id="3-3-nginx-uWSGI-django启动后，访问media目录的图片、文件、视频出现403-forbidden提示"><a href="#3-3-nginx-uWSGI-django启动后，访问media目录的图片、文件、视频出现403-forbidden提示" class="headerlink" title="3.3  nginx+uWSGI+django启动后，访问media目录的图片、文件、视频出现403 forbidden提示"></a>3.3  nginx+uWSGI+django启动后，访问media目录的图片、文件、视频出现403 forbidden提示</h4><p>这个问题，在csdn绝大部分nginx+uWSGI+django部署文章都没提及如何处理，因为大部分文章没有测试到这部分内容，只是测试nginx可以正常获取static目录下的js、css、html文件，页面显示正常即结束，但如果项目的代码中，例如有视频课程这么一个功能，上传视频是放在media目录下的course目录里，那么当网页访问该视频时，就会提示该资源403 forbidden状态。<br>==<strong>原因：nginx worker对media整个目录没有读权限</strong>==<br>nginx默认使用名字为“nginx”用户，这一点可在其主配置文件找到，也可以通过进程详情看到</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# ps -ef|grep nginx</span><br><span class="line">root     28759     1  0 02:01 ?        00:00:00 nginx: master process /usr/sbin/nginx</span><br><span class="line">nginx    28760 28759  0 02:01 ?        00:00:00 nginx: worker process</span><br><span class="line">root     28784 28382  0 02:26 pts/0    00:00:00 grep --color=auto nginx</span><br></pre></td></tr></table></figure>
<p>再看mywebapp/media权限，指向root用户</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x.  1 root  root     804 ** manage.py</span><br><span class="line">drwxr-xr-x.  4 root  root     49 ** media</span><br><span class="line">drwxr-xr-x.  3 root  root      93 ** mysite</span><br><span class="line">drwxr-xr-x.  9 root  root     113 ** static</span><br></pre></td></tr></table></figure>
<p>故需要将相关目录的读权限给到nginx用户，对django项目根目录下的static、media两个目录赋权给nginx user</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# chown -R nginx:nginx /opt/mywebapp/static/</span><br><span class="line">[root@nn mywebapp]# chmod -R ug+r /opt/mywebapp/static/</span><br><span class="line">[root@nn mywebapp]# chown -R nginx:nginx /opt/mywebapp/media/</span><br><span class="line">[root@nn mywebapp]# chmod -R ug+r /opt/mywebapp/media/</span><br></pre></td></tr></table></figure>
<p>聪明的同学可能此时会立刻联想到：既然nginx访问django项目静态文件要赋权，那么前面第2部分的uWSGI进程也是否需要赋权呢？答案：需要看uwsgi.ini配置了什么用户。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">uid=root</span><br><span class="line">gid=root</span><br></pre></td></tr></table></figure>
<p>这里配置了root用户，而且对于django整个项目文件的rwx权限，root用户本已具备，因此不需要再赋权，除非uwsgi.ini配置了一个非root用户，例如blog_user用户，那么就需要重启赋权，目录是整个django项目，例如以下可行的赋权：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# chown -R blog_user:blog_user /opt/mywebapp</span><br><span class="line">[root@nn mywebapp]# chmod -R ug+r /opt/mywebapp</span><br></pre></td></tr></table></figure>


<p>==第3部分的请求流程可以表示为：==<br><img src="https://img-blog.csdnimg.cn/20190825195924701.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="4、关于nginx配置django的静态文件讨论"><a href="#4、关于nginx配置django的静态文件讨论" class="headerlink" title="4、关于nginx配置django的静态文件讨论"></a>4、关于nginx配置django的静态文件讨论</h3><p>关于静态文件的配置，其过程有些地方非常容易引起混淆，在这里一一指出。<br>首先，在本文中，nginx服务和uWSGI服务部署在同一台服务器，因此在nginx配置中，location的静态文件因为的是本地django项目里面的静态文件，个人把这种配置过程nginx代理本地静态文件配置，另外一种django项目里面的静态文件放置在远程服务器上，由远程的nginx来代理，这种称为nginx代理远程静态文件配置。<br>==为什么要做这样的部署测试？<br>大部分文章都是基于同一台服务器进行nginx服务和uWSGI服务部署，很少有讨论在不同服务器上部署，事实上，如果生产环境比较严格，nginx服务器本身要做冗余和负载均衡，uWSGI服务器也是要做冗余和负载均衡，数据库MariaDB本身主-主模式。==</p>
<h4 id="4-1-nginx代理本地静态文件配置"><a href="#4-1-nginx代理本地静态文件配置" class="headerlink" title="4.1 nginx代理本地静态文件配置"></a>4.1 nginx代理本地静态文件配置</h4><p>在第三部分的uWSGI的配置文件中，socket配置为loopback地址，说明uWSGI进程与nginx进行本地通信，nginx代理本地静态文件。在第3部分可测试过程中，除了<a href="http://192.168.100.5:9090/admin%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E6%98%BE%E7%A4%BA%E9%A1%B5%E9%9D%A2%EF%BC%88%E6%89%80%E6%9C%89%E5%85%B3%E4%BA%8Eadmin%E7%AE%A1%E7%90%86%E9%A1%B5%E9%9D%A2%E5%90%8E%E5%8F%B0%E7%9A%84%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6nginx%E6%97%A0%E6%B3%95%E6%89%BE%E5%88%B0%EF%BC%89%EF%BC%8C%E5%85%B6%E4%BB%96%E5%AD%90%E8%B7%AF%E5%BE%84%E4%BE%8B%E5%A6%82http://192.168.100.5:9090/blog,http://192.168.100.5:9090/article,%E9%83%BD%E5%8F%AF%E4%BB%A5%E6%AD%A3%E5%B8%B8%E6%98%BE%E7%A4%BA%E9%A1%B5%E9%9D%A2%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%83%85%E5%86%B5%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%EF%BC%9F">http://192.168.100.5:9090/admin无法正常显示页面（所有关于admin管理页面后台的静态文件nginx无法找到），其他子路径例如http://192.168.100.5:9090/blog,http://192.168.100.5:9090/article,都可以正常显示页面，这种情况如何处理？</a><br>从nginx配置的项目静态文件路径为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如果写成/static/,nginx无法找到项目静态文件路径,注意避免配置语法出错</span></span><br><span class="line">location /static &#123;</span><br><span class="line">    alias /opt/mywebapp/static;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看其目录文件，发现并没有django项目admin的后台所需的静态文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn mywebapp]# ls</span><br><span class="line">account     blog        image        media   </span><br><span class="line">all_static  course      __init__.py  mysite  templates</span><br><span class="line">article     db.sqlite3  manage.py    static  uwsgi_conf</span><br><span class="line">[root@nn mywebapp]# ls static</span><br><span class="line">css  editor  fonts  images  ImgCrop  js</span><br></pre></td></tr></table></figure>
<p>通过<code>python manage.py collectstatic</code>将admin后台包含所有的静态文件都拷贝到mywebapp根目录下，在执行命令之前，需要在settings.py设置一个放置整个mywebapp项目静态文件目录</p>
<ul>
<li>原静态文件目录的设置：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">STATIC_URL = &#x27;/static/&#x27;</span><br><span class="line">STATICFILES_DIRS = (</span><br><span class="line">    os.path.join(BASE_DIR, &quot;static&quot;),</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>加入admin静态文件的设置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">STATIC_URL = &#x27;/static/&#x27;</span><br><span class="line">STATICFILES_DIRS = (</span><br><span class="line">    os.path.join(BASE_DIR, &quot;static&quot;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">STATIC_ROOT=&#x27;all_static&#x27;</span><br></pre></td></tr></table></figure>

<p>==注意：若STATIC_ROOT=’static’，collect无法拷贝admin静态文件到项目中，会有提示==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> django.core.exceptions.ImproperlyConfigured: The STATICFILES_DIRS setting should not contain the STATIC_ROOT setting</span></span><br></pre></td></tr></table></figure>
<p>==能否在settings.py不指定STATIC_ROOT，利用已指定的mywebapp/static目录放置collect static？不行==，看以下提示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># django.core.exceptions.ImproperlyConfigured: You&#39;re using the staticfiles app without having set the STATIC_ROOT setting to a filesystem path.</span><br></pre></td></tr></table></figure>

<p>当正确拷贝所有静态文件到mywebapp/all_static目录后，查看其内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(py36) [root@nn mywebapp]# ls all_static/</span><br><span class="line">admin  css  editor  fonts  images  ImgCrop  js</span><br></pre></td></tr></table></figure>
<p>原来 <code>python manage.py collectstatic</code> 把项目下的static目录静态文件和django自带的后台admin静态文件打包一起放在/mywebapp/all_static目录里。<br>就个人对项目目录管理习惯而言，把all_static/下的admin目录整个拷贝到static目录下，并删除all_static目录，settings的静态文件路径注释掉<code>STATIC_ROOT=&#39;all_static&#39;</code>，==这样既可保持整个django项目仅有一个static目录，而且该目录已经包含项目所需的所有静态文件（js、css、html等）==，注意media目录路径不做改变，还是位于根项目路径下。</p>
<h4 id="4-2-远程静态目录配置"><a href="#4-2-远程静态目录配置" class="headerlink" title="4.2 远程静态目录配置"></a>4.2 远程静态目录配置</h4><h5 id="4-2-1-拷贝静态文件到远程nginx服务器"><a href="#4-2-1-拷贝静态文件到远程nginx服务器" class="headerlink" title="4.2.1 拷贝静态文件到远程nginx服务器"></a>4.2.1 拷贝静态文件到远程nginx服务器</h5><p>如果已经理解了nginx代理本地静态文件的配置，其实远程方式其实也简单，本文的远程nginx服务器地址为192.168.100.6，在创建与uWSGI相同的application目录（可以使用不同路径，这里是为方便管理）：<code>/opt/mywebapp/</code>，该目录只放置static和media目录，文件可以手工同步过来，注意要赋权给nginx用户</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 mywebapp]# ls -al</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 4 root  root   33 Aug 17 20:29 .</span><br><span class="line">drwxr-xr-x. 3 root  root   67 Aug 17 20:03 ..</span><br><span class="line">drwxr-xr-x. 4 nginx nginx  35 Aug 19  2019 media</span><br><span class="line">drwxr-xr-x. 9 nginx nginx 113 Aug 18  2019 static</span><br><span class="line"></span><br><span class="line">[root@dn1 mywebapp]# ls static/</span><br><span class="line">admin  css  editor  fonts  images  ImgCrop  js</span><br><span class="line">[root@dn1 mywebapp]# ls media/</span><br><span class="line">courses  images</span><br></pre></td></tr></table></figure>
<h5 id="4-2-2-更改-5uWSGI服务器配置文件socket"><a href="#4-2-2-更改-5uWSGI服务器配置文件socket" class="headerlink" title="4.2.2 更改.5uWSGI服务器配置文件socket"></a>4.2.2 更改.5uWSGI服务器配置文件socket</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[uwsgi]</span><br><span class="line"># 项目所在的根目录</span><br><span class="line">chdir&#x3D;&#x2F;opt&#x2F;mywebapp&#x2F;</span><br><span class="line"># 指定项目的application,区别于启动命令--wsgi-filemysite&#x2F;wsgi.py</span><br><span class="line">module&#x3D;mysite.wsgi:application</span><br><span class="line"># 不再使用loopback地址，对外其他服务器暴露uWSGI服务</span><br><span class="line">socket&#x3D;192.168.1005:9001</span><br></pre></td></tr></table></figure>

<h5 id="4-2-3-更改-6服务器的nginx配置文件，"><a href="#4-2-3-更改-6服务器的nginx配置文件，" class="headerlink" title="4.2.3 更改.6服务器的nginx配置文件，"></a>4.2.3 更改.6服务器的nginx配置文件，</h5><p>为了方便管理，路径与uWSGI一致：<br><code>/etc/nginx/conf.d/myblog.conf</code><br>远程nginx服务器配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">upstream blog_app &#123;</span><br><span class="line"><span class="meta">   #</span><span class="bash"> 连接远程uWSGI服务器的socket</span> </span><br><span class="line">   server 192.168.88.5:9001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 9090;</span><br><span class="line">    server_name 192.168.100.6;</span><br><span class="line">    access_log /var/log/nginx/access.log;</span><br><span class="line">    charset utf-8;</span><br><span class="line">    gzip_types text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;</span><br><span class="line">    error_page 404 /404.html;</span><br><span class="line">    error_page 500 502 503 504 /50x.html;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        include uwsgi_params;</span><br><span class="line">        uwsgi_connect_timeout 30;</span><br><span class="line">        uwsgi_pass ;</span><br><span class="line">    &#125;</span><br><span class="line">    # 这就是为何在远程nginx服务器上，保持与uWSGI静态文件路径一致的原因，方便管理和理解配置文件</span><br><span class="line">    location /static &#123;</span><br><span class="line">        alias /opt/mywebapp/static;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    location /media &#123;</span><br><span class="line">        alias /opt/mywebapp/media;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以.6IP访问动态资源和静态，可以正常请求。</p>
<h3 id="5-、关于sock文件的理解"><a href="#5-、关于sock文件的理解" class="headerlink" title="5 、关于sock文件的理解"></a>5 、关于sock文件的理解</h3><p>在uwsgi.ini和nginx配置文件里面，我们需要配置socket，以便他们之间进行进程通信，这里socket的配置方式，可以选择一个以.sock作为后缀的文件，而大家更熟悉的方式是通过socket pair进行socket通信，这里如何理解sock文件？<br>这里整理收集的资料概括为：</p>
<p>在Unix/Linux系统里面，“一切皆文件”，这里文件是由什么组成的？这些文件其实是可以读取和写入的普通字节（字节流）的集合。如果你持有一个文件引用（也就是文件描述符），就可以使用“打开open –&gt; 读写write/read –&gt; 关闭close”模式（抽象成一组Linux 文件操作API）来进行 IO 操作，无论设备的类型和底层硬件是什么。</p>
<p>所以进程（进程本身也是文件）之间的通信。个人理解uwsgi.sock文件用到的就是该模式的一个实现，socket就是这么一种特殊的套接字文件，也即是说nginx通过uwsgi.sock作为载体，与本地的uWSGI进程进行通信。</p>
<p>这种基于特殊的套接字文件来保持通信是无法进行远程通信，这时需要用到TCP/IP协议，通过远程IP+端口号的socket pair，基于TCP连接进行远程通信（若用loopback的IP地址127.0.0.1，就变成本地通信），所以这启发我们可以将nginx部署在另外一台服务器上，本文中，uWSGI+application部署在192.168.100.5，另外一台服务器192.168.100.6 作为nginx服务器，要实现他们之间的远程通信，<br>只需uwsgi配置文件uwsgi.ini里面socket行改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#the local unix socket file than commnuincate to Nginx</span><br><span class="line"># 指定sock的文件路径，则限制本地通信</span><br><span class="line"># 指定loopback 地址+端口号，则限制本地通信</span><br><span class="line"># 指定全网地址或者本机真实IP，则可以实现远程通信</span><br><span class="line">socket&#x3D;192.168.100.5:9001</span><br></pre></td></tr></table></figure>
<p>在.6服务器上，nginx的配置myblog.conf的upstream模块个改为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream blog_app &#123;</span><br><span class="line"> # 指向远程uWSGI</span><br><span class="line">   server 192.168.100.5:9001;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过以上设置，可以把static静态文件目录和media目录放置在远程.6的nginx服务器上，.5服务器则负责application业务逻辑，两服务器之间的静态文件可以通过rsync实时同步。rsync的配置不再给出，也可参考<a href="https://blog.csdn.net/pysense/article/details/99289090">本blog文章</a></p>
<p>若考虑对uWSGI做负载均衡，比如第二台uWSGI服务器192.168.100.4:9001;可以加入upstream并设定相关负载算法，若还考虑对nginx服务器进行负载均衡，则需要用keepalived，通过VIP对外透明服务，负载均衡的配置会更有趣。</p>
]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>uWSGI</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Centos7.5完整部署分布式Hadoop3.1.2</title>
    <url>/blog/2019/10/10/%E5%9F%BA%E4%BA%8ECentos7.5%E5%AE%8C%E6%95%B4%E9%83%A8%E7%BD%B2%E5%88%86%E5%B8%83%E5%BC%8FHadoop3.1.2/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文基于虚拟机以及有限的计算资源搭建了非HA模式下分布式的hadoop集群，主要是为了后续开发基于大数据的实时计算项目提供hadoop服务。</p>
<h3 id="1、相关安装包以及规划"><a href="#1、相关安装包以及规划" class="headerlink" title="1、相关安装包以及规划"></a>1、相关安装包以及规划</h3><p>考虑本地测试使用，这里所使用的三台服务器均有虚拟机创建，每台配置：1个vCPU+1G内存+9G硬盘，基本组件版本</p>
<table>
<thead>
<tr>
<th>Ip</th>
<th>角色</th>
<th>hadoop路径</th>
<th>Hostname</th>
<th>jdk路径</th>
<th>linux版本</th>
</tr>
</thead>
<tbody><tr>
<td>192.188.0.4</td>
<td>NameNode,Datanode,NodeManager</td>
<td>/opt/hadoop-3.1.2</td>
<td>nn</td>
<td>/opt/jdk1.8.0_161</td>
<td>Centos7.5</td>
</tr>
<tr>
<td>192.188.0.5</td>
<td>DataNode,ResourceManager,NodeManager，JobHistoryServer</td>
<td>/opt/hadoop-3.1.2</td>
<td>dn1</td>
<td>/opt/jdk1.8.0_161</td>
<td>Centos7.5</td>
</tr>
<tr>
<td>192.188.0.6</td>
<td>DataNode,Secondarynode,NodeManager</td>
<td>/opt/hadoop-3.1.2</td>
<td>dn2</td>
<td>/opt/jdk1.8.0_161</td>
<td>Centos7.5</td>
</tr>
</tbody></table>
<a id="more"></a>

<p>这里列出节点服务的基础介绍：</p>
<p>hadoop平台相关：</p>
<p>NameNode：</p>
<blockquote>
<p>接收用户操作请求<br>维护文件系统的目录结构<br>管理文件与block之间关系，block与datanode之间关系</p>
</blockquote>
<p>DataNode：</p>
<blockquote>
<p>存储文件<br>文件被分成block存储在磁盘上<br>为保证数据安全，文件会有多个副本</p>
</blockquote>
<p>Secondary NameNode：</p>
<blockquote>
<p>合并来自namenode的fsimage和edits文件来更新namenode的metedata</p>
</blockquote>
<p>yarn平台相关：<br>ResourceManager:</p>
<blockquote>
<p>集群中所有资源的统一管理和分配，它接受来自各个节点的NodeManager的资源汇报信息，并把这些信息按照一定的策略分配给各个应用程序，是整个yarn集群中最重要的组件之一。</p>
</blockquote>
<p>JobHistoryServer：</p>
<blockquote>
<p>历史服务器，可以通过历史服务器查看已经运行完成的Mapreduce作业记录，比如用了多少个Map、多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。默认情况下，历史服务器是没有启动的，需要进行参数配置才能启动</p>
</blockquote>
<p>NodeManager：</p>
<blockquote>
<p>运行在单个节点上的代理，管理hadoop集群中单个计算节点，它需要与相应用程序ApplicationMaster和集群管理者ResourceManager交互<br>从ApplicationMaster上接收有关Contioner的命令并执行<br>向ResourceManager汇报各个Container运行状态和节点健康状况，并领取有关的Container的命令并执行</p>
</blockquote>
<h3 id="2、设置hostname"><a href="#2、设置hostname" class="headerlink" title="2、设置hostname"></a>2、设置hostname</h3><p>分别对三个节点更改对应的hostname</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# vi /etc/hostname </span><br><span class="line">nn</span><br><span class="line">[root@dn1 ~]# vi /etc/hostname </span><br><span class="line">dn1</span><br><span class="line">[root@dn2 ~]# vi /etc/hostname </span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>



<p>配置域名解析，三个节点都需要配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.188.0.4  nn</span><br><span class="line">192.188.0.5 dn1</span><br><span class="line">192.188.0.6 dn2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 无需重启，直接ping主机名称</span></span><br><span class="line">[root@dn2 ~]# ping nn</span><br><span class="line">PING nn (192.188.0.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from nn (192.188.0.4): icmp_seq=1 ttl=64 time=0.322 ms</span><br><span class="line">64 bytes from nn (192.188.0.4): icmp_seq=2 ttl=64 time=0.347 ms</span><br></pre></td></tr></table></figure>

<h3 id="3、配置免密ssh"><a href="#3、配置免密ssh" class="headerlink" title="3、配置免密ssh"></a>3、配置免密ssh</h3><h4 id="3-1-对三台服务器设置ssh公钥"><a href="#3-1-对三台服务器设置ssh公钥" class="headerlink" title="3.1 对三台服务器设置ssh公钥"></a>3.1 对三台服务器设置ssh公钥</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn &#x2F;]# ssh-keygen -t rsa</span><br><span class="line"># 手动创建 authorized_keys文件</span><br><span class="line">[root@nn .ssh]# ls</span><br><span class="line">id_rsa  id_rsa.pub</span><br><span class="line">[root@nn .ssh]# cp id_rsa.pub authorized_keys</span><br></pre></td></tr></table></figure>
<p>其他两个节点同样操作</p>
<h4 id="3-2-在nn节点将自己公钥拷贝到其他两个节"><a href="#3-2-在nn节点将自己公钥拷贝到其他两个节" class="headerlink" title="3.2  在nn节点将自己公钥拷贝到其他两个节"></a>3.2  在nn节点将自己公钥拷贝到其他两个节</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 三个节点都需要操作</span><br><span class="line">[root@nn ~]# ssh-copy-id nn</span><br><span class="line">[root@nn ~]# ssh-copy-id dn1</span><br><span class="line">[root@nn ~]# ssh-copy-id dn2</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 测试免密登录</span><br><span class="line">[root@nn ~]# ssh dn1</span><br><span class="line">[root@nn ~]# ssh dn2</span><br></pre></td></tr></table></figure>

<h3 id="4、配置Java环境"><a href="#4、配置Java环境" class="headerlink" title="4、配置Java环境"></a>4、配置Java环境</h3><p>本项目中，java包、hadoop包、spark包都放在/opt目录下，三个节点都需配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># java包路径</span><br><span class="line">[root@nn jdk1.8.0_161]# pwd</span><br><span class="line">&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"># 配置环境变量</span><br><span class="line"># vi &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line">export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line"></span><br><span class="line"># 生效配置</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 查看版本</span><br><span class="line">[root@nn jdk1.8.0_161]# java -version</span><br><span class="line">java version &quot;1.8.0_161&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_161-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)</span><br></pre></td></tr></table></figure>

<h3 id="5、配置Hadoop环境"><a href="#5、配置Hadoop环境" class="headerlink" title="5、配置Hadoop环境"></a>5、配置Hadoop环境</h3><p>Hadoop路径<code>/opt/hadoop-3.1.2</code>，以下为hadoop文件目录的简要说明</p>
<table>
<thead>
<tr>
<th>Bin</th>
<th>Hadoop最基本的管理脚本和使用脚本的目录，这些脚本是sbin目录下管理脚本的基础实现 。用户可以直接使用这些脚本管理和使用Hadoop</th>
</tr>
</thead>
<tbody><tr>
<td>include</td>
<td>对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</td>
</tr>
<tr>
<td>etc</td>
<td>Hadoop的配置文件所在的目录，各类**.xml配置文件夹</td>
</tr>
<tr>
<td>lib</td>
<td>该目录下存放的是Hadoop运行时依赖的jar包，Hadoop在执行时会把lib目录下面的jar全部加到classpath中。</td>
</tr>
<tr>
<td>libexec</td>
<td>各个服务对用的shell配置文件所在的目录，可用于配置日志输出、启动参数（比如JVM参数）等基本信息。</td>
</tr>
<tr>
<td>sbin</td>
<td>Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的启动/关闭脚本，</td>
</tr>
<tr>
<td>share</td>
<td>Hadoop各个模块编译后的jar包所在的目录，也官方自带的doc手册</td>
</tr>
<tr>
<td>logs</td>
<td>（hadoop初始化之后才会自动生成）该目录存放的是Hadoop运行的日志，查看日志对寻找Hadoop运行错误非常有帮助。</td>
</tr>
<tr>
<td>namenode_dir</td>
<td>在hdfs-site.xml配置后，hadoop首次启动会创建该目录，目录下包含edit文件和fsimage</td>
</tr>
<tr>
<td>datanode_dir</td>
<td>在hdfs-site.xml配置后，hadoop首次启动会创建该目录：存放数据文件</td>
</tr>
</tbody></table>
<h4 id="5-1-配置hadoop-env-sh"><a href="#5-1-配置hadoop-env-sh" class="headerlink" title="5.1 配置hadoop-env.sh"></a>5.1 配置hadoop-env.sh</h4><p>给hadoop配置Java路径，三个节点都需要配置，但无需每台去设置，因为后面会把整个/opt/hadoop-3.1.2/etc/hadoop拷贝到另外两个dn节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line">vi hadoop-env.sh</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br></pre></td></tr></table></figure>

<h4 id="5-2-core-site-xml"><a href="#5-2-core-site-xml" class="headerlink" title="5.2 core-site.xml"></a>5.2 core-site.xml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- namenode用9000根datanode通信 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;nn:9000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--hadoop临时文件路径 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>



<h4 id="5-2-hdfs-site-xml"><a href="#5-2-hdfs-site-xml" class="headerlink" title="5.2 hdfs-site.xml"></a>5.2 hdfs-site.xml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-把dn2 设为secondary namenode，端口不能缺少 --&gt;</span><br><span class="line">          &lt;property&gt;</span><br><span class="line">                  &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">                 &lt;value&gt;dn2:50090&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;    </span><br><span class="line">    &lt;!-- namenode 上存储 hdfs 名字空间元数据--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- datanode 上数据块的物理存储位置--&gt;  </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 设置 hdfs 副本数量 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>



<h4 id="5-3-mapred-site-xml"><a href="#5-3-mapred-site-xml" class="headerlink" title="5.3 mapred-site.xml"></a>5.3 mapred-site.xml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定Yyarn运行--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;Yyarn&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">	&lt;value&gt;dn1:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定dn1作为jobhistory服务器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;dn1:19888&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 注意这里的路径不是Linux文件路径，而是hdfs文件系统上的路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;history&#x2F;done&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;history&#x2F;done_intermediate&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- mp所需要hadoop环境 --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;Yyarn.app.mapreduce.am.env&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.map.env&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.reduce.env&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>



<h4 id="5-4-yarn-site-xm"><a href="#5-4-yarn-site-xm" class="headerlink" title="5.4 yarn-site.xm;"></a>5.4 yarn-site.xm;</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific yarn configuration properties --&gt;</span><br><span class="line">    &lt;!-- 指定ResourceManager的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;dn1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;  </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>



<h4 id="5-5-workers"><a href="#5-5-workers" class="headerlink" title="5.5 workers"></a>5.5 workers</h4><p>三个节点都设为datanode，当然也生产环境中，负责数据物理文件存储DD不要跟DN放在同一台服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi etc&#x2F;hadoop&#x2F;workers </span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>

<h4 id="5-6-设置start-dfs-sh-和-stop-dfs-sh"><a href="#5-6-设置start-dfs-sh-和-stop-dfs-sh" class="headerlink" title="5.6 设置start-dfs.sh 和 stop-dfs.sh"></a>5.6 设置start-dfs.sh 和 stop-dfs.sh</h4><p>在/opt/hadoop-3.1.2/sbin/start-dfs.sh 文件开头</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi sbin/start-dfs.sh</span><br><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure>

<h4 id="5-7-设置start-yarn-sh-和-stop-yarn-sh"><a href="#5-7-设置start-yarn-sh-和-stop-yarn-sh" class="headerlink" title="5.7 设置start-yarn.sh 和 stop-yarn.sh"></a>5.7 设置start-yarn.sh 和 stop-yarn.sh</h4><p>都是在文件开头处添加</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi sbin/start-yarn.sh </span><br><span class="line">yarn_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">yarn_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>



<h4 id="5-8-将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置"><a href="#5-8-将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置" class="headerlink" title="5.8 将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置"></a>5.8 将hadoop包添加到linux环境变量，三个节点都需要加这个hadoop环境设置</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">export HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>



<p><strong>直接将以上的配置文件所在目录拷贝到另外两个节点上，避免繁琐配置</strong></p>
<p>[root@nn hadoop-3.1.2]# scp -r /opt/hadoop-3.1.2/etc/hadoop/   dn1:/opt/hadoop-3.1.2/etc/</p>
<p>[root@nn hadoop-3.1.2]# scp -r /opt/hadoop-3.1.2/sbin   dn1:/opt/hadoop-3.1.2/</p>
<h4 id="5-9-初始化hadoop文件系统"><a href="#5-9-初始化hadoop文件系统" class="headerlink" title="5.9 初始化hadoop文件系统"></a>5.9 初始化hadoop文件系统</h4><p>因为nn是作为namenode管理节点，因此只需在nn节点进行相应的格式化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn bin]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;bin</span><br><span class="line">[root@nn bin]# hdfs namenode -format</span><br><span class="line">****</span><br><span class="line">*** INFO common.Storage: Storage directory &#x2F;opt&#x2F;hadoop-3.1.2&#x2F;namenode has been successfully formatted.</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at nn&#x2F;192.188.0.4</span><br></pre></td></tr></table></figure>

<p>以上说明namenode格式化成功</p>
<h3 id="6-启动hadoop服务"><a href="#6-启动hadoop服务" class="headerlink" title="6 启动hadoop服务"></a>6 启动hadoop服务</h3><h4 id="6-1-在namenode上启动服务"><a href="#6-1-在namenode上启动服务" class="headerlink" title="6.1 在namenode上启动服务"></a>6.1 在namenode上启动服务</h4><p>一键启动所有：如果使用start-all.sh，表示把集群的所有配置的服务都启动，它会调用start-dfs.sh和start-yarn.sh</p>
<p>单个节点启动：使用start-dfs.sh和start-yarn.sh，这里要注意，比如nn节点是作为namenode节点，那么在nn节点执行start-dfs.sh，无需执行start-yarn.sh</p>
<p>网上绝大部分教程会教你用start-all.sh启用集群服务，但这不是官方的推荐方式，个人推荐在每个节点启动相应服务</p>
<p>nn节点：NameNode,Datanode,NodeManager，只需运行start-dfs.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>dn1节点：DataNode,ResourceManager,NodeManager，因为需要使用yarn服务，且作为ResourceManager节点（本身也是NodeManager）,需运行start-yarn.sh</p>
<p>此外：dn1节点还是作为yarn主节点的JobHistoryServer服务，还需通过命令<code>mapred --daemon start historyserver </code>启动之，启动JobHistoryServer后，可以在yarn的web服务直观查看每个job的运行历史，后面会给截图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# start-yarn.sh </span><br><span class="line">[root@dn1 sbin]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;sbin</span><br><span class="line">[root@dn1 sbin]# mapred --daemon start historyserver </span><br></pre></td></tr></table></figure>



<p>dn2节点：DataNode,Secondarynode,NodeManager，因为nn节点的hdfs-site.xml已经配置了dn2节点作为sn节点，那么nn节点启动服务时，就已经自动在dn2节点启动了Secondarynode进程。</p>
<p>查看各个节点服务进程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# jps</span><br><span class="line">9957 NameNode</span><br><span class="line">10553 Jps</span><br><span class="line">10092 DataNode</span><br><span class="line">10430 NodeManager</span><br><span class="line"></span><br><span class="line">[root@dn1 ~]# jps</span><br><span class="line">31792 DataNode</span><br><span class="line">32133 NodeManager</span><br><span class="line">32492 Jps</span><br><span class="line">31998 ResourceManager</span><br><span class="line">17428 JobHistoryServer</span><br><span class="line"></span><br><span class="line">[root@dn2 ~]# jps</span><br><span class="line">31105 NodeManager</span><br><span class="line">30898 DataNode</span><br><span class="line">31235 Jps</span><br><span class="line">31005 SecondaryNameNode</span><br></pre></td></tr></table></figure>

<p>也可通过查看web服务来确认NameNode服务和yarn服务<br>NN入口：<code>http://192.188.0.4:9870/</code><br><img src="https://img-blog.csdnimg.cn/20191010210825857.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>yarn入口：<code>http://192.188.0.5:8088</code><br><img src="https://img-blog.csdnimg.cn/20191010205253651.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>也可通过起一个python http服务查看hadoop自带的手册，手册html文件在</p>
<p><code>/opt/hadoop-3.1.2/share/doc/hadoop</code>，里面有index.html,故只需在该目录下，开启一个后台web 服务，即可在 浏览器打开其网页</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop]# ls</span><br><span class="line">api                                  hadoop-fs2img</span><br><span class="line">css                                  hadoop-gridmix</span><br><span class="line">dependency-analysis.html             hadoop-hdfs-httpfs</span><br><span class="line">***</span><br><span class="line">***</span><br><span class="line">hadoop-dist                          images</span><br><span class="line">hadoop-distcp                        index.html</span><br><span class="line">hadoop-extras                        project-reports.html</span><br><span class="line"></span><br><span class="line">[root@nn hadoop]# python -m SimpleHTTPServer 8000 &amp;</span><br></pre></td></tr></table></figure>

<h3 id="7、跑个wordcount-测试"><a href="#7、跑个wordcount-测试" class="headerlink" title="7、跑个wordcount 测试"></a>7、跑个wordcount 测试</h3><h4 id="7-1-在namenode节点上的hadoop文件系统的根目录路上创建一个目录"><a href="#7-1-在namenode节点上的hadoop文件系统的根目录路上创建一个目录" class="headerlink" title="7.1 在namenode节点上的hadoop文件系统的根目录路上创建一个目录"></a>7.1 在namenode节点上的hadoop文件系统的根目录路上创建一个目录</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建测试目录,可以使用</span><br><span class="line">[root@nn hadoop-3.1.2]# hadoop fs -mkdir &#x2F;app</span><br><span class="line"># 或者</span><br><span class="line">[root@nn hadoop-3.1.2]# hdfs dfs -mkdir &#x2F;app</span><br><span class="line"># 查看hadoop文件系统的根目录下的结构</span><br><span class="line">[root@nn hadoop-3.1.2]# hadoop fs -ls &#x2F;</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          ** &#x2F;app</span><br><span class="line">drwxr-xr-x   - root supergroup          ** &#x2F;word-count-app</span><br></pre></td></tr></table></figure>

<p>以上目录的创建，也会在datanode同步创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# hadoop fs -ls &#x2F;</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** &#x2F;app</span><br><span class="line">drwxr-xr-x   - root supergroup          0 ** &#x2F;word-count-app</span><br></pre></td></tr></table></figure>





<h4 id="7-2-hdfs常用命令"><a href="#7-2-hdfs常用命令" class="headerlink" title="7.2  hdfs常用命令"></a>7.2  hdfs常用命令</h4><p>具体详细命令用户，官网给出非常仔细的说明：</p>
<p><code>http://hadoop.apache.org/docs/r3.1.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">列出 hdfs 下的文件</span><br><span class="line">$ hdfs dfs -ls</span><br><span class="line">列出 hdfs &#x2F; 路径下的所有文件，文件夹  </span><br><span class="line">$ hdfs dfs -ls -R &#x2F;</span><br><span class="line">创建目录 &#x2F;app</span><br><span class="line">$ hdfs dfs -mkdir &#x2F;app</span><br><span class="line">列出 hsfs 名为 input 的文件夹中的文件</span><br><span class="line">$ hadoop dfs -ls app</span><br><span class="line">将 words.txt 上传到 hdfs 中</span><br><span class="line">$ hdfs dfs -put &#x2F;hadoop_test&#x2F;words.txt &#x2F;app</span><br><span class="line"></span><br><span class="line">将 hsdf 中的 words.txt 文件保存到本地</span><br><span class="line">$ hdfs dfs -get &#x2F;app&#x2F;words.txt &#x2F;hadoop_test&#x2F;words.txt</span><br><span class="line"></span><br><span class="line">删除 hdfs 上的 test.txt 文件</span><br><span class="line">$ hadoop dfs -rmr &#x2F;hadoop_test&#x2F;words.txt</span><br><span class="line"></span><br><span class="line">查看 hdfs 下 app 文件夹中的内容</span><br><span class="line">$ hadoop fs -cat app&#x2F;*</span><br><span class="line">进入安全模式</span><br><span class="line">$ hadoop dfsadmin –safemode enter</span><br><span class="line">退出安全模式</span><br><span class="line">$ hadoop dfsadmin -safemode leave</span><br><span class="line">报告 hdfs 的基本统计情况</span><br><span class="line">$ hadoop dfsadmin -report</span><br></pre></td></tr></table></figure>



<h4 id="7-1-将测试文件添加到hadoop系统的指定目录"><a href="#7-1-将测试文件添加到hadoop系统的指定目录" class="headerlink" title="7.1 将测试文件添加到hadoop系统的指定目录"></a>7.1 将测试文件添加到hadoop系统的指定目录</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# cat hadoop_example&#x2F;words.txt </span><br><span class="line">foo is foo</span><br><span class="line">bar is not bar</span><br><span class="line">hadoop file system is the infrastructure of bigdata </span><br><span class="line"></span><br><span class="line">[root@nn opt]# hdfs dfs -put hadoop_example&#x2F;words.txt &#x2F;app</span><br><span class="line"></span><br><span class="line">[root@nn opt]# hdfs dfs -ls &#x2F;app      </span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup         76 ** &#x2F;app&#x2F;words.txt</span><br></pre></td></tr></table></figure>
<p>也可以通过web端查看fs目录结构和文件内容，直观易用</p>
<p><img src="https://img-blog.csdnimg.cn/20191010204829139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="7-2-运行word-count-java示例程序"><a href="#7-2-运行word-count-java示例程序" class="headerlink" title="7.2 运行word count java示例程序"></a>7.2 运行word count java示例程序</h4><p>例程序在此路径：<code>/opt/hadoop-3.1.2/share/hadoop/mapreduce</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn mapreduce]# hadoop jar hadoop-mapreduce-examples-3.1.2.jar wordcount &#x2F;app &#x2F;result</span><br><span class="line"></span><br><span class="line">**,500 INFO mapreduce.Job:  map 50% reduce 0%</span><br><span class="line">**,653 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">**,685 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">**,715 INFO mapreduce.Job: Job job_1***** completed successfully</span><br><span class="line">**,837 INFO mapreduce.Job: Counters: 55</span><br></pre></td></tr></table></figure>

<p>以上表示成功运行一个计算实例</p>
<p>查看计算结果，计算放在hdfs文件系统/result目录下,其中 /result/part-r-00000为计算结果，可以查看其输出内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# hdfs dfs -cat &#x2F;result&#x2F;part-r-00000</span><br><span class="line">bar     2</span><br><span class="line">big     1</span><br><span class="line">data    1</span><br><span class="line">file    1</span><br><span class="line">foo     2</span><br><span class="line">hadoop  2</span><br><span class="line">infrastructure  1</span><br><span class="line">is      3</span><br><span class="line">not     1</span><br><span class="line">spark   2</span><br><span class="line">system  1</span><br><span class="line">the     1</span><br><span class="line">zookeeper       2</span><br></pre></td></tr></table></figure>

<p>当然，因为我们引入yarn调度框架，并且有dn1节点提供yarn服务，当然可以对此次map-reduce计算任务job在web端查看。<br><img src="https://img-blog.csdnimg.cn/20191010211317781.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>以上为完整的基于hadoop3.1.2真实集群并引入yarn管理的文章讨论，给出了完整部署流程和测试案例，保证本次部署过程的可行性。注意到本文的hadoop集群中还不是HA模式，生产环境需要部署HA模式，后面的文章中我们将引入Zookeeper，给出HA模式的部署过程（zk文章在本博客已经有深入的探讨，目的也是为了后面大数据架构部署）</p>
<h3 id="Trouble-shooting"><a href="#Trouble-shooting" class="headerlink" title="Trouble shooting"></a>Trouble shooting</h3><p>1、运行word count时，yarn提示任务虚拟运行内存不足</p>
<p>Container [pid=17786,containerID=container_****002_01_000003] is running 459045376B beyond the ‘VIRTUAL’ memory limit. Current usage: 61.8 MB of 1 GB physical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing container</p>
<p>回答这个问题前，首先要理解yarn集群中Container的概念</p>
<ul>
<li><p>在yarn的NodeManager节点上，会将集群中所有节点的CPU和内存的一定值抽离出来，组成一个“资源池”，例如资源池是100，这个资源池根据配置（例如设置大哥容器申请的资源最大值为10）可以分成多个Container（100/10=10个可供Job使用的容器），当Application（在MapReduce时期叫Job）提出申请时，就会分配相应的Container资源，因此Container其实是yarn中的一个动态资源分配的概念，其拥有一定的内存，核数，由RM分配给ApplicationMaster或者MapTask或者ReduceTask使用，这些task就在Container为基础的容器中运行起来。</p>
</li>
<li><p>通俗点说，Container就是“一组资源：内存+CPU”，它跟Linux Container没有任何关系，仅仅是yarn提出的一个概念，当有一个Application来想RM节点申请资源是，第一个Container用来跑ApplicationMaster，然后ApplicationMaster再申请一些Container来跑Mapper，之后再申请一些Container来跑Reducer。</p>
</li>
<li><p>当Mapper或者Reducer所需的“资源之一虚拟内存大于Container默认提供值时”，以上问题就会出现：beyond the ‘VIRTUAL’ memory limit.</p>
</li>
</ul>
<p>解决办法有两种</p>
<p>A、降低Mapper或者Reducer所需内存资源配置值，在mapred-site.xml 进行配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;每个Map任务的物理内存限制&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;200&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;每个Reduce任务的物理内存限制&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.java.opts&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx100m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.java.opts&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx200m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>





<p>B、配置RM针对单个Container能申请的最大资源或者RM本身能配置的最大内存</p>
<p>配置解释：单个容器可申请的最小与最大内存，Application在运行申请内存时不能超过最大值，小于最小值则分配最小值，例如在本文测试中，因计算任务较为简单，无需太多资源，故最小值设为50M，最大值设为100M</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;50&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>配置解释：NM的内存资源配置，主要是通过下面两个参数进行的</p>
<p>第一个参数：每个节点可用的最大内存，默认值为-1，代表着yarn的NodeManager占总内存的80%，本文中，物理内存为1G</p>
<p>第二个参数：NM的虚拟内存和物理内存的比率，默认为2.1倍</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>vmem-pmem-ratio的默认值为2.1，由于本机器中，每个节点的物理内存为1G，因此单个RM拿到最大虚拟内存为2.1G，从<code>2.5 GB of 2.1 GB virtual memory used. Killing container</code>,可知，Container申请的资源为2.5G，已经超过默认值2.1G，当改为3倍时，虚拟化够用，故解决了问题。</p>
<p>2、org.apache.hadoop.yarn.exceptions.yarnException:Unauthorized request to start container</p>
<p>出错原因：Hadoop集群（本文也包含yarn集群）中多个节点的时间不同步导致.</p>
<p>解决：修改多个节点的时间为相同的时间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> # 将硬件时间写到系统时间</span><br><span class="line">[root@dn1 ~]# hwclock -s </span><br><span class="line">保存时钟</span><br><span class="line">[root@dn1 ~]# clock -w</span><br></pre></td></tr></table></figure>









]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop集群</tag>
      </tags>
  </entry>
  <entry>
    <title>在hadoopHA节点上部署flume高可用组件</title>
    <url>/blog/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式</p>
<a id="more"></a>

<table>
<thead>
<tr>
<th>hadoop节点</th>
<th>数据源节点</th>
<th>角色</th>
</tr>
</thead>
<tbody><tr>
<td>nn</td>
<td>nn</td>
<td>NameNode,DataNode数据源</td>
</tr>
<tr>
<td>dn2</td>
<td>dn2</td>
<td>NameNode,DateNode数据源</td>
</tr>
<tr>
<td>dn1</td>
<td>dn1</td>
<td>DataNode,数据源</td>
</tr>
</tbody></table>
<p>其他hbase、hive等不再此表给出，因为前面的文件已有相关表格。</p>
<h4 id="1、flume-的基本介绍"><a href="#1、flume-的基本介绍" class="headerlink" title="1、flume 的基本介绍"></a>1、flume 的基本介绍</h4><h5 id="1-1-基本介绍"><a href="#1-1-基本介绍" class="headerlink" title="1.1 基本介绍"></a>1.1 基本介绍</h5><blockquote>
<p>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.  It uses a simple extensible data model that allows for online analytic application.</p>
</blockquote>
<p>简单来说：flume 是一个分布式、高可靠、高可用的用来收集、聚合、转移不同来源的大量日志数据到中央数据仓库的工具</p>
<p>目前flume最新版本为今年1月发布的 Flume 1.9.0，具体优化的内容：</p>
<blockquote>
<ul>
<li>Better SSL/TLS support</li>
<li>Configuration Filters to provide a way to inject sensitive information like passwords into the configuration</li>
<li>Float and Double value support in Context</li>
<li>Kafka client upgraded to 2.0</li>
<li>HBase 2 support</li>
</ul>
</blockquote>
<p>环境要求：</p>
<p>Java Runtime Environment - Java 1.8 or later</p>
<h5 id="1-2-数据流模型"><a href="#1-2-数据流模型" class="headerlink" title="1.2 数据流模型"></a>1.2 数据流模型</h5><p>这里以Flume收取web的日志再将其写入到hdfs作为数据流模型示例说明。<br>（需要注意的flume支持多级配置、扇入、扇出，这里仅介绍单级、单输入、单输出的模式）<br><img src="https://img-blog.csdnimg.cn/20191123154225919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>==Event==<br>Event是Flume定义的一个数据流传输的最小单元。Agent就是一个Flume的实例，本质是一个JVM进程，该JVM进程控制Event数据流从外部日志生产者那里传输到目的地（或者是下一个Agent）。<br>在Flume中，event表示数据传输的一个最小单位，从上图可以看出Agent就是Flume的一个部署实例， 一个完整的Agent中包含了三个组件Source、Channel和Sink，Source是指数据的来源和方式，Channel是一个数据的缓冲池，Sink定义了数据输出的方式和目的地。</p>
<p>==Source组件==<br>Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。（对于实时大数据项目，这个source其实就是不断滚动的log数据文件。）<br>Flume提供了各种source的实现，具体参考官方文档：<br>Flume Sources、Avro Source、Thrift Source、Exec Source、JMS Source、Taildir Source、Kafka Source、NetCat TCP Source、NetCat UDP Source、Syslog Sources、HTTP Source、Custom Source等</p>
<p>==Channel组件==<br>Channel是连接Source和Sink的组件，可以看作一个数据的缓冲区，它可以将事件暂存到内存中也可以持久化到磁盘上， 直到Sink处理完该事件。flume提供多个channel提供对event数据的缓存：<br>Memory Channel、JDBC Channel、Kafka Channel、File Channel、Spillable Memory Channel、Pseudo Transaction Channel、Custom Channel<br>（在本大数据实时项目中，使用Memory Channel即可，生产环境需要根据具体需求而定）</p>
<p>==Sink组件==<br>Sink从Channel取出event数据，并将其写入到文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。<br>Flume也提供了各种sink的实现，具体参考官方说明：<br>HDFS Sink、Hive Sink、Logger Sink、Avro Sink、Thrift Sink、IRC Sink、File Roll Sink、Null Sink、HBaseSinks、AsyncHBaseSink、ElasticSearchSink、Kite Dataset Sink、Kafka Sink、HTTP Sink、Custom Sink<br>（在实时大数据项目中，用sink配置将数据写入kafka sink）</p>
<p>以上的数据模型的详细介绍可以参考中文翻译文档：<a href="https://flume.liyifeng.org/">地址</a><br>(注意：该翻译文档对应flume1.8版本的内容，若想查阅最新的flume，参考官网1.9原文)</p>
<h4 id="2、flume的配置文件说明"><a href="#2、flume的配置文件说明" class="headerlink" title="2、flume的配置文件说明"></a>2、flume的配置文件说明</h4><p>flume配置遵循Java properties文件格式的文本文件。一个或多个Agent配置可放在同一个配置文件里。配置文件包含Agent的source，sink和channel的各个属性以及他们的数据流连接。</p>
<p>完整的配置流程如下：<br>命名各个组件（定义流）–&gt;连接各个组件–&gt;配置各个组件的属性–&gt;启动Agent</p>
<h5 id="2-1-配置过程"><a href="#2-1-配置过程" class="headerlink" title="2.1 配置过程"></a>2.1 配置过程</h5><p>这里以后面文章——flume整合kafka的配置文件说明。<br>（有较多的blog文章会照搬a1.sources=r1,a1.sinks=k1,a1.channels=c1这样的写法，a1?r1?k1?c1?到底指代什么？所以建议要参考官方文档，否则一头雾水？当然熟悉flume后，可以使用短命名，设置配置属性字符串不会显得太长）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、列出Agent的所有Source、Channel、Sink</span></span><br><span class="line">&lt;Agent&gt;.sources = &lt;Source&gt;</span><br><span class="line">&lt;Agent&gt;.sinks = &lt;Sink&gt;</span><br><span class="line">&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接各个组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Source的关联</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Sink的关联</span></span><br><span class="line">&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、为每个组件配置属性，这些属性就是flume的性能参数，控制flume的各种工作方式，调优配置就在这部分了。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sources</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> channels</span></span><br><span class="line">&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sinks</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br></pre></td></tr></table></figure>

<p>对于本blog的实时大数据项目的配置，Agent名字为：agent_log从本地服务器读取log数据文件，使用内存channel缓存，然后通过kafka Sink从channel读取后发送到kafka集群，它的配置文件应该这样配：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、定义Agent的所有<span class="built_in">source</span>、sink和channel组件</span></span><br><span class="line">agent_log.sources = log-src</span><br><span class="line">agent_log.sinks = kafka-sink</span><br><span class="line">agent_log.channels = log-mem-channel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接三个组件</span></span><br><span class="line">agent_log.sources.log-src.channels =log-mem-channel    # 指定与source:log-src相连接的channel是log-mem-channel</span><br><span class="line">agent_foo.sinks.kafka-sink.channel = log-mem-channel   # 指定与sink:kafka-sink相连接的channel是log-mem-channel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、配置各个组件的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置avro-AppSrv-source的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> log-src 的类型是spooldir，官方建议不要使用tail -F抽取数据文件因会出现丢失</span></span><br><span class="line">agent_log.sources.log-src.type = spooldir         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置<span class="built_in">source</span>数据文件的路径</span></span><br><span class="line">agent_log.sources.log-src.spoolDir = /opt/flume_agent/web_log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置log-mem-channel的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> channel的类型是内存channel</span></span><br><span class="line">agent_log.channels.log-mem-channel.type = memory  </span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> channel的最大容量是1000</span></span><br><span class="line">agent_log.channels.log-mem-channel.capacity = 1000         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>和sink每次从channel写入和读取的Event数量</span></span><br><span class="line">agent_log.channels.log-mem-channel.transactionCapacity = 100    </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置kafka-sink的属性，将数据写入到kafka集群指定topic，实现Flume与Kafka集成</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接channel到kafkasink</span></span><br><span class="line">agent_log.sinks.kafka-sink.channel = log-mem-channel </span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定kafka sink</span></span><br><span class="line">agent_log.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka存放数据的topic</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.topic = webtopic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka sink 使用的 kafka 集群的实例列表</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每批要发送到kafka的消息数量</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.flumeBatchSize = 20</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在成功写入之前，要求有1个副本必须确认消息，保证数据一致性</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.producer.acks = 1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>通过上面的配置，就形成了[log-src]-&gt;[log-mem-channel]-&gt;[log-sink]的数据流，这将使Event通过内存channel（log-mem-channel）从log-src流向Kafka-sink，从而实现数据源-flume-kafka的实时数据流动。</p>
<h4 id="3、单点flume-agent测试"><a href="#3、单点flume-agent测试" class="headerlink" title="3、单点flume agent测试"></a>3、单点flume agent测试</h4><p>本节主要在name节点上部署单个flume agent，用于测试单agent的使用。<br>数据流向：<br>手动写日志内容—&gt;flume spooldir抽取—&gt;flume sink到hadoop集群文件系统上</p>
<h5 id="3-1-基本安装"><a href="#3-1-基本安装" class="headerlink" title="3.1 基本安装"></a>3.1 基本安装</h5><p>个人习惯将所有的hadoop组件都放置在同一个dir下，方便管理，如下所示</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hive-3.1.2       xcall.sh       </span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    scala-2.13.1         </span><br><span class="line">hbase-2.1.7   mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br></pre></td></tr></table></figure>
<p>配置flume-env.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">[root@nn conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line">vi flume-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置java1.8的路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> JAVA_HOME=/opt/jdk1.8.0_16</span></span><br></pre></td></tr></table></figure>
<p>这里的配置要注意的点：如果已经在系统的环境变量配置JAVA_HOME，那么flume-env.sh可以不用再配置java路径</p>
<p>配置flume-conf.properties<br>这里就是用于配置flume agent的文件。有了第2章节的介绍后，这里有关source、channel、sink配置则相对简单，因此可以使用短字符进行配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>数据源为本地某个文件目录，flume监听这个目录下日志文件</span></span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里不需要写成web_log/</span></span><br><span class="line">a1.sources.r1.spoolDir = /opt/flume_log/web_log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录，hdapp为hadoop集群名称，不需要加入端口，否则flume无法写入，</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType=SequenceFile</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat=Writable</span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放在hdfs的文件文件命名方式，其实还有更详细的配置，这里仅给出简单示例，具体可参考官网。</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从flume过来的数据，每128M分割成一个文件</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128000000  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终在hdfs的文件名称为：%Y-%m-%d.TimeStamp.txt</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这里在配置source.type要注意的是，配成spooldir类型后：允许把要收集的文件放入磁盘上的某个指定目录。它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来。数据解析逻辑是可配置的。<br>与Exec Source不同，Spooling Directory Source是可靠的，即使Flume重新启动或被kill，也不会丢失数据。<br>但这种可靠有一定的代价和限制：指定目录中的文件必须是不可变的、唯一命名的。Flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常：<br>如果文件在写入完成后又被再次写入新内容，Flume将向其日志文件（这是指Flume自己logs目录下的日志文件）打印错误并停止处理。如果在以后重新使用以前的文件名，Flume将向其日志文件打印错误并停止处理。<br>为了避免上述问题，最好在生成新文件的时候文件名加上时间戳，可以通过加入属性项实现：a1.sinks.k1.hdfs.useLocalTimeStamp = true</p>
<h5 id="3-2-启动flume-agent进程"><a href="#3-2-启动flume-agent进程" class="headerlink" title="3.2 启动flume agent进程"></a>3.2 启动flume agent进程</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 命令含义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commands:</span><br><span class="line">  help                      display this help text</span><br><span class="line">  agent                     run a Flume agent</span><br><span class="line">  avro-client               run an avro Flume client</span><br><span class="line">  version                   show Flume version info</span><br><span class="line"> global options:</span><br><span class="line">    --conf,-c &lt;conf&gt;          use configs in &lt;conf&gt; directory</span><br><span class="line"> agent options:</span><br><span class="line">    --name,-n &lt;name&gt;          the name of this agent (required)</span><br><span class="line">  --conf-file,-f &lt;file&gt;     specify a config file (required if -z missing)</span><br></pre></td></tr></table></figure>
<p>创建数据文件，测试flume 能否成功把目录下的文件推到hdfs指定目录上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn web_log]# pwd</span><br><span class="line">/opt/flume_log/web_log</span><br><span class="line">[root@nn web_log]#  vi log.txt</span><br><span class="line">aaa</span><br><span class="line">bbb</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当文件创建后，发现该log.txt被命名为log.txt.COMPLETED，说明已经被flume 读取过</span></span><br><span class="line">[root@nn web_log]# ls</span><br><span class="line">log.txt.COMPLETED</span><br></pre></td></tr></table></figure>
<p>hdfs上可看到数据文件已经上传到到/flume/web_log（这里打码了时间）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn web_log]# hdfs dfs -ls /flume/web_log </span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup        161 **** /flume/web_log/2019-**-**.15**0.txt</span><br></pre></td></tr></table></figure>

<h5 id="3-3-将source-type配成tail-F"><a href="#3-3-将source-type配成tail-F" class="headerlink" title="3.3 将source.type配成tail F"></a>3.3 将source.type配成tail F</h5><p>Spooling Directory Source是可靠的，它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来，当此种方式不适合本blog后面开发的实时大数据项目需求。具体说明如下：<br>本blog后面开发的实时大数据项目需求：<br>实时抽取access.log的访问日志，access.log每插入一行，flume 就会把它实时sink到hdfs上（本文用于测试所以先sink到hdfs，若已经到开发阶段，这里会改为sink到kalka集群上）。<br>对于spooldir模式，当log.txt被sink后其文件名变为log.txt.COMM，若继续向log.txt.COMPLETED append数据行，flume不会再抽取该文件，也说明无法把新来的数据sink到hdfs上，显然不符合需求。<br>source需做以下调整，使用exec source：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改<span class="built_in">source</span> <span class="built_in">type</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>
<p>这里sink的文件滚动策略很重要，若配置不当，flume sink会在hdfs不断滚动生成多个小文件，例如access.log新增一行，触发flume sink在hdfs新增一个对应的文件。<br>以下的配置：access.log在hdfs存放的形式为：<br>/flume/web_log/2019-05-31.1579*.txt<br>每达到128M则开始滚动新建一个文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128M</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br></pre></td></tr></table></figure>

<p>但exec source方式也有缺点：会丢失数据，例如当flume 挂了重启，之前进来的日志行将不会被重启后flume抽取到，正官方的提示：<br>The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost.</p>
<p>这种数据丢失其实还可以接受，毕竟大部分日志收集应用场景还没到高级事务的严格标准，而且服务器集群运行以及进程运行稳定，即使宕机、断电再重启，也只是一小部分日志行丢失。</p>
<p>==测试结果：==<br>启动flume agent，并将日志实时打印在shell</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn flume-1.9.0]#  bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将日志追加新数据行</span></span><br><span class="line">[root@nn web_log]# echo &#x27;test&#x27;&gt;&gt;access.log </span><br></pre></td></tr></table></figure>
<p>在hdfs上，存放的文件会以文件名.txt.tmp形式保持打开状态，供flume实时写入，若达到滚动条件，则会生成日期+时间戳.txt的数据文件，再新建另外一个日期+时间戳.txt.tmp文件。</p>
<p>至此，已完成flume的部署，下一步，在三个节点上配置高可用的flume集群。</p>
<h4 id="4、flume高可用配置"><a href="#4、flume高可用配置" class="headerlink" title="4、flume高可用配置"></a>4、flume高可用配置</h4><p>Flume高可用又称Flume NG高可用，NG：Next Generation。<br>flume高可用的实现思路比较清晰：多个节点flume agent  avro sink 到 多个flume collector avro source上，这些flume collector 会有优先级，优先级高的collector负责把数据sink到hdfs或者kafka上。因为agent和collector是多节点运行，在agent端：某个agent挂了，还有其他agent工作；在collecor端，某个collector挂了，还有其他collector继续工作。<br>架构图如下：<br><img src="https://img-blog.csdnimg.cn/20191124103515197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">各个节点规划，考虑到测试虚拟机资源有限，其中两个节点都分布运行agent和collector进程。<br>| 节点 |  flume 角色|<br>|–|–|<br>| nn | agent1，collector 1|<br>| dn1 | agen2 |<br>| dn2 | agent3，collector2 |</p>
<h5 id="4-1-三个agent的flume配置"><a href="#4-1-三个agent的flume配置" class="headerlink" title="4.1  三个agent的flume配置"></a>4.1  三个agent的flume配置</h5><p>三个agent的配置其实都一样，不同的部分：每个agent命名不同。<br>在nn节点的/opt/flume-1.9.0/conf新建一个avro-agent.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出agent1的组件，sinks有两个，分别去到collector1和collector2</span></span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>属性</span></span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel</span></span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector1</span></span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = nn</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector2</span></span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = dn2</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建sink groups，将多个sinks绑定为一个组，agent会向这些组sink 数据，将k1和k2设置负载均衡模式，也可以设置为failover模式，本文使用load_balance</span></span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">agent1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agent1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">agent1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">agent1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> failover模式，只有collector1工作。仅当collector1挂了后，collector2才能启动服务。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> agent1.sinkgroups.g1.processor.type = failover</span></span><br><span class="line"><span class="meta">#</span><span class="bash">值越大，优先级越高，collector1优先级最高</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k1 = 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k2 = 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">发生异常的sink最大故障转移时间（毫秒），这里设为10秒</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.maxpenalty = 10000</span></span><br></pre></td></tr></table></figure>
<p>将avro-agent.properties拷贝到dn1和dn2节点，agent1这个名字可改，可不改。</p>
<h5 id="4-2-配置-collector"><a href="#4-2-配置-collector" class="headerlink" title="4.2  配置 collector"></a>4.2  配置 collector</h5><p>分别在nn和dn2节点的/opt/flume-1.9.0/conf新建一个avro-collector.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在dn2节点上，则改为collector2，不改也没关系，这里只是为了区分两个collector</span></span><br><span class="line">collector1.sources = r1</span><br><span class="line">collector1.sinks = k1</span><br><span class="line">collector1.channels = c1</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义<span class="built_in">source</span>：这里的<span class="built_in">source</span>配成avro，从而连接agent端sink avro</span></span><br><span class="line">collector1.sources.r1.channels = c1</span><br><span class="line">collector1.sources.r1.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span>的属性：dn2节点需改为dn2</span></span><br><span class="line">collector1.sources.r1.bind = nn</span><br><span class="line">collector1.sources.r1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">定义channel</span></span><br><span class="line">collector1.channels.c1.type = memory</span><br><span class="line">collector1.channels.c1.capacity = 1000</span><br><span class="line">collector1.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">定义sinks：由collector将数据event推到hdfs上</span></span><br><span class="line">collector1.sinks.k1.channel=c1</span><br><span class="line">collector1.sinks.k1.type=hdfs</span><br><span class="line">collector1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">collector1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">collector1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">collector1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">collector1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">collector1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<h5 id="4-3-测试flume高可用"><a href="#4-3-测试flume高可用" class="headerlink" title="4.3 测试flume高可用"></a>4.3 测试flume高可用</h5><p>在nn节点和dn2节点启动各自的collector</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">nn节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector1，所以这里启动的--name 为collector1</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">dn2节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector2，所以这里启动的--name 为collector2</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>


<p>在nn、dn1和dn2节点启动各自的agent，在shell可以看到以下agent 进程打印的信息，说明三个agent都可以连接到两个collector的source组件k1和k2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k1 started.</span><br><span class="line"> ......</span><br><span class="line">Monitored counter group for type: SINK, name: k2: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k2 started.</span><br></pre></td></tr></table></figure>
<p>在nn节点上的access.log新增信息 echo ‘foo’ &gt;&gt;access.log后，在hdfs上可以看到***.txt.tmp文件可以相应的文件内容<br>停止collector1经常，此时测试collector2可以正常接替服务。</p>
<p>至此，已完成本文内容。下一篇文章为Hadoop引入Kafka组件，在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是本文测试的hdfs。</p>
]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume高可用</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Docker单机部署ZooKeeper集群</title>
    <url>/blog/2019/09/04/%E5%9F%BA%E4%BA%8EDocker%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2ZooKeeper%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面的文章部署zk服务，直接在裸机上部署，较为不便，现在很多服务如果不做docker化，无论在故障恢复、运维都增加很大困难，无法做到自动化部署，这种低效率的IT运营模式是比较难接受的，对于我们开发而已，必须是一键式优雅部署，所以本篇文章采用docker方式部署zk集群，可以从中对比裸机部署过程的不同以及优势</p>
<h3 id="1、部署docker和docker-compose"><a href="#1、部署docker和docker-compose" class="headerlink" title="1、部署docker和docker-compose"></a>1、部署docker和docker-compose</h3><p>参考本博客文章：<a href="https://blog.csdn.net/pysense/article/details/100547816">链接</a></p>
<h3 id="2、部署zookeeper集群"><a href="#2、部署zookeeper集群" class="headerlink" title="2、部署zookeeper集群"></a>2、部署zookeeper集群</h3><p>&#8195;&#8195;拉取zk镜像，可以dockerhub上面看下目前的zk官方镜像的tag有什么版本，默认是latest，接着是3.5.5以及3.4.14，这里用的stable版本3.4.14</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 opt]# docker pull zookeeper:3.4.14</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在宿主机上新建一个存放docker集群zk服务器目录（仅为了方便管理），并在该目录下新建一个compose配置文件</p>
<a id="more"></a>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# pwd</span><br><span class="line">/opt/zk_docker_cluster</span><br><span class="line">[root@dn2 zk_docker_cluster]# vi docker-compose.yml</span><br><span class="line">version: &#x27;3.3&#x27;</span><br><span class="line">services:</span><br><span class="line">  zoo1:</span><br><span class="line">    # 使用zookeeper:3.4.14镜像，加上tag标签</span><br><span class="line">    image: zookeeper:3.4.14</span><br><span class="line">    restart: always</span><br><span class="line">    hostname: zoo1</span><br><span class="line">    container_name: zk1</span><br><span class="line">    ports:</span><br><span class="line">      - 2181:2181</span><br><span class="line">    volumes:</span><br><span class="line">    # 宿主机目录路径无需手工创建，docker-compose有权限进行自行创建挂载的目录路径    </span><br><span class="line">      - /opt/zk_docker_cluster/zoo1/data:/data</span><br><span class="line">      - /opt/zk_docker_cluster/zoo1/datalog:/datalog</span><br><span class="line">      - /opt/zk_docker_cluster/zoo1/logs:/logs</span><br><span class="line">      </span><br><span class="line">    environment:</span><br><span class="line">      ZOO_MY_ID: 1</span><br><span class="line">			ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class="line"></span><br><span class="line">  zoo2:</span><br><span class="line">    image: zookeeper</span><br><span class="line">    restart: always</span><br><span class="line">    hostname: zoo2</span><br><span class="line">    container_name: zk2</span><br><span class="line">    ports:</span><br><span class="line">      - 2182:2181</span><br><span class="line">    volumes:</span><br><span class="line">      - /opt/zk_docker_cluster/zoo2/data:/data</span><br><span class="line">      - /opt/zk_docker_cluster/zoo2/datalog:/datalog</span><br><span class="line">      - /opt/zk_docker_cluster/zoo2/logs:/logs      </span><br><span class="line">    environment:</span><br><span class="line">      ZOO_MY_ID: 2</span><br><span class="line">			ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class="line"></span><br><span class="line">  zoo3:</span><br><span class="line">    image: zookeeper</span><br><span class="line">    restart: always</span><br><span class="line">    hostname: zoo3</span><br><span class="line">    container_name: zk3</span><br><span class="line">    ports:</span><br><span class="line">      - 2183:2181</span><br><span class="line">    volumes:</span><br><span class="line">      - /opt/zk_docker_cluster/zoo3/data:/data</span><br><span class="line">      - /opt/zk_docker_cluster/zoo3/datalog:/datalog</span><br><span class="line">      - /opt/zk_docker_cluster/zoo3/logs:/logs      </span><br><span class="line">    environment:</span><br><span class="line">      ZOO_MY_ID: 3</span><br><span class="line">			ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>配置文件需要注意的地方</p>
<p>在运行前，可以用<code>docker-compose -f docker-compose.yml config </code>检查配置文件是否正确</p>
<p>1）version 版本号不能随便改，例如这里改为1.0，提示不支持</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# docker-compose -f docker-compose.yml config </span><br><span class="line">ERROR: Version in &quot;./docker-compose.yml&quot; is unsupported. You might be seeing this error because you&#x27;re using the wrong Compose file version. Either specify a supported version (e.g &quot;2.2&quot; or &quot;3.3&quot;) and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.</span><br></pre></td></tr></table></figure>

<p>2）注意yaml语法的层次表达</p>
<p>例如在这里，故意把zoo1放置在service同层次上，引起解析出错，所以在编排zk的配置时，要注意这些细节</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ERROR: yaml.parser.ParserError: while parsing a block mapping</span><br><span class="line">  in &quot;.&#x2F;docker-compose .yml&quot;, line 1, column 1</span><br><span class="line">expected &lt;block end&gt;, but found &#39;&lt;block mapping start&gt;&#39;</span><br><span class="line">  in &quot;.&#x2F;docker-compose .yml&quot;, line 17, column 3</span><br></pre></td></tr></table></figure>

<p>==3) 注意到docker-compose yml跟裸机部署集群的不同==</p>
<ul>
<li>例如hostname：</li>
</ul>
<p>在docker中，无需要指明具体的ip地址，因为docker使用其内部私网为zk服务自动分配私网ip，而且自动DNS解析主机名，因此配置文件可以直接用zoo1这样的主机名</p>
<p>而在裸机部署中，裸机自己的网络设置需要指定具体IP地址，如果zoo.cfg配置用了主机名代替服务器IP，那么要求裸机网卡设定的DNS需支持zk网段的主机名解析</p>
<ul>
<li>再例如设定集群的server.n：</li>
</ul>
<p>在docker-compose里，直接主机名，server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 ，前面已经提过，docker内部其实已经对三个zk容器都分配相应的私网地址，通过以下命令查看：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出所有docker容器IP</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# docker inspect --format=&#x27;&#123;&#123;.Name&#125;&#125; - &#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27; $(docker ps -aq)</span><br><span class="line">/zk1 - 172.18.0.4</span><br><span class="line">/zk3 - 172.18.0.2</span><br><span class="line">/zk2 - 172.18.0.3</span><br><span class="line"><span class="meta">#</span><span class="bash"> 也可以用docker inspect zk2 查看容器内部具体的信息，这里截取一部分</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# docker inspect zk2</span><br><span class="line">        &quot;HostConfig&quot;: &#123;</span><br><span class="line">            &quot;Binds&quot;: [</span><br><span class="line">               # datalog:rw，说明docker对宿挂载的宿主机有读写权限</span><br><span class="line">                &quot;/opt/zk_docker_cluster/zoo2/datalog:/datalog:rw&quot;,</span><br><span class="line">                &quot;/opt/zk_docker_cluster/zoo2/data:/data:rw&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;ContainerIDFile&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LogConfig&quot;: &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;json-file&quot;,</span><br><span class="line">                &quot;Config&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;NetworkMode&quot;: &quot;zk_docker_cluster_default&quot;,</span><br><span class="line">            &quot;PortBindings&quot;: &#123;</span><br><span class="line">                # 绑定宿主机的端口号</span><br><span class="line">                &quot;2181/tcp&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;HostIp&quot;: &quot;&quot;,</span><br><span class="line">                        # zoo2容器内部的zk服务监听端口号</span><br><span class="line">                        &quot;HostPort&quot;: &quot;2182&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            </span><br><span class="line">            .......</span><br><span class="line">            </span><br><span class="line">            &quot;Networks&quot;: &#123;</span><br><span class="line">                &quot;zk_docker_cluster_default&quot;: &#123;</span><br><span class="line">                    &quot;IPAMConfig&quot;: null,</span><br><span class="line">                    &quot;Links&quot;: null,</span><br><span class="line">                    &quot;Aliases&quot;: [</span><br><span class="line">                        &quot;0595457ea13d&quot;,</span><br><span class="line">                        # hostname主机名</span><br><span class="line">                        &quot;zoo2&quot;</span><br><span class="line">                    ],</span><br><span class="line">                    &quot;NetworkID&quot;: &quot;46f7dbb34f0eefb1181729aeaaf6a1080d64a46fdba935b21d5e37a3b1aea34e&quot;,</span><br><span class="line">                    &quot;EndpointID&quot;: &quot;9dd1d2726ba6850ab851f0227fb04aa1a027c35e84d36cd308208a4d4fb42f7d&quot;,</span><br><span class="line">                    &quot;Gateway&quot;: &quot;172.18.0.1&quot;,</span><br><span class="line">                    &quot;IPAddress&quot;: &quot;172.18.0.3&quot;,</span><br><span class="line">                    &quot;IPPrefixLen&quot;: 16,</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>docker内部的私网段可以在宿主机上<code>ip a</code> 命令查看到，这是个docker的网桥网络，</p>
<p>地址池：172.18.0.1/16</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">8: br-46f7dbb34f0e: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:82:51:27:a6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-46f7dbb34f0e</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:82ff:fe51:27a6/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>三个zk容器服务分别从这个地址池获取三个ip，网关为172.18.0.1，相当于三台独立服务器，因此在server.n设置端口都可以指定为2888:3888相同端口，也即</p>
<p>zoo1:2888:3888等于172.18.0.4:2888:3888<br>zoo2:2888:3888等于172.18.0.3:2888:3888</p>
<p>zoo3:2888:3888等于172.18.0.2:2888:3888</p>
<p>总之，docker内部出色的网络结构设计，使得管理员从相对繁琐的网络配置解放出来。</p>
<p>而在单台裸机部署集群的配置中，则要指明ip（若有dns或者配置主机名解析，可无须指定IP地址）以及不同的管理端口号（使用不同端口是防止在同一服务器上端口冲突）：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server.1=192.168.4.100:42182:42183</span><br><span class="line">server.2=192.168.4.100:42184:42185</span><br><span class="line">server.3=192.168.4.100:42186:42187</span><br></pre></td></tr></table></figure>



<p><strong>启动docker-compose</strong></p>
<p>注意：所有的操作都应该在对应的docker-compse项目下进行，这是因为命令docker-compose自动读取本目录下的docker-compose.yml配置，注意这里仅当配置文件名为默认值<code>docker-compose.yml</code>，运行docker-compose命令才无需传入配置文件，否则如果项目目录下，yml配置文件为其它名字，例如</p>
<p>zk_docker_cluster.yml，每次执行docker-compose命令都需要指定配置文件：</p>
<p><code>docker-compose -f zk_docker_cluster.yml up -d</code></p>
<p>改了默认文件名的配置文件，在执行命令没传人配置文件，docker-compose提示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# ls</span><br><span class="line">zk_docker_cluster.yml  zoo1  zoo2  zoo3</span><br><span class="line">[root@dn2 zk_docker_cluster]# docker-compose ps</span><br><span class="line">ERROR: </span><br><span class="line">        Can&#x27;t find a suitable configuration file in this directory or any</span><br><span class="line">        parent. Are you in the right directory?</span><br><span class="line"></span><br><span class="line">        Supported filenames: docker-compose.yml, docker-compose.yaml</span><br><span class="line"><span class="meta">#</span><span class="bash"> 它这里提示在当前目录或者docker-compose的默认目录，都没有砸到配合文件，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 支持两种使用默认值命名的文件：docker-compose.yml, docker-compose.yaml</span></span><br></pre></td></tr></table></figure>



<p>执行相关命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# docker-compose  up -d</span><br><span class="line">Starting zk2 ... done</span><br><span class="line">Creating zk1 ... done</span><br><span class="line">Creating zk3 ... done</span><br><span class="line"></span><br><span class="line">[root@dn2 zk_docker_cluster]# docker-compose stop</span><br><span class="line">Stopping zk1 ... done</span><br><span class="line">Stopping zk3 ... done</span><br><span class="line">Stopping zk2 ... done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看zk_docker_cluster目录结构，目录路径由docker根据compose配置自动创建，无需手工预先创建</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# tree </span><br><span class="line">.</span><br><span class="line">├── docker-compose.yml</span><br><span class="line">├── zoo1</span><br><span class="line">│   ├── data</span><br><span class="line">│   │   ├── myid</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   │       ├── acceptedEpoch</span><br><span class="line">│   │       ├── currentEpoch</span><br><span class="line">│   │       ├── snapshot.0</span><br><span class="line">│   │       └── snapshot.400000000</span><br><span class="line">│   ├── datalog</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   └── logs</span><br><span class="line">├── zoo2</span><br><span class="line">│   ├── data</span><br><span class="line">│   │   ├── myid</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   │       ├── acceptedEpoch</span><br><span class="line">│   │       ├── currentEpoch</span><br><span class="line">│   │       ├── snapshot.0</span><br><span class="line">│   │       └── snapshot.400000000</span><br><span class="line">│   ├── datalog</span><br><span class="line">│   │   └── version-2</span><br><span class="line">│   └── logs</span><br><span class="line">└── zoo3</span><br><span class="line">    ├── data</span><br><span class="line">    │   ├── myid</span><br><span class="line">    │   └── version-2</span><br><span class="line">    │       ├── acceptedEpoch</span><br><span class="line">    │       ├── currentEpoch</span><br><span class="line">    │       ├── snapshot.0</span><br><span class="line">    │       └── snapshot.400000000</span><br><span class="line">    ├── datalog</span><br><span class="line">    │   └── version-2</span><br><span class="line">    └── logs</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可以直接进入容器内部查看</span></span><br><span class="line">[root@dn2 ~]# docker exec -it zk1 /bin/bash</span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在zk容器内部使用Cli登录并创建节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create /foo 1</span><br><span class="line">Created /foo</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create /app_conf 1</span><br><span class="line">Created /app_conf</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] ls /</span><br><span class="line">[zookeeper,app_conf,foo]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建临时顺序节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] create -e -s /foo 1</span><br><span class="line">Created /foo0000000001</span><br><span class="line">[zk: localhost:2181(CONNECTED) 7] create -e -s /foo 1</span><br><span class="line">Created /foo0000000002</span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] create -e -s /foo 1</span><br><span class="line">Created /foo0000000003</span><br></pre></td></tr></table></figure>



<p>在zk的docker容器内部，指定zk容器ip进入相应的服务</p>
<p>在前面已经给出三个zk服务在docker内部分配的私网IP，若想进入指定的zk容器，则需要用到这些私网网IP</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 三个zk服务器在docker内部分配到的IP</span></span><br><span class="line">(docker ps -aq)</span><br><span class="line">/zk3 - 172.18.0.3</span><br><span class="line">/zk1 - 172.18.0.4</span><br><span class="line">/zk2 - 172.18.0.2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 先选一个容器进入其内部，例如其内部zk client环境连接到其他zk服务实例，</span></span><br><span class="line">[root@dn2 bin]# docker exec -it zk1 /bin/bash</span><br><span class="line">root@zoo1:/zookeeper-3.4.14# </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接zk1服务</span></span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkCli.sh -server 172.18.0.4</span><br><span class="line">[zk: 172.18.0.4:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper, app_conf, foo]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接zk2服务</span></span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkCli.sh -server 172.18.0.2</span><br><span class="line">[zk: 172.18.0.2:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper, app_conf, foo]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接zk1服务</span></span><br><span class="line">root@zoo1:/zookeeper-3.4.14# ./bin/zkCli.sh -server 172.18.0.3</span><br><span class="line">[zk: 172.18.0.3:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper, app_conf, foo]</span><br></pre></td></tr></table></figure>





<h3 id="3、用zk的四字命令查看zk集群状态"><a href="#3、用zk的四字命令查看zk集群状态" class="headerlink" title="3、用zk的四字命令查看zk集群状态"></a>3、用zk的四字命令查看zk集群状态</h3><p>&#8195;&#8195;通过进入容器查看zk状态显然不优雅，zk中有快捷的命令可以查看服务器的运行状态，它们的长度为4个英文字母缩写，又叫“四字命令”，需要结合nc命令，服务器安装nmap-ncat.x86_64（可通过yum install nc）</p>
<h4 id="state"><a href="#state" class="headerlink" title="state"></a>state</h4><p>stat命令用于获取zk的运行时状态信息，包括基本的zk版本、打包信息、运行时角色、集群数据节点个数等信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 详细信息</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2181    </span><br><span class="line">Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">Clients:</span><br><span class="line"> /172.18.0.1:38660[0](queued=0,recved=1,sent=0)</span><br><span class="line"></span><br><span class="line">Latency min/avg/max: 2/18/49</span><br><span class="line">Received: 7</span><br><span class="line">Sent: 6</span><br><span class="line">Connections: 1</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x300000002</span><br><span class="line">Mode: follower</span><br><span class="line">Node count: 4</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个zk实例的角色</span></span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2181|grep Mode </span><br><span class="line">Mode: follower</span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2182|grep Mode </span><br><span class="line">Mode: follower</span><br><span class="line">[root@dn2 zk_docker_cluster]# echo stat | nc 127.0.0.1 2183|grep Mode</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="conf"><a href="#conf" class="headerlink" title="conf"></a>conf</h4><p>conf命令用于输出ZooKeeper服务器运行时使用的基本配置信息，包括clientPort、dataDir和tickTime等。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo conf | nc 127.0.0.1 2181</span><br><span class="line">clientPort=2181</span><br><span class="line">dataDir=/data/version-2</span><br><span class="line">dataLogDir=/datalog/version-2</span><br><span class="line">tickTime=2000</span><br><span class="line">maxClientCnxns=60</span><br><span class="line">minSessionTimeout=4000</span><br><span class="line">maxSessionTimeout=40000</span><br><span class="line">serverId=1</span><br><span class="line">initLimit=5</span><br><span class="line">syncLimit=2</span><br><span class="line">electionAlg=3</span><br><span class="line">electionPort=3888</span><br><span class="line">quorumPort=2888</span><br><span class="line">peerType=0</span><br></pre></td></tr></table></figure>



<h4 id="mntr"><a href="#mntr" class="headerlink" title="mntr"></a>mntr</h4><p>mntr命令用于输出比stat命令更为详尽的服务器统计信息，包括请求处理的延迟情况、服务器内存数据库大小和集群的数据同步情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo mntr | nc 127.0.0.1 2181    </span><br><span class="line">zk_version      3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">zk_avg_latency  18</span><br><span class="line">zk_max_latency  49</span><br><span class="line">zk_min_latency  2</span><br><span class="line">zk_packets_received     8</span><br><span class="line">zk_packets_sent 7</span><br><span class="line">zk_num_alive_connections        1</span><br><span class="line">zk_outstanding_requests 0</span><br><span class="line">zk_server_state follower</span><br><span class="line">zk_znode_count  4</span><br><span class="line">zk_watch_count  0</span><br><span class="line">zk_ephemerals_count     0</span><br><span class="line">zk_approximate_data_size        27</span><br><span class="line">zk_open_file_descriptor_count   31</span><br><span class="line">zk_max_file_descriptor_count    1048576</span><br><span class="line">zk_fsync_threshold_exceed_count 0</span><br></pre></td></tr></table></figure>



<h4 id="crst"><a href="#crst" class="headerlink" title="crst"></a>crst</h4><p>crst命令是一个功能性命令(client reset)，用于重置所有的客户端连接统计信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo crst | nc 127.0.0.1 2181    </span><br><span class="line">Connection stats reset.</span><br></pre></td></tr></table></figure>



<h4 id="srvr"><a href="#srvr" class="headerlink" title="srvr"></a>srvr</h4><p>srvr命令和stat命令的功能一致，唯一的区别是srvr不会将客户端的连接情况输出，仅仅输出服务器的自身信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">imok[root@dn2 zk_docker_cluster]# echo srvr | nc 127.0.0.1 2181    </span><br><span class="line">Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">Latency min/avg/max: 0/0/49</span><br><span class="line">Received: 327</span><br><span class="line">Sent: 326</span><br><span class="line">Connections: 2</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x300000004</span><br><span class="line">Mode: follower</span><br><span class="line">Node count: 5</span><br></pre></td></tr></table></figure>



<h4 id="dump"><a href="#dump" class="headerlink" title="dump"></a>dump</h4><p>dump命令用于输出当前集群的所有会话信息，包括这些会话的会话ID，以及每个会话创建的临时节点等信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo dump | nc 127.0.0.1 2181</span><br><span class="line">SessionTracker dump:</span><br><span class="line">org.apache.zookeeper.server.quorum.LearnerSessionTracker@77315813</span><br><span class="line">ephemeral nodes dump:</span><br><span class="line">Sessions with Ephemerals (0):</span><br></pre></td></tr></table></figure>



<h4 id="envi"><a href="#envi" class="headerlink" title="envi"></a>envi</h4><p>envi命令用于输出ZooKeeper所在服务器环境以及一些runtime环境，包括os.version、java.version和user.home等</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo envi | nc 127.0.0.1 2181    </span><br><span class="line">Environment:</span><br><span class="line">zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT</span><br><span class="line">host.name=zoo1</span><br><span class="line">java.version=1.8.0_222</span><br><span class="line">java.vendor=Oracle Corporation</span><br><span class="line">java.home=/usr/local/openjdk-8</span><br><span class="line">java.class.path=/zookeeper-3.4.14/bin/../zookeeper-server/target/classes:/zookeeper-3.4.14/bin/../build/classes:/zookeeper-3.4.14/bin/../zookeeper-server/target/lib/*.jar:/zookeeper-3.4.14/bin/../build/lib/*.jar:/zookeeper-3.4.14/bin/../lib/slf4j-log4j12-1.7.25.jar:/zookeeper-3.4.14/bin/../lib/slf4j-api-1.7.25.jar:/zookeeper-3.4.14/bin/../lib/netty-3.10.6.Final.jar:/zookeeper-3.4.14/bin/../lib/log4j-1.2.17.jar:/zookeeper-3.4.14/bin/../lib/jline-0.9.94.jar:/zookeeper-3.4.14/bin/../lib/audience-annotations-0.5.0.jar:/zookeeper-3.4.14/bin/../zookeeper-3.4.14.jar:/zookeeper-3.4.14/bin/../zookeeper-server/src/main/resources/lib/*.jar:/conf:</span><br><span class="line">java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span><br><span class="line">java.io.tmpdir=/tmp</span><br><span class="line">java.compiler=&lt;NA&gt;</span><br><span class="line">os.name=Linux</span><br><span class="line">os.arch=amd64</span><br><span class="line">os.version=3.10.0-957.27.2.el7.x86_64</span><br><span class="line">user.name=zookeeper</span><br><span class="line">user.home=/home/zookeeper</span><br><span class="line">user.dir=/zookeeper-3.4.14</span><br></pre></td></tr></table></figure>



<h4 id="ruok"><a href="#ruok" class="headerlink" title="ruok"></a>ruok</h4><p>ruok命令用于输出当前ZooKeeper服务器是否正在运行，“Are you ok”的缩写？如果当前ZooKeeper服务器正在运行，那么返回“imok”(I am ok)，否则没有任何响应输出。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo ruok | nc 127.0.0.1 2181    </span><br><span class="line">imok</span><br></pre></td></tr></table></figure>



<h4 id="wchs"><a href="#wchs" class="headerlink" title="wchs"></a>wchs</h4><p>wchs命令用于输出当前服务器上管理的Watcher的概要信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 zk_docker_cluster]# echo wchs | nc 127.0.0.1 2181     </span><br><span class="line"> connections watching 0 paths</span><br><span class="line">Total watches:0</span><br></pre></td></tr></table></figure>



<h4 id="wchc"><a href="#wchc" class="headerlink" title="wchc"></a>wchc</h4><p>wchc命令用于输出当前服务器上管理的Watcher的详细信息，以会话为单位进行归组，同时列出被该会话注册了Watcher的节点路径</p>
<h4 id="wchp"><a href="#wchp" class="headerlink" title="wchp"></a>wchp</h4><p>wchp命令和wchc命令非常类似，也是用于输出当前服务器上管理的Watcher的详细信息，不同点在于wchp命令的输出信息以节点路径为单位进行归组。</p>
]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
  </entry>
  <entry>
    <title>基于Gitee Pages和Hexo搭建个人开源博客</title>
    <url>/blog/2020/11/22/%E5%9F%BA%E4%BA%8EGitee%20Pages%E5%92%8CHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%BC%80%E6%BA%90%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>&#8195;&#8195;本blog用于归档如何在GitHub搭建个人博客的过程，内容参考来自本篇文章<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog">《如何用 GitHub 从零开始搭建一个博客》</a> 以及hexo中文官网的<a href="https://hexo.io/zh-cn/docs/">文档</a>。</p>
<p>更新：这篇文章写于今年年初，个人博客在年初已使用GitHub Pages搭建开源博客主页，今年下半年GitHub Pages所有以github.io为尾的博客地址在国内都无法正常访问，因此将其切换到国内Gitee才是正确选择，确实也需支持国内开源平台。</p>
<p><img src="https://img-blog.csdnimg.cn/2020112219074494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<a id="more"></a>

<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><h4 id="选用GitHub-Gitee发布开源博客的理由"><a href="#选用GitHub-Gitee发布开源博客的理由" class="headerlink" title="选用GitHub/Gitee发布开源博客的理由"></a>选用GitHub/Gitee发布开源博客的理由</h4><p>个人就使用CSDN相关看法：</p>
<ul>
<li>优势：<br>首先CSDN流量不愁，不同博主之间可以进行留言、私信等方式进行技术交流等，其次CSDN提供不错Markdown编辑器工具，例如在线草稿、离线草稿、图片拖曳与上传、定时保存等功能相对完善。</li>
<li>不足：<br>发布文章后，博客页面两栏窗口过多广告，对于推崇简洁主义的开发者来说（尤其已经习惯MacOS暗黑模式高度focus的UI）较为烦人，毕竟CSDN平台运营存在大量的商业行为。<br>而GitHub/Gitee搭建的博客为开源博客，以静态文件方式发布，通过整合一些博客框架提供的简洁模板，可以为技术文章撰写者和阅读者提供“无打扰”的简约而清爽的阅读环境。</li>
</ul>
<h4 id="GitHub-Gitee可构建个人开源博客主页"><a href="#GitHub-Gitee可构建个人开源博客主页" class="headerlink" title="GitHub/Gitee可构建个人开源博客主页"></a>GitHub/Gitee可构建个人开源博客主页</h4><p>GitHub/Gitee上除了最重要的代码仓库功能，还有GitHub Pages （国内为Gitee Pages）功能，可用于部署静态网页文件，只要发布者整合基本的工具，通过博客框架将本地Markdown文件编译为静态网页文件，再部署到GitHub/Gitee仓库，访问URL即可看到该静态网页。因此技术撰写者需要做的事情为：</p>
<ul>
<li>配置GitHub /GitHub Pages 本地博客开发环境</li>
<li>在本地完成Markdown文章</li>
<li>push 到GitHub/Gitee完成公网的博客发布</li>
</ul>
<p>目前GitHub Pages在国内已无法正常访问（VPN除外），因此开源博客主页在个人的Gitee仓库部署。</p>
<h3 id="Gitee-pages相关环境配置"><a href="#Gitee-pages相关环境配置" class="headerlink" title="Gitee pages相关环境配置"></a>Gitee pages相关环境配置</h3><h4 id="新建一个Repository用于存博客文件"><a href="#新建一个Repository用于存博客文件" class="headerlink" title="新建一个Repository用于存博客文件"></a>新建一个Repository用于存博客文件</h4><p>首先在个人Gitee 新建一个仓库，名称为blog，当然gitee开启pages功能后，主页url会被gitee自动设为</p>
<p><code>https://yield-bytes.gitee.io/blog</code></p>
<p>需注意：若基于GitHub Pages 部署博客，需要按以下规则建立仓库名称：</p>
<p>名称为 {yourblogname}.github.io，仓库名称必须以github.io 为后缀结尾，国内的gitee不需要此方式。</p>
<h4 id="本地git的配置"><a href="#本地git的配置" class="headerlink" title="本地git的配置"></a>本地git的配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name  &quot;用户名&quot; </span><br><span class="line"></span><br><span class="line">注意这里gitee的邮箱配置：如果在gitee设置了不公开个人邮箱，那么gitee会帮你设置一个隐私邮箱***@user.noreply.gitee.com，后续git命令提交都需要设置这个隐私邮箱，而不是设置注册邮箱</span><br><span class="line"></span><br><span class="line">git config --global user.email &quot;***@user.noreply.gitee.com&quot; </span><br><span class="line"></span><br><span class="line"># 避免git gui中的中文乱码</span><br><span class="line">git config --global gui.encoding utf-8</span><br><span class="line"></span><br><span class="line"># 避免git status显示的中文名乱码</span><br><span class="line">git config --global core.quotepath off</span><br></pre></td></tr></table></figure>

<h4 id="配置码云-ssh-key"><a href="#配置码云-ssh-key" class="headerlink" title="配置码云 ssh key"></a>配置码云 ssh key</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &#39;注册码云的邮箱&#39;</span><br></pre></td></tr></table></figure>

<p>其中-t指定密钥类型，这里设置rsa即可，-C是密钥的注释，这里设置成邮箱方便分辨</p>
<p>将公钥打印出来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat ~&#x2F;.ssh&#x2F;id_rsa.pub</span><br></pre></td></tr></table></figure>

<p> 在码云上添加ssh 公钥：在个人设置里面找到相应的ssh key添加界面处理即可</p>
<p>测试连接是否成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**** % ssh gitee@gitee.com</span><br><span class="line">Hi ***! You&#39;ve successfully authenticated, but GITEE.COM does not provide shell access.</span><br><span class="line">Connection to gitee.com closed.</span><br></pre></td></tr></table></figure>

<p>测试往仓库推一个新建文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir MyTecBlog</span><br><span class="line">cd MyTecBlog</span><br><span class="line">git init</span><br><span class="line">touch README.md</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;首次提交&quot;</span><br><span class="line">git remote add origin git@gitee.com:yield-bytes&#x2F;blog.git # 指定push的线上仓库，这里就是前面创建的blog仓库</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure>

<p>在web端的gitee仓库可看到README.md文件以及提交记录</p>
<h3 id="Hexo环境配置"><a href="#Hexo环境配置" class="headerlink" title="Hexo环境配置"></a>Hexo环境配置</h3><p>Hexo依赖node.js，借用相关npm包，使得搭建出来的GitHub博客不会过于简单。<br>MacOS直接用brew安装即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew install node </span><br><span class="line">node -v # node.js的版本</span><br><span class="line">npm -v # 包管理版本</span><br></pre></td></tr></table></figure>
<p>安装 Hexo博客框架<br>借助Hexo博客框架，可以快速部署和设计博客，Hexo自带命令行命令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#拉取源设置为淘宝镜像</span><br><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<p>在macOS安装 hexo-cli可能会提示未安装xcode CommandLineTools</p>
<ul>
<li><p><code>xcode-select --print-path</code>查看 command-line tools 的安装路径，不出意外显示的结果应该是<code>/Library/Developer/CommandLineTools</code></p>
</li>
<li><p><code>sudo rm -r -f /Library/Developer/CommandLineTools，</code>卸载 command-line tools </p>
</li>
<li><p><code>xcode-select --install，重新安装</code>command-line tools </p>
</li>
</ul>
<h3 id="创建hexo博客项目"><a href="#创建hexo博客项目" class="headerlink" title="创建hexo博客项目"></a>创建hexo博客项目</h3><p>创建一个名为yield-bytes的博客项目（名字可以任意取）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder hexo init yield-bytes</span><br><span class="line">yymac@wonder yield-bytes % ls</span><br><span class="line">_config.yml		package.json		themes</span><br><span class="line">node_modules		scaffolds</span><br><span class="line">package-lock.json	source</span><br></pre></td></tr></table></figure>
<p>可以看到yield-bytes为一个目录，该目录下有相应的node模块目录、配置文件、主题目录等。<br>将项目编译成静态文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo generate</span><br></pre></td></tr></table></figure>
<p>项目目录下多了一个public目录，可以看到这些就是静态网页内容：css、html、js等</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder public % ls</span><br><span class="line">2020		css		index.html</span><br><span class="line">archives	fancybox	js</span><br></pre></td></tr></table></figure>
<p>其中index.html就是hexo的demo博客展示页面，通过在public目录下运行<code>hexo server</code>将页面作为web服务跑起来</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder public % hexo server</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure>
<p>在浏览器可以看到hexo的demo博客主页</p>
<h4 id="将本地demo博客部署到gitee"><a href="#将本地demo博客部署到gitee" class="headerlink" title="将本地demo博客部署到gitee"></a>将本地demo博客部署到gitee</h4><p>只需要在hexo创建的项目目录下的<code>_config.yml</code>加入gitee.io仓库的相关配置，hexo可一键部署到gitee上。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p>修改<code>_config.yml</code>，需要修改两处地方</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line"># URL</span><br><span class="line"># If your site is put in a subdirectory, set url as &#x27;http://example.com/child&#x27; and root as &#x27;/child/&#x27;</span><br><span class="line">url: https://yield-bytes.gitee.io/blog</span><br><span class="line">root: /blog/</span><br><span class="line">  </span><br><span class="line"># Deployment</span><br><span class="line"># Docs: https://hexo.io/docs/one-command-deployment</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@gitee.com:yield-bytes/blog.git</span><br><span class="line">  branch: master</span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<p>上述提示本地静态网页文件已经push到gitee的blog仓库<br>打开网址<code>https://yield-bytes.gitee.io/blog</code>，即可看到demo主页</p>
<h3 id="配置和完善个人博客"><a href="#配置和完善个人博客" class="headerlink" title="配置和完善个人博客"></a>配置和完善个人博客</h3><p>前面章节仅完成基本的搭建，从本章开始，将完善个人博客的配置以及UI。在<code>_config.yml </code>文件里面，分了很多部分，都可用于配置博客不同功能</p>
<h4 id="修改博客简介等基本内容："><a href="#修改博客简介等基本内容：" class="headerlink" title="修改博客简介等基本内容："></a>修改博客简介等基本内容：</h4> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  4 # Site</span><br><span class="line"> 5 title: Yield-Bytes</span><br><span class="line"> 6 subtitle: &#39;分享与沉淀&#39;</span><br><span class="line"> 7 description: &#39;这是一个非常专注于技术总结与分享的博客&#39;</span><br><span class="line"> 8 keywords: &quot;Python,BigData,Web开发,数据分析,深度学习&quot;</span><br><span class="line"> 9 author: yield-bytes</span><br><span class="line">10 language: zh-CN</span><br><span class="line">11 timezone: &#39;Asia&#x2F;Shanghai&#39;</span><br></pre></td></tr></table></figure>
<p> 本地运行刷新即可看到主页修改后的效果。</p>
<h4 id="为博客设置主题"><a href="#为博客设置主题" class="headerlink" title="为博客设置主题"></a>为博客设置主题</h4><p>博客当然可以设置为不同风格的UI，称为主题，在前十年那会，网易163博客还很流行，博客主可根据平台提供不同模板将自己的博客打造更加高级，有些模板还需要VIP或者付费。在当今开源时代，Hexo提供很多不错的博客模板，网址:<code>https://hexo.io/themes/</code>，可直接clone使用，以next主题为例子的配置过程：<br>在<code>yield-bytes/themes</code>目录下仅有一个默认的landscape主题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yymac@wonder themes % ls</span><br><span class="line">landscape</span><br></pre></td></tr></table></figure>
<p>将next主题拉到该目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder themes % ls</span><br><span class="line">yymac@wonder themes % git clone https://github.com/theme-next/hexo-theme-next</span><br><span class="line">hexo-theme-next	landscape</span><br></pre></td></tr></table></figure>

<p>在yield-bytes根目录下修改_config.yml 文件，在 theme 配置部分，修改为 hexo-theme-next:</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">4 # Extensions</span><br><span class="line">3 ## Plugins: https://hexo.io/plugins/</span><br><span class="line">2 ## Themes: https://hexo.io/themes/ # 官方提供更多开源的主题以及插件</span><br><span class="line">1 theme:  hexo-theme-next  # 更换主题</span><br></pre></td></tr></table></figure>
<p>重启hexo server即可看到主页已经换成next主题</p>
<h4 id="对主题进行深度定制"><a href="#对主题进行深度定制" class="headerlink" title="对主题进行深度定制"></a>对主题进行深度定制</h4><ul>
<li>更换样式</li>
</ul>
<p>每个主题的目录下也有一个<code>_config.yml</code>博客样式配置文件，这里可进行深度定制</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder hexo-theme-next % ls</span><br><span class="line">LICENSE.md	crowdin.yml	languages	scripts</span><br><span class="line">README.md	docs		layout		source</span><br><span class="line">_config.yml	gulpfile.js	package.json</span><br></pre></td></tr></table></figure>
<p>next 默认有四个样式，这里设为Pisces样式，也可打开暗黑模式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br><span class="line"></span><br><span class="line"># Dark Mode</span><br><span class="line">darkmode: true</span><br></pre></td></tr></table></figure>
<ul>
<li>更换title的logo</li>
</ul>
<p>由于logo需要出裁剪为一定尺寸的png图片，需自行设计。在next主题下的<code>_config.yml</code>的favicon部分可进行更换logo的配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">   small: &#x2F;images&#x2F;favicon-16x16-next.png</span><br><span class="line">   medium: &#x2F;images&#x2F;favicon-32x32-next.png</span><br><span class="line">   apple_touch_icon: &#x2F;images&#x2F;apple-touch-icon-next.png</span><br><span class="line">   safari_pinned_tab: &#x2F;images&#x2F;logo.svg</span><br></pre></td></tr></table></figure>


<ul>
<li>avatar 设置头像后者主页的标识图像</li>
</ul>
<p>只需将图片放在该路径：themes/hexo-theme-next/source/images 路径，在next主题下的<code>_config.yml </code>文件下配置为该图片路径，也可设为网络图片路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7 avatar:</span><br><span class="line">6   # Replace the default image and set the url here.</span><br><span class="line">5   url: #&#x2F;images&#x2F;avatar.gif</span><br><span class="line"># url: #&#x2F;images&#x2F;avatar.gif</span><br><span class="line">4   # If true, the avatar will be dispalyed in circle.</span><br><span class="line">3   rounded: true</span><br><span class="line">2   # If true, the avatar will be rotated with the cursor.</span><br><span class="line">1   rotated: false</span><br></pre></td></tr></table></figure>

<ul>
<li><p>为个人博客开启RSS(用处不大)<br>在hexo创建的博客项目目录下安装feed插件,博客项目安装的所有插件都放置在node_modules目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder  yield-bytes % npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure>
<p>安装完成之后不需要其他的配置，以后每次编译生成站点的时候就会自动生成 RSS Feed 文件</p>
</li>
<li><p>修改code代码的显示样式</p>
</li>
</ul>
<p>在文章中，经常需要贴上代码，为保证阅读效果，可将默认浅灰色样式设为Mac样式，看起还不错。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">12 codeblock:</span><br><span class="line">11   # Code Highlight theme</span><br><span class="line">10   # Available values: normal | night | night eighties | night blue | night  bright | solarized | solarized dark | galactic</span><br><span class="line"> 9   # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line"> 8   highlight_theme: solarized dark</span><br><span class="line"> 7   # Add copy button on codeblock</span><br><span class="line"> 6   copy_button:</span><br><span class="line"> 5     enable: true</span><br><span class="line"> 4     # Show text copy result.</span><br><span class="line"> 3     show_result: true</span><br><span class="line"> 2     # Available values: default | flat | mac</span><br><span class="line"> 1     style: mac</span><br></pre></td></tr></table></figure>

<p>default样式：<br><img src="https://img-blog.csdnimg.cn/20200201140722675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>mac样式：<br><img src="https://img-blog.csdnimg.cn/2020020114043519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>back2top设置页面滚动逻辑、阅读进度</li>
</ul>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">14 back2top:</span><br><span class="line">13   enable: true</span><br><span class="line">12   # Back to top in sidebar.</span><br><span class="line">11   sidebar: true</span><br><span class="line">10   # Scroll percent label in b2t button.</span><br><span class="line"> 9   scrollpercent: true</span><br><span class="line"> 8</span><br><span class="line"> 7 # Reading progress bar</span><br><span class="line"> 6 reading_progress:</span><br><span class="line"> 5   enable: true</span><br><span class="line"> 4   # Available values: top | bottom</span><br><span class="line"> 3   position: top</span><br><span class="line"> 2   color: &quot;#37c6c0&quot;</span><br><span class="line"> 1   height: 3px</span><br><span class="line"></span><br><span class="line">10 # Bookmark Support</span><br><span class="line"> 9 bookmark:</span><br><span class="line"> 8   enable: true</span><br><span class="line"> 7   # Customize the color of the bookmark.</span><br><span class="line"> 6   color: &quot;#222&quot;</span><br><span class="line"> 5   # If auto, save the reading progress when closing the page or clicking th     e bookmark-icon.</span><br><span class="line"> 4   # If manual, only save it by clicking the bookmark-icon.</span><br><span class="line"> 3   save: auto</span><br><span class="line"> 2</span><br></pre></td></tr></table></figure>
<p>打开Bookmark Support功能后，可以提示阅读体验，例如关闭该文章后，再打开浏览，可以恢复到上次阅读位置，类似微信阅读文章的体验。</p>
<ul>
<li>为每篇文章关联GitHub Banner</li>
</ul>
<p>大家阅读一些技术文章应该经常看到文章右上角有GitHub的图标，该链接就是去GitHub Repository:</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">   4 # `Follow me on GitHub` banner in the top-right corner.</span><br><span class="line">   3 github_banner:</span><br><span class="line">   2   enable: true</span><br><span class="line">   1   permalink: https://github.com/yield-bytes/yield-bytes.github.io</span><br><span class="line">403    title: Follow me on GitHub</span><br></pre></td></tr></table></figure>

<ul>
<li>为博客正确显示math 的Markdown语法</li>
</ul>
<p>为了能让文章中能正常显示数学公式（常见数据挖掘、深度学习等文章），可通过hexo为next主题引入相关插件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % npm install  hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>

<p>注意：这里不要安装hexo-renderer-pandoc，该库的js有bug，无法正确解析Markdown文章，会导致hexo运行报错</p>
<p>在主题配置打开math配置即可：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">   1 math:</span><br><span class="line">512    enable: true</span><br><span class="line">   1   # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">   2   # That is it only render those page which has `mathjax: true` in Front-ma     tter.</span><br><span class="line">   3   # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">   4   per_page: true</span><br><span class="line">   5</span><br><span class="line">   6   # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJa     x support.</span><br><span class="line">   7   mathjax:</span><br><span class="line">   8     enable: true</span><br><span class="line">   9     # See: https://mhchem.github.io/MathJax-mhchem/</span><br><span class="line">  10     mhchem: true</span><br></pre></td></tr></table></figure>

<ul>
<li>开启无刷新加载页面</li>
</ul>
<p>为进一步提升博客体验，hexo支持页面实现无刷新加载，借助pjax插件即可。<br>首先在配置文件里开启pjax</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">1   # pjax: //cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js</span><br><span class="line">938    pjax: true</span><br></pre></td></tr></table></figure>
<p>在hexo-theme-next目录下，安装该插件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder hexo-theme-next % pwd</span><br><span class="line">***/yield-bytes/themes/hexo-theme-next</span><br><span class="line">yymac@wonder hexo-theme-next % git clone https://github.com/theme-next/theme-next-pjax source/lib/pjax</span><br></pre></td></tr></table></figure>

<p>到处，已经完成对搭建博客的较为深度的定制，其他定制可参考官网：<a href="https://theme-next.org/docs/getting-started/%EF%BC%8C%E8%BF%99%E9%87%8C%E4%B8%8D%E5%86%8D%E7%B4%AF%E8%B5%98%E3%80%82">https://theme-next.org/docs/getting-started/，这里不再累赘。</a></p>
<h3 id="文章设置"><a href="#文章设置" class="headerlink" title="文章设置"></a>文章设置</h3><p>前置章节主要对博客主题及其UI做定制配置，本章节介绍如何在搭建的Gitee Pages上发布新文章等内容。</p>
<h4 id="增加文章"><a href="#增加文章" class="headerlink" title="增加文章"></a>增加文章</h4><p>新增一篇名为《深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）》的文章<br>在本地yield-bytes根目录创建文章，文章类型为Markdown格式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo new 深入理解异步IO的底层逻辑——IO多路复 用（select、poll、epoll）</span><br><span class="line">INFO  Created:***yield-bytes/source/_posts/深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）.md</span><br></pre></td></tr></table></figure>
<p>所有新建的文章的都拷贝到该目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % cd source/_posts/</span><br><span class="line">yymac@wonder _posts % ls</span><br><span class="line">hello-world.md</span><br><span class="line">深入理解异步IO的底层逻辑——IO多路复用(select、poll、epoll).md</span><br></pre></td></tr></table></figure>
<p>每篇文章的头部为该文章的元数据，例如标题，创建时间，标签，文章分类，有关文章属性更为详细的配置，可以参考：<a href="https://hexo.io/zh-cn/docs/writing.html">https://hexo.io/zh-cn/docs/writing.html</a></p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">  ---</span><br><span class="line">  title: 深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）</span><br><span class="line">  date: 2020-02-01 21:50:46</span><br><span class="line">tags: </span><br><span class="line"><span class="bullet">-</span> IO多路复用</span><br><span class="line"><span class="bullet">-</span> epoll</span><br><span class="line">categories:</span><br><span class="line"><span class="bullet">-</span> Python进阶</span><br><span class="line">  ---</span><br></pre></td></tr></table></figure>
<p>头部元数据之后就是Markdown的正文</p>
<h4 id="修改文章字体"><a href="#修改文章字体" class="headerlink" title="修改文章字体"></a>修改文章字体</h4><p>在next主题的配置文件<code>_config.yml</code>里面，font 部分可以设置文章显示字体，默认博客站点的字体大小为1，个人在global设为0.8后，文字看起相对舒服</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">16 font:</span><br><span class="line">15   enable: true</span><br><span class="line">14</span><br><span class="line">13   # Uri of fonts host, e.g. //fonts.googleapis.com (Default).</span><br><span class="line">12   host:</span><br><span class="line">11</span><br><span class="line">10   # Font options:</span><br><span class="line"> 9   # <span class="code">`external: true`</span> will load this font family from <span class="code">`host`</span> above.</span><br><span class="line"> 8   # <span class="code">`family: Times New Roman`</span>. Without any quotes.</span><br><span class="line"> 7   # <span class="code">`size: x.x`</span>. Use <span class="code">`em`</span> as unit. Default: 1 (16px)</span><br><span class="line"> 6</span><br><span class="line"> 5   # Global font settings used for all elements inside <span class="xml"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span>.</span><br><span class="line"> 4   global:</span><br><span class="line"> 3     external: true</span><br><span class="line"> 2     family: Lato</span><br><span class="line"> 1     size: 0.8</span><br></pre></td></tr></table></figure>

<h4 id="为博客左侧增加标签页、分类页等链接"><a href="#为博客左侧增加标签页、分类页等链接" class="headerlink" title="为博客左侧增加标签页、分类页等链接"></a>为博客左侧增加标签页、分类页等链接</h4><p>上面搭建的博客仅有少量栏目例如文章栏目、主页栏目，每次发布文章前，我们需要为其打个标签，以便当文章数量较多时，读者可在标签页查看相关的标签，从而快速找到相关文章，例如一篇streaming和kafka整合的文章，标签可以打为：streaming、kafka、实时流计算等。此外还需多文章进行分类，例如Python进阶分类、spark相关的分类、数据分析与挖掘的分类、数据结构与算法的分类、Linux相关的分类等<br>创建标签页和分类页：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo new page tags</span><br><span class="line">INFO  Created: ***/yield-bytes/source/tags/index.md</span><br><span class="line"></span><br><span class="line">yymac@wonder yield-bytes % hexo new page categories</span><br><span class="line">INFO  Created: ***/yield-bytes/source/categories/index.md</span><br></pre></td></tr></table></figure>
<p>创建的标签页以及分类页都是Markdown文件<br>标签页的md：</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: tags</span><br><span class="line">  2 date: <span class="strong">****</span> 10:58:03</span><br><span class="line">  3 ---</span><br><span class="line">~</span><br></pre></td></tr></table></figure>
<p>分类页的md：</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: categories</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 11:32:51</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 ---</span></span></span><br></pre></td></tr></table></figure>

<p>将tags的md指定为标签页：</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: tags</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 10:58:03</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 type: tags</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  4 comments: false  # 这里表示关闭当前文章的评论</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  5 ---</span></span></span><br></pre></td></tr></table></figure>
<p>将categories制定为分类页：</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: categories</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 11:32:51</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 type: categories</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  4 comments: false</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  5 ---</span></span></span><br></pre></td></tr></table></figure>

<p>在next主题的配置文章的menu部分新增tags访问路径和categories访问路径，这里就是配置页面路由的地方，可自行新增路径。</p>
<p>配置说明：<code> tags: /tags/ || fa fa-tags</code>表示tags的路径为/tags/，对应的icon图标为fa fa-tags</p>
<p>这里有一个图标与名称对应的地址：<code>https://v3.bootcss.com/components/#glyphicons</code>  ，将选中的icon图标名称如glyphicons glyphicons-th-large改为fa fa-th-large即可</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  #about: /about/ || fa fa-user</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th-large</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line">  #schedule: /schedule/ || fa fa-calendar</span><br><span class="line">  #sitemap: /sitemap.xml || fa fa-sitemap</span><br><span class="line">  #commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure>
<p>刷新后访问<code>http://localhost:4000/tags/</code>即可<br><img src="https://img-blog.csdnimg.cn/20200202132540606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200202132613168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">若需要对文章添加多个分类，用以下格式书写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">categories:</span><br><span class="line">	- [Spark,kafka]</span><br></pre></td></tr></table></figure>

<p>再增加一个404页面：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new 404</span><br></pre></td></tr></table></figure>

<p>在yield-bytes/source/404/路径下，更改index.md内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">   title: 404 Not Found</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">   &lt;center&gt;</span><br><span class="line">   对不起，您所访问的页面不存在或者已删除。</span><br><span class="line">   您可以&lt;a href&#x3D;&quot;https:&#x2F;&#x2F;yield-bytes.gitee.io&#x2F;blog&quot;&gt;点击此处&lt;&#x2F;a&gt;返回首页。</span><br><span class="line">   &lt;&#x2F;center&gt;</span><br><span class="line"></span><br><span class="line">  &lt;blockquote class&#x3D;&quot;blockquote-center&quot;&gt;</span><br><span class="line">      yield-bytes</span><br><span class="line">  &lt;&#x2F;blockquote&gt;</span><br></pre></td></tr></table></figure>



<h4 id="博客左侧menu菜单显示图片"><a href="#博客左侧menu菜单显示图片" class="headerlink" title="博客左侧menu菜单显示图片"></a>博客左侧menu菜单显示图片</h4><p>在next主题的配置文件_config.yml下加入Avatar地址即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar:</span><br><span class="line">  # Replace the default image and set the url here.</span><br><span class="line">  url: your avatar image url</span><br><span class="line">  # If true, the avatar will be dispalyed in circle.</span><br><span class="line">  rounded: true</span><br><span class="line">  # If true, the avatar will be rotated with the cursor.</span><br><span class="line">  rotated: false</span><br></pre></td></tr></table></figure>

<h4 id="多篇文章在首页的展示逻辑"><a href="#多篇文章在首页的展示逻辑" class="headerlink" title="多篇文章在首页的展示逻辑"></a>多篇文章在首页的展示逻辑</h4><p>若博客已发布多篇文章，next主题默认在首页里将在可视化区域窗口内展示所有文章的所有完整内容，而不是只显示每篇文章的摘要部分，这将导致无法预览多篇文章内容。处理方式很简单，hexo支持对每篇文章提供只显示摘要部分，文章的更多内容则用more提示来指引读者。<br>处理方式：<br>在每篇Markdown文章比较靠前的位置，加入<code>&lt;!--more--&gt;</code>标识即可，例如下面的两篇文章：<br>第一篇文章，在首页中，只给它显示前言的内容即可<br><img src="https://img-blog.csdnimg.cn/2020020213171413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>第二篇spark streaming的文章，在首页，将该文的第1章节前面几句作为文章摘要显示<br><img src="https://img-blog.csdnimg.cn/2020020213193456.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最后在查看首尔显示效果：<br><img src="https://img-blog.csdnimg.cn/20200202132150362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到首页的多篇文章只展示<code>&lt;!--more--&gt;</code> 前面的内容。</p>
<h4 id="为博客增加全站搜索功能"><a href="#为博客增加全站搜索功能" class="headerlink" title="为博客增加全站搜索功能"></a>为博客增加全站搜索功能</h4><p>开启博客全文搜索功能需要加入相关插件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>

<p>首先在yield-bytes项目的<code>_config.yml</code>新增搜索配置：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">2 search:</span><br><span class="line">3   path: search.xml</span><br><span class="line">4   field: post</span><br><span class="line">5   format: html</span><br><span class="line">6   limit: 1000</span><br></pre></td></tr></table></figure>

<p>再到next主题的<code>_config.yml</code>开启相关检索配置，这里启动 Local Search功能，这里的配置也提示了需要安装依赖hexo-generator-searchdb插件：</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">   8 # Local Search</span><br><span class="line">   7 # Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span><br><span class="line">   6 local<span class="emphasis">_search:</span></span><br><span class="line"><span class="emphasis">   5   enable: true</span></span><br><span class="line"><span class="emphasis">   4   # If auto, trigger search by changing input.</span></span><br><span class="line"><span class="emphasis">   3   # If manual, trigger search by pressing enter key or search button.</span></span><br><span class="line"><span class="emphasis">   2   trigger: auto</span></span><br><span class="line"><span class="emphasis">   1   # Show top n results per article, show all results by setting to -1</span></span><br><span class="line"><span class="emphasis">761    top_</span>n<span class="emphasis">_per_</span>article: 5</span><br><span class="line">   1   # Unescape html strings to the readable one.</span><br><span class="line">   2   unescape: false</span><br><span class="line">   3   # Preload the search data when the page loads.</span><br><span class="line">   4   preload: false</span><br><span class="line">   5</span><br></pre></td></tr></table></figure>
<p>上述表示输入关键字后自动触发搜索，并只显示5条命中记录。从效果来看，不得不佩服hexo博客框架（结合各类第三方插件）确实强大。（在独立开发的博客项目中，若要启用全文检索，则需引入elasticsearch技术栈）<br><img src="https://img-blog.csdnimg.cn/20200202151834914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>至此，博客的文章板块已经定制完毕，重新编译部署到gitee即可看到效果</p>
<h4 id="为GitHub-Pages配置自定义域名-注意以下不是Gitee-Pages的配置"><a href="#为GitHub-Pages配置自定义域名-注意以下不是Gitee-Pages的配置" class="headerlink" title="为GitHub Pages配置自定义域名(注意以下不是Gitee Pages的配置)"></a>为GitHub Pages配置自定义域名(注意以下不是Gitee Pages的配置)</h4><p>GitHub pages为git用户提供免费的自定义域名服务，所搭建的博客站点可无需使用类似<code>https://yourrepo.github.io</code>作为访问地址</p>
<p>在github博客项目仓库的Settings里面的GitHub Pages 可进行自定义域名设置：</p>
<p><a href="https://io.yield-bytes.cn/">https://io.yield-bytes.cn</a></p>
<h4 id="为文章添加字数统计和阅读时长"><a href="#为文章添加字数统计和阅读时长" class="headerlink" title="为文章添加字数统计和阅读时长"></a>为文章添加字数统计和阅读时长</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_wordcount:</span><br><span class="line">  item_text: true</span><br><span class="line">  wordcount: true         # 单篇 字数统计</span><br><span class="line">  min2read: true          # 单篇 阅读时长</span><br><span class="line">  totalcount: false       # 网站 字数统计</span><br><span class="line">  separated_meta: true</span><br><span class="line">  </span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<p>安装相关插件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yymac@wonder yield-bytes % $ npm install eslint --save</span><br><span class="line">yymac@wonder yield-bytes % $ npm install hexo-wordcount --save</span><br><span class="line">yymac@wonder yield-bytes % $ npm install hexo-symbols-count-time --save</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在博客yield-bytes项目根目录的_config.yml文件的最后加入以下配置</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true                # 文章字数统计</span><br><span class="line">  time: true                   # 文章阅读时长</span><br><span class="line">  total_symbols: true          # 站点总字数统计</span><br><span class="line">  total_time: true             # 站点总阅读时长</span><br><span class="line">  exclude_codeblock: false     # 排除代码字数统计</span><br></pre></td></tr></table></figure>
<p>在next主题的配置文件已有相关文章计数配置</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true     # 是否另起一行（true的话不和发表时间等同一行）</span><br><span class="line">  item_text_post: true     # 首页文章统计数量前是否显示文字描述（本文字数、阅读时长）</span><br><span class="line">  item_text_total: false   # 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长）</span><br></pre></td></tr></table></figure>
<p>注意字体统计效果需重新hexo clean 和hexo generate才有效果，直接重启本地hexo server将无效</p>
<p><img src="https://img-blog.csdnimg.cn/20200204182425284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>最后重新部署一遍博客即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>



<h3 id="自动更新Gitee-Pages"><a href="#自动更新Gitee-Pages" class="headerlink" title="自动更新Gitee Pages"></a>自动更新Gitee Pages</h3><p>GitHub Pages只要 push上去，主页文章就会自动更新，但Gitee Pages的个人版无法实现自动更新，需要手动在仓库设置中点击更新按钮，若想自动化该过程，可以用selenium爬虫工具实现，npm有一个插件可以实现该过程——官网<a href="https://developer.aliyun.com/mirror/npm/package/gitee-publish/v/1.0.18">地址</a></p>
<p>当然使用Python也可以快速开发一个自动更新脚本</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title>基于PySpark整合Spark Streaming与Kafka</title>
    <url>/blog/2020/03/06/%E5%9F%BA%E4%BA%8EPySpark%E6%95%B4%E5%90%88Spark%20Streaming%E4%B8%8EKafka/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文内容主要给出基于PySpark程序，整合Spark Streaming和Kafka，实现实时消费和处理topic消息，为PySpark开发大数据实时计算项目提供基本参考。</p>
<h4 id="1-程序环境准备："><a href="#1-程序环境准备：" class="headerlink" title="1 程序环境准备："></a>1 程序环境准备：</h4><p>&#8195;&#8195;这里不再使用Spark的集群环境，因涉及的计算资源测试环境受限，目前两台虚拟机：1个vcore+2G内存，其中一台虚拟机启动Spark Streaming服务进程，另外一台虚拟机启动kafka进程。</p>
<ul>
<li>虚拟机A：启动单实例kafka服务</li>
<li>虚拟机B：运行PySpark程序</li>
</ul>
<p>&#8195;&#8195;在VM A，程序环境要求安装jdk1.8以上以及与kafka匹配版本的scala版本<br>版本兼容说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kafka：kafka_2.11-2.4.0</span><br><span class="line">java：java version &quot;1.8.0_11&quot;</span><br><span class="line">scala： Scala 2.12.0</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;这里需要注意：如果使用kafka_2.12版本以上，需要使用jdk1.8.0_212以上；kafka_2.12与jdk1.8.0_11有不兼容地方，kafka启动报错提示<code>java.lang.VerifyError: Uninitialized object exists on backward branch 209</code>。</p>
<a id="more"></a>

<h5 id="1-1-基本配置"><a href="#1-1-基本配置" class="headerlink" title="1.1 基本配置"></a>1.1 基本配置</h5><p>（1）配置单机zk这里无需依赖ZooKeeper集群，只需使用kafka自带的zk服务即可<br>vim /opt/kafka_2.11-2.4.0/config/zookeeper.properties </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;zookeeper # zk的snapshot数据存储路径</span><br><span class="line">clientPort&#x3D;2181 # 按默认端口</span><br></pre></td></tr></table></figure>

<p>（2）配置kafka的，路径<code>/opt/kafka_2.11-2.4.0/config/ server.properties</code></p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">log.dirs=/opt/kafka-logs # 存放kafka数据目录</span><br><span class="line">zookeeper.connect=127.0.0.1:2181 # 按默认连接本机zk即可</span><br></pre></td></tr></table></figure>
<h5 id="1-2-启动zk和kafka"><a href="#1-2-启动zk和kafka" class="headerlink" title="1.2 启动zk和kafka"></a>1.2 启动zk和kafka</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# pwd</span><br><span class="line">/opt/kafka_2.12-2.4.0</span><br><span class="line"></span><br><span class="line">[root@nn kafka_2.11-2.4.0]#  nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>kafka server后台启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# nohup bin/kafka-server-start.sh config/server.properties 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<h5 id="1-3-测试单实例Kafka"><a href="#1-3-测试单实例Kafka" class="headerlink" title="1.3 测试单实例Kafka"></a>1.3 测试单实例Kafka</h5><p>&#8195;&#8195;对于kafka单节点而言，这里只能使用1个分区且1个replication-factor，topic名称为sparkapp</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sparkapp</span><br><span class="line">Created topic sparkapp.</span><br></pre></td></tr></table></figure>

<p>打开一个新的shell,用于启动producer</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sparkapp</span><br></pre></td></tr></table></figure>

<p>再打开一个新的shell,用于启动consumer</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic sparkapp</span><br></pre></td></tr></table></figure>

<p>&#8195;&#8195;在producer shell输入字符串，consumer端可以看到相应输出，说明单机的kafka可以正常运行，下面将使用Spark Streaming实时读取kafka的输入流</p>
<h4 id="2-整合streaming和kafka"><a href="#2-整合streaming和kafka" class="headerlink" title="2  整合streaming和kafka"></a>2  整合streaming和kafka</h4><h5 id="2-1-配置依赖包"><a href="#2-1-配置依赖包" class="headerlink" title="2.1 配置依赖包"></a>2.1 配置依赖包</h5><p>&#8195;&#8195;具体说明<a href="http://spark.apache.org/docs/2.4.4/streaming-kafka-integration.html">参考官方文档</a>spark streaming连接kafka需要依赖两个jar包（注意版本号）：<br>spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar： <a href="http://archiva-maven-storage-prod.oss-cn-beijing.aliyuncs.com/repository/central/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.3/spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar?Expires=1579170671&OSSAccessKeyId=LTAIfU51SusnnfCC&Signature=Yh6l7ZwWEfW0QPkwKlrDAdXrGxs=">下载链接</a><br>spark-streaming-kafka-0-8_2.11-2.4.4.jar：  <a href="https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.4/spark-streaming-kafka-0-8_2.11-2.4.4.jar">下载链接</a><br>&#8195;&#8195;将这两个jar包放在spark 的jars目录下，需要注意的是：这两个jar包缺一不可，如果是在Spark集群上做测试，那么每个Spark节点都需要放置这两个jars包：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn jars]# pwd</span><br><span class="line">/opt/spark-2.4.4-bin-hadoop2.7/jars</span><br><span class="line"></span><br><span class="line">[root@nn jars]# ls spark-streaming-kafka-0-8</span><br><span class="line">spark-streaming-kafka-0-8_2.11-2.4.4.jar</span><br><span class="line">spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;(关于spark-streaming-kafka的jar包依赖说明：就像python连接kafka，需要使用pip 安装kafka这个库）</p>
<h5 id="2-2-Spark-Streaming实时消费Kafka消息"><a href="#2-2-Spark-Streaming实时消费Kafka消息" class="headerlink" title="2.2 Spark Streaming实时消费Kafka消息"></a>2.2 Spark Streaming实时消费Kafka消息</h5><p>&#8195;&#8195;使用spark自带的直连kafka，实现实时计算wordcount，可以看到写普通的PySpark逻辑相对简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.kafka <span class="keyword">import</span> KafkaUtils</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(appName=<span class="string">&quot;streamingkafka&quot;</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少shell打印日志</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>) <span class="comment"># 5秒的计算窗口</span></span><br><span class="line">    brokers=<span class="string">&#x27;127.0.0.1:9092&#x27;</span></span><br><span class="line">    topic = <span class="string">&#x27;sparkapp&#x27;</span></span><br><span class="line">    <span class="comment"># 使用streaming使用直连模式消费kafka </span></span><br><span class="line">    kafka_streaming_rdd = KafkaUtils.createDirectStream(ssc, [topic], &#123;<span class="string">&quot;metadata.broker.list&quot;</span>: brokers&#125;)</span><br><span class="line">    lines_rdd = kafka_streaming_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    counts = lines_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">        .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    <span class="comment"># 将workcount结果打印到当前shell    </span></span><br><span class="line">    counts.pprint()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>spark streaming流默认接收的是utf-8编码的字符串</p>
<p>KafkaUtils接口<code>createDirectStream</code>说明：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Parameters:	</span><br><span class="line">    ssc – StreamingContext object.</span><br><span class="line">    topics – list of topic_name to consume.</span><br><span class="line">    kafkaParams – Additional params for Kafka.</span><br><span class="line">    fromOffsets – Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.</span><br><span class="line">    keyDecoder – A function used to decode key (default is utf8_decoder).</span><br><span class="line">    valueDecoder – A function used to decode value (default is utf8_decoder).</span><br><span class="line">    messageHandler – A function used to convert KafkaMessageAndMetadata. You can assess meta using messageHandler (default is None).</span><br><span class="line"></span><br><span class="line">Returns:	</span><br><span class="line">A DStream object</span><br></pre></td></tr></table></figure>

<p>spark streaming 从 kafka 接收数据，有两种方式<br>（1）使用Direct API，这是更底层的kafka API<br>（2）使用receivers方式，这是更为高层次的API</p>
<p>&#8195;&#8195;在本博客后面讨论streaming的原理同时也给出Direct模式的相关详细的解析。当前测试使用为Direct模式，在虚拟机B的Spark目录下，启动application，启动命令需要带上指定的jars包。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/spark-submit --jars spark-streaming-kafka-0-8_2.11-2.4.4.jar direct_stream.py </span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;在虚拟机A的producer shell端，输入字符串句子</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-producer.sh --broker-list localhost:9 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">welcome to pyspark kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">从这里开始  将开发一个 由sparkstreaming 完成的 实时计算的 大数据项目</span></span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;在spark-submit窗口，可以看到spark streaming消费并处理kafka生成的实时流字符串结果：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:28</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;welcome&#x27;, 1)</span><br><span class="line">(&#x27;to&#x27;, 1)</span><br><span class="line">(&#x27;pyspark&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:34</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;从这里开始&#x27;, 1)</span><br><span class="line">(&#x27;&#x27;, 1)</span><br><span class="line">(&#x27;将开发一个&#x27;, 1)</span><br><span class="line">(&#x27;由sparkstreaming&#x27;, 1)</span><br><span class="line">(&#x27;完成的&#x27;, 1)</span><br><span class="line">(&#x27;实时计算的&#x27;, 1)</span><br><span class="line">(&#x27;大数据项目&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:36</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure>


<p>&#8195;&#8195;以上完成基于PySpark整合Spark Streaming与Kafka的测试。</p>
<h5 id="2-3-关于以上测试过程有关offset简单说明"><a href="#2-3-关于以上测试过程有关offset简单说明" class="headerlink" title="2.3 关于以上测试过程有关offset简单说明"></a>2.3 关于以上测试过程有关offset简单说明</h5><p>&#8195;&#8195;该测试并没有给出consumer自己管理消息的offset，在上面测试中，例如，producer连续生产5条消息，那么消息体可以看出以下简单构成：<br>| offset| msg |<br>|–|–|<br>|  0|123@qq.com|<br>|  1|124@qq.com  |<br>|  2|125@qq.com |<br>|  3|126@qq.com  |<br>|  5|127@qq.com  |<br>&#8195;&#8195;上面的测试中，streaming 以Direct模式连接kafka，每消费一条消息，streaming默认自动commit offset到kafka，以期实现当下一批streaming去kafka取消息时，是按顺延下一条来取，保证没有重复处理消息，也不会漏了消息，这是什么意思呢？<br>&#8195;&#8195;例如当前streaming 消费offset=1的消息后，自动将消费位置offset=1告诉kafka：你记住我已经把第1个位置消息处理了，如果我下次找你kafka消费，请你找出offset=2的消息给我，但如果你将offset=0的消息给我，说明你让我重复消费消息，如果将offset=4消息给我，说明你让我漏了处理offset=3的消息。<br>&#8195;&#8195;根据以上说明，例如producer已经生产了offset=9共10消息，即使将当前spark streaming进程再消费offset=1的消息后，被退出，之后重启，spark streaming从kafka消费的消息将是offset=2的消息，而不是offset=10的消息。虽然默认配置有一定合理性，但也有这种情况，导致无法实现“仅消费一次而且保证业务正常”，参考以下场景：<br>&#8195;&#8195;spark streaming当前进程消费了offset=1的消息后，在业务处理过程中程序出错导致没有将办理业务详情发送到用户<code>124@qq.com</code>，因为spark streaming默认自动提交offset的位置给到kafka，因此spark streaming在一批处理中将消费offset=2的消息。若你想倒回去重新处理offset=1的消息，以保证邮件正确送到给用户，那么只能自己用外部数据库存放成功完成业务的offset，也即是自行管理offset，而不是被动的自动提交到kafka保存消费的offset。<br>&#8195;&#8195;kafka的offset消费位置的管理详解将在之后的文章给出，只有将offset的消费位置交由客户端自行管理，才能灵活实现各种需求：重新消费、只消费一次等</p>
<h4 id="3-Spark-Streaming与Kafka整合的两种方式"><a href="#3-Spark-Streaming与Kafka整合的两种方式" class="headerlink" title="3 Spark Streaming与Kafka整合的两种方式"></a>3 Spark Streaming与Kafka整合的两种方式</h4><p>&#8195;&#8195;在上面的整合测试里，用的streaming直连kafka进行消费消息。目前Spark Streaming 与 Kafka 的结合主要有两种方式：Receiver Dstream和Direct Dstream，目前企业实际项目主要采用 Direct Dstream 的模式，为何我这边可以断言企业主要使用Direct Dstream模式呢？因为在企业中，他们主力用Java和Scala，考虑企业需求方面，肯定使用spark-streaming-kafka-0-10版本的整合包，而这一版本不再支持Receiver模式。除非某些企业用了Pyspark作为spark应用开发，否则基本没人用Receiver模式。Spark官网也给出整合Kafka的指引<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html">链接</a><br><img src="https://img-blog.csdnimg.cn/20200205144614446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;因为基于PySpark开发实时流计算程序，这里只能选择spark-streaming-kafka-0-8开发包，从官方提示可知，spark-streaming-kafka-0-10是stable版本而且支持ssl安全传输，支持offset commit（支持手动提交，这个非常重要，自行控制消息位置从哪条开始处理，保证准确消费）和dynamic topic subscription，这就是为何要用Scala语言开发面向高级需求的Spark程序或者streaming程序，亲儿子！<br>&#8195;&#8195;对于两种连接连接方式，有必要给出讨论和对比，以便加深streaming消费kafka topic更深理论知识。</p>
<h5 id="3-1-基于Receiver消费消息方式"><a href="#3-1-基于Receiver消费消息方式" class="headerlink" title="3.1 基于Receiver消费消息方式"></a>3.1 基于Receiver消费消息方式</h5><p><strong>原理图（已启用WAL机制）</strong>：<br><img src="https://img-blog.csdnimg.cn/20200303193251128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   （原理图需要注意的地方：如果Receiver模式下，未开启WAL用于备份接收的消息，那么图中Save data to WAL是不存在的。）<br>&#8195;&#8195;早期版本的Spark Streaming与Kafka的整合方式为Receiver从Kafka消费消息，在提交Spark Streaming任务后，Spark会划出指定的Receiver来持续不断、异步读取kafka数据，这个Receiver其实是Executor（jvm进程）的一个常驻线程，跟task类似，为何它是常驻的？因为它需要不断监听Kafka的Producer生产的消息，从这点也可以看出，Receiver收到的消息是存放在Executor的内存中，换句话说，占用了Executor的内存。至于Receiver线程内部使用哪种数据结构存放接收的消息？对于先进先消费，后进后消费场景，显然使用queue最适合（通过队列实现多线程的生产-消费编程逻辑）。当Driver这边提交job后，Executors从Receiver拿到消息去交给task处理。在执行完之后，Receiver向Kafka的Zookeeper提交offset，告诉Kafka记主它当前已消费的位置。<br>&#8195;&#8195;早期的设计中，Spark Streaming为了零丢失地消费kafka消息，增加对接收到的消息进行预写日志处理（Write Ahead Log， WAL）这个WAL是放在hdfs的checkpoint 目录下，开启该功能后，Receiver除了将接收到消息存放到Executor内存中，还将其同步写入到hdfs上的WAL日志文件。因此，当一直运行的Spark Streaming任务突然挂了，后期启动时，Streaming也可以自动从hfds的checkpoint目录下的WAL日志找回丢失的消息。</p>
<h6 id="Receiver连接方式的缺点"><a href="#Receiver连接方式的缺点" class="headerlink" title="Receiver连接方式的缺点"></a>Receiver连接方式的缺点</h6><p>&#8195;&#8195;从上面receiver工作原理可以总结其缺点出将出现在内存方面、wal日志影响吞吐量等方面存在设计上的缺点：<br><strong>（1）占用cpu+内存</strong>：每个receiver需要单独占用一个vcore以及相应内存，如果Receiver并发数量多，占用Executor更多cpu和内存资源，这些资源本应用来跑tasks做计算用的，这就出现浪费资源的情况。</p>
<p><strong>（2）WAL拖累整体处理效率</strong>：为了不丢数据需要开启WAL，也即Receiver将接收到的数据写一份备份到文件系统上（hdfs的checkpoint目录），既然有落到磁盘自然会有IO，这降低了<code>kafka+streaming</code>这个组合实时处理消息的效率，换句话说：增加job的执行时间。此外，开启WAL，还有造成重复消费的可能。</p>
<p><strong>（3）接收数量大于处理速率</strong>： 若Receiver并发数量设置不合理，接受消息速率大于streaming处理消息的速率，就会出现数据积压在队列中，最终可能会导致程序异常退出。这里也是面试常见的问题：例如提高Receiver的并发数量，就可以提高streaming处理能力吗？首先，Receiver异步接收kafka消息，不参与计算，真正执行计算的是streaming，如果streaming并发性没有调高，整个计算能力也没有提高。一定要记着：kafka跟streaming是需要两边同时调优，才能打得计算能力的整体提升，不能只调优一边，这一个组合！！</p>
<p>（补充知识点：Receiver的并发数据量是怎么确定？<br>&#8195;&#8195;在KafkaUtils.createStream()中，可以指定topic的partition数量，该数量就是Receiver消费此topic的并发数（其实就是Executor 启动消费此topic的线程数量）但需要指出的是：Kafka中topic的partition与Spark中RDD的partition是两个不同的概念，两者没有关联关系。）</p>
<h5 id="3-2-基于Direct消费消息方式"><a href="#3-2-基于Direct消费消息方式" class="headerlink" title="3.2 基于Direct消费消息方式"></a>3.2 基于Direct消费消息方式</h5><p>原理图：<br><img src="https://img-blog.csdnimg.cn/20200305173531689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;当Receiver的工作原理及其缺点理解后，Direct模式将更容易理解。Driect模式下，Streaming定时主动查询Kafka，以获得指定topic的所有partition的最新offset，结合上一批次已保存的offset位置，Streaming就可以确定出每个批次拉取消息offset的范围，例如第1批次的消息（offset范围0-100）正在处理过程中，streaming指定特定的线程定时去Kafka查询第2批次最新的offset，发现最新值为300，那么如果streaming没有限制每批次的最大消费速率，在第2批次取消息时，会一次性取回offset=101到300的消息记录，这个就是所谓的offset ranges。当让如果streaming没有限制每批次的最大消费速率就是每批次100，那么即使最新的offset位置为300，第2批次消费的offset 访问只能是101~200共计100条消费记录。<br>&#8195;&#8195;当处理数据的job启动时，就会使用kafka的简单Consumer API来获取kafka中指定offset范围的数据。此外，Streaming已消费的offset不再交由Zookeeper来管理，而是手动采用外部存储数据库如mysql、redis等存放和管理已消费的offset。<br>以下为Scala代码演示从rdd拿到offset ranges属性的逻辑（rdd当然本身包含消息数据）<br>​```java<br>directKafkaStream.map {<br>           …<br> }.foreachRDD { batchRdd =&gt;<br>    // 获取当前rdd数据对应的offset<br>    val offsetRanges = batchRdd.asInstanceOf[HasOffsetRanges].offsetRanges<br>    // 运行计算任务<br>    doCompute(batchRdd)<br>    // 使用外部数据库自行保存和管理offsetRanges<br>    saveToRedis(offsetRanges)<br> }</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&amp;#8195;&amp;#8195;而Receiver方式下没有关于offset的处理逻辑，这是因为streaming在该模式下内部通过kafka consumer high level API 提交到zk保存。</span><br><span class="line">​&#96;&#96;&#96;java</span><br><span class="line">receiverkafkaStream.map &#123;</span><br><span class="line">           ...</span><br><span class="line"> &#125;.foreachRDD &#123; streamRdd &#x3D;&gt;</span><br><span class="line">    &#x2F;&#x2F; 运行计算任务</span><br><span class="line">    doCompute(rdd)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h6 id="Direct连接方式的优点"><a href="#Direct连接方式的优点" class="headerlink" title="Direct连接方式的优点"></a>Direct连接方式的优点</h6><p><strong>（1）提高计算资源利率</strong>：不像Receiver那样还占用Executor的一部分内存和计算资源，Direct方式下的Executor的代码实现踢掉Receiver这块设计，因此可以实现计算和内存资源全部用在计算任务，因为streaming定时主动去kafka拉取batch 消息，拉过来直接计算，而不是像Receiver不断接收消息不断地存放在内存中。</p>
<p><strong>（2）无需开启WAL</strong>：Receiver方式需要开启WAL机制以保证不丢失消息，这种方式加大了集群的计算延迟和效率，而Direct的方式，无需开启WAL机制，因为Kafka集群有partition做了高可用，只要streaming消费方自己存放和管理好已经消费过的offset，那么即使程序异常退出等，也可利用已存储的offset去Kafka消费丢失的消息。</p>
<p><strong>（3）可保证exactly once的消费语义</strong>：基于Receiver的方式，使用kafka的高阶API来在Zookeeper中保存消费过的offset。这是消费kafka数据的传统方式。这种方式配合WAL机制，可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和Zookeeper之间可能是不同步的。基于Direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据时消费一次且仅消费一次。</p>
<p><strong>（4）计算程序更稳定</strong>：Receiver模式是通过异步持续不断的读取数据，当集群出现网络、计算负载跟不上等因素，导致streaming计算任务侧出现延迟和堆积，而Receiver却还在持续接收kafka消息，此种情况容易导致Executor内存溢出或者其他异常抛出，从而引起计算程序退出，换句话说，Receiver模式的streaming实时计算可靠性和稳定性欠缺。对于Direct模式，Driver在触发batch计算任务时，才会去kafka拉消息回来并计算，而且给streaming加入最大消费速率控制后，整个实时计算集群鲁棒性更强。</p>
<p><strong>（5）Dstream 的rdd分区数与kafka分区一致</strong>：<br>&#8195;&#8195;Direct模式下，Spark Streaming创建的rdd分区数跟Kafka的partition数量一致，也就是说Kafka partitions和streaming rdd partitions之间有一对一的映射关系，这样的好处是明显和直观的：只要增加kafka topic partition数量，就可以直接增大spark streaming的计算的并发数。<br>&#8195;&#8195;当然，Direct模式不足的地方就是需要自行实现可靠的offset管理逻辑，但对于开发方向来说，这点很容易实现，我个人若对offset管理，将优先选用redis，而且是集群！<br>&#8195;&#8195;以上有关Spark Streaming 整合Kafka的方式和原理分析必须要理解，否则在后面的实时计算平台的代码开发上，有些逻辑你不一定能处理好。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Streaming</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Sentinel模式部署高可用Redis</title>
    <url>/blog/2019/12/20/%E5%9F%BA%E4%BA%8ESentinel%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8Redis/</url>
    <content><![CDATA[<p>&#8195;&#8195;在本博客前面的文章给出redis-cluster模式的配置和测试<a href="https://blog.csdn.net/pysense/article/details/100827689">《一篇文章掌握redis-cluster原理及其部署、测试》</a>，redis还有另外一种failover自动切换的部署方式，也即是本文给出的——Sentinel模式（哨兵模式），这两种方式部署的redis服务其实在普通的项目完全够用，例如个人在Django项目使用的Sentinel模式保证了”查询缓存服务以及一些频繁读取配置参数服务“的高可用。对于并发量大的需求，可以使用国内知名Codis——分布式Redis集群代理中间件，可配置规模更大的redis集群服务。</p>
<a id="more"></a>

<h4 id="1、安装redis"><a href="#1、安装redis" class="headerlink" title="1、安装redis"></a>1、安装redis</h4><p>&#8195;&#8195;为保持文章内容完整，这里给出redis的安装过程。两种方式，一种为yum 安装，另外一种下载包安装。这里选择bin包下载安装。目前redis稳定版为5.0.7，tar包为仅为1.7M，不愧为缓冲界的宠儿。安装包放在opt下，个人喜好将所有关开发的相关组件安装包放置于/opt目录，例如前面大数据各个组件的安装包，还是为了方便记忆、管理和查找。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# pwd</span><br><span class="line">/opt/redis-5.0.7</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget http://download.redis.io/releases/redis-5.0.7.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar xzf redis-5.0.7.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> redis-5.0.7</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make</span></span><br></pre></td></tr></table></figure>

<p>将redis启动命令所在的src路径加入系统变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# source ~/.bash_profile  </span><br><span class="line">PATH=$PATH:$HOME/bin:/opt/redis-5.0.7/src/  </span><br></pre></td></tr></table></figure>

<p>查看版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# redis-server -v</span><br><span class="line">Redis server v=5.0.7 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=864a7319aeb56c9b</span><br></pre></td></tr></table></figure>
<p>启动redis-server后台进程：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# redis-server &amp;</span><br><span class="line">[root@nn opt]# ps -ef |grep redis</span><br><span class="line">root      91054  60098  0 11:47 pts/0    00:00:00 redis-server *:6379</span><br><span class="line"></span><br><span class="line">[root@nn opt]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; set msg 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get msg</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure>

<h4 id="2、Sentinel-的配置说明"><a href="#2、Sentinel-的配置说明" class="headerlink" title="2、Sentinel 的配置说明"></a>2、Sentinel 的配置说明</h4><h5 id="2-1-官网有关Sentinel模式的基本信息"><a href="#2-1-官网有关Sentinel模式的基本信息" class="headerlink" title="2.1 官网有关Sentinel模式的基本信息"></a>2.1 官网有关Sentinel模式的基本信息</h5><blockquote>
<p>The current version of Sentinel is called <strong>Sentinel 2</strong>，A stable release of Redis Sentinel is shipped since Redis 2.8.<br>Sentinels by default run <strong>listening for connections to TCP port 26379</strong>,<br>If you are using the <code>redis-sentinel</code> executable， you can run Sentinel<br>with the following command line:<br><code>redis-sentinel /path/to/sentinel.conf</code></p>
</blockquote>
<p>redis要求启动Sentinel服务时必须带上其配置文件，否则直接返回启动失败。<br>启动Sentinel模式前的基本要求：</p>
<ul>
<li>You need at least three Sentinel instances for a robust deployment.（至少3个Sentinel实例，多数票选举）</li>
<li>最好在不同物理机上或者虚拟机上启动每个Sentinel 实例（在测试环境下，当然也可在同一台服务器里面，启动不同端口的多个实例也可完成测试。）</li>
<li>Sentinel + Redis distributed system does not guarantee that acknowledged writes are retained during failures，since Redis uses asynchronous replication.（Sentinel+redis分布式集群环境下，节点出现故障时，不保证写一致性，因redis异步复制方式实现集群数据同步）</li>
</ul>
<h5 id="2-2-redis官网Sentinel模式说明"><a href="#2-2-redis官网Sentinel模式说明" class="headerlink" title="2.2 redis官网Sentinel模式说明"></a>2.2 redis官网Sentinel模式说明</h5><p>有些缩写需要说明：</p>
<ul>
<li>Masters are called M1, M2, M3, …, Mn.</li>
<li>replicas are called R1, R2, R3, …, Rn (R stands for <em>replica</em>).（replica也就是slave角色，因为slave有歧视语义，很多中间件不再使用该词描述副角色，例如kafka备份分区的：replica）</li>
<li>Sentinels are called S1, S2, S3, …, Sn.</li>
<li>Clients are called C1, C2, C3, …, Cn.</li>
</ul>
<p>首先看官方首推的 basic setup with three boxes:It is based on three boxes, each box running both a Redis process and a Sentinel process. 每个box代表一个redis节点，确认master失败的选票数为2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">       +----+</span><br><span class="line">       | M1 |</span><br><span class="line">       | S1 |</span><br><span class="line">       +----+</span><br><span class="line">          |</span><br><span class="line">+----+    |    +----+</span><br><span class="line">| R2 |----+----| R3 |</span><br><span class="line">| S2 |         | S3 |</span><br><span class="line">+----+         +----+</span><br><span class="line"></span><br><span class="line">Configuration: quorum &#x3D; 2</span><br></pre></td></tr></table></figure>

<p>If the master M1 fails, S2 and S3 will agree about the failure and will<br>be able to authorize a failover, making clients able to continue.</p>
<p>如果主redis M1宕机（哨兵S1当然也会挂掉），那么其他节点上哨兵S2和哨兵S3发现与S1心跳失败，两者一致同意此时进入故障转移，选举R2为新的master M2。</p>
<p>redis sentinel实现高可用，但也会在某种程度下有丢失有些写数据。例如下面的情况：客户端C1原来与M1连接，写入M1，当M1挂了，到M2起来的这个过程，C1在这一过程写的部分数据会丢失。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">         +----+</span><br><span class="line">         | M1 |</span><br><span class="line">         | S1 | &lt;- C1 (writes will be lost)</span><br><span class="line">         +----+</span><br><span class="line">            |</span><br><span class="line">            &#x2F;</span><br><span class="line">            &#x2F;</span><br><span class="line">+------+    |    +----+</span><br><span class="line">| [M2] |----+----| R3 |</span><br><span class="line">| S2   |         | S3 |</span><br><span class="line">+------+         +----+</span><br></pre></td></tr></table></figure>

<p>&#8195;&#8195;以上情况可通过以下两个配置实现数据丢失最小化。that allows to stop accepting writes if a master detects thatit is no longer able to transfer its writes to the specified number of replicas。<br>这里用到replica关键字单词，在本博客前面kafka文章里面，kafka也有自己的replica名词，不过kafka的replica是指top 分区后的副本，redis这里replica是从服务器（开源界不建议使用slave这个带有歧视的单词）。通过以下设置，只有当前master主机至少还有一个alive的replica才准许外部客户端写入数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br></pre></td></tr></table></figure>

<h4 id="3、一主两从的redis架构配置"><a href="#3、一主两从的redis架构配置" class="headerlink" title="3、一主两从的redis架构配置"></a>3、一主两从的redis架构配置</h4><p>&#8195;&#8195;第2部分的sentinel 2 高可用的前提是基于1主2两从的架构基础上实现的，如下架构，故首先得让一主两从的redis小集群跑起来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">       +----+</span><br><span class="line">       | M1 |</span><br><span class="line">       | S1 |</span><br><span class="line">       +----+</span><br><span class="line">          |</span><br><span class="line">+----+    |    +----+</span><br><span class="line">| R2 |----+----| R3 |</span><br><span class="line">| S2 |         | S3 |</span><br><span class="line">+----+         +----+</span><br><span class="line"></span><br><span class="line">Configuration: quorum &#x3D; 2</span><br></pre></td></tr></table></figure>

<p>&#8195;&#8195;M1为1主，两从：R2、R3，在此基础上，每个节点运行sentinel进程，即可实现redis高可用架构。</p>
<h5 id="3-1-配置主从的redis-conf文件"><a href="#3-1-配置主从的redis-conf文件" class="headerlink" title="3.1 配置主从的redis.conf文件"></a>3.1 配置主从的<strong>redis.conf</strong>文件</h5><p>redis的配置文件的注释有分段说明，这里列出仅需修改的地方：<br>”一主redis“的配置说明：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# NETWORK</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定本机IP</span></span><br><span class="line">bind 182.0.0.10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ GENERAL</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台守护进程运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### SNAPSHOTTING</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放快照（数据日志文件的目录）dump.rdb</span></span><br><span class="line">dir /opt/redis-5.0.7/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ REPLICATION</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1主两从架构里，至少一个有个从服务器在线且在10秒以内延迟，主redis才能对外提供写服务器，否则客户端无法写</span></span><br><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br><span class="line"><span class="meta">#</span><span class="bash">这里也需设置，因为当该master挂了再重启，变成replica后，需要密码去认证新的master</span></span><br><span class="line">masterauth foo123</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SECURITY</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 为master设置认证密码</span></span><br><span class="line">requirepass foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## CLIENTS</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# MEMORY MANAGEMENT</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br></pre></td></tr></table></figure>

<p>”2个replica“节点的redis.conf配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# NETWORK</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定本机IP</span></span><br><span class="line">bind 182.0.0.11</span><br><span class="line"><span class="meta">#</span><span class="bash"> 另外一台从的IP为182.0.0.12</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ GENERAL</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台守护进程运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### SNAPSHOTTING</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放快照（数据日志文件的目录）dump.rdb</span></span><br><span class="line">dir /opt/redis-5.0.7/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ REPLICATION</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告诉从服务器主服务器的认证密码以及IP端口号，新版redis不再使用slave争议词，原版是slaveof</span></span><br><span class="line">replicaof 182.0.0.10 6379</span><br><span class="line">masterauth foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1主两从架构里，至少一个有个从服务器在线且在10秒以内延迟，主redis才能对外提供写服务器，否则客户端无法写</span></span><br><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SECURITY</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 从redis需要密码认证</span></span><br><span class="line">requirepass foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## CLIENTS</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# MEMORY MANAGEMENT</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br></pre></td></tr></table></figure>

<h5 id="3-2-启动和测试主从"><a href="#3-2-启动和测试主从" class="headerlink" title="3.2 启动和测试主从"></a>3.2 启动和测试主从</h5><p>启动所有主从redis-server，后台守护进程运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@p1 opt]# redis-server /opt/redis-5.0.7/redis.conf </span><br></pre></td></tr></table></figure>
<p>注意：<br>如果只启动master，从服务器还未启动，提示没有足够的从服务器在线，无法对外提供写服务。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set test 1</span><br><span class="line">(error) NOREPLICAS Not enough good replicas to write.</span><br></pre></td></tr></table></figure>
<p>这是因为<code>min-replicas-to-write 1</code> 要求最少1个从redis在线后master才能接收客户端写数据。<br>在master set一个key</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@p1 opt]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; set foo 1</span><br><span class="line">​```shell</span><br><span class="line">在两个从服务器get key</span><br><span class="line">​```shell</span><br><span class="line">[root@p2 redis-5.0.7]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@p3 redis-5.0.7]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure>

<p>通过在master 查看主从信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; INFO Replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">min_slaves_good_slaves:2</span><br><span class="line">slave0:ip=182.0.0.11,port=6379,state=online,offset=1958,lag=0</span><br><span class="line">slave1:ip=182.0.0.12,port=6379,state=online,offset=1958,lag=1</span><br><span class="line">master_replid:1f69dd42ecea58d245859fd716c4eaee83a6e753</span><br><span class="line">master_replid2:0000000000000000000000000000000000000000</span><br><span class="line">master_repl_offset:1958</span><br><span class="line">second_repl_offset:-1</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:1</span><br><span class="line">repl_backlog_histlen:1958</span><br></pre></td></tr></table></figure>
<p>注：INFO [section]命令可以查看多个部分信息，也可指定查看某个section的信息<br>以上说明1主两从redis架构已经构建，该模式下，只有master才能写数据，replica只能get数据。如果尝试在replica上写数据，将提示readonly：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; <span class="built_in">set</span> bar <span class="number">2</span></span><br><span class="line">(error) READONLY You can<span class="string">&#x27;t write against a read only replica.</span></span><br></pre></td></tr></table></figure>
<h4 id="4、sentinel-高可用配置"><a href="#4、sentinel-高可用配置" class="headerlink" title="4、sentinel 高可用配置"></a>4、sentinel 高可用配置</h4><p>&#8195;&#8195;sentinel 高可用是基于主-从-从正常运行情况下配置，经过前面2.3点，相信很容易理解该sentinel的逻辑，</p>
<h5 id="4-1-配置sentinel-conf"><a href="#4-1-配置sentinel-conf" class="headerlink" title="4.1 配置sentinel.conf"></a>4.1 配置sentinel.conf</h5><p>&#8195;&#8195;主、从的sentinel.conf都一样，而且也很简单，更改两项属性即可，其他可以按默认值，如果需要调优，可自行参考conf的说明设置相应值。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bind 0.0.0.0</span><br><span class="line">port 26379</span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;</span></span><br><span class="line">sentinel monitor  mymaster  182.0.0.10 6379 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果master设置了密码，那么也告诉sentinel密码</span></span><br><span class="line">sentinel auth-pass mymaster foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  &lt;master-name&gt; 自行定义名称，这里使用mymaster默认值，后面的配置项都用了mymaster这个名，在django项目的settings，redis缓存设置也需要用到该master-name，无特殊需求不用改。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  &lt;quorum&gt; 裁定master挂了的最低通过票数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Tells Sentinel to monitor this master, and to consider it <span class="keyword">in</span> O_DOWN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (Objectively Down) state only <span class="keyword">if</span> at least &lt;quorum&gt; sentinels agree.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告诉Sentinel通监控master，如果有两个sentinel认为master挂了，说明master真的挂了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="4-2-测试redis高可用"><a href="#4-2-测试redis高可用" class="headerlink" title="4.2 测试redis高可用"></a>4.2 测试redis高可用</h5><p>启动所有主从的sentinel 服务，注意如果用 redis-server启动命令，需要带上选项 —-sentinel</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# redis-server /opt/redis-5.0.7/sentinel.conf --sentinel</span><br><span class="line">或者使用</span><br><span class="line">[root@nn opt]# redis-sentinel /opt/redis-5.0.7/sentinel.conf</span><br></pre></td></tr></table></figure>
<p>在master上查看sentinel状态，只需连接sentinel的工作端口即可，可以看到该master下带了两个replica，状态正常：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# redis-cli -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;182.0.0.10&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> 7) &quot;runid&quot;</span><br><span class="line"> 8) &quot;7cdc7e518b592168c94268f7d55fc2d237449118&quot;</span><br><span class="line"> 9) &quot;flags&quot;</span><br><span class="line">10) &quot;master&quot;</span><br><span class="line">11) &quot;link-pending-commands&quot;</span><br><span class="line">12) &quot;0&quot;</span><br><span class="line">13) &quot;link-refcount&quot;</span><br><span class="line">14) &quot;1&quot;</span><br><span class="line">15) &quot;last-ping-sent&quot;</span><br><span class="line">16) &quot;0&quot;</span><br><span class="line">17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">18) &quot;516&quot;</span><br><span class="line">19) &quot;last-ping-reply&quot;</span><br><span class="line">20) &quot;516&quot;</span><br><span class="line">21) &quot;down-after-milliseconds&quot;</span><br><span class="line">22) &quot;30000&quot;</span><br><span class="line">23) &quot;info-refresh&quot;</span><br><span class="line">24) &quot;3335&quot;</span><br><span class="line">25) &quot;role-reported&quot;</span><br><span class="line">26) &quot;master&quot;</span><br><span class="line">27) &quot;role-reported-time&quot;</span><br><span class="line">28) &quot;113804&quot;</span><br><span class="line">29) &quot;config-epoch&quot;</span><br><span class="line">30) &quot;0&quot;</span><br><span class="line">31) &quot;num-slaves&quot;</span><br><span class="line">32) &quot;2&quot;</span><br><span class="line">33) &quot;num-other-sentinels&quot;</span><br><span class="line">34) &quot;1&quot;</span><br><span class="line">35) &quot;quorum&quot;</span><br><span class="line">36) &quot;2&quot;</span><br><span class="line">37) &quot;failover-timeout&quot;</span><br><span class="line">38) &quot;180000&quot;</span><br><span class="line">39) &quot;parallel-syncs&quot;</span><br><span class="line">40) &quot;1&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>查看两个replica上的sentinel服务也正常运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:26379&gt; SENTINEL slaves mymaster</span><br><span class="line">1)  1) &quot;name&quot;</span><br><span class="line">    2) &quot;182.0.0.11:6379&quot;</span><br><span class="line">    3) &quot;ip&quot;</span><br><span class="line">    4) &quot;182.0.0.11&quot;</span><br><span class="line">    5) &quot;port&quot;</span><br><span class="line">    6) &quot;6379&quot;</span><br><span class="line">    7) &quot;runid&quot;</span><br><span class="line">    8) &quot;b7a9000c355584be472fe2409406b772b755b0ed&quot;</span><br><span class="line">    9) &quot;flags&quot;</span><br><span class="line">   10) &quot;slave&quot;</span><br><span class="line">   11) &quot;link-pending-commands&quot;</span><br><span class="line">   12) &quot;0&quot;</span><br><span class="line">   13) &quot;link-refcount&quot;</span><br><span class="line">   14) &quot;1&quot;</span><br><span class="line">   15) &quot;last-ping-sent&quot;</span><br><span class="line">   16) &quot;0&quot;</span><br><span class="line">   17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">   18) &quot;430&quot;</span><br><span class="line">   19) &quot;last-ping-reply&quot;</span><br><span class="line">   20) &quot;430&quot;</span><br><span class="line">   21) &quot;down-after-milliseconds&quot;</span><br><span class="line">   22) &quot;30000&quot;</span><br><span class="line">   23) &quot;info-refresh&quot;</span><br><span class="line">   24) &quot;9197&quot;</span><br><span class="line">   25) &quot;role-reported&quot;</span><br><span class="line">   26) &quot;slave&quot;</span><br><span class="line">   27) &quot;role-reported-time&quot;</span><br><span class="line">   28) &quot;169854&quot;</span><br><span class="line">   29) &quot;master-link-down-time&quot;</span><br><span class="line">   30) &quot;0&quot;</span><br><span class="line">   31) &quot;master-link-status&quot;</span><br><span class="line">   32) &quot;ok&quot;</span><br><span class="line">   33) &quot;master-host&quot;</span><br><span class="line">   34) &quot;182.0.0.10&quot;</span><br><span class="line">   35) &quot;master-port&quot;</span><br><span class="line">   36) &quot;6379&quot;</span><br><span class="line">   37) &quot;slave-priority&quot;</span><br><span class="line">   38) &quot;100&quot;</span><br><span class="line">   39) &quot;slave-repl-offset&quot;</span><br><span class="line">   40) &quot;24585&quot;</span><br><span class="line">2)  1) &quot;name&quot;</span><br><span class="line">    2) &quot;182.0.0.12:6379&quot;</span><br><span class="line">    3) &quot;ip&quot;</span><br><span class="line">    4) &quot;182.0.0.12.5&quot;</span><br><span class="line">    5) &quot;port&quot;</span><br><span class="line">    6) &quot;6379&quot;</span><br><span class="line">    7) &quot;runid&quot;</span><br><span class="line">    8) &quot;39edb70865b99916b1fd0013740e457135fe42e4&quot;</span><br><span class="line">    9) &quot;flags&quot;</span><br><span class="line">   10) &quot;slave&quot;</span><br><span class="line">   11) &quot;link-pending-commands&quot;</span><br><span class="line">   12) &quot;0&quot;</span><br><span class="line">   13) &quot;link-refcount&quot;</span><br><span class="line">   14) &quot;1&quot;</span><br><span class="line">   15) &quot;last-ping-sent&quot;</span><br><span class="line">   16) &quot;0&quot;</span><br><span class="line">   17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">   18) &quot;430&quot;</span><br><span class="line">   19) &quot;last-ping-reply&quot;</span><br><span class="line">   20) &quot;430&quot;</span><br><span class="line">   21) &quot;down-after-milliseconds&quot;</span><br><span class="line">   22) &quot;30000&quot;</span><br><span class="line">   23) &quot;info-refresh&quot;</span><br><span class="line">   24) &quot;9197&quot;</span><br><span class="line">   25) &quot;role-reported&quot;</span><br><span class="line">   26) &quot;slave&quot;</span><br><span class="line">   27) &quot;role-reported-time&quot;</span><br><span class="line">   28) &quot;169855&quot;</span><br><span class="line">   29) &quot;master-link-down-time&quot;</span><br><span class="line">   30) &quot;0&quot;</span><br><span class="line">   31) &quot;master-link-status&quot;</span><br><span class="line">   32) &quot;ok&quot;</span><br><span class="line">   33) &quot;master-host&quot;</span><br><span class="line">   34) &quot;182.0.0.10&quot;</span><br><span class="line">   35) &quot;master-port&quot;</span><br><span class="line">   36) &quot;6379&quot;</span><br><span class="line">   37) &quot;slave-priority&quot;</span><br><span class="line">   38) &quot;100&quot;</span><br><span class="line">   39) &quot;slave-repl-offset&quot;</span><br><span class="line">   40) &quot;24585&quot;</span><br></pre></td></tr></table></figure>



<p>高可用测试：</p>
<p>kill 掉master的redis-server进程和redis-sentinel进程，模拟master服务器宕机情况：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# ps -ef |grep redis</span><br><span class="line">root       7385      1  0 *        00:02:35 redis-server 0.0.0.0:6379</span><br><span class="line">root      20367  20127  0 *    00:00:00 redis-cli -a foo123</span><br><span class="line">root      23760      1  0 *       00:00:03 redis-sentinel 0.0.0.0:26379 [sentinel]</span><br><span class="line">[root@nn redis-5.0.7]# kill -9 7385</span><br><span class="line">[root@nn redis-5.0.7]# kill -9 23760</span><br></pre></td></tr></table></figure>

<p>在replica 1查看目前是否已转移到：可以看到两个replica已经选举出新的master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;182.0.0.11&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p>也可通过redis-cli上查看：目前182.0.0.11已成为新的master，有个正常连接的replica</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info Replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:1</span><br><span class="line">min_slaves_good_slaves:1</span><br><span class="line">slave0:ip=182.0.0.12,port=6379,state=online,offset=146353,lag=1</span><br><span class="line">master_replid:4c1679086e6e4bb0bdd4956bcf979a6b964a8503</span><br><span class="line">master_replid2:ff0d944d22232a7b3489a8544c3109350aac6cd5</span><br><span class="line">master_repl_offset:146494</span><br><span class="line">second_repl_offset:145062</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:130700</span><br><span class="line">repl_backlog_histlen:15795</span><br></pre></td></tr></table></figure>

<p>在新maser set 值，可在剩余的一个replica get到相应的key</p>
<p>将原宕机的master恢复redis进程和sentinel进程，在新的master：182.0.0.11上，查看10节点已加入到replicas列表:<br>slave0:ip=182.0.0.12,port=6379,state=online,offset=211840,lag=1<br>slave1:ip=182.0.0.10,port=6379,state=online,offset=211840,lag=1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">min_slaves_good_slaves:2</span><br><span class="line">slave0:ip=182.0.0.12,port=6379,state=online,offset=211840,lag=1</span><br><span class="line">slave1:ip=182.0.0.10,port=6379,state=online,offset=211840,lag=1</span><br><span class="line">master_replid:4c1679086e6e4bb0bdd4956bcf979a6b964a8503</span><br><span class="line">master_replid2:ff0d944d22232a7b3489a8544c3109350aac6cd5</span><br><span class="line">master_repl_offset:211840</span><br><span class="line">second_repl_offset:145062</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:130700</span><br><span class="line">repl_backlog_histlen:81141</span><br></pre></td></tr></table></figure>
<p>以上完成redis 1主2从的高可用配置和测试，下面将在实际项目中引入。</p>
<h4 id="5、在python项目或者django的项目引入sentinel集群"><a href="#5、在python项目或者django的项目引入sentinel集群" class="headerlink" title="5、在python项目或者django的项目引入sentinel集群"></a>5、在python项目或者django的项目引入sentinel集群</h4><h5 id="5-1-python项目连接sentinel集群"><a href="#5-1-python项目连接sentinel集群" class="headerlink" title="5.1 python项目连接sentinel集群"></a>5.1 python项目连接sentinel集群</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> redis.sentinel <span class="keyword">import</span> Sentinel  </span><br><span class="line">In [<span class="number">4</span>]: st=Sentinel([(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="number">26379</span>),(<span class="string">&#x27;182.0.0.11&#x27;</span>,<span class="number">26379</span>),(<span class="string">&#x27;182.0.0.12&#x27;</span>,<span class="number">26379</span>)]) </span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: st.discover_master(<span class="string">&#x27;mymaster&#x27;</span>)                                     </span><br><span class="line">Out[<span class="number">7</span>]: (<span class="string">&#x27;182.0.0.10&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line">In [<span class="number">8</span>]: st.discover_slaves(<span class="string">&#x27;mymaster&#x27;</span>)                                     </span><br><span class="line">Out[<span class="number">8</span>]: [(<span class="string">&#x27;182.0.0.11&#x27;</span>, <span class="number">6379</span>), (<span class="string">&#x27;182.0.0.12&#x27;</span>, <span class="number">6379</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看用法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [5]: ?st.master_for                                                                               </span><br><span class="line">Signature:</span><br><span class="line">st.master_for(</span><br><span class="line">    service_name,</span><br><span class="line">    redis_class=&lt;class &#x27;redis.client.Redis&#x27;&gt;,</span><br><span class="line">    connection_pool_class=&lt;class &#x27;redis.sentinel.SentinelConnectionPool&#x27;&gt;,</span><br><span class="line">    **kwargs,</span><br><span class="line">)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a redis client instance <span class="keyword">for</span> the ``service_name`` master.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>创建连接实例，kwargs参数跟redis.Redis入参一致</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意别漏了sentinel集群设了密码</span></span><br><span class="line">In [<span class="number">9</span>]: master_rd=st.master_for(service_name=<span class="string">&#x27;mymaster&#x27;</span>,password=<span class="string">&#x27;foo123&#x27;</span>,db=<span class="number">0</span>)  </span><br><span class="line">In [<span class="number">10</span>]: replica_rd=st.slave_for(service_name=<span class="string">&#x27;mymaster&#x27;</span>,password=<span class="string">&#x27;foo123&#x27;</span>,db=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: master_rd              </span><br><span class="line">Out[<span class="number">19</span>]: Redis&lt;SentinelConnectionPool&lt;service=mymaster(master)&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向master写数据，不仅实现高可用，而且还实现读写分离</span></span><br><span class="line">In [<span class="number">23</span>]: master_rd.<span class="built_in">set</span>(<span class="string">&#x27;redis-HA&#x27;</span>,<span class="string">&#x27;good&#x27;</span>)                 </span><br><span class="line">Out[<span class="number">23</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: master_rd.get(<span class="string">&#x27;redis-HA&#x27;</span>)                </span><br><span class="line">Out[<span class="number">24</span>]: <span class="string">b&#x27;good&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果尝试向replica写数据则出错提示：replica只能读数据</span></span><br><span class="line">In [<span class="number">25</span>]: replica_rd.<span class="built_in">set</span>(<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;HA&#x27;</span>)    </span><br><span class="line">ReadOnlyError: You can<span class="string">&#x27;t write against a read only replica.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 从replica读数据，不仅实现高可用，而且还实现读写分离</span></span><br><span class="line"><span class="string">In [28]: replica_rd.get(&#x27;</span>redis-HA<span class="string">&#x27;)            </span></span><br><span class="line"><span class="string">Out[28]: b&#x27;</span>good<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure>

<p>在服务器上把当前10节点master kill掉，再看看python取值是否被影响</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 经过3秒左右，再次获取最新的master可以看到已转移到11节点上，所以这3秒时间实际也是丢失数据的时间窗口</span></span><br><span class="line">In [<span class="number">43</span>]: st.discover_master(<span class="string">&#x27;mymaster&#x27;</span>)                                     Out[<span class="number">43</span>]: (<span class="string">&#x27;182.0.0.11&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前集群仅有一个replica</span></span><br><span class="line">In [<span class="number">44</span>]: st.discover_slaves(<span class="string">&#x27;mymaster&#x27;</span>)</span><br><span class="line">Out[<span class="number">44</span>]: [(<span class="string">&#x27;182.0.0.12&#x27;</span>, <span class="number">6379</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主从切换后，不影响程序获取原有数据</span></span><br><span class="line">In [<span class="number">45</span>]: master_rd.get(<span class="string">&#x27;redis-HA&#x27;</span>)             </span><br><span class="line">Out[<span class="number">45</span>]: <span class="string">b&#x27;good&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: master_rd.<span class="built_in">set</span>(<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;test&#x27;</span>)               </span><br><span class="line">Out[<span class="number">47</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: replica_rd.get(<span class="string">&#x27;bar&#x27;</span>)         </span><br><span class="line">Out[<span class="number">48</span>]: <span class="string">b&#x27;test&#x27;</span></span><br></pre></td></tr></table></figure>
<h5 id="5-2-django项目中使用引入sentinel集群"><a href="#5-2-django项目中使用引入sentinel集群" class="headerlink" title="5.2 django项目中使用引入sentinel集群"></a>5.2 django项目中使用引入sentinel集群</h5><h6 id="5-2-1-单redis实例"><a href="#5-2-1-单redis实例" class="headerlink" title="5.2.1 单redis实例"></a>5.2.1 单redis实例</h6><p>在Django项目开发中，一般可以redis作为django后端cache的中间件，很多需求可以满足：例如验证码、session、缓存查询数据等。</p>
<p>首先需要django-redis这个库支持：pip install django-redis，具体用法参考<a href="http://niwinz.github.io/django-redis/latest/">官方doc</a></p>
<p>如果是单实例redis，在setting的设置相对简单：redis的0号db作为默认缓存，1号db作为hp这个App的数据缓存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将redis设为django缓存</span></span><br><span class="line">CACHES = &#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;BACKEND&#x27;</span>: <span class="string">&#x27;django_redis.cache.RedisCache&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;LOCATION&#x27;</span>: [<span class="string">&#x27;redis://182.0.0.10:6379/0&#x27;</span>], //连接redis url</span><br><span class="line">        <span class="string">&#x27;KEY_PREFIX&#x27;</span>: <span class="string">&#x27;dj&#x27;</span>,   //前缀名</span><br><span class="line">        <span class="string">&#x27;OPTIONS&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;CLIENT_CLASS&#x27;</span>: <span class="string">&#x27;django_redis.client.DefaultClient&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;CONNECTTON_POOL_KWARGS&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;max_connections&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;hp&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;BACKEND&#x27;</span>: <span class="string">&#x27;django_redis.cache.RedisCache&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;LOCATION&#x27;</span>: [<span class="string">&#x27;redis://182.0.0.10:6379/1&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;KEY_PREFIX&#x27;</span>: <span class="string">&#x27;dj:bi&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;OPTIONS&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;CLIENT_CLASS&#x27;</span>: <span class="string">&#x27;django_redis.client.DefaultClient&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;CONNECTTON_POOL_KWARGS&#x27;</span>: &#123;</span><br><span class="line">                    <span class="string">&#x27;max_connections&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">SESSION_ENGINE = <span class="string">&#x27;django.contrib.sessions.backends.cache&#x27;</span></span><br><span class="line"><span class="comment">#django自身运行上下文使用默认数据库redis缓存</span></span><br><span class="line">SESSION_CACHE_ALIAS = <span class="string">&#x27;default&#x27;</span>  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>启动django shell测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@nn hp]<span class="comment"># python manage.py shell</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="keyword">from</span> django.core.cache <span class="keyword">import</span> cache,caches     </span><br><span class="line">In [<span class="number">4</span>]: cache.<span class="built_in">set</span>(<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;fofo&#x27;</span>)                     </span><br><span class="line">Out[<span class="number">4</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用default 库</span></span><br><span class="line">In [<span class="number">5</span>]: cache.get(<span class="string">&#x27;name&#x27;</span>)                                                   </span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">&#x27;fofo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用hp 库</span></span><br><span class="line">In [<span class="number">11</span>]: caches[<span class="string">&#x27;hp&#x27;</span>].<span class="built_in">set</span>(<span class="string">&#x27;code&#x27;</span>,<span class="number">133</span>)                                       </span><br><span class="line">Out[<span class="number">11</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: caches[<span class="string">&#x27;hp&#x27;</span>].get(<span class="string">&#x27;code&#x27;</span>)                                           </span><br><span class="line">Out[<span class="number">12</span>]: <span class="number">133</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：如果使用cache方法，则默认使用default数据库，若需要使用其他名称的数据，需要用caches方法，并通过settings里面redis数据库名称来索引</p>
<p>在redis服务器查看default库相应的key：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) &quot;bar&quot;</span><br><span class="line">2) &quot;redis-HA&quot;</span><br><span class="line">3) &quot;dj:1:name&quot;</span><br></pre></td></tr></table></figure>
<p>因为在cache里面设置key的前缀为dj，加上cache会给key再打上<code>:1:</code>这个默认前缀，故实际存储的完整key名称:”dj:1:name”<br>同理查看db1号库的key</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SELECT 1</span><br><span class="line">OK</span><br><span class="line"> </span><br><span class="line">127.0.0.1:6379[1]&gt; keys *</span><br><span class="line">1) &quot;dj:bi:1:code&quot;</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379[1]&gt; get dj:bi:1:code</span><br><span class="line">&quot;133&quot;</span><br></pre></td></tr></table></figure>

<p>若Django项目中需要使用更多进阶的原生client功能连接redis（支持redis所有方法)），需使用get_redis_connection，建议在实际项目使用该方法获取redis连接对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">13</span>]: <span class="keyword">from</span> django_redis <span class="keyword">import</span> get_redis_connection</span><br><span class="line">In [<span class="number">14</span>]: conn = get_redis_connection()                                     </span><br><span class="line">In [<span class="number">15</span>]: conn.<span class="built_in">set</span>(<span class="string">&#x27;fb&#x27;</span>,<span class="number">12</span>)                                                 </span><br><span class="line">Out[<span class="number">15</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: conn.get(<span class="string">&#x27;fb&#x27;</span>)                                                     </span><br><span class="line">Out[<span class="number">16</span>]: <span class="string">b&#x27;12&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h6 id="5-2-2-django项目的cache引入sentinel模式"><a href="#5-2-2-django项目的cache引入sentinel模式" class="headerlink" title="5.2.2  django项目的cache引入sentinel模式"></a>5.2.2  django项目的cache引入sentinel模式</h6><p>该模式需要安装新的django插件，<a href="https://github.com/KabbageInc/django-redis-sentinel">github地址</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install django-redis-sentinel</span><br><span class="line"><span class="meta">#</span><span class="bash">或者pip install django-redis-sentinel-redux 0.2.0</span> </span><br></pre></td></tr></table></figure>
<p>这个插件其实封装了redis.sentinel将其作为django_redis的插件之一，注意，该插件已不再维护，如果在重要项目上，不建议使用。（重要项目直接用codis，或者redis-cluster，以便可以pip install到相关的redis连接插件，或者自己参考模板写一个）</p>
<p>在setting.py中cache的设置：<br>Location 格式: master_name/sentinel_server:port,sentinel_server:port/db_id</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CACHES = &#123;</span><br><span class="line">        <span class="string">&quot;default&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;BACKEND&quot;</span>: <span class="string">&quot;django_redis.cache.RedisCache&quot;</span>,</span><br><span class="line">            <span class="string">&quot;LOCATION&quot;</span>: <span class="string">&quot;mymaster/188.0.0.10:26379,188.0.0.11:26379,188.0.0.12:26379/0&quot;</span></span><br><span class="line">            <span class="string">&quot;OPTIONS&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;PASSWORD&quot;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">                <span class="string">&quot;CLIENT_CLASS&quot;</span>: <span class="string">&quot;django_redis_sentinel.SentinelClient&quot;</span>,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>该 django-redis-sentinel的插件代码实现不到120行，主要看看connect方法，写数据时用sentinel.discover_master(master_name)获取master去写，读数据时，随机取一个replica 读数据random.choice(sentinel.discover_slaves(master_name))</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect</span>(<span class="params">self, write=<span class="literal">True</span>, SentinelClass=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates a redis connection with connection pool.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> SentinelClass <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        SentinelClass = Sentinel</span><br><span class="line">    self.log.debug(<span class="string">&quot;connect called: write=%s&quot;</span>, write)</span><br><span class="line">    master_name, sentinel_hosts, db = self.parse_connection_string(self._connection_string)</span><br><span class="line"></span><br><span class="line">    sentinel_timeout = self._options.get(<span class="string">&#x27;SENTINEL_TIMEOUT&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">    password = self._options.get(<span class="string">&#x27;PASSWORD&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    sentinel = SentinelClass(sentinel_hosts,</span><br><span class="line">                             socket_timeout=sentinel_timeout,</span><br><span class="line">                             password=password)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> write:</span><br><span class="line">        host, port = sentinel.discover_master(master_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            host, port = random.choice(sentinel.discover_slaves(master_name))</span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            self.log.debug(<span class="string">&quot;no slaves are available. using master for read.&quot;</span>)</span><br><span class="line">            host, port = sentinel.discover_master(master_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> password:</span><br><span class="line">        connection_url = <span class="string">&quot;redis://:%s@%s:%s/%s&quot;</span> % (password, host, port, db)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        connection_url = <span class="string">&quot;redis://%s:%s/%s&quot;</span> % (host, port, db)</span><br><span class="line">    <span class="keyword">return</span> self.connection_factory.connect(connection_url)</span><br></pre></td></tr></table></figure>

<p>注意：如果使用sentinel这个插件，那么需使用django_redis的get_redis_connection，而不是使用django.core.cache的cache</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> django_redis <span class="keyword">import</span> get_redis_connection   </span><br><span class="line">In [<span class="number">2</span>]: conn = get_redis_connection()       </span><br><span class="line">In [<span class="number">3</span>]: conn.<span class="built_in">set</span>(<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;airpods&#x27;</span>)               </span><br><span class="line">Out[<span class="number">3</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: conn.get(<span class="string">&#x27;apple&#x27;</span>)                              </span><br><span class="line">Out[<span class="number">4</span>]: <span class="string">b&#x27;airpods&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：get_redis_connection()有bug，当使用conn实例获取哨兵集群的master或者replica，get_redis_connection()调用了<code>your pythonpath/python3.7/site-packages/redis/client.py </code>里面的<code> self.execute_command(&#39;SENTINEL MASTER&#39;, service_name)</code>，然而该方法是在6379端口连接的client执行<code>SENTINEL MASTER mymaster</code>，由于所有的SENTINEL 命令只能在26379端口下启动的client才能执行，因此直接用get_redis_connection()获取sentinel信息会提示未知命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">8</span>]: conn.sentinel_master(<span class="string">&#x27;mymaster&#x27;</span>) </span><br><span class="line">ResponseError: unknown command `SENTINEL`, <span class="keyword">with</span> args beginning <span class="keyword">with</span>: `MASTER`, `mymaster`, </span><br></pre></td></tr></table></figure>

<p>测试以上出错信息也简单：连接redis-server的6379端口client</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -a foo123 </span><br><span class="line">127.0.0.1:6379&gt; sentinel master mymaster</span><br><span class="line">(error) ERR unknown command `sentinel`, with args beginning with: `master`, `mymaster`, </span><br></pre></td></tr></table></figure>

<p>由于sentinel集群监听的是26379端口来执行有关查询命令（端口号在sentinel.conf文件配置），而get_redis_connection()使用的是6379，用此报错。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -a foo123 -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;188.0.0.10&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> 7) &quot;runid&quot;</span><br><span class="line"> 8) &quot;5537ec765629633406942061f5993e475c42df8e&quot;</span><br><span class="line"> 9) &quot;flags&quot;</span><br><span class="line">10) &quot;master&quot;</span><br></pre></td></tr></table></figure>
<p>解决办法有三种种：<br>第一种：修改redis源码，改get_redis_connection，也不难<br>第二种：sentinel监听端口设为默认6379，redis-server设为其他端口号即可<br>第三种：不需要使用django redis插件，直接用redis原生库自行封装相关sentinel的操作</p>
<p>&#8195;&#8195;综上完成基于sentinel模式的redisHA配置以及详细解释了在python项目和django项目中如何使用sentinel集群模式，个人认为，目前该方案足够支持大部分中小企业内部自行开发项目。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Sentinel模式</tag>
        <tag>redis集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于YARN HA集群的Spark HA集群</title>
    <url>/blog/2019/12/08/%E5%9F%BA%E4%BA%8EYARN%20HA%E9%9B%86%E7%BE%A4%E7%9A%84Spark%20HA%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>，这篇是基于非HA模式下hadoop集群的spark集群HA配置，而本文将给出基于HA模式下hadoop集群的spark集群HA配置，并将yarn HA集群映入到spark中，做资源管理。为何要做些环境的配置呢？因为到本篇文章为止，已经完成hadoop HA集群、hbaseHA集群，hive集群（非HA）、sparkHA集群、flumeHA集群、kafka HA集群，实现实时数据流动，接下的文章重点探讨spark streaming、spark以及pyspark相关知识，这将涉及多个计算任务以及相关计算资源的分配，因此需要借助yarn HA集群强大的资源管理服务来管理spark的计算任务，从而实现完整的、接近生产环境的、HA模式下的大数据实时分析项目的架构。</p>
<a id="more"></a>

<p>服务器资源分配表(仅列出yarn和spark)：</p>
<table>
<thead>
<tr>
<th>节点</th>
<th>yarn 角色</th>
<th>spark 角色</th>
</tr>
</thead>
<tbody><tr>
<td>nn</td>
<td>ResourceManager， NodeManager</td>
<td>Master，Worker</td>
</tr>
<tr>
<td>dn1</td>
<td>NodeManager</td>
<td>Worker</td>
</tr>
<tr>
<td>dn2</td>
<td>ResourceManager， NodeManager</td>
<td>Master，Worker</td>
</tr>
</tbody></table>
<p>&#8195;&#8195;这里再提下yarn管理大数据集群计算中对资源有效管理（主要指CPU、物理内存以及虚拟内存）的重要性：</p>
<blockquote>
<p>&#8195;&#8195;整个集群的计算任务由ResourceManager和NodeManager共同完成，其中，ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为计算任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。</p>
</blockquote>
<p>&#8195;&#8195;因为spark就是负责计算，有大量计算任务要运行，每个任务总得分配cpu和内存给它用，否则某些计算任务会被“饿死”（巧妇难为无米之炊），这种比喻比较形象。</p>
<h3 id="YARN-HA模式的配置"><a href="#YARN-HA模式的配置" class="headerlink" title="YARN HA模式的配置"></a>YARN HA模式的配置</h3><p>&#8195;&#8195;yarn HA模式的运行是于hadoop HA模式运行的，关于hadoop HA部署和测试可以参考本博客文章<a href="https://blog.csdn.net/pysense/article/details/102635656">《基于Hadoop HA集群部署HBase HA集群（详细版）》</a>的第6章内容，考虑到后面文章将会给出各种spark计算任务，结合测试服务器本身cpu和内存资源有限，这里主要重点介绍yarn-site.xml和mapred-site.xml配置文件说明。</p>
<h4 id="完整-yarn-site-xml配置"><a href="#完整-yarn-site-xml配置" class="headerlink" title="完整 yarn-site.xml配置"></a>完整 yarn-site.xml配置</h4><p>&#8195;&#8195;yarn-site的配置其实分为两大块：第一部分为yarn HA集群的配置，第二部分为根据现有测试服务器资源来优化yarn配置。<br>==yarn-site.xml在三个节点上都使用相同配置，无需更改==<br>第一部分：yarn HA集群的配置<br>（注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code>&lt;configuration&gt;&lt;/configuration&gt;</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;!-- 启用yarn HA高可用性 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定resourcemanager的名字，自行命名，跟服务器hostname无关 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hayarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定nn节点为rm1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定dn2节点为rm2  --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;dn2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定当前机器nn作为主rm1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定zookeeper集群机器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上将nn和dn2作为yarn集群主备节点，对应的id为rm1、rm2</p>
<p>第二部分：yarn的优化配置<br>A、禁止检查每个任务正使用的物理内存量、虚拟内存量是否可用<br>若任务超出分配值，则将其杀掉。考虑到作为测试环境，希望看到每个job都能正常运行，以便记录其他观测事项，这里将其关闭。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> 	&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line"> 	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> 	&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line"> 	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>B、配置RM针对单个Container能申请的最大资源或者RM本身能配置的最大内存<br>配置解释：单个容器可申请的最小与最大内存，Application在运行申请内存时不能超过最大值，小于最小值则分配最小值，例如在本文测试中，因计算任务较为简单，无需太多资源，故最小值设为512M，最大值设为1024M。注意最大最不小于1G，因为yarn给一个executor分配512M时，还需要另外动态的384M内存（Required executor memory (512), overhead (384 MB)）。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;512&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>若将yarn.scheduler.maximum-allocation-mb设为例如512M，spark on yarn就会启动失败。</p>
<p>C、NM的内存资源配置，主要是通过下面两个参数进行的</p>
<p>第一个参数：每个节点可用的最大内存，默认值为-1，代表着yarn的NodeManager占总内存的80%，本文中，物理内存为1G</p>
<p>第二个参数：NM的虚拟内存和物理内存的比率，默认为2.1倍</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;nm向本机申请的最大物理内存，默认8G&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>vmem-pmem-ratio的默认值为2.1，由于本机器中，每个节点的物理内存为1G，因此单个RM拿到最大虚拟内存为2.1G，例如在跑spark任务，会出现<code>2.5 GB of 2.1 GB virtual memory used. Killing container</code>的提示，Container申请的资源为2.5G，已经超过默认值2.1G，当改为3倍时，虚拟化够用，故解决可该虚拟不足的情况。</p>
<h4 id="mapred-site-xml的配置文件说明"><a href="#mapred-site-xml的配置文件说明" class="headerlink" title="mapred-site.xml的配置文件说明"></a>mapred-site.xml的配置文件说明</h4><p>mapred-site的配置其实分为两大块：第一部分为mapreduce的基本配置，第二部分为根据现有测试服务器资源来优化mapreduce计算资源分配的优化配置。<br>==mapred-site.xml在三个节点上都需要配置，只需把nn主机名改为当前节点的主机名即可==<br>第一部分：mapreduce的基本配置<br>（注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code>&lt;configuration&gt;&lt;/configuration&gt;</code>）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> &lt;!-- 使用yarn框架来管理MapReduce --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!-- mp所需要hadoop环境 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:10020&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定nn作为jobhistory服务器 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">  		&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line"> 		 &lt;value&gt;nn:19888&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!--存放已完成job的历史日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放正在运行job的历史日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done_intermediate&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放yarn stage的日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/history/staging&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>这里主要配置开启jobhistory服务以及MapReduce多种日志存放</p>
<p>第二部分：mapreduce的优化项</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个mapper任务的物理内存限制&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;200&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个reducer任务的物理内存限制&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;每个mapper任务申请的虚拟cpu核心数，默认1&lt;/description&gt; </span><br><span class="line"> &lt;/property&gt; </span><br><span class="line"> </span><br><span class="line"> &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;每个reducer任务申请的虚拟cpu核心数，默认1&lt;/description&gt; </span><br><span class="line"> &lt;/property&gt; </span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx100m&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;mapper阶段的JVM的堆大小&lt;/description&gt;     </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx200m&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;reduce阶段的JVM的堆大小&lt;/description&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>根据当前服务器物理配置资源，在内存和CPU方面给mapper和reducer任务进行调优。</p>
<h4 id="yarn-HA的启动"><a href="#yarn-HA的启动" class="headerlink" title="yarn HA的启动"></a>yarn HA的启动</h4><p>首先确保hadoop HA集群已正常启动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure>
<p>启动yarn HA服务，只需在nn节点启动yarn后，其他节点会自动启动相应服务。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# start-yarn.sh </span><br><span class="line">[root@nn sbin]# yarn rmadmin -getServiceState rm1</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# yarn rmadmin -getServiceState rm2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure>
<p>以上完成yarn HA配置，因为涉及hadoop HA和调优，因此不建议刚入门的同学就按此配置继续测试，建议从最原始、最简单的非HA hadoop开始着手。<br>下面开始配置spark。</p>
<h3 id="spark-HA-集群及其基本测试"><a href="#spark-HA-集群及其基本测试" class="headerlink" title="spark HA 集群及其基本测试"></a>spark HA 集群及其基本测试</h3><h4 id="修改spark配置"><a href="#修改spark配置" class="headerlink" title="修改spark配置"></a>修改spark配置</h4><p>&#8195;&#8195;经历第1章节繁琐的yarn HA配置后， 当资源管理问题得到妥善解决，那么接下的计算任务将实现的非常流畅。<br>spark HA集群详细的部署和测试，请参考<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>的第8章节，本文不再累赘。<br>&#8195;&#8195;把spark 的任务交给yarn管理还需要在HA集群上再加入部分配置，改动也简单 ，只需在spark-defaults.conf和spark-env.sh改动。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf]# vi spark-defaults.conf</span><br><span class="line"></span><br><span class="line">#spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"></span><br><span class="line"># spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;hdapp&#x2F;directory</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.yarn.jars                  hdfs:&#x2F;&#x2F;hdapp&#x2F;spark_jars&#x2F;*</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure>
<p>重点配置项目说明：</p>
<p>原standalone模式下：spark.master设为 spark://nn:7077</p>
<p>因为spark已经配成HA模式，因此无需指定master是谁，交由zookeeper管理。</p>
<p>spark.eventLog.dir              hdfs://hdapp/directory<br>这里hdfs路径从nn:9000改为hdapp，是因为hadoop已经配置为HA模式，注意集群模式下是不需要加上端口： hdfs://hdapp:9000/directory，这会导致NameNode无法解析host部分。</p>
<p>spark.yarn.jars                  hdfs://hdapp/spark_jars/*<br>这里需要将spark跟目录下的jar包都上传到hdfs指定的spark_jars目录下，若不这么处理，每次提交spark job时，客户端每次得先上传这些jar包到hdfs，然后再分发到每个NodeManager，导致任务启动很慢。而且启动spark也会提示：<br>==WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.==</p>
<p>解决办法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -mkdir  &#x2F;spark_jars</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -put  jars&#x2F;*  &#x2F;spark_jars</span><br></pre></td></tr></table></figure>
<p>spark-defaults.conf在三个节点上使用相同配置。</p>
<p>spark-env.sh的配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf]# vi spark-env.sh</span><br><span class="line"># 基本集群配置</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;opt&#x2F;scala-2.12.8</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url&#x3D;nn:2181,dn1:2181,dn2:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark&quot;</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line"></span><br><span class="line"># yarn模式下的调优配置</span><br><span class="line"># Options read in YARN client&#x2F;cluster mode</span><br><span class="line">export SPARK_WORKER_MEMORY&#x3D;512M</span><br><span class="line"># - SPARK_CONF_DIR, Alternate conf dir. (Default: $&#123;SPARK_HOME&#125;&#x2F;conf) 无需设置，使用默认值</span><br><span class="line"># - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line"># - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN 上面HADOOP_CONF_DIR以已设置即可</span><br><span class="line"># - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1). 无需设置，默认使用1个vcpu</span><br><span class="line"># - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">export SPARK_EXECUTOR_MEMORY&#x3D;512M</span><br><span class="line"># - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">export SPARK_EXECUTOR_MEMORY&#x3D;512M</span><br><span class="line"></span><br><span class="line"># 存放计算过程的日志</span><br><span class="line">export SPARK_HISTORY_OPTS&#x3D;&quot;</span><br><span class="line">-Dspark.history.ui.port&#x3D;9001</span><br><span class="line">-Dspark.history.retainedApplications&#x3D;5</span><br><span class="line">-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;hdapp&#x2F;directory&quot;</span><br></pre></td></tr></table></figure>
<p>以上的driver和executor的可用内存设为512M，考虑到测试服务器内存有限的调优。若生产服务器，一般32G或者更大的内存，则可以任性设置。</p>
<h4 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h4><p>在nn节点上，启动wokers： start-slaves.sh，该命令自动启动其他节点的worker<br>在nn节点和dn2节点启动master进程：start-master.sh<br>查看nn:8080和dn2:8080的spark web UI是否有active以及standby模式。<br>跑一个wordcount例子，测试spark集群能否正常计算结果。<br>创建一个本地文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  vi &#x2F;opt&#x2F;foo.txt</span><br><span class="line">spark on yarn</span><br><span class="line">yarn </span><br><span class="line">spark HA</span><br></pre></td></tr></table></figure>
<p>启动pyspark，连接到spark集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  .&#x2F;bin&#x2F;pyspark --name bar --driver-memory 512M   --master  spark:&#x2F;&#x2F;nn:7077</span><br><span class="line"># 读取本地文件&#x2F;opt&#x2F;foo.txt</span><br><span class="line">&gt;&gt;&gt; df&#x3D;sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;opt&#x2F;foo.txt&quot;)</span><br><span class="line"># 切分单词，过滤空值</span><br><span class="line">&gt;&gt;&gt; words &#x3D; df.flatMap(lambda line: line.split(&#39; &#39;)).filter(lambda x: x !&#x3D;&quot;&quot;)</span><br><span class="line">&gt;&gt;&gt; words.collect()</span><br><span class="line">[u&#39;spark&#39;, u&#39;on&#39;, u&#39;yarn&#39;, u&#39;yarn&#39;,u&#39;spark&#39;, u&#39;HA&#39;]</span><br><span class="line"># 将个word映射为（word，1）这样的元组，在reduce汇总。</span><br><span class="line">&gt;&gt;&gt; counts &#x3D; words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)</span><br><span class="line">&gt;&gt;&gt; counts.collect()</span><br><span class="line">[(u&#39;spark&#39;, 2), (u&#39;yarn&#39;, 2), (u&#39;on&#39;, 1), (u&#39;HA&#39;, 1)]   </span><br></pre></td></tr></table></figure>
<p>以上完成spark HA集群和测试。</p>
<h3 id="spark-on-yarn"><a href="#spark-on-yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h3><p>spark on yarn意思是将spark计算人任务提交到yarn集群上运行。</p>
<h4 id="spark集群跑在yarn上的两种方式"><a href="#spark集群跑在yarn上的两种方式" class="headerlink" title="spark集群跑在yarn上的两种方式"></a>spark集群跑在yarn上的两种方式</h4><p>根据spark官网的<a href="http://spark.apache.org/docs/latest/running-on-yarn.html">文档说明</a>，这里引用其内容：</p>
<blockquote>
<p>There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>
</blockquote>
<p>cluster模式下，spark driver 在 AM里运行，客户端（或者应用程序）在提交完任务（初始化）后可直接退出，作业会继续在 YARN 上运行。显然cluster 模式不适合交互式操作。cluster模式的spark计算结果可以保持到<br>外部数据库，例如hbase。这部分内容将是spark streaming可以完成的环境，spark streaming以yarn cluster模式运行，实时将处理结果存到hbase里，web BI 应用再从hbase取数据。</p>
<p>client模式下，spark driver是在本地环境运行，AM仅负责向yarn请求计算资源（Executor 容器），例如交互式运行基本的操作。</p>
<p>在前面第2节的word count例子里，用下面的启动命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# pyspark --name bar --driver-memory 512M   --master  spark:&#x2F;&#x2F;nn:7077</span><br></pre></td></tr></table></figure>
<p>该命令启动是一个spark shell进程，没有引入yarn管理其资源，因此在yarn集群的管理页面<code>http://nn:8088/cluster/apps/RUNNING</code>，将不会 bar这个application。</p>
<h4 id="测试spark-on-yarn"><a href="#测试spark-on-yarn" class="headerlink" title="测试spark on yarn"></a>测试spark on yarn</h4><p>只需在启动spark shell时，将<code>--master spark://nn:7077</code> 改为<br><code> --master yarn --deploy-mode cluster</code>或者<code> --master yarn --deploy-mode client</code>，那么spark提交的任务就会交由yarn集群管理<br>还是以word count为例，使用yarn client模式启动spark<br>创建测试文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;opt&#x2F;yarn-word-count.txt</span><br><span class="line">spark on yarn </span><br><span class="line">spark HA </span><br><span class="line">yarn HA</span><br></pre></td></tr></table></figure>
<p>启动driver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  pyspark --name client_app	 --driver-memory 512M  --executor-memory 512M  --master yarn --deploy-mode client</span><br><span class="line">Python 2.7.5 (default, Oct 30 2018, 23:45:53) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     &#x2F; __&#x2F;__  ___ _____&#x2F; &#x2F;__</span><br><span class="line">    _\ \&#x2F; _ \&#x2F; _ &#96;&#x2F; __&#x2F;  &#39;_&#x2F;</span><br><span class="line">   &#x2F;__ &#x2F; .__&#x2F;\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\_\   version 2.4.4</span><br><span class="line">      &#x2F;_&#x2F;</span><br><span class="line"></span><br><span class="line">Using Python version 2.7.5 (default, Oct 30 2018 23:45:53)</span><br><span class="line">SparkSession available as &#39;spark&#39;.</span><br><span class="line">&gt;&gt;&gt; sc</span><br><span class="line">&lt;SparkContext master&#x3D;yarn appName&#x3D;client_app	&gt;</span><br></pre></td></tr></table></figure>

<p>这里driver和executor都是以最小可用内存512来启动spark-shell<br>因为该spark 任务是提交到yarn 上运行，所以在spark web ui后台：<code>http://nn:8080</code>，running application 为0<br><img src="https://img-blog.csdnimg.cn/20191208110757819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这是需要去yarn后台入口：<code>http://nn:8088</code>，可以看到刚提交的计算任务：<br><img src="https://img-blog.csdnimg.cn/20191208111208596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到该application（计算任务）分配了3个Container<br><img src="https://img-blog.csdnimg.cn/20191208111547983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">通过查看该applicationMaster管理页面，可以看到client-yarn这个app更为详细的计算过程，例如该wordcount在reduceByKey DAG可视化过程。<br><img src="https://img-blog.csdnimg.cn/20191208111947653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>yarn cluster模式下，因为它不是打开一个spark shell让你交互式输入数据处理逻辑，所以需先把处理逻辑封装成一个py模块。<br>以上面的word count为例：<br>word_count.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">word_count</span>():</span></span><br><span class="line">	    conf = SparkConf().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&#x27;cluster-yarn&#x27;</span>)</span><br><span class="line">	    sc = SparkContext(conf=conf)</span><br><span class="line">	    <span class="comment"># 统计文件中包含mape的行数，并打印第一行</span></span><br><span class="line">	    df = sc.textFile(<span class="string">&quot;/tmp/words.txt&quot;</span>)</span><br><span class="line">	    words = df.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&#x27; &#x27;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x !=<span class="string">&quot;&quot;</span>)</span><br><span class="line">	    <span class="built_in">print</span> words.collect()</span><br><span class="line">	    counts = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">	    <span class="built_in">print</span> counts.collect()</span><br><span class="line">	    sc.stop</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	word_count()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>需要使用spark-submit 提交到yarn</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 spark-2.4.4-bin-hadoop2.7]#  ./bin/spark-submit  --driver-memory 512M  --executor-memory 512M  --master yarn  --deploy-mode cluster  --py-files word_count.py</span><br></pre></td></tr></table></figure>
<p>在yarn管理也可以看到该app，application的命名好像直接用脚本名字，而不是指定的cluster-yarn<br><img src="https://img-blog.csdnimg.cn/20191208120528893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">关于如何提交py文件，官方也给出指引：</p>
<blockquote>
<p>For Python, you can use the –py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application. If you depend on multiple Python files we recommend packaging them into a .zip or .egg.</p>
</blockquote>
<p>如有多个py文件（例如1.py依赖2.py和3.py），需要通过将其打包为.zip或者.egg包： –py-files tasks.zip</p>
<h4 id="提交spark-application的多种方式"><a href="#提交spark-application的多种方式" class="headerlink" title="提交spark application的多种方式"></a>提交spark application的多种方式</h4><p>spark运行有standalone模式（分local、cluster）、on yarn模式（分client、cluster）还有on k8s，而且可以附带jar包或者py包，多种提交的方式的命令模板怎么写？网上其实很多类似文章，但都是给的某个模式的某种文件的提交方式，其实在spark官网的<a href="http://spark.apache.org/docs/latest/submitting-applications.html">submitting-applications</a>章节给出详细的多种相关命令模板。这里统一汇总：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run application locally on 8 cores 本地模式</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master local[8] \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  standalone 集群下的client模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  standalone 集群下的cluster模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> on yarn 集群，且用的class文件和jar包</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a YARN cluster</span></span><br><span class="line">export HADOOP_CONF_DIR=XXX</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \  # can be client for client mode</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里给出如何传入py文件，可以不写 --py-files 选项</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Mesos cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master mesos://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Kubernetes cluster <span class="keyword">in</span> cluster deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master k8s://xx.yy.zz.ww:443 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure>

<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&#8195;&#8195;本文内容主要为后面的文章——spark streaming 与kafka集群的实时数据计算做铺垫，考虑到测试环境环境资源有限，在做spark streaming的时候，将不会以spark HA模式运行，也不会将任务提交到yarn集群上，而是用一节点作为spark streaming计算节点，具体规划参考该文。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>YARN集群</tag>
        <tag>Spark集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于hadoop3.1.2分布式平台上部署spark HA集群</title>
    <url>/blog/2019/10/13/%E5%9F%BA%E4%BA%8Ehadoop3.1.2%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B3%E5%8F%B0%E4%B8%8A%E9%83%A8%E7%BD%B2spark%20HA%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>&#8195;&#8195;在此文章<a href="https://blog.csdn.net/pysense/article/details/102490212">《基于Centos7.5完整部署分布式Hadoop3.1.2》</a>里，已经给出详细的hadoop和yarn的部署过程，既然已经解决了大数据开发中“hdfs”的数据存储部署，那么就要考虑如何基于底层分布式文件基础上运行计算框架，以便进行更高层次的应用开发。在本篇文章中，将给出完整部署spark计算框架集群。</p>
<a id="more"></a>

<h3 id="1、spark版本（仅列出spark相关）"><a href="#1、spark版本（仅列出spark相关）" class="headerlink" title="1、spark版本（仅列出spark相关）"></a>1、spark版本（仅列出spark相关）</h3><p>spark-2.4.4-bin-hadoop2.7，该版本的spark支持hadoop2.7以及之后的版本</p>
<p>scala-2.13.1：使用Scala语言开发数据处理逻辑，当然也可使用python进行spark数据处理逻辑开发，官网有给出pyspark相关指导教程。</p>
<p>三台节点都需要配置，目录放置路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">hadoop-3.1.2    jdk1.8.0_161  scala-2.13.1  spark-2.4.4-bin-hadoop2.7</span><br></pre></td></tr></table></figure>

<p>spark HA集群规划，这里只列出spark HA集群的有关进程，hadoop的进程不再列出</p>
<table>
<thead>
<tr>
<th>IP，hostname</th>
<th>spark集群中负责的角色</th>
<th>Spark 路径</th>
<th>Scala路径</th>
<th>物理内存</th>
</tr>
</thead>
<tbody><tr>
<td>192.188.0.4，nn</td>
<td>master，worker，spark-history-server</td>
<td>/opt/spark-2.4.4-bin-hadoop2.7</td>
<td>/opt/scala-2.13.1</td>
<td>2G</td>
</tr>
<tr>
<td>192.188.0.5，dn1</td>
<td>master，worker</td>
<td>/opt/spark-2.4.4-bin-hadoop2.7</td>
<td>/opt/scala-2.13.1</td>
<td>1G</td>
</tr>
<tr>
<td>192.188.0.6，dn2</td>
<td>master，worker</td>
<td>/opt/spark-2.4.4-bin-hadoop2.7</td>
<td>/opt/scala-2.13.1</td>
<td>1G</td>
</tr>
</tbody></table>
<p>这里spark master节点nn的物理内存给了2G，因为该节点不仅仅启动了spark相关主服务，还得启动hadoop相关主服务，如果物理内存不足，在后面章节中启动spark-shell或者跑application都无法正常启动，提示资源不足。</p>
<h3 id="2、设置path环境"><a href="#2、设置path环境" class="headerlink" title="2、设置path环境"></a>2、设置path环境</h3><p>三个节点都需要设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8.0_161</span><br><span class="line">export HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export SCALA_HOME=/opt/scala-2.13.1</span><br><span class="line">export SPARK_HOME=/opt/spark-2.4.4-bin-hadoop2.7/</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SCALA_HOME/bin:</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure>



<h3 id="3、配置spark集群的相关文件"><a href="#3、配置spark集群的相关文件" class="headerlink" title="3、配置spark集群的相关文件"></a>3、配置spark集群的相关文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 拷贝一份spark-env.sh文件用于配置spark环境</span></span><br><span class="line">[root@dn1 ~]# cp /opt/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh.template /opt/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh</span><br><span class="line">[root@dn1 ~]# cd /opt/spark-2.4.4-bin-hadoop2.7/</span><br><span class="line"></span><br><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# ls conf/</span><br><span class="line">docker.properties.template   slaves.template</span><br><span class="line">fairscheduler.xml.template   spark-defaults.conf.template</span><br><span class="line">log4j.properties.template    spark-env.sh</span><br><span class="line">metrics.properties.template  spark-env.sh.template</span><br><span class="line"></span><br><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# vi conf/spark-env.sh</span><br></pre></td></tr></table></figure>


<p>只需在spark-env.sh文件头部加入以下环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export SCALA_HOME=/opt/scala-2.12.8</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8.0_161</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设定192.188.0.4，nn节点为spark master</span></span><br><span class="line">export SPARK_MASTER_IP=nn</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop的配置文件**site.xml所在目录</span></span><br><span class="line">export HADOOP_CONF_DIR=/opt/hadoop-3.1.2/etc/hadoop</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>修改conf目录下的slaves文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@dn1 conf]# cp slaves.template slaves</span><br><span class="line">[root@dn1 conf]# vi slaves</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>为减少spark主节点nn的内存资源消耗，这里不再将nn设为Worker角色</p>
<p>将修改过的两个文件拷贝到其他两个节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# scp -r conf&#x2F; dn1:&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;</span><br><span class="line"></span><br><span class="line">[root@dn1 spark-2.4.4-bin-hadoop2.7]# scp -r conf&#x2F; dn2:&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;</span><br></pre></td></tr></table></figure>

<h3 id="4、启动spark集群进程"><a href="#4、启动spark集群进程" class="headerlink" title="4、启动spark集群进程"></a>4、启动spark集群进程</h3><h4 id="4-1-启动spark-master进程"><a href="#4-1-启动spark-master进程" class="headerlink" title="4.1 启动spark-master进程"></a>4.1 启动spark-master进程</h4><p>spark的进程启动是有步骤的，需先启动master服务，再启动worker进程，因为worker启动需要通过spark://nn:7077 spark协议的7077端口与master节点通信，否则master节点和worker之间无法形成集群。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# .&#x2F;start-master.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-nn.out</span><br><span class="line"></span><br><span class="line"># nn节点上</span><br><span class="line">[root@nn sbin]# jps</span><br><span class="line">24292 DataNode</span><br><span class="line">24155 NameNode</span><br><span class="line">25339 NodeManager</span><br><span class="line">30638 Master</span><br><span class="line">30750 Jps</span><br><span class="line"></span><br><span class="line"># dn1节点上：</span><br><span class="line">[root@dn1 ~]# jps</span><br><span class="line">18480 Jps</span><br><span class="line">12805 ResourceManager</span><br><span class="line">12365 DataNode</span><br><span class="line">12942 NodeManager</span><br><span class="line"># dn2节点上：</span><br><span class="line">[root@dn2 ~]# jps</span><br><span class="line">13144 DataNode</span><br><span class="line">13244 SecondaryNameNode</span><br><span class="line">19437 Jps</span><br><span class="line">13599 NodeManager</span><br></pre></td></tr></table></figure>

<p>以上表示主节点已经启动Master进程，其他节点dn1和dn2还未启动Worker进程。可以通过log日志文件内容看到其启动过程，这里不再给出，当然更直观的方式是在web端查看：页面<code>http://nn:8080/</code>或者<code>http://192.188.0.4:8080</code>可以直观看到master状态，此时workers还未启动,可以按到显示workers数量为0</p>
<h4 id="4-2-在spark-master启动后，启动Worker节点"><a href="#4-2-在spark-master启动后，启动Worker节点" class="headerlink" title="4.2 在spark master启动后，启动Worker节点"></a>4.2 在spark master启动后，启动Worker节点</h4><p>在spark主节点上nn，启动workers，这些workers的对应的节点就是路径<code>/opt/spark-2.4.4-bin-hadoop2.7/conf</code>下slaves文件配置到2个节点：dn1,dn2。</p>
<ul>
<li><p>启动spark集群上所有的workers节点命令：start-slaves.sh</p>
</li>
<li><p>启动本节点上的work进程：start-slave.sh</p>
</li>
<li><p>可以对比其shell脚本的差别，在start-slaves.sh脚本后面可以看到</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;$&#123;SPARK_HOME&#125;&#x2F;sbin&#x2F;start-slave.sh&quot; &quot;spark:&#x2F;&#x2F;$SPARK_MASTER_HOST:$SPARK_MASTER_PORT&quot;</span><br></pre></td></tr></table></figure>

<p>start-slaves.sh其实是在其他节点运行<code>./start-slave.sh spark://nn:7077</code>实现批量启动其他work节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;sbin</span><br><span class="line">[root@nn sbin]# .&#x2F;start-slaves.sh </span><br><span class="line">dn1: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn1.out</span><br><span class="line">dn2: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn2.out</span><br><span class="line"></span><br><span class="line"># # nn节点上spark Master</span><br><span class="line">[root@nn sbin]# jps</span><br><span class="line">24292 DataNode</span><br><span class="line">24155 NameNode</span><br><span class="line">25339 NodeManager</span><br><span class="line">30638 Master</span><br><span class="line">30750 Jps</span><br><span class="line"></span><br><span class="line"># dn1节点上spark Worker进程</span><br><span class="line">[root@dn1 ~]# jps</span><br><span class="line">12805 ResourceManager</span><br><span class="line">23045 Jps</span><br><span class="line">23000 Worker</span><br><span class="line">12365 DataNode</span><br><span class="line">12942 NodeManager</span><br><span class="line"></span><br><span class="line"># dn2节点上spark Worker进程</span><br><span class="line">[root@dn2 ~]# jps</span><br><span class="line">24789 Worker</span><br><span class="line">24837 Jps</span><br><span class="line">13144 DataNode</span><br><span class="line">13244 SecondaryNameNode</span><br><span class="line">13599 NodeManager</span><br></pre></td></tr></table></figure>
<p>在spark的master web端:<code>http://nn:8080</code>或者<code>http://192.188.0.4:8080</code>可以看到2个worker均active<br><img src="https://img-blog.csdnimg.cn/20191013195910122.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">每个worker的最大可用内存512m，vCPU 1颗</p>
<h3 id="5、设置启动spark-shell的默认环境（非常关键的配置）"><a href="#5、设置启动spark-shell的默认环境（非常关键的配置）" class="headerlink" title="5、设置启动spark-shell的默认环境（非常关键的配置）"></a>5、设置启动spark-shell的默认环境（非常关键的配置）</h3><h4 id="5-1-配置spark-defaults-conf"><a href="#5-1-配置spark-defaults-conf" class="headerlink" title="5.1 配置spark-defaults.conf"></a>5.1 配置spark-defaults.conf</h4><p>注意，在启动spark-shell之前，如果需要对/opt/spark-2.4.4-bin-hadoop2.7/conf目录下的配置文件：<code>spark-defaults.conf.template</code>相关参数进行修改，例如需要结合spark-history-server的配置，那么除了新建一份<code>spark-defaults.conf</code>，还需要对里面参数正确，否则启动spark-shell会提示出错并退出</p>
<p>因为本文测试使用2G内存，所以需要对配置文件里面做修改，修改如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# cp spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">[root@nn conf] spark-defaults.conf</span><br><span class="line"></span><br><span class="line"># spark集群主节点的入口</span><br><span class="line">spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"># 在hadoop core-site.xml设置的hdfs入口地址，directory需自行在hdfs文件系统上创建</span><br><span class="line"># 通过命令可创建：hdfs dfs -mkdir &#x2F;directory</span><br><span class="line"># 同时日志目录作为spark-history-server的日志目录</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line"></span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark主节点driver内存，默认为5G，这里设为1g</span><br><span class="line">spark.driver.memory              1g</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure>

<p>如果以上入口地址设错，或者未在namenode节点的hdfs文件系统上创建directory目录，都会导致无法启动spark-shell</p>
<p>==若不对spark-defaults.conf.template参数修改，例如不需要启动history服务，则无需创建spark-defaults.conf文件，也无需进行上述设置，可以直接启动spark-shell==</p>
<h4 id="5-2-spark-defaults-conf的详细的设置"><a href="#5-2-spark-defaults-conf的详细的设置" class="headerlink" title="5.2  spark-defaults.conf的详细的设置"></a>5.2  spark-defaults.conf的详细的设置</h4><p>参考<a href="http://spark.apache.org/docs/latest/configuration.html">官网配置指引</a><br>其实该配置就是用来spark集群调优的关键配置</p>
<p>主要分为几大部分的参数配置：</p>
<ul>
<li>Application Properties</li>
<li>Runtime Environment</li>
<li>Spark UI</li>
<li> Compression and Serialization</li>
<li>Memory Management</li>
<li>Execution Behavior</li>
<li>Networking</li>
<li>Scheduling</li>
<li>Dynamic Allocation</li>
</ul>
<h4 id="5-3-启动spark-shell"><a href="#5-3-启动spark-shell" class="headerlink" title="5.3 启动spark-shell"></a>5.3 启动spark-shell</h4><p>首次启动spark-shell时，会出现‘WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform’的提示，参考文章提示：centos预装的glibc库是2.17版本，而hadoop期望是2.14版本，可以忽略该警告，在hadoop日志配置文件设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn2 ~]# vi &#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop&#x2F;log4j.properties </span><br><span class="line"># 新增以下内容</span><br><span class="line">log4j.logger.org.apache.hadoop.util.NativeCodeLoader&#x3D;ERROR</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动成功提示</span><br><span class="line">[root@nn bin]# spark-shell                </span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Spark context Web UI available at http:&#x2F;&#x2F;nn:4040</span><br><span class="line">Spark context available as &#39;sc&#39; (master &#x3D; spark:&#x2F;&#x2F;nn:7077, app id &#x3D; app-2019*****-0004).</span><br><span class="line">Spark session available as &#39;spark&#39;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     &#x2F; __&#x2F;__  ___ _____&#x2F; &#x2F;__</span><br><span class="line">    _\ \&#x2F; _ \&#x2F; _ &#96;&#x2F; __&#x2F;  &#39;_&#x2F;</span><br><span class="line">   &#x2F;___&#x2F; .__&#x2F;\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\_\   version 2.4.4</span><br><span class="line">      &#x2F;_&#x2F;</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_161)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br></pre></td></tr></table></figure>

<p>可以在<code>http://nn:4040</code>查看，若有计算任务提交，可以直观查看spark job 、excutors等进度，参考官方说明：</p>
<blockquote>
<p>Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:</p>
<ul>
<li>A list of scheduler stages and tasks</li>
<li>A summary of RDD sizes and memory usage</li>
<li>Environmental information.</li>
<li>Information about the running executors</li>
</ul>
</blockquote>
<p>但以上启动是有问题的，表面上看，spark-shell已正常启动，但测试机器最大内存为2G，启动spark-shell若不限定executor-memory内存使用（默认值1G）那么在执行计算任务时，spark-shell会一直提示 scheduler资源不足：</p>
<blockquote>
<p>WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory</p>
</blockquote>
<p>导致job一直waiting状态<br>解决办法：<br>启动spark-shell限制相关资源的使用:</p>
<p><code>spark-shell --executor-memory 512m  --total-executor-cores 3 --executor-cores 1</code></p>
<h3 id="6、在spark-shell交互式计算words"><a href="#6、在spark-shell交互式计算words" class="headerlink" title="6、在spark-shell交互式计算words"></a>6、在spark-shell交互式计算words</h3><h4 id="6-1-存放words的文件已经上传到hdfs文件系统上的-app目录下"><a href="#6-1-存放words的文件已经上传到hdfs文件系统上的-app目录下" class="headerlink" title="6.1 存放words的文件已经上传到hdfs文件系统上的/app目录下"></a>6.1 存放words的文件已经上传到hdfs文件系统上的/app目录下</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# hdfs dfs -ls &#x2F;app</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup         41 ** &#x2F;app&#x2F;title.txt</span><br><span class="line">-rw-r--r--   3 root supergroup         76 ** &#x2F;app&#x2F;words.txt</span><br><span class="line"></span><br><span class="line"># title.txt内容：</span><br><span class="line">hadoop spark zookeeper</span><br><span class="line"> spark zookeeper</span><br><span class="line"></span><br><span class="line"># words.txt内容：</span><br><span class="line">foo is foo</span><br><span class="line">bar is not bar</span><br><span class="line">hadoop file system is the infrastructure of big data </span><br></pre></td></tr></table></figure>


<h4 id="6-2带参数启动spark-shell"><a href="#6-2带参数启动spark-shell" class="headerlink" title="6.2带参数启动spark-shell"></a>6.2带参数启动spark-shell</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 ~]# spark-shell --master spark:&#x2F;&#x2F;nn:7077 --executor-memory 512m  --total-executor-cores 3 --executor-cores 1  --num-executors 2</span><br><span class="line"></span><br><span class="line"># SparkContext,也可以在web端查看http:&#x2F;&#x2F;nn:4040</span><br><span class="line">scala&gt; sc</span><br><span class="line">res2: org.apache.spark.SparkContext &#x3D; org.apache.spark.SparkContext@e71bd92</span><br><span class="line"></span><br><span class="line"># 统计hdfs目录&#x2F;app下所有文件里面words，scala语言的链式调用</span><br><span class="line">scala&gt; sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect</span><br><span class="line"></span><br><span class="line"># 统计结果返回一个scala数组</span><br><span class="line">res0: Array[(String, Int)] &#x3D; Array((is,3), (&quot;&quot;,2), (bar,2), (foo,2), (spark,2), (hadoop,2), (zookeeper,2), (not,1), (system,1), (big,1), (infrastructure,1), (the,1), (data,1), (file,1))</span><br></pre></td></tr></table></figure>

<p>或者在此spark-shell上交互式使用Scala写简单的统计语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val file&#x3D;sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;)</span><br><span class="line">file: org.apache.spark.rdd.RDD[String] &#x3D; hdfs:&#x2F;&#x2F;nn:9000&#x2F;app MapPartitionsRDD[21] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd &#x3D; file.flatMap(line &#x3D;&gt; line.split(&quot; &quot;)).map(word &#x3D;&gt; (word,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(String, Int)] &#x3D; MapPartitionsRDD[29] at sortBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res6: Array[(String, Int)] &#x3D; Array((is,3), (&quot;&quot;,2), (bar,2), (foo,2), (hadoop,2), (zookeeper,2), (spark,2), (not,1), (system,1), (data,1), (file,1), (big,1), (infrastructure,1), (the,1))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）"><a href="#7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）" class="headerlink" title="7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）"></a>7、使用spark的相关web服务页面查看application执行计算作业的详细过程（非常重要）</h3><p>下面以一个Application 执行job前和执行job后的页面来说明application，job，task等内容</p>
<p>==<strong>Application 执行job前</strong>==</p>
<p><strong>A、查看application执行的详情页面</strong></p>
<p>在nn节点上，启动一个名字为word-count的application：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# spark-shell --executor-memory 512m  --total-executor-cores 3 --executor-cores 1  --num-executors 2 --name word-count</span><br></pre></td></tr></table></figure>
<p>在spark-shell启动后，会提示：<br>Spark context Web UI available at <code>http://nn:4040</code><br>Spark context available as ‘sc’ (master = spark://nn:7077, app id = app-2019*****-0002).</p>
<p><code>http://nn:4040</code>针对当前运行application的job详情，如果执行统计命令后退出spark-shell，那么web服务退出，<code>http://nn:4040</code>将无法访问，也即无法查看当前application执行过程的情况，所有需要配置application 的spark-history-server，用来查看之前已经完成或者未完成的application情况的历史记录<br><code>http://nn:4040</code>页面：<br>app id = app-2019*<strong><strong>-0002Jobs栏目内容：<br>可以看到目前该application没有job需要执行<br><img src="https://img-blog.csdnimg.cn/2019101320274522.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">app id = app-2019*</strong></strong>-0002的executor内容：<br>该application分配了两个executor，分别为dn1节点和dn2节点，nn节点则作为driver<br><img src="https://img-blog.csdnimg.cn/20191013203133344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>B、查看所有正在完成、已完成的application管理页面：spark master：</strong><code>http://nn:8080</code><br>该页面可以看到spark集群的资源分配情况、worker情况、正在runing的application以及已经完成的application<br><img src="https://img-blog.csdnimg.cn/20191013203738203.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>C、在A部分提到如果要回看已经完成application运行情况，则需要启动spark-history-server，这里给出配置文件说明</strong><br>==配置Spark History Server服务==<br>history只需在spark主节点上配置，无需在其他两个节点上配置。<br>spark-history-server其实就是一个web服务，spark.eventLog.dir存放所有application事件日志，web服务通过把这些application运行日志内容以web UI提供查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# vi spark-env.sh</span><br><span class="line"># 从配置说明可以看出，所有配置hisory服务的属性值可由以下属性设定</span><br><span class="line"># - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. &quot;-Dx&#x3D;y&quot;)</span><br><span class="line"></span><br><span class="line"># 配置history日志存放目录，可以配置多个属性值</span><br><span class="line"></span><br><span class="line">SPARK_HISTORY_OPTS&#x3D;&quot;</span><br><span class="line">-Dspark.history.ui.port&#x3D;9001 </span><br><span class="line">-Dspark.history.retainedApplications&#x3D;5</span><br><span class="line">-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory&quot;</span><br></pre></td></tr></table></figure>
<p>web访问端口为9001，保留最近5个application的日志，application的日志存放在<code>hdfs://nn:9000/directory</code></p>
<p>其他配置项</p>
<p>其他相关参数:</p>
<table>
<thead>
<tr>
<th align="center">Property Name</th>
<th align="center">Default</th>
<th align="center">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="center">spark.history.fs.update.interval</td>
<td align="center">10s</td>
<td align="center">文件系统历史提供程序在日志目录中检查新日志或更新日志的周期。较短的间隔可以更快地检测新应用程序，但代价是需要更多的服务器负载重新读取更新的应用程序。一旦更新完成，已完成和未完成的应用程序的清单将反映更改</td>
</tr>
<tr>
<td align="center">spark.history.retainedApplications</td>
<td align="center">50</td>
<td align="center">在缓存中保留UI数据的应用程序数量。如果超过这个上限，那么最老的应用程序将从缓存中删除。如果应用程序不在缓存中，则必须从磁盘加载它(如果是从UI访问它)</td>
</tr>
<tr>
<td align="center">spark.history.fs.cleaner.enabled</td>
<td align="center">false</td>
<td align="center">是否周期性的删除storage中的event log(生产必定是true)</td>
</tr>
<tr>
<td align="center">spark.history.fs.cleaner.interval</td>
<td align="center">1d</td>
<td align="center">多久删除一次</td>
</tr>
<tr>
<td align="center">spark.history.fs.cleaner.maxAge</td>
<td align="center">7d</td>
<td align="center">每次删除多久的event log，配合上一个参数就是每天删除前七天的数据</td>
</tr>
<tr>
<td align="center">spark-history页面截图：</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><img src="https://img-blog.csdnimg.cn/2019101321104720.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">==<strong>Application 执行job后</strong>==</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">当application开始runing后，可以看到相关job运行情况</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><strong>A、application的jobs图示</strong></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">该word-count app启动了两个job</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><img src="https://img-blog.csdnimg.cn/20191013214149503.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">job-0主要负责作业中Transformation链操作：</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><code>sc.textFile(&quot;hdfs://nn:9000/app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</code></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">job-0分解</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p>job-1负责作业最后阶段Action操作：<br><code>.collect</code></p>
<p>application、job、stage、task构成关系<br>这里job-1的stage2是skip的，因为job-0已经完成了同样的操作，其他job无法重复执行。<br><img src="https://img-blog.csdnimg.cn/2019101322183877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>B、job-0、job-1对于的stage图</strong><br>job-0：stage-0和stage-1<br><img src="https://img-blog.csdnimg.cn/20191013222359342.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">job-1：stage-2、stage-3、stage-4<br><img src="https://img-blog.csdnimg.cn/20191013222504141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>C、以job-0为例：stage-0和stage-1的具体任务执行图DAG调度过程</strong><br>==job-0：stage-0，其实就是map阶段，对应shuffle write==<br><img src="https://img-blog.csdnimg.cn/20191013222734564.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>==job-1：stage-1，其实就是reduce阶段，对应shuffle read==<br><img src="https://img-blog.csdnimg.cn/20191013222849671.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="8、启动spark-HA集群"><a href="#8、启动spark-HA集群" class="headerlink" title="8、启动spark HA集群"></a>8、启动spark HA集群</h3><p>前面的测试都是基于一个master带2个slave节点的集群，若nn节点上的master进程挂了，显然无法达到高可用集群，因此本章节也给出其配置过程，后面多篇文章会有大数据实时项目相关组件的部署，全部组件都基于HA方式运行，近可能贴近生产环境。<br>spark HA集群基于zookeeper集群实现，因此需要环境配置好并启动zookeeper服务，这里不再累赘，可以参考本人blog中有关zk集群的配置过程。</p>
<h4 id="8-1-配置文件"><a href="#8-1-配置文件" class="headerlink" title="8.1 配置文件"></a>8.1 配置文件</h4><p>spark HA配置相对简单，改动三个文件： spark-defaults.conf，slaves，spark-env.sh</p>
<p>将三个节点spark-defaults.conf都做以下配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf] vi  spark-defaults.conf</span><br><span class="line"># 因为spark要配成HA模式，因此不再指定nn节点为active节点</span><br><span class="line">#spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"># 注意这里eventLog.dir，因为本文中hadoop 集群还不是HA模式，NameNode主节点仅有nn节点，因此设为nn:9000。若hadoop集群为HA模式，这里的路径需要设为  ：hdfs:&#x2F;&#x2F;hdapp&#x2F;directory。在后面的spark on yarn 文章也还会提到这一点。</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure>


<p>将3个节点都为加入到slaves文件，每个节点都需配置该slaves文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn1 conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@dn1 conf]# cp slaves.template slaves</span><br><span class="line">[root@dn1 conf]# vi slaves</span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>更改spark-env.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export SCALA_HOME&#x3D;&#x2F;opt&#x2F;scala-2.12.8</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line"># spark HA配置里，不再指定某个节点为master</span><br><span class="line">#export SPARK_MASTER_IP&#x3D;182.10.0.4</span><br><span class="line">export SPARK_WORKER_MEMORY&#x3D;512m</span><br><span class="line"># 加入zookeeper集群，由zk统一管理</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url&#x3D;nn:2181,dn1:2181,dn2:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark&quot;</span><br><span class="line">#hadoop的配置文件**site.xml所在目录</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br></pre></td></tr></table></figure>
<p>在三个节点上都需按以上内容做相同配置。</p>
<h4 id="8-2-启动和测试spark-HA"><a href="#8-2-启动和测试spark-HA" class="headerlink" title="8.2 启动和测试spark HA"></a>8.2 启动和测试spark HA</h4><p>首先启动nn节点上slaves进程,此时三个节点都是worker角色</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# .&#x2F;sbin&#x2F;start-slaves.sh </span><br><span class="line">nn: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-nn.out</span><br><span class="line">dn1: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn1.out</span><br><span class="line">dn2: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-dn2.out</span><br></pre></td></tr></table></figure>
<p>接着在nn节点上启动master进程，此时nn节点将被选举为active状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# .&#x2F;sbin&#x2F;start-master.sh </span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to &#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-nn.out</span><br></pre></td></tr></table></figure>
<p>最后，分别在dn1和dn2节点上启动master进程，此时因nn节点已经优先成为active角色，故这两个节点虽然启动master，但会处于standby模式<br>通过spark web UI查看以上集群情况：<br>首先访问<code>http://nn:8080</code>，可以看到当前nn节点为active状态且有3个alive workers<br><img src="https://img-blog.csdnimg.cn/20191205225822814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">访问<code>http://dn1:8080</code>，dn1节点为standby模式，而且无自己的workers<br><img src="https://img-blog.csdnimg.cn/20191205230110964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">访问<code>http://dn2:8080</code>，dn2节点为standby模式，而且无自己的workers<br><img src="https://img-blog.csdnimg.cn/20191205230256782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">kill掉nn上master进程，观测spark 集群的master切换情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# jps</span><br><span class="line">7094 Master</span><br><span class="line">7322 Jps</span><br><span class="line">7019 Worker</span><br><span class="line">4892 QuorumPeerMain</span><br><span class="line">5853 NameNode</span><br><span class="line">5933 DataNode</span><br><span class="line">6253 DFSZKFailoverController</span><br><span class="line">6062 JournalNode</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# kill -9 7094</span><br></pre></td></tr></table></figure>

<p>访问<code>http://dn1:8080</code>，dn1节点由standby变为active模式且有3个alive workers，而dn2仍然standby模式，说明HA部署正常。<br><img src="https://img-blog.csdnimg.cn/20191205230624602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="9、结束"><a href="#9、结束" class="headerlink" title="9、结束"></a>9、结束</h3><p>本文详细讨论了基于hadoop上搭建spark HA集群，并对执行的application做了简单的介绍，注意到，这里spark HA集群并没有引入yarn资源调度服务，后面的文章会给出配置过程。同时本文没有对spark架构及其原理做更多的探讨，相关文章也在之后给出。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>spark集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于redis实现分布式锁（多实例redis+RedLock算法）</title>
    <url>/blog/2019/09/22/%E5%9F%BA%E4%BA%8Eredis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%88%E5%A4%9A%E5%AE%9E%E4%BE%8Bredis+RedLock%E7%AE%97%E6%B3%95%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章中，已经实现单实例redis分布式锁，但这种实现是基于单个redis服务，若redis服务不可用，显然所有客户端无法加锁，该实现还未到高可用的水平，因此需要进一步提升分布式的锁的逻辑，好在redis官方提供了相应的权威描述并称之为Redlock，具体参考文章：<a href="https://redis.io/topics/distlock">DLM</a>，这个锁的算法实现了多redis实例（各个redis是相互独立的，没有主从、集群模式）的情况，实现了真正高可用分布式锁。</p>
<a id="more"></a>

<h3 id="高可用的分布式锁要求："><a href="#高可用的分布式锁要求：" class="headerlink" title="高可用的分布式锁要求："></a>高可用的分布式锁要求：</h3><p>1）Mutual exclusion，互斥性，任何时刻只能有一个client获取锁</p>
<p>2）Deadlock free，死锁也必须能释放，即使锁定资源的redis服务器崩溃或者分区，仍然能释放锁</p>
<p>3）Fault tolerance；只要多数互相独立的redis节点，这里不是指主从模式两个节点或者集群模式，（一半以上）在使用，client才可以进行加锁，而且加锁成功的redis服务数量超过半数，且锁的有效时长还未过期，才认为加锁成功，否则加锁失败</p>
<h3 id="Redlock算法说明"><a href="#Redlock算法说明" class="headerlink" title="Redlock算法说明"></a>Redlock算法说明</h3><p>&#8195;&#8195;首先需理解时钟漂移clock drift概念：服务器时钟偏离绝对参考时钟的差值，例如在分布式系统中，有5台服务器，所有服务器时钟在初始情况下都设置成相同的时间（服务器上没有设置ntp同步）例如都为2019-08-01 10:00:00，随着时间的流逝，例如经过1年后，再“观察”这5台服务器的时间，服务器之间的时间对比，将有可能出现一定的快慢差异：</p>
<p>Server1显示一年后的时间：2020-08-01 10:00:01          </p>
<p>Server2显示一年后的时间：2020-08-01 10:00:02         </p>
<p>Server3显示一年后的时间：2020-08-01 10:00:02           </p>
<p>Server4显示一年后的时间：2020-08-01 09:59:58          </p>
<p>Server5显示一年后的时间：2020-08-01 10:00:01           </p>
<p>那么由这5台服务器组成的分布式系统，在外侧观察，时钟漂移为=2020-08-01 10:00:02减去2020-08-01 09:59:58=4秒，当然这是累计一年的时钟漂移时长，于是可以计算每秒的时间漂移刻度=4/(3600*24*365)，该刻度时长极小完全可以忽略不计，这是redis官方提供这个概念，让分布式锁的redis实现看起来更高级。</p>
<p>redlock加锁流程，假设客户端A按顺序分别在5个完全独立的redis实例作为加锁，如图所示：<br><img src="https://img-blog.csdnimg.cn/20190922224528783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>1）客户端A在redis01加锁操作前，获取当前时间戳T1</p>
<p>2）客户端A使用相同的key和uuid按顺序在5个redis上set key加锁和设定键的过期时长（有效时长），因为set key操作需要一定时间，因此在set过期时长时，需要set大于加锁所消耗的时长，否则客户端A还未在超过半数redis实例加锁成功前，前面redis set的key就已经先失效了，</p>
<p>错误设置：TTL为1s，例如客户端A在redis01加锁耗时为0.1秒、在redis02加锁耗时为0.5秒，但在redis03加锁耗时为1秒，此时redis01、redis02的key已失效，导致客户端A没能在超过半数（3个）的redis实例上加锁成功</p>
<p>正确设置：TTL为5s，例如客户端A在redis01加锁耗时为0.1秒、在redis02加锁耗时为0.5秒，redis03加锁耗时为1秒，此时redis01、redis02、redis03 key还未失效，客户端A成功在超过半数（3个）的redis实例上加锁，但此时客户端A还不能严格意义上成功获得了分布式锁，还需要进行第3步骤的判断</p>
<p>3）客户端A完成在多个redis实例上加锁后，此刻，锁真正有效时间不是一开始设置TTL的10秒，而是由以下得出：</p>
<p>在5个redis上加锁完后所消耗的时长：set_lock_cost=T5-T1=4s</p>
<p>实际锁的最小有效时长：min_validity=TTL-set_lock_cost-时钟漂移耗时</p>
<p>实际锁的最小有效时长=10s-4s-1s=5s，也就是说客户端A虽然在redis服务器设置有效时长为10s，但扣除一系列的加锁操作耗时后，“redis服务端”留给客户端A的实际有效时长为5秒。如果客户端A能在这5秒内完成任务，且按顺序释放锁，那么客户端A完成了一个完整流程的分布式锁条件的任务。</p>
<p>4）如果客户端A超时等原因无法获得超过半数（3）个以上，则必须解锁所有redis实例，否则影响其他进程加锁</p>
<h3 id="RedLock代码实现："><a href="#RedLock代码实现：" class="headerlink" title="RedLock代码实现："></a>RedLock代码实现：</h3><p>&#8195;&#8195;前一篇文章中已经实现的单服务的redis分布式锁，基于该基础上，实现redlock并不复杂，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,datetime</span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLockException</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedLock</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, locker_key, connection_conf_list=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 retry_times=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 retry_interval=<span class="number">200</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 ttl=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 clock_drift=<span class="number">500</span></span>):</span></span><br><span class="line">        self.locker_key = locker_key</span><br><span class="line">        self.retry_times = retry_times</span><br><span class="line">        self.retry_interval = retry_interval</span><br><span class="line">        self.global_ttl = ttl</span><br><span class="line">        self.clock_drift = clock_drift</span><br><span class="line">        self.locker_id = <span class="literal">None</span></span><br><span class="line">        self.is_get_lock = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> connection_conf_list:</span><br><span class="line">            connection_conf_list = [&#123;</span><br><span class="line">                <span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;port&#x27;</span>: <span class="number">6379</span>,</span><br><span class="line">                <span class="string">&#x27;db&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;socket_connect_timeout&#x27;</span>:<span class="number">1</span></span><br><span class="line">            &#125;]</span><br><span class="line"></span><br><span class="line">        self.all_redis_nodes = [redis.StrictRedis(**each_conf) <span class="keyword">for</span> each_conf <span class="keyword">in</span> connection_conf_list]</span><br><span class="line">        self.majority_nodes = <span class="built_in">len</span>(self.all_redis_nodes) // <span class="number">2</span> + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_release_single_lock</span>(<span class="params">self, node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在redis服务端执行原生lua脚本，只能删除加锁者自己的id，而且是原子删除</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        lua_script = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then</span></span><br><span class="line"><span class="string">            return redis.call(&quot;del&quot;,KEYS[1])</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            return 0</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            lua_func = node.register_script(lua_script)</span><br><span class="line">            lua_func(keys=[self.locker_key], args=[self.locker_id])</span><br><span class="line">        <span class="keyword">except</span>(redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_acquire_single_lock</span>(<span class="params">self, node</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在单个redis加锁</span></span><br><span class="line"><span class="string">        :param node:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result = node.<span class="built_in">set</span>(self.locker_key, self.locker_id, nx=<span class="literal">True</span>, px=self.global_ttl)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">except</span>(redis.exceptions.ConnectionError, redis.exceptions.TimeoutError):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_acquire</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.locker_id = <span class="built_in">str</span>(uuid.uuid1())</span><br><span class="line">        loop = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> loop &lt;= self.retry_times:</span><br><span class="line">           <span class="comment"># 这里需要注意：多线程并发模拟过程中，需要在任务执行前加锁，否则线程不安全</span></span><br><span class="line">            ok_lock_count = <span class="number">0</span></span><br><span class="line">            start = time.monotonic()</span><br><span class="line">            <span class="comment"># 按顺序在每个redis上尝试set key 加锁</span></span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> self.all_redis_nodes:</span><br><span class="line">                <span class="keyword">if</span> self._acquire_single_lock(node):</span><br><span class="line">                    print(<span class="string">&#x27;&#123;&#125;：成功加锁&#x27;</span>.<span class="built_in">format</span>(node))</span><br><span class="line">                    ok_lock_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            end = time.monotonic()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在多个redis实例上加锁所消耗的时长</span></span><br><span class="line">            set_lock_cost = (end - start)</span><br><span class="line">            <span class="comment"># 扣除相关操作耗时，得出实际锁的有效时长</span></span><br><span class="line">            real_ttl = self.global_ttl - set_lock_cost - self.clock_drift</span><br><span class="line">            print(<span class="string">&#x27;本次加锁耗时：&#123;0:,.4f&#125; ms 锁实际有效时长：&#123;1:,.4f&#125; ms&#x27;</span>.<span class="built_in">format</span>(set_lock_cost,real_ttl))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果加锁数量超过半数，且实际锁的有效时长大于0，则说明客户端本次成功获得分布式锁</span></span><br><span class="line">            <span class="keyword">if</span> ok_lock_count &gt;= self.majority_nodes <span class="keyword">and</span> real_ttl &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span>, real_ttl</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 客户端本次未能获得分布式锁，需释放本次申请的所有锁</span></span><br><span class="line">                <span class="keyword">if</span> real_ttl &lt;= <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">&#x27;客户端加锁失败，因为锁的实际有效时间太短&#x27;</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">&#x27;客户端加锁失败，因为成功加锁的redis实例少于总数的一半&#x27;</span>)</span><br><span class="line">                <span class="keyword">for</span> node <span class="keyword">in</span> self.all_redis_nodes:</span><br><span class="line">                    self._release_single_lock(node)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 随机休眠后，客户端继续下一轮加锁</span></span><br><span class="line">            loop += <span class="number">1</span></span><br><span class="line">            time.sleep(random.randint(<span class="number">0</span>, self.retry_interval) / <span class="number">1000</span>)</span><br><span class="line">        print(<span class="string">&#x27;超过&#123;&#125;次加锁失败&#x27;</span>.<span class="built_in">format</span>(self.retry_times))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>,<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire</span>(<span class="params">self</span>):</span></span><br><span class="line">        is_lock, validity = self._acquire()</span><br><span class="line">        <span class="keyword">return</span> is_lock</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire_with_validity</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :return: 返回加锁是否成功和锁的有效期</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        is_lock, validity = self._acquire()</span><br><span class="line">        <span class="keyword">return</span> is_lock, validity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">release</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.all_redis_nodes:</span><br><span class="line">            self._release_single_lock(node)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        is_lock, validity = self._acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_lock:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="comment"># raise RedLockException(&#x27;unable to acquire distributed lock&#x27;)</span></span><br><span class="line">        <span class="keyword">return</span> is_lock, validity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r</span>):</span></span><br><span class="line">    <span class="keyword">with</span> RedLock(<span class="string">&#x27;locker_test&#x27;</span>):</span><br><span class="line">        thread_name = threading.currentThread().name</span><br><span class="line">        bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">        total = r.get(bonus)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">            print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">            <span class="keyword">return</span>          </span><br><span class="line">        result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">        print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;,时间:&#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    start_time=time.monotonic()</span><br><span class="line">    thread_nums=<span class="number">100</span></span><br><span class="line">    pool_obj = redis.ConnectionPool(host=<span class="string">&#x27;192.168.100.5&#x27;</span>, port=<span class="number">8002</span>, socket_connect_timeout=<span class="number">5</span>)</span><br><span class="line">    r_conn = redis.Redis(connection_pool=pool_obj)</span><br><span class="line"></span><br><span class="line">    threads = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(thread_nums):</span><br><span class="line">        t = threading.Thread(target=doing_jobs, args=(r_conn,))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    cost=time.monotonic()-start_time</span><br><span class="line">    print(<span class="string">&#x27;任务耗时:&#123;:,.2f&#125; ms&#x27;</span>.<span class="built_in">format</span>(cost))</span><br></pre></td></tr></table></figure>



<h4 id="在单个redis下测试redlock"><a href="#在单个redis下测试redlock" class="headerlink" title="在单个redis下测试redlock"></a>在单个redis下测试redlock</h4><h4 id="1）单redis，100个并发请求"><a href="#1）单redis，100个并发请求" class="headerlink" title="1）单redis，100个并发请求"></a>1）单redis，100个并发请求</h4><p>手动在redis单服务set值，测试客户端发来的100个并发抢资源时，基于redlock的分布式锁是否逻辑正确，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set money 300</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line"># 运行结果</span><br><span class="line">客户端:Thread-100抢到奖金，还剩299,时间:*** 22:14:40.358679</span><br><span class="line">客户端:Thread-9抢到奖金，还剩298,时间:*** 22:14:40.363933</span><br><span class="line">客户端:Thread-36抢到奖金，还剩297,时间:*** 22:14:40.371695</span><br><span class="line"></span><br><span class="line">客户端:Thread-16抢到奖金，还剩202,时间:*** 22:14:40.808868</span><br><span class="line">客户端:Thread-34抢到奖金，还剩201,时间:*** 22:14:40.811364</span><br><span class="line">客户端:Thread-52抢到奖金，还剩200,时间:*** 22:14:40.817055</span><br></pre></td></tr></table></figure>

<p>可以看到，在同一秒内，100个线程都有序的抢到锁和资源</p>
<h4 id="2）5个redis实例，1个并发请求"><a href="#2）5个redis实例，1个并发请求" class="headerlink" title="2）5个redis实例，1个并发请求"></a>2）5个redis实例，1个并发请求</h4><p>在5个独立redis实例下验证redlock分布式锁有效性(这里的5个实例是在同一服务器下开启，模拟5台redis服务)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 redis-5.0.5]# pwd</span><br><span class="line">/opt/redis/redis-5.0.5</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在redis目录下直接拷贝redis.conf，重命名，且只需修改里面的端口项即可，这里端口为8000~8004</span></span><br><span class="line">redis8001.conf  redis8002.conf  redis8003.conf  redis8004.conf  </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 逐个启动redis实例</span></span><br><span class="line">[root@dn2 redis-5.0.5]# redis-server redis8001.conf </span><br><span class="line"></span><br><span class="line">[root@dn2 redis-5.0.5]# ps -ef|grep redis           </span><br><span class="line">root     30321     1  0 20:58 ?        00:00:00 redis-server *:8000</span><br><span class="line">root     30326     1  0 20:58 ?        00:00:00 redis-server *:8001</span><br><span class="line">root     30372     1  0 21:02 ?        00:00:00 redis-server *:8002</span><br><span class="line">root     30381     1  0 21:03 ?        00:00:00 redis-server *:8003</span><br><span class="line">root     30386     1  0 21:03 ?        00:00:00 redis-server *:8004</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 登录其中一个实例<span class="built_in">set</span> key</span></span><br><span class="line">[root@dn2 redis-5.0.5]# redis-cli -p 8002</span><br><span class="line">127.0.0.1:8002&gt; set foo 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:8002&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure>


<p>只有一个并发的条件下，客户端在5个实例加锁情况，在8002实例上加入资源：</p>
<p>以上代码小改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加入5个redis实例连接配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r</span>):</span></span><br><span class="line">    redis_nodes_conf=[</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,<span class="string">&#x27;port&#x27;</span>:<span class="number">8000</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8001</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8002</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8003</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8004</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">with</span> RedLock(locker_key=<span class="string">&#x27;Redlock&#x27;</span>,connection_conf_list=redis_nodes_conf):</span><br><span class="line">        thread_name = threading.currentThread().name</span><br><span class="line">        bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">        total = r.get(bonus)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">            print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">        print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;,时间:&#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result,datetime.datetime.now()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p> 在其中一个redis实例加入资源</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 redis-5.0.5]# redis-cli -p 8002</span><br><span class="line">127.0.0.1:8002&gt; set money 10</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>可以看到，客户端首先在五个实例上按顺序加锁，执行任务，获得1个资源完成任务后，接着再顺序释放锁，其中加锁耗时0.01ms，锁实际有效时长：4,499.99 ms，任务耗时0.02ms，说明锁的实际有效时长足够大，以至于可以保证任务执行过程中，保持锁不失效。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8000,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8001,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8002,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.01 ms 锁实际有效时长：4,499.99 ms</span><br><span class="line">客户端:Thread-1抢到奖金，还剩9,时间:*** 22:10:46.490385</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8000,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8001,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8002,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">任务耗时:0.02 ms</span><br></pre></td></tr></table></figure>


<h4 id="3）redis实例工作数量小于半数，1个并发请求"><a href="#3）redis实例工作数量小于半数，1个并发请求" class="headerlink" title="3）redis实例工作数量小于半数，1个并发请求"></a>3）redis实例工作数量小于半数，1个并发请求</h4><p>只有一个并发的条件下，客户端在小于3个实例加锁情况，只需把8000、8001、8002端口改掉，模拟只有两个redis实例正常服务。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.0670 ms 锁实际有效时长：4,499.9330 ms</span><br><span class="line">客户端加锁失败，因为成功加锁的redis实例少于总数的一半</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.0728 ms 锁实际有效时长：4,499.9272 ms</span><br><span class="line">客户端加锁失败，因为成功加锁的redis实例少于总数的一半</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功加锁</span><br><span class="line">本次加锁耗时：0.0548 ms 锁实际有效时长：4,499.9452 ms</span><br><span class="line">客户端加锁失败，因为成功加锁的redis实例少于总数的一半</span><br><span class="line">超过3次加锁失败</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=80000,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=80001,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=80002,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8003,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">Redis&lt;ConnectionPool&lt;Connection&lt;host=192.168.100.5,port=8004,db=0&gt;&gt;&gt;：成功释放锁</span><br><span class="line">任务耗时:0.53 ms</span><br></pre></td></tr></table></figure>

<p>&#8195;&#8195;可以看到，客户端尝试3次加锁，在给定的5个redis实例里仅能成功加锁2个，少于半数，故本次分布式加锁失败。当然也可以模拟把锁的ttl设置小值，例如500ms，那么将出现即使加完锁，因为锁的有实效时长太短，导致无法最终得到分布式锁，这里不在模拟。</p>
<h3 id="支持多线程的redlock算法"><a href="#支持多线程的redlock算法" class="headerlink" title="支持多线程的redlock算法"></a>支持多线程的redlock算法</h3><p>&#8195;&#8195;以上未模拟1个线程并发，但其实现不支持多线程，如果要模拟多个并发例如：100个并发，因为在同一进程里，涉及到对多个线程同一时刻更改ok_lock_count的值，因此，在执行任务前，就需要出传入线程锁，保证同一时刻，仅有一个线程更新这个ok_lock_count（本线程在多个redis实例上成功set key 的计数）</p>
<p>任务执行代码小改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doing_jobs</span>(<span class="params">r,thread_lock</span>):</span></span><br><span class="line">    redis_nodes_conf=[</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>:<span class="string">&#x27;192.168.100.5&#x27;</span>,<span class="string">&#x27;port&#x27;</span>:<span class="number">8000</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8001</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8002</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8003</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;192.168.100.5&#x27;</span>, <span class="string">&#x27;port&#x27;</span>: <span class="number">8004</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 这里的多线程锁是为了处理&quot;模拟并发情况下&quot;，对ok_lock_count变量进行更新时，保证同一时间只能有一个线程来操作</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> thread_lock:</span><br><span class="line">        <span class="keyword">with</span> RedLock(locker_key=<span class="string">&#x27;Redlock&#x27;</span>,connection_conf_list=redis_nodes_conf) <span class="keyword">as</span> (is_lock,validity):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> is_lock:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            thread_name = threading.currentThread().name</span><br><span class="line">            bonus = <span class="string">&#x27;money&#x27;</span></span><br><span class="line">            total = r.get(bonus)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> total:</span><br><span class="line">                print(<span class="string">&#x27;奖金池没设置&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">int</span>(total) == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">&#x27;奖金已被抢完&#x27;</span>.<span class="built_in">format</span>(thread_name))</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">            result = r.decr(bonus, <span class="number">1</span>)</span><br><span class="line">            print(<span class="string">&#x27;客户端:&#123;0&#125;抢到奖金，还剩&#123;1&#125;,时间:&#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(thread_name, result,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_time=time.monotonic()</span><br><span class="line">    thread_nums=<span class="number">100</span></span><br><span class="line">    pool_obj = redis.ConnectionPool(host=<span class="string">&#x27;192.168.100.5&#x27;</span>, port=<span class="number">8002</span>, socket_connect_timeout=<span class="number">5</span>)</span><br><span class="line">    r_conn = redis.Redis(connection_pool=pool_obj)</span><br><span class="line">    thread_lock=threading.RLock()</span><br><span class="line">    threads = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(thread_nums):</span><br><span class="line">        t = threading.Thread(target=doing_jobs, args=(r_conn,thread_lock))</span><br><span class="line">        threads.append(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">        t.join()</span><br><span class="line"></span><br><span class="line">    cost=time.monotonic()-start_time</span><br><span class="line">    print(<span class="string">&#x27;任务耗时:&#123;:,.2f&#125; ms&#x27;</span>.<span class="built_in">format</span>(cost))</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;以上完成redlock完整的分析、实现和测试，现在回看redlock的实现，它提出的所谓加锁耗时、时钟漂移等，都可以用最简单的方式代替：只需要把key的ttl设置足够长的时间，那么就无需担心在加锁过程中key突然失效。</p>
<p>&#8195;&#8195;综上，个人认为redis实现分布式锁的过程过于繁琐（注意不是复杂），而且要求redis实例之间是独立运行，反正我个人不会在项目中使用这种逻辑，因此Zookeeper在分布式锁方面的可用性，无疑是最优的。</p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title>深入functools.wraps、partial</title>
    <url>/blog/2019/11/17/%E6%B7%B1%E5%85%A5functools.wraps%E3%80%81partial/</url>
    <content><![CDATA[<p>&#8195;&#8195;在装饰器的定义中，经常引用functools.wraps，本篇文章将深入其内部源码，由于该方法的定义中，还引入其他重要的函数或者类，因此根据其调用链，对每个函数或者方法或者类进行单独分析，所以文章的结构大致如下：</p>
<h4 id="第一部分内容："><a href="#第一部分内容：" class="headerlink" title="第一部分内容："></a>第一部分内容：</h4><p>根据其调用链：functools.wraps—-&gt;partial—-&gt;update_wrapper<br>functools.wraps需要调用partial，因此需要解析partial的源码<br>partial调用了update_wrapper函数，因此需要解析update_wrapper的源码</p>
<a id="more"></a>

<h4 id="第二部分内容："><a href="#第二部分内容：" class="headerlink" title="第二部分内容："></a>第二部分内容：</h4><p>patial作为关键函数，在第三方库造轮子里，使用频率较高。这里以borax第三方库里面的fetch方法说明partial的使用场合。考虑到fetch还使用的python内建的attrgetter和itemgetter，故还对这两个类进行解析。<br>其调用链为：fetch—&gt;partial/attrgetter/itemgetter</p>
<h4 id="第三部分内容："><a href="#第三部分内容：" class="headerlink" title="第三部分内容："></a>第三部分内容：</h4><p>在python的内建方法中，attrgetter/itemgetter类，里面用了<code>__slots__</code>方法，本文也给出关于该方法作用的内容</p>
<p>考虑到以上有多个知识点混合，本文采用倒叙方式，文章的内容组织如下</p>
<ul>
<li>1、python的魔法方法<code>__slots__</code>的作用</li>
<li>2、attrgetter/itemgetter类的解析</li>
<li>3、borax.fetch的用法</li>
<li>4、partial的解析和用法</li>
<li>5、update_wrapper的解析和用法</li>
<li>6、functools.wraps的解析和用法</li>
</ul>
<h3 id="1、python的魔法方法-slots-的作用"><a href="#1、python的魔法方法-slots-的作用" class="headerlink" title="1、python的魔法方法__slots__的作用"></a>1、python的魔法方法<code>__slots__</code>的作用</h3><p>首先看看attrgetter/itemgetter的源代码定义(这里仅给出方法名)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class attrgetter:</span><br><span class="line">    __slots__ &#x3D; (&#39;_attrs&#39;, &#39;_call&#39;)</span><br><span class="line">    def __init__(self, attr, *attrs):</span><br><span class="line">    def __call__(self, obj):</span><br><span class="line">    def __repr__(self):</span><br><span class="line">    def __reduce__(self):</span><br></pre></td></tr></table></figure>
<p>该类里面引用了<code>__slots__</code>方法，仅有两个私有属性：<code> (&#39;_attrs&#39;, &#39;_call&#39;)</code></p>
<p>对于attrgetter，其作用：</p>
<ul>
<li>给类指定一个固定大小的空间存放属性，用于极致减少对象的内存占用，例如当十几万个小类（数据类），对象占用内存利用率将更有效。</li>
<li>更快的属性访问速度</li>
<li>实例后限制绑定新属性</li>
</ul>
<p>为何<code>_slots_</code>方法有以上作用?</p>
<p>&#8195;&#8195;这是因为，在定义个对象时（定义类），Python默认用一个字典来保存一个该对象实例属性。然而，对于有着已知属性的小对象类来说（例如一个坐标点类，仅有几个属性即可），当创建几十万个这些实例时，将有几十万个这样的字典占用大量内存，因此可通过slots方法告诉Python不使用字典，使用一个元组作为这几个属性的存放位置，以节省每个小对象的存储空间。<br>==slots这里不建议使用列表，因为列表占用空间比元组大。==</p>
<h5 id="1-1、-slots方法保证实例不会创建-dict-方法"><a href="#1-1、-slots方法保证实例不会创建-dict-方法" class="headerlink" title="1.1、 slots方法保证实例不会创建__dict__方法"></a>1.1、 slots方法保证实例不会创建<code>__dict__</code>方法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,x,y</span>):</span></span><br><span class="line">        self._x=x</span><br><span class="line">        self._y=y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Point&lt;&#123;0&#125;,&#123;1&#125;&gt;&#x27;</span>.<span class="built_in">format</span>(self._x,self._y)</span><br><span class="line"></span><br><span class="line">a=Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.__dict__)</span><br><span class="line">a.test=<span class="number">3</span></span><br><span class="line">print(a.test)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Point&lt;<span class="number">1</span>,<span class="number">2</span>&gt;</span><br><span class="line">&#123;<span class="string">&#x27;_x&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;_y&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入slot之后</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    __slots__ = (<span class="string">&#x27;_x&#x27;</span>,<span class="string">&#x27;_y&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,x,y</span>):</span></span><br><span class="line">        self._x=x</span><br><span class="line">        self._y=y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Point&lt;&#123;0&#125;,&#123;1&#125;&gt;&#x27;</span>.<span class="built_in">format</span>(self._x,self._y)</span><br><span class="line">a=Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.__dict__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">Point&lt;<span class="number">1</span>,<span class="number">2</span>&gt;</span><br><span class="line"><span class="comment">#AttributeError: &#x27;Point&#x27; object has no attribute &#x27;__dict__&#x27;</span></span><br><span class="line"><span class="comment">#可见实例没有使用dict字典存放属性</span></span><br></pre></td></tr></table></figure>

<p>不过需要注意的是：slots魔法方法定义的属性仅对当前类实例起作用，对继承的子类是无效的</p>
<h5 id="1-2、为何列表占用空间比元组大？"><a href="#1-2、为何列表占用空间比元组大？" class="headerlink" title="1.2、为何列表占用空间比元组大？"></a>1.2、为何列表占用空间比元组大？</h5><ul>
<li>==内存占用有区别==</li>
</ul>
<p>首先列表和元组最重要的区别就是，列表是动态的、可变的对象、可读可写，而元组是静态的、不可变的对象，可读不可写。</p>
<p>下面通过实例看看它们存储以及占用空间的区别</p>
<p>查看列表的内存占用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l=[]</span><br><span class="line">l.__sizeof__()</span><br><span class="line"><span class="comment"># 40</span></span><br></pre></td></tr></table></figure>

<p>加入4个字符，每个字符为8字节空间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_obj=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>]</span><br><span class="line">list_obj.__sizeof__()</span><br><span class="line"><span class="comment"># 结果为72字节=列表自身40+4个字符*8</span></span><br></pre></td></tr></table></figure>

<p>此外列表的空间是动态增加的，在数据结构与算法里，大家在设计列表这种数据结构应该知道，当调用append方法时，内部会判断当前列表预留空间是否满足用于存放新元素，若空间不足，会再动态申请新内存，申请的逻辑为：</p>
<p>（1）当原底层数组存满时，list类会自动请求一个空间为原列表两倍的新列表</p>
<p>（2）原列表的所有元素将被一次存入新列表里</p>
<p>（3）删除原列表，并初始化新列表</p>
<p>例如下面测试，原list_obj有4个字符元素，总计为72个字节，再加一个字符，是等于80个字节吗？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_obj.append(<span class="string">&#x27;e&#x27;</span>)</span><br><span class="line">list_obj.__sizeof__()</span><br><span class="line"><span class="comment"># 结果为104字节=列表自身42+原4个字符*8+新1个字符*8+原4个字符*8的新申请预留空间</span></span><br></pre></td></tr></table></figure>
<p>这里列表自身从40变为42字节，是因为增加了一个1索引。</p>
<p>查看元组的内存占用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t=()</span><br><span class="line">t.__sizeof__()</span><br><span class="line"><span class="comment"># 24</span></span><br></pre></td></tr></table></figure>

<p>加入4个字符，每个字符为8字节空间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple_obj=(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">tuple_obj.__sizeof__()</span><br><span class="line"><span class="comment"># 结果为56字节=元组自身24+4个字符*8</span></span><br></pre></td></tr></table></figure>
<p>可以看到，存储5个字符，列表用了72个字节，元组只用了56个字节。</p>
<ul>
<li>==对象创建时间有区别==</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line">t1 = timeit.Timer(<span class="string">&#x27;list_obj=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]&#x27;</span>)</span><br><span class="line">t1.timeit()</span><br><span class="line"><span class="comment"># 0.0844487198985604</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t2 = timeit.Timer(<span class="string">&#x27;tuple_obj=(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)&#x27;</span>)</span><br><span class="line">t2.timeit()</span><br><span class="line"><span class="comment"># 0.01631815598959463</span></span><br></pre></td></tr></table></figure>

<p>因为列表数据结构初始化需要方法逻辑比元组负责，而且需要预占空间，可以看到它们之间创建时间差别大，列表创建时间是元组的5倍左右。</p>
<h3 id="2、attrgetter-itemgetter类的解析"><a href="#2、attrgetter-itemgetter类的解析" class="headerlink" title="2、attrgetter/itemgetter类的解析"></a>2、attrgetter/itemgetter类的解析</h3><h4 id="2-1-attrgetter的使用场景"><a href="#2-1-attrgetter的使用场景" class="headerlink" title="2.1 attrgetter的使用场景"></a>2.1 attrgetter的使用场景</h4><p>attrgetter主要用于快速获取对象的keys或者属性。<br>以下以数据类型为BlogItem对象为例，该数据对象有三个attribute，分别博客网址、作者、博客文章数量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogItem</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, website, author, blog_nums</span>):</span></span><br><span class="line">        self.website = website</span><br><span class="line">        self.author = author</span><br><span class="line">        self.blog_nums = blog_nums</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&#123;0&#125;:&#123;1&#125;&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__,self.website)</span><br><span class="line"></span><br><span class="line">    __repr__ = __str__</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">blog_object_list = \</span><br><span class="line">    [BlogItem(<span class="string">&quot;www.aoo.cn&quot;</span>, <span class="string">&#x27;aoo&#x27;</span>, <span class="number">10</span>),</span><br><span class="line">     BlogItem(<span class="string">&quot;www.boo.cn&quot;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="number">5</span>),</span><br><span class="line">     BlogItem(<span class="string">&quot;www.coo.cn&quot;</span>, <span class="string">&#x27;coo&#x27;</span>, <span class="number">20</span>)</span><br><span class="line">     ]</span><br><span class="line">print(blog_object_list)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[BlogItem:www.aoo.cn, BlogItem:www.boo.cn, BlogItem:www.coo.cn]</span><br></pre></td></tr></table></figure>
<p>现要获取每行数据对象的blog属性，并对其实施排序，通常会使用lambda表达式实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">sorted</span>(blog_object_list, key=<span class="keyword">lambda</span> item: item.blog_nums))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[BlogItem:www.boo.cn, BlogItem:www.aoo.cn, BlogItem:www.coo.cn]</span><br></pre></td></tr></table></figure>

<p>有了attrgetter方法后，更方便调用获取对象的属性，例如下面的用法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">sorted</span>(blog_object_list,key=attrgetter(<span class="string">&#x27;blog_nums&#x27;</span>)))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[BlogItem:www.boo.cn, BlogItem:www.aoo.cn, BlogItem:www.coo.cn]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>也可以传入多个属性，按多个属性进行排序，例如这里先根据blog排序、再根据author排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">sorted</span>(blog_object_list,key=attrgetter(<span class="string">&#x27;blog_nums&#x27;</span>,<span class="string">&#x27;author&#x27;</span>)))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">[BlogItem:www.boo.cn, BlogItem:www.aoo.cn, BlogItem:www.coo.cn]</span><br></pre></td></tr></table></figure>

<h4 id="2-2-attrgetter的内部实现："><a href="#2-2-attrgetter的内部实现：" class="headerlink" title="2.2 attrgetter的内部实现："></a>2.2 attrgetter的内部实现：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">attrgetter</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return a callable object that fetches the given attribute(s) from its `.</span></span><br><span class="line"><span class="string">    After f = attrgetter(&#x27;name&#x27;), the call f(r) returns r.name.</span></span><br><span class="line"><span class="string">    After g = attrgetter(&#x27;name&#x27;, &#x27;date&#x27;), the call g(r) returns (r.name, r.date).</span></span><br><span class="line"><span class="string">    After h = attrgetter(&#x27;name.first&#x27;, &#x27;name.last&#x27;), the call h(r) returns (r.name.first, r.name.last).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __slots__ = (<span class="string">&#x27;_attrs&#x27;</span>, <span class="string">&#x27;_call&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, attr, *attrs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> attrs:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(attr, <span class="built_in">str</span>):</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">&#x27;attribute name must be a string&#x27;</span>)</span><br><span class="line">            self._attrs = (attr,)</span><br><span class="line">            names = attr.split(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">getattr</span>(obj, name)</span><br><span class="line">            self._call = func</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#  如果获取多个属性，使用map对每个属性实施attrgetter</span></span><br><span class="line">            self._attrs = (attr,) + attrs</span><br><span class="line">            getters = <span class="built_in">tuple</span>(<span class="built_in">map</span>(attrgetter, self._attrs))</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">tuple</span>(getter(obj) <span class="keyword">for</span> getter <span class="keyword">in</span> getters)</span><br><span class="line">            self._call = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, obj</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._call(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s.%s(%s)&#x27;</span> % (self.__class__.__module__,</span><br><span class="line">                              self.__class__.__qualname__,</span><br><span class="line">                              <span class="string">&#x27;, &#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">repr</span>, self._attrs)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__, self._attrs</span><br></pre></td></tr></table></figure>


<p>attrgetter在内部定义了一个闭包函数func，该函数其实就是getattr(obj, name)的功能。<br>attrgetter内有个特殊的魔法方法<code>__reduce__</code>，该方法用于pickle反序列化后可以找到该对象绑定的类及其入参，若要想对某对象进行pickle，那么该对象要定义<code>__reduce__</code>方法，否则在序列化时（使用pickle.load）无法持久化存储，查看其源码pickle.dumps可以看到相关逻辑</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reduce = <span class="built_in">getattr</span>(obj, <span class="string">&quot;__reduce_ex__&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">if</span> reduce <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    rv = reduce(self.proto)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 获取被处理的对象的__reduce__方法，若不存在提示无法pickle持久化（或者称为无法被序列化），所以attrgetter对象或者其实例是可以被序列化</span></span><br><span class="line">    reduce = <span class="built_in">getattr</span>(obj, <span class="string">&quot;__reduce__&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> reduce <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        rv = reduce()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> PicklingError(<span class="string">&quot;Can&#x27;t pickle %r object: %r&quot;</span> %</span><br><span class="line">                            (t.__name__, obj))</span><br></pre></td></tr></table></figure>

<p><code>attrgetter的doc文档说：f = attrgetter(&#39;name&#39;), the call f(r) returns r.name。 __call__方法是为了实现f(r）用法，等价于getattr(r, name)，用于获取给定对象的属性值</code>，</p>
<p>与attrgetter不同的是：getattr只能获取单个属性值而且getattr也是python工厂函数，在builtins.py内部定义；而attrgetter可以获取多个属性值，是对getattr的再次封装和加强，只不过它的封装逻辑写得还不错，即清晰易懂。<br>==对比getattr与attrgetter的区别==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">R</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line"></span><br><span class="line"><span class="comment"># getattr用法</span></span><br><span class="line">r=R(<span class="string">&#x27;foo&#x27;</span>,<span class="number">10</span>)</span><br><span class="line">print(<span class="built_in">getattr</span>(r,<span class="string">&#x27;name&#x27;</span>))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">foo</span><br><span class="line"><span class="comment"># attrgetter用法</span></span><br><span class="line">f=attrgetter(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">print(f(r))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">foo</span><br></pre></td></tr></table></figure>

<p>这里给出<code>f=attrgetter(&#39;name&#39;)==&gt;f(r)==&gt;&#39;foo&#39;</code>的调用过程，有两种情况</p>
<p>A、只获取对象的一个属性<br><code> f=attrgetter(&#39;name&#39;)==&gt;f(r)==&gt;触发__call__==&gt;self._call(obj)，因为self._call = func==&gt;func(obj)，根据func的定义，就是返回getattr(obj, name)==&gt;getattr(r,&quot;name&quot;)==&gt;“foo”</code></p>
<p>B、获取对象两个属性以上的情况</p>
<p>例如attrgetter(‘name’，‘age’)，那么其传递过程为<br><code>f=attrgetter(&#39;name&#39;，‘age&#39;)==&gt;f(r)==&gt;触发__call__==&gt;self._call(obj)，因为self._call = func==&gt;func(obj)==&gt;因为要获取两个属性，故递归调用attrgetter： getters = tuple(map(attrgetter, self._attrs))==&gt;(attrgetter(&#39;name&#39;)，attrgetter(&#39;age&#39;))==&gt;对元组里面的元素按照步A的传递路线==&gt;(getattr(r,&quot;name&quot;),getattr(r,&quot;age&quot;))==&gt;(&#39;foo&#39;,10)</code></p>
<p>从上面的分析可知，attrgetter的实现有点绕了，所以python的闭包机制虽然可以基于原始函数上封装出具备更强功能的函数，但其代价就像符合函数，层层封装。<br>例如复合函数h，h=f(g(e(x)))，复合函数h作为加强版方法，通过封装f方法，f方法封装g方法，g方法封装e方法，从而使得h方法的功能比最初始e方法具备更强大的功能，但你需要一路往内部追踪，才知道h函数最里面的函数为e函数。</p>
<h4 id="2-3-itemgetter的使用场景"><a href="#2-3-itemgetter的使用场景" class="headerlink" title="2.3 itemgetter的使用场景"></a>2.3 itemgetter的使用场景</h4><p>attrgetter可以理解给定单个key或者多个key，返回这些key的值，而对应另外一个类itemgetter，它用来实现对数据对象在给定索引号后，返回索引号对应的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r=[</span><br><span class="line">    (<span class="string">&quot;www.boo.cn&quot;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="number">20</span>),</span><br><span class="line">    (<span class="string">&quot;www.aoo.cn&quot;</span>, <span class="string">&#x27;aoo&#x27;</span>, <span class="number">10</span>),</span><br><span class="line">    (<span class="string">&quot;www.coo.cn&quot;</span>, <span class="string">&#x27;coo&#x27;</span>, <span class="number">15</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">f=itemgetter(<span class="number">1</span>)</span><br><span class="line">print(f(r))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">(<span class="string">&#x27;www.aoo.cn&#x27;</span>, <span class="string">&#x27;aoo&#x27;</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">f=itemgetter(<span class="number">0</span>，<span class="number">2</span>) <span class="comment"># 注意这里不是范围，而是索引0和索引2</span></span><br><span class="line">print(f(r))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">((<span class="string">&#x27;www.boo.cn&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="number">20</span>), (<span class="string">&#x27;www.coo.cn&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>, <span class="number">15</span>))</span><br></pre></td></tr></table></figure>

<p>其源代码也简单，实现跟attrgetter逻辑一直，attrgetter使用getter(obj，key)获取值，itemgetter使用obj[index]的方式获得值，但要求obj内部必须实现<code> __getitem__(self, item)</code>方法。</p>
<h4 id="2-4-itemgetter的源码分析"><a href="#2-4-itemgetter的源码分析" class="headerlink" title="2.4 itemgetter的源码分析"></a>2.4 itemgetter的源码分析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">itemgetter</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return a callable object that fetches the given item(s) from its operand.</span></span><br><span class="line"><span class="string">    After f = itemgetter(2), the call f(r) returns r[2].</span></span><br><span class="line"><span class="string">    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __slots__ = (<span class="string">&#x27;_items&#x27;</span>, <span class="string">&#x27;_call&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, item, *items</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> items:</span><br><span class="line">            self._items = (item,)</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="comment"># 若只提供一个索引号，则直接按getitem的写法获取该对象的值</span></span><br><span class="line">                <span class="keyword">return</span> obj[item]</span><br><span class="line">            self._call = func</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._items = items = (item,) + items</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">obj</span>):</span></span><br><span class="line">                <span class="comment"># 若获取多个索引号，则遍历这些索引号，获取每个对象的值</span></span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">tuple</span>(obj[i] <span class="keyword">for</span> i <span class="keyword">in</span> items)</span><br><span class="line">            self._call = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, obj</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._call(obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s.%s(%s)&#x27;</span> % (self.__class__.__module__,</span><br><span class="line">                              self.__class__.__name__,</span><br><span class="line">                              <span class="string">&#x27;, &#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">repr</span>, self._items)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__, self._items</span><br></pre></td></tr></table></figure>

<p>取值传递过程有两种情况</p>
<p>A、当给定1个索引号</p>
<p><code>f=itemgetter(1)==&gt;self._call(obj)==&gt;因为self._call = func，等价于func(obj)==&gt;根据func的定义，obj[item]==&gt;r[1]==&gt;也即是列表索引取值的方式，(&#39;www.aoo.cn&#39;, &#39;aoo&#39;, 10)</code></p>
<p>B、给定多个索引号</p>
<p><code>f=itemgetter(0,2)==&gt;self._call(obj)==&gt;因为self._call = func，等价于func(obj)==&gt;根据func的定义以及入参大于1 ==&gt;tuple(obj[i] for i in items)==&gt;(obj[0],obj[1])==&gt;(r[0],r[1])==&gt;((&#39;www.boo.cn&#39;, &#39;boo&#39;, 20), (&#39;www.coo.cn&#39;, &#39;coo&#39;, 15))</code></p>
<h3 id="3、broax-fetch的用法"><a href="#3、broax-fetch的用法" class="headerlink" title="3、broax.fetch的用法"></a>3、broax.fetch的用法</h3><p>borax是一个python第三库轻量库，里面有一些基本中国农历函数、choice、数据结构、设计模式以及fetch函数。它的doc</p>
<blockquote>
<p>Borax is a utils collections for python3 development, which contains<br>some common data structures and the implementation of design patterns</p>
<p>主要的module:</p>
<ul>
<li>borax.calendars : A Chinese lunar calendar package, which contains lunar,festivals, birthday.</li>
<li>borax.choices : choices a enhance module using class-style define for const choices.</li>
<li>borax.fetch : A function sets for fetch the values of some axises.</li>
<li>borax.structures : A useful data structure for dictionary/list/set .</li>
<li>borax.patterns : A implementation for the design patterns.</li>
</ul>
</blockquote>
<p>fetch函数功能：从数据序列中选择一个或多个字段的数据，它很好展示了partial函数的实际项目的用法。</p>
<blockquote>
<p>在这里插播之后会写一篇blog的通告：<br>fetch是从已有的数据序列中，根据指定key或者属性对应的记录行，而records库则是从各类关系型数据库取出数据记录行（当然可完成增删查改），发现records源代码清晰简单，但实现功能确如此强大，所以接下来会单独给出一篇blog用于解析records源码，records不到550行，封装逻辑通俗易懂。</p>
</blockquote>
<blockquote>
<p>records是kennethreitz的for Humans™系列的库，用于近乎易懂的方式操作数据库，kennethreitz是requests库的作者–<a href="https://github.com/kennethreitz">github地址</a>），kennethreitz总能把底层较为繁琐的逻辑封装成易用的逻辑，其开源的项目的源代码具有不错的学习价值。</p>
</blockquote>
<h4 id="3-1-fetch模块的源码简析"><a href="#3-1-fetch模块的源码简析" class="headerlink" title="3.1  fetch模块的源码简析"></a>3.1  fetch模块的源码简析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> tee</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="comment"># 当使用from fetch import * 时，通过__all__属性来限制import *的导出范围</span></span><br><span class="line">__all__ = [<span class="string">&#x27;fetch&#x27;</span>, <span class="string">&#x27;ifetch&#x27;</span>, <span class="string">&#x27;fetch_single&#x27;</span>, <span class="string">&#x27;ifetch_multiple&#x27;</span>, <span class="string">&#x27;ifetch_single&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Empty</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">EMPTY = Empty()</span><br><span class="line"><span class="comment"># 以下iterable就是要处理的数据序列例如以下数据对象Person序列</span></span><br><span class="line"><span class="comment"># [Person(&#x27;aoo&#x27;,10&#x27;),Person(&#x27;boo&#x27;,21),....,Person(&#x27;coo&#x27;,13)]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ifetch_single</span>(<span class="params">iterable, key, default=EMPTY, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    getter() g(item, key):pass</span></span><br><span class="line"><span class="string">    # 给定单个key或者属性或者索引号，用于获取数据序列对象的值，例如获取每个Person数据对象name的值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_getter</span>(<span class="params">item</span>):</span></span><br><span class="line">        <span class="keyword">if</span> getter:</span><br><span class="line">            <span class="comment"># 这里就是第三库如何把partial引用到自己的代码实现里面的一个示例，这里留到后面章节给出其解释</span></span><br><span class="line">            custom_getter = partial(getter, key=key)</span><br><span class="line">            <span class="keyword">return</span> custom_getter(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 用了第2章节内容提到的attrgetter获取属性值</span></span><br><span class="line">                <span class="comment"># 其实就是return getattr(item,key)，即获取单个数据项key对应的值</span></span><br><span class="line">                attrgetter = operator.attrgetter(key)</span><br><span class="line">                <span class="keyword">return</span> attrgetter(item)</span><br><span class="line">            <span class="keyword">except</span> AttributeError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 用了第2章节内容提到的itemgetter，给定索引号，获取值</span></span><br><span class="line">                <span class="comment"># 其实就是return item[key],即获取给定索引号的单个数据项对应的值           </span></span><br><span class="line">                itemgetter = operator.itemgetter(key)</span><br><span class="line">                <span class="keyword">return</span> itemgetter(item)</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">if</span> default <span class="keyword">is</span> <span class="keyword">not</span> EMPTY:</span><br><span class="line">                <span class="keyword">return</span> default</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Item %r has no attr or key for %r&#x27;</span> % (item, key))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">map</span>(_getter, iterable)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_single</span>(<span class="params">iterable, key, default=EMPTY, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 因为ifetch_single返回的map对象，因此需要list化后，才能得到整个列表数据值</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(ifetch_single(iterable, key, default=default, getter=getter))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ifetch_multiple</span>(<span class="params">iterable, *keys, defaults=<span class="literal">None</span>, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 用于处理给定的key或者属性或者索引号的入参大于1个的情况，例如要获取每个Person数据对象的name属性的值、age属性的值、phone属性的值，所以有三个key：name、age、phone</span></span><br><span class="line">    defaults = defaults <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(keys) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 根据给定key的个数n，生成对应n个数据序列的迭代器，其实就是把要处理的数据序列变成迭代器后，并复制了多份,显然存在设计不合理的地方，拷贝多份数据，占用空间。</span></span><br><span class="line">        <span class="comment">#例如3个key对应生成3个迭代器 iters = (iterable,iterable,iterable)</span></span><br><span class="line">        iters = tee(iterable, <span class="built_in">len</span>(keys))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        iters = (iterable,)</span><br><span class="line">  <span class="comment">#结果为：[ifetch_single(data_list,&#x27;name&#x27;),ifetch_single(data_list,&#x27;age&#x27;),ifetch_single(data_list,&#x27;phone&#x27;)]    </span></span><br><span class="line">    iters = [ifetch_single(it, key, default=defaults.get(key, EMPTY), getter=getter) <span class="keyword">for</span> it, key <span class="keyword">in</span> <span class="built_in">zip</span>(iters, keys)]</span><br><span class="line">    <span class="keyword">return</span> iters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ifetch</span>(<span class="params">iterable, key, *keys, default=EMPTY, defaults=<span class="literal">None</span>, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 该函数就是通过判断需要获取1个属性还是多个属性来决定调用ifetch_single还是ifetch_multiple</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(keys) &gt; <span class="number">0</span>:</span><br><span class="line">        keys = (key,) + keys</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">map</span>(<span class="built_in">list</span>, ifetch_multiple(iterable, *keys, defaults=defaults, getter=getter))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ifetch_single(iterable, key, default=default, getter=getter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">iterable, key, *keys, default=EMPTY, defaults=<span class="literal">None</span>, getter=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 这个fetch其实多此一举，可以直接在ifetch返回处加入list方法即可。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(ifetch(iterable, key, *keys, default=default, defaults=defaults, getter=getter))</span><br></pre></td></tr></table></figure>

<h4 id="3-2-fetch的使用示例"><a href="#3-2-fetch的使用示例" class="headerlink" title="3.2 fetch的使用示例"></a>3.2 fetch的使用示例</h4><p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_list = [</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;aro&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">10</span>,<span class="string">&#x27;phone&#x27;</span>:<span class="number">131</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;bro&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">13</span>,<span class="string">&#x27;phone&#x27;</span>:<span class="number">132</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;cro&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">15</span>,<span class="string">&#x27;phone&#x27;</span>:<span class="number">143</span>&#125;,</span><br><span class="line">]</span><br><span class="line">result = fetch(data_list,<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment">#[&#x27;aro&#x27;,&#x27;bro&#x27;,&#x27;cro&#x27;]</span></span><br><span class="line"></span><br><span class="line">result = fetch(data_list,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment">#[[&#x27;aro&#x27;,&#x27;bro&#x27;,&#x27;cro&#x27;],[10,13,15]]</span></span><br></pre></td></tr></table></figure>
<p>当了解fetch里面的调用了itemgetter的内部逻辑后，其实就是字典的取值：data[‘name’]，data[‘age’]</p>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">R</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">data_list=[</span><br><span class="line">	R(<span class="string">&#x27;aro&#x27;</span>,<span class="number">10</span>),</span><br><span class="line">	R(<span class="string">&#x27;bro&#x27;</span>,<span class="number">12</span>),</span><br><span class="line">	R(<span class="string">&#x27;cro&#x27;</span>,<span class="number">19</span>)</span><br><span class="line">]</span><br><span class="line">print(fetch(data_list,<span class="string">&#x27;name&#x27;</span>))      </span><br><span class="line"><span class="comment">#[&#x27;aro&#x27;, &#x27;bro&#x27;, &#x27;cro&#x27;]    </span></span><br></pre></td></tr></table></figure>
<p>当了解fetch里面的调用了attrgetter的内部逻辑后，其实就是使用内建方法getattr获取对象属性的值：[getattr(data1,’name’),getattr(data2,’name’),getattr(data3,’name’)]</p>
<p>示例3<br>自定义getter，这里用到了partial</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, <span class="built_in">id</span>, name, age, phone</span>):</span></span><br><span class="line">        self.<span class="built_in">id</span> = <span class="built_in">id</span></span><br><span class="line">        self._data = &#123;<span class="string">&#x27;name&#x27;</span>: name, <span class="string">&#x27;age&#x27;</span>: age, <span class="string">&#x27;phone&#x27;</span>: phone&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, key</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._data.get(key)</span><br><span class="line">data_item = [</span><br><span class="line">    Person(<span class="number">1001</span>,<span class="string">&#x27;Aerk&#x27;</span>, <span class="number">22</span>, <span class="number">141</span>),</span><br><span class="line">    Person(<span class="number">1002</span>, <span class="string">&#x27;Berk&#x27;</span>, <span class="number">25</span>, <span class="number">151</span>),</span><br><span class="line">    Person(<span class="number">1003</span>, <span class="string">&#x27;Derk&#x27;</span>, <span class="number">21</span>, <span class="number">181</span>)</span><br><span class="line">]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_getter</span>(<span class="params">item,key</span>):</span></span><br><span class="line">    <span class="keyword">return</span> item.get(key)</span><br><span class="line">values = fetch(data_item, <span class="string">&#x27;name&#x27;</span>, getter=my_getter)</span><br><span class="line">print(values)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment"># [&#x27;Aerk&#x27;, &#x27;Berk&#x27;, &#x27;Derk&#x27;]</span></span><br></pre></td></tr></table></figure>


<h3 id="4、本文核心内容"><a href="#4、本文核心内容" class="headerlink" title="4、本文核心内容"></a>4、本文核心内容</h3><p>经过前面3个章节多个知识点的铺垫后，再来看本章节内容，则会更容易理解。本节内容对应前言第一部分：<br>wrap装饰器的调用过程：functools.wraps—-&gt;partial—-&gt;update_wrapper<br>先看看functools.wraps的示例。</p>
<h4 id="4-1-login-require的装饰器例子"><a href="#4-1-login-require的装饰器例子" class="headerlink" title="4.1 login_require的装饰器例子"></a>4.1 login_require的装饰器例子</h4><p>在Django的app开发中，一般会使用装饰器鉴权，大致如下结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># 未使用functools.wraps</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line"></span><br><span class="line"><span class="meta">@login_require</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_blog_list</span>(<span class="params">request</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取blog列表的function&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">200</span></span><br><span class="line"></span><br><span class="line">print(get_blog_list.__doc__)</span><br><span class="line">print(get_blog_list.__name__)</span><br><span class="line">print(get_blog_list.__qualname__)</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  用于对外部的request做是否已经登录请求鉴权</span></span><br><span class="line"><span class="comment"># inner_wrap</span></span><br><span class="line"><span class="comment"># login_require.&lt;locals&gt;.inner_wrap</span></span><br></pre></td></tr></table></figure>
<p>这里显然不符合需求，get_blog_list的<code>__doc__</code>、<code>__name__</code>、<code>__qualname__</code>被改成login_require里面闭包函数inner_wrap对应属性的值<br>这里如何保证被装饰函数get_blog_list的属性值不被改动呢？加入functools.wraps(func)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line">print(get_blog_list.__doc__)</span><br><span class="line">print(get_blog_list.__name__)</span><br><span class="line">print(get_blog_list.__qualname__)</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">获取blog列表的function</span><br><span class="line">get_blog_list</span><br><span class="line">get_blog_list</span><br></pre></td></tr></table></figure>
<p>这次，get_blog_list的<code>__doc__</code>、<code>__name__</code>、<code>__qualname__</code>属性保持不变。</p>
<p>不使用functools.wraps(func)，也可以实现get_blog_list被装饰后，其属性值保持不变，通过setattr处理即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    inner_wrap.__doc__=func.__doc__</span><br><span class="line">    inner_wrap.__name__=func.__name__</span><br><span class="line">    inner_wrap.__qualname__=func.__qualname__</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line">    </span><br><span class="line"><span class="comment">#或者使用setattr设置属性值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_require</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner_wrap</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用于对外部的request做是否已经登录请求鉴权&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__doc__&#x27;</span>,func.__doc__)</span><br><span class="line">    <span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__name__&#x27;</span>,func.__name__)</span><br><span class="line">    <span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__qualname__&#x27;</span>,func.__qualname__)</span><br><span class="line">    <span class="keyword">return</span> inner_wrap</span><br><span class="line">print(get_blog_list.__doc__)</span><br><span class="line">print(get_blog_list.__name__)</span><br><span class="line">print(get_blog_list.__qualname__)    </span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">获取blog列表的function</span><br><span class="line">get_blog_list</span><br><span class="line">get_blog_list</span><br></pre></td></tr></table></figure>
<p>update_wrapper正是使用上述setter方式实现对原函数被装饰后，新函数属性和原函数属性和保持一致。<br>但在软件工程中，这种写法是过程式设计，初级的写法，无法被重用，因此需要使用更优雅的方式将这些逻辑封装打包，对外可以重用。</p>
<h4 id="4-2-functools-wraps的定义"><a href="#4-2-functools-wraps的定义" class="headerlink" title="4.2 functools.wraps的定义"></a>4.2 functools.wraps的定义</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wraps</span>(<span class="params">wrapped,</span></span></span><br><span class="line"><span class="function"><span class="params">          assigned = WRAPPER_ASSIGNMENTS,</span></span></span><br><span class="line"><span class="function"><span class="params">          updated = WRAPPER_UPDATES</span>):</span></span><br><span class="line">    <span class="keyword">return</span> partial(update_wrapper, wrapped=wrapped,</span><br><span class="line">                   assigned=assigned, updated=updated)</span><br></pre></td></tr></table></figure>
<p>wraps里面调用partial，partial里面调用update_wrapper，应用在get_blog_list也就是：partial(update_wrapper, wrapped=get_blog_list)<br>接下来，先看看update_wrapper到底实现的什么功能</p>
<h4 id="4-3-update-wrapper的源码分析"><a href="#4-3-update-wrapper的源码分析" class="headerlink" title="4.3 update_wrapper的源码分析"></a>4.3 update_wrapper的源码分析</h4><p>以login_require的装饰器例子，wrapper就是inner_wrap函数，而wrapped则是get_blog_list</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">WRAPPER_ASSIGNMENTS = (<span class="string">&#x27;__module__&#x27;</span>, <span class="string">&#x27;__name__&#x27;</span>, <span class="string">&#x27;__qualname__&#x27;</span>, <span class="string">&#x27;__doc__&#x27;</span>,<span class="string">&#x27;__annotations__&#x27;</span>)</span><br><span class="line">WRAPPER_UPDATES = (<span class="string">&#x27;__dict__&#x27;</span>,)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_wrapper</span>(<span class="params">wrapper,</span></span></span><br><span class="line"><span class="function"><span class="params">                   wrapped,</span></span></span><br><span class="line"><span class="function"><span class="params">                   assigned = WRAPPER_ASSIGNMENTS,</span></span></span><br><span class="line"><span class="function"><span class="params">                   updated = WRAPPER_UPDATES</span>):</span></span><br><span class="line">                   </span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> assigned:</span><br><span class="line">        <span class="comment"># 将原函数指定的5个属性（__name__、__doc__等）更新到（注册到/覆盖到）新函数，使得原函数被装饰后，指定的5个属性也保不变。</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 获取原函数的属性值</span></span><br><span class="line">            value = <span class="built_in">getattr</span>(wrapped, attr)</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 将原函数的属性值更新到相应的新函数属性中</span></span><br><span class="line">            <span class="built_in">setattr</span>(wrapper, attr, value)</span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> updated:</span><br><span class="line">        <span class="comment"># 原函数的__dict__属性更新到（注册到/覆盖到）新函数的__dict__属性，从而实现原函数被装饰后其__dict__属性保持不变。</span></span><br><span class="line">        <span class="built_in">getattr</span>(wrapper, attr).update(<span class="built_in">getattr</span>(wrapped, attr, &#123;&#125;))</span><br><span class="line">    wrapper.__wrapped__ = wrapped</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<p>所以update_wrapper设计简约，实现了4.1章节内容所提如下内容的功能</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__doc__&#x27;</span>,func.__doc__)</span><br><span class="line"><span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__name__&#x27;</span>,func.__name__)</span><br><span class="line"><span class="built_in">setattr</span>(inner_wrap,<span class="string">&#x27;__qualname__&#x27;</span>,func.__qualname__)</span><br></pre></td></tr></table></figure>

<h4 id="4-3-partial的源码分析"><a href="#4-3-partial的源码分析" class="headerlink" title="4.3 partial的源码分析"></a>4.3 partial的源码分析</h4><p>文章到了这里，对partial的了解将会更加深入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial</span>(<span class="params">func, *args, **keywords</span>):</span></span><br><span class="line">    <span class="comment">#对于login_require的装饰器例子，这里的func参数就是update_wrapper，args参数为空为:keywords就是wrapped=get_blog_list被装饰函数,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES</span></span><br><span class="line">    <span class="comment"># 因为update_wrapper没有`func`属性，所以跳过这部分处理</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(func, <span class="string">&#x27;func&#x27;</span>):</span><br><span class="line">        args = func.args + args</span><br><span class="line">        tmpkw = func.keywords.copy()</span><br><span class="line">        tmpkw.update(keywords)</span><br><span class="line">        keywords = tmpkw</span><br><span class="line">        <span class="keyword">del</span> tmpkw</span><br><span class="line">        func = func.func</span><br><span class="line"></span><br><span class="line"><span class="comment">#   这里newfunc是整个partial设计为最巧妙的地方！！！乃至functools.wraps里面设计最为巧妙的环节。将新加入的位置参数和关键字参数追加到func里</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">newfunc</span>(<span class="params">*fargs, **fkeywords</span>):</span></span><br><span class="line">        <span class="comment">#wrapped=get_blog_list被装饰函数,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES拷贝到newkeywords</span></span><br><span class="line">        newkeywords = keywords.copy()</span><br><span class="line">        newkeywords.update(fkeywords)</span><br><span class="line">        <span class="comment"># 对于login_require例子，这里fkeywords为空</span></span><br><span class="line">        <span class="keyword">return</span> func(*(args + fargs), **newkeywords)</span><br><span class="line"></span><br><span class="line">    newfunc.func = func</span><br><span class="line">    newfunc.args = args</span><br><span class="line">    newfunc.keywords = keywords</span><br><span class="line">    <span class="keyword">return</span> newfunc</span><br></pre></td></tr></table></figure>
<p>当使用以下写法时</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@functools.wraps(<span class="params">func</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_wrapper</span></span></span><br></pre></td></tr></table></figure>
<p>就会触发partial调用newfunc，而newfunc被定义Wie函数，是可以被<code>__call__</code>的，它返回func(*(args + fargs), **newkeywords)，func就是update_wrapper函数，args为空参数，fargs就是inner_wrapper函数，newkeywords就是wrapped=get_blog_list被装饰函数,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES<br>所以以下这一小段代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@functools.wraps(<span class="params">func</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_wrapper</span></span></span><br></pre></td></tr></table></figure>

<p>就是转换为以下语句的调用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">update_wrapper(wrapper=inner_wrapper,wrapped=get_blog_list,assigned = WRAPPER_ASSIGNMENTS,updated = WRAPPER_UPDATES)</span><br></pre></td></tr></table></figure>

<p>4.3章节已经详细指出update_wrapper的作用：<br> 将原函数get_blog_list指定的5个属性（<code>__name__</code>、<code>__doc__</code>等）更新到（注册到/覆盖到）新函数inner_wrapper，使得原函数被装饰后，指定的5个属性也保不变。<br>这就是functools.wraps(func)的内部基于partial的实现逻辑</p>
<h4 id="4-4-再论partial"><a href="#4-4-再论partial" class="headerlink" title="4.4 再论partial"></a>4.4 再论partial</h4><p>partical的官方说明：</p>
<blockquote>
<p>functools.partial(func, *args, **keywords)<br>Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords. If more arguments are supplied to the call, they are appended to args. If additional keyword arguments are supplied, they extend and override keywords. </p>
</blockquote>
<p>从英文的解释来看（不建议翻译为中文，直接理解英文更加准确），partial不应该翻译成“偏函数”，partial词意中：有不完整的意思，翻译成“待补全外部参数”的类函数对象貌似更贴切，<br>以下面例子说明</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">x,y,z=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> x+y+z</span><br><span class="line"><span class="comment"># 先给定1个位置参数和1个关键字参数，对于add函数，参数是不完整的，需要待后面补全多一个外部位置参数    </span></span><br><span class="line">add_15=partial(add,<span class="number">5</span>,z=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#这里add_15如果直接调用，会提示缺少一个位置参数，因此对于add_15，3就是待补全外部参数，</span></span><br><span class="line">print(add_15(<span class="number">3</span>)) <span class="comment"># 输出18</span></span><br></pre></td></tr></table></figure>
<p>以上的实际执行过程如下：<br>partial返回func(*(args + fargs), **newkeywords)，这里的func为add函数，args为(5,10)两个位置参数，3为后面补全的参数，就是fargs的值，newkeywords定义为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">newkeywords = keywords.copy()</span><br><span class="line">newkeywords.update(fkeywords)</span><br></pre></td></tr></table></figure>
<p>add的z=10，就是keywords，fkeywords为空，因此newkeywords为{‘z’:10}<br>所以有以下等价链<br><code>partial(add,5,z=10)&lt;===&gt;func(*(args + fargs), **newkeywords)&lt;===&gt;add(*((5,)+(3,)),z=10)&lt;===&gt;add(5,3,z=10)</code></p>
<p>从上面分析可以看出，paritial最大的用处就是基于某个原函数和原函数参数基础上生成一个”待补全外部参数新函数“，提供给该新函数的入参个数比原函数少了，得到高效简洁地调用指定函数，例如新函数add_15只需要提供1个参数即可，而原函数add需要提供3个参数。</p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>functool.wraps</tag>
        <tag>partial</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Kafka</title>
    <url>/blog/2019/12/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章<a href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，介绍大数据实时分析平台生态圈组件——kafka，前向用于连接flume，后向连接spark streaming。在研究Kafka过程中，发现该中间件的设计很巧妙，因此专设一篇文章用于深入理解Kafka核心知识。Kafka已经纳入个人目前最欣赏的中间件list：redis，zookeeper，kafka</p>
<h4 id="1、kafka集群架构图"><a href="#1、kafka集群架构图" class="headerlink" title="1、kafka集群架构图"></a>1、kafka集群架构图</h4><p>以下为kafka集群一种经典的架构图，该图以《在hadoopHA节点上部署kafka集群组件》文章的kafka集群以及sparkapp topic作为示例绘成，本文的内容将以该图为标准作为说明。<br><img src="https://img-blog.csdnimg.cn/20191127231934698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="kafka集群架构图">图1 kafka集群架构图</p>
<a id="more"></a>

<h4 id="2、kafka-高性能读写的设计"><a href="#2、kafka-高性能读写的设计" class="headerlink" title="2、kafka 高性能读写的设计"></a>2、kafka 高性能读写的设计</h4><h5 id="2-1、利用read-ahead-和-write-behind提升写性能"><a href="#2-1、利用read-ahead-和-write-behind提升写性能" class="headerlink" title="2.1、利用read-ahead 和 write-behind提升写性能"></a>2.1、利用read-ahead 和 write-behind提升写性能</h5><p>&#8195;&#8195;kafka底层设计高度依赖现代磁盘优化技术和文件系统的优化技术。在kafka官方文档的：<a href="http://kafka.apache.org/documentation/#design">don’t fear the filesystem</a>章节说明了kafka是如何利用磁盘已有的高性能读写技术：read-ahead 和 write-behind 实现日志在磁盘山高性能顺序写。<br>&#8195;&#8195;read-ahead 是以大的 data block 为单位预先读取数据。write-behind（后写） 是将多个小型的逻辑写合并成一次大型的物理磁盘写入，producer向kafka写入消息日志时，因为消息是一条一条的过来，而且消息本身payload很小，如果每条消息进来立刻执行写入磁盘，显然IO非常高，因此需要将进来的消息先缓存，然后到一定数量或者到一定容量时再触发写入磁盘，kafka用了pagecache实现write-behind而不是通过内存。<br>&#8195;&#8195;官方举例说明用廉价的RAID-5模式sata硬盘可以去到600MB/秒，但随机写入的性能仅约为100k/秒，相差6000倍以上。</p>
<h5 id="2-2、使用pagecache缓存程序数据提升读写性能"><a href="#2-2、使用pagecache缓存程序数据提升读写性能" class="headerlink" title="2.2、使用pagecache缓存程序数据提升读写性能"></a>2.2、使用pagecache缓存程序数据提升读写性能</h5><p>&#8195;&#8195;同样，在kafka官方文档的：<a href="http://kafka.apache.org/documentation/#design">don’t fear the filesystem</a>章节还提到另外一个技术：pagecache。kafka利用了现代操作系统主动将所有空闲内存用作磁盘caching这一机制（代价是在内存回收时性能会有所降低），再次提升基于filesystem的读写性能的效果。<br>&#8195;&#8195;kafka 跑在 jvm之上，那么jvm一定会有复杂的GC情况：</p>
<ul>
<li>对象的内存开销非常高，通常是所存储的数据的两倍(甚至更多)。</li>
<li>随着堆中数据的增加，Java 的垃圾回收变得越来越复杂和缓慢。</li>
</ul>
<p>&#8195;&#8195;受这些因素影响， 维护in-memory cache就会显得很复杂，而kafka通过文件系统方式和 pagecache 读写消息反而显得更有优势（避免复杂低效率的GC），通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番，例如32GB内存的服务器，它的 pagecache缓存容量可以达到28-30GB，并且不会产生额外的 GC 负担。kafka自己也说还有重要一点：简化核心代码。<br>为何这么设计？<br>&#8195;&#8195;kafka自己这么解释：因为相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将消息数据 flush 到文件系统的，kafka写过程把这个过程倒过来：所有消息数据一开始就被写入（write-behind）到文件系统的持久化日志中，而不用在in-memory cache 空间不足的时候 flush 到磁盘。实际上，是先把数据被转移到了内核的 pagecache 中。<br>&#8195;&#8195;这里可以联想到Hbase的MemStore设计：MemStore基于in-memory cache，MemStore 在内存中存在，保存修改key-value数据，当MemStore的大小达到一个阀值（默认64MB）时，MemStore里面的数据会被flush到Hfile文件上，也就是flush到磁盘上。</p>
<p>==为何page cache 会加速读过程？==<br>linux的文件cache分为两层，一个是page cache，另一个是buffer cache；每一个page cache包含若干个buffer cache，结构图如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191130122756221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>page cache：文件系统层级的缓存，从磁盘里读取数据缓存到page cache（属于内核空间，而不是应用用户的空间），这样应用读磁盘数据会被加速，例如使用find等命令查找文件时，第一次会慢很多，第二次查找相同文件时会瞬间读取到。如果page cache的数据被修改过后，也即脏数据，等到写入磁盘时机到来时，会把数据转移到buffer cache 而不是直接写入到磁盘。<br>buffer cache：磁盘等块设备的缓冲。<br>大致流程：<br><img src="https://img-blog.csdnimg.cn/20191130120633422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">page cache其优化读的工作过程如下：<br>A、文件的第一次读请求<br>系统读入所请求的page页并读入紧随其后的的少数几个页面，这种读取方式称为同步预读。<br>B、文件的第二次读请求:<br>如果page页不在第一次的cache中，说明不是顺序读，所以又会重新继续第一次那种同步预读过程。</p>
<p>==如果page页面在cache中，说明是顺序读，Linux会将预读group扩大一倍==，继续把不在首次cache中的文件数据读进来，此为异步预读。kafka之所以设计按顺序读写，完全就是按照底层page cahe的这种预读机制来设计，所以在文件系统底层就已经有不错的性能了。</p>
<h5 id="2-3-通过sendfile（零拷贝机制）提高消费者端的读吞吐量"><a href="#2-3-通过sendfile（零拷贝机制）提高消费者端的读吞吐量" class="headerlink" title="2.3 通过sendfile（零拷贝机制）提高消费者端的读吞吐量"></a>2.3 通过sendfile（零拷贝机制）提高消费者端的读吞吐量</h5><p>&#8195;&#8195;在kafka官方文档的Efficiency章节解释了kafka通过使用sendfile （零拷贝技术）继续提高消费者端的读性能。<br>&#8195;&#8195;前面2.1和2.2解释了kafka里利用相关底层机制，解决了磁盘访问模式不佳的情况。接下来，还需要解决以下两个影响kafka性能的情况：<br>too many small I/O operations, and excessive byte copying<br>（大量的小型 I/O 操作以及过多的字节拷贝 ）</p>
<ul>
<li><p>A、 The small I/O problem happens both between the client and the server and in the server’s own persistent operations.<br>（大量小型的 I/O 操作表现在client和broker之间以及broker服务端自身持久化操作中）<br>解决方式：kafka用一个称为 “消息块” 的抽象基础上，合理将消息分组。 这使得网络请求将多个消息打包成一组，而不是每次发送一条消息，从而使整组消息分担网络中往返的开销。consumer 每次获取多个大型有序的消息块，并由服务端依次将消息块一次加载到它的日志中。<br>这个简单的优化对速度有着数量级的提升。批处理允许更大的网络数据包，更大的顺序读写磁盘操作，连续的内存块等等</p>
</li>
<li><p>B、excessive byte copying<br>另一个低效率的操作是字节拷贝，在消息量少时，这不是什么问题，但是在高负载的情况下，影响就不容忽视。为了避免这种情况，kafka在producer、broker 和 consumer 都是用相同标准化的二进制消息格式，这样数据块不用修改就能在他们之间传递。<br>broker 维护的消息日志本身就是一个文件目录，每个segment文件都由一系列以相同格式消息组成，保持这种通用格式将非常有利于消息日志文件的网络传输的效率。 现代的unix 操作系统提供了一个高度优化的编码方式，用于将数据从 pagecache 转移到 socket 网络连接中，减少内核拷贝次数；在 Linux 中系统调用<a href="http://man7.org/linux/man-pages/man2/sendfile.2.html"> sendfile </a>方式做到这一点。 </p>
</li>
</ul>
<p>先看看数据从磁盘文件到套接字的拷贝过程：<br><code>File.read(fileDesc, buf, len);</code><br><code>Socket.send(socket, buf, len);</code><br>以上两个操作是java语义的读取文件和socket发送数据包，一共有两次拷贝？当然不是的：<br>1） 操作系统从磁盘读取数据到内核空间的 page cache<br>2）应用程序从内核空间 page cache读取数据到用户空间的缓冲区（应用程序的地址空间）<br>3）应用程序将数据(用户空间的缓冲区)写回内核空间到套接字缓冲区(内核空间)<br>4）操作系统将数据从套接字缓冲区(内核空间)复制到通过网络发送的 NIC 缓冲区<br>以上过程有四次 copy 操作和两次系统调用，如果数据传输吞吐量大时，对Kafka来说低效率，如何减少拷贝次数？<br>使用 内核提供的sendfile 方法，使用零拷贝的应用程序要求内核直接将数据从磁盘文件拷贝到套接字，而无需通过应用程序。零拷贝不仅大大地提高了应用程序的性能，而且还减少了内核与用户模式间的上下文切换。例如一个 topic 被多消费者消费时，使用上面zero-copy（零拷贝）优化，消息在使用时只会被复制到pagecache 中一次，节省了每次拷贝到用户空间内存中，再从用户空间进行读取的消耗。这使得消息能够以接近服务器网卡Gb级别的网速来进行消费。</p>
<blockquote>
<p>在应用程序和网络之间提供更快的数据传输方法，从而可以有效地降低通信延迟，提高网络吞吐率。零拷贝技术是实现主机或者路由器等设备高速网络接口的主要技术之一。举例来说，一个 1 GHz 的处理器可以对 1Gbit/s 的网络链接进行传统的数据拷贝操作，但是如果是 10 Gbit/s 的网络，那么对于相同的处理器来说，零拷贝技术就变得非常重要了。</p>
</blockquote>
<p>page cache 和 sendfile 的组合使用意味着，在一个kafka集群中，大多数 consumer 消费时，==将看不到磁盘上的读取活动，因为数据将完全由缓存提供==。<br>有关zero-copy以及Linux IO详细内容，推荐IBM Developer中国区四篇高质量文章：<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-directio/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中直接 I/O 机制的介绍》</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中的零拷贝技术，第 1 部分》</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中的零拷贝技术，第 2 部分》</a><br><a href="https://www.ibm.com/developerworks/cn/java/j-zerocopy/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《通过零拷贝实现有效数据传输》</a>，这篇文章翻译了IBM Developer官方英文原文：<a href="https://developer.ibm.com/articles/j-zerocopy/">《Efficient data transfer through zero copy》</a><br>阅读这几篇文章可以说收益匪浅，不仅深度理解了kafka使用filesystem作为消息队列的底层文件IO，而且也有利于理解任何基于文件系统上的中间件的部分实现机制。</p>
<h4 id="3、kafka的repilcas副本机制"><a href="#3、kafka的repilcas副本机制" class="headerlink" title="3、kafka的repilcas副本机制"></a>3、kafka的repilcas副本机制</h4><h5 id="3-1-主分区的副本"><a href="#3-1-主分区的副本" class="headerlink" title="3.1 主分区的副本"></a>3.1 主分区的副本</h5><p>&#8195;&#8195;Kafka 允许 topic 的 partition 拥有若干副本，也就是说每个partition都有一个 leader 和零或多个 followers，例如图1 kafka集群架构图中，sparkapp这个topic，在broker1有主分区（leader）partition-0，在broker-1和broker-2有followers副本分区（replica）partition-0。  总的副本数是包含 leader 分区的总和。 所有的读写操作都由 leader 处理，各分区的 leader 均 匀的分布在brokers 中，一个topic在当前broker只能有一个leader主分区。followers节点就像普通的 consumer 那样从 leader 节点那里拉取消息并保存在自己的日志文件中。</p>
<h5 id="3-2-leade如何管理follower节点"><a href="#3-2-leade如何管理follower节点" class="headerlink" title="3.2 leade如何管理follower节点"></a>3.2 leade如何管理follower节点</h5><p>&#8195;&#8195;Kafka 判断节点是否存活有两种方式。</p>
<ul>
<li>首先follower所在的broker服务器在线，Zookeeper 通过心跳机制检查每个broker的连接，对应的znode路径/brokers/ids。</li>
<li>要求follower角色的同步进程 ，它必须能及时的同步 leader 的写操作，并且延时不能太多。 </li>
</ul>
<p>&#8195;&#8195;kafka认为满足这两个条件的节点处于 “in sync” 状态， Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本集合ISR中移除。这个ISR列表在zookeeper可以看到，a set of In-Sync Replicas，简称：ISR：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 36] get &#x2F;brokers&#x2F;topics&#x2F;sparkapp&#x2F;partitions&#x2F;1&#x2F;state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:6,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:2,&quot;isr&quot;:[11,12,10]&#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;以上说明：sparkapp的partition-1这个主分区在brokerid为10的服务器上，其他follower的brokerid分别为11和12。<br>可配置在ISR移除follower的触发条件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果leader发现follower超过10秒没有向它发起同步请求，那么leader会认为follower无法正常同步主分区日志，就把它从ISR集合中中移除。</span><br><span class="line"> rerplica.lag.time.max.ms&#x3D;10000 # 默认值</span><br><span class="line"> # 相差1000条就从ISR集合移除该follower</span><br><span class="line"> rerplica.lag.max.messages&#x3D;1000# 默认值</span><br></pre></td></tr></table></figure>

<h5 id="3-3-Replica如何均匀分布到整个kafka集群"><a href="#3-3-Replica如何均匀分布到整个kafka集群" class="headerlink" title="3.3 Replica如何均匀分布到整个kafka集群"></a>3.3 Replica如何均匀分布到整个kafka集群</h5><p>&#8195;&#8195;为了更好的做负载均衡以及HA，Kafka尽量降所有的replicas均匀分配到整个集群上。为了更直观partition的副本是如何被分布到不同节点上，这里以一个小例子为例：创建一个fooTopic（可以先把它理解为消息队列queue，类似RabbitMQ的队列）且有五个分区，每个分区有三个副本replica，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181 --replication-factor 3 --partitions 5 --topic fooTopic</span><br><span class="line">[root@dn1 kafka-2.12]#  bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181 --topic fooTopic</span><br><span class="line">Topic:fooTopic  PartitionCount:5        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: fooTopic Partition: 0    Leader: 11      Replicas: 11,10,12      Isr: 11,10,12</span><br><span class="line">        Topic: fooTopic Partition: 1    Leader: 12      Replicas: 12,11,10      Isr: 12,11,10</span><br><span class="line">        Topic: fooTopic Partition: 2    Leader: 10      Replicas: 10,12,11      Isr: 10,12,11</span><br><span class="line">        Topic: fooTopic Partition: 3    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: fooTopic Partition: 4    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Kafka分配资源跟很多中间件一样：通过取余实现，具体的规则如下：<br> 1）序号为i的Partition分配到第（i mod n）个Broker上，n为集群的broker总数<br> 2）序列号为i的Partition的第j个Replica分配到第（(i + j) mod n）个Broker上</p>
<p>以上述fooTopic为例，给出其分布过程：首先查看broker ids的列表为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[11, 12, 10]</span><br></pre></td></tr></table></figure>
<p>[11, 12, 10]列表的项的索引从0开始，因为kafka是用Scala语言开发，Scala获取zk这个ids值后，肯定是转为Scala数组类型，它的索引从0开始。当然也适用replicas数组（列表）。（这里为何不是[10, 11, 12]？因为本次获取结果是最新的集群选举结果数组）<br>假设a=[11, 12, 10]那么a[0]=11,a=[1]=12,a[2]=10</p>
<ul>
<li><p>按规则1）对于partition的序号i，它会分配到第（i mod n）个broker上：<br>那么对于partition0，0 mod 3=0，所以该在a[0]=11这个broker上，<br>同理有：<br>partition1，1 mod 3=1，所以该在a[1]=12这个broker上<br>partition2，2 mod 3=2，所以该在a[2]=10这个broker上<br>partition3，3 mod 3=0，所以该在a[0]=11这个broker上<br>partition4，4 mod 3=1，所以该在a[1]=12这个broker上</p>
</li>
<li><p>按规则 2）序列号为i的Partition的第j个Replica分配到第（(i + j) mod n）个Broker上<br>那么对于序列号为4的partition和序列号为0的replica，（4+0）mod 3=1，所以该在a[1]=12这个broker上，那么这个就是主leader分区，符合规则1partition4在12这个broker的计算结果。<br>那么对于序列号为4的partition和序列号为1的replica，（4+1）mod 3=2，所以该在a[2]=10这个broker上，<br>那么对于序列号为4的partition和序列号为2的replica，（4+2）mod 3=0，所以该在a[0]=11这个broker上<br>也就说partition4的replicas为[12,10,11]</p>
</li>
</ul>
<h4 id="4、Kafka消息的ack机制"><a href="#4、Kafka消息的ack机制" class="headerlink" title="4、Kafka消息的ack机制"></a>4、Kafka消息的ack机制</h4><p>&#8195;&#8195;这里是指producer向broker写消息的确认机制，这直接影响到Kafka集群的吞吐量和消息可靠性。而吞吐量和可靠性是矛盾的，两者不可兼得，只能平衡。<br>&#8195;&#8195;在第3章节提到leader和follower节点日志同步的内容，kafka动态维护了一个同步状态的副本的集合，在这个集合中的节点都是和leader保持高度一致的，任何一条消息只有被这个集合中的每个节点读取并追加到日志中，才会向外部通知说“这个消息已经committed。<br>&#8195;&#8195;也就是说只有当消息被ISR上所有的followers加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。这一环节就是决定了消息队列吞吐量和可靠性的环节。消息从leader复制到follower，可通过producer是否等待消息被提交的通知(ack)来区分同步复制和异步复制。<br>ack有3个可选值，分别是1，0，-1，可通过server.properties进行配置：<br>request.required.asks=0<br>==ack=0==:相当于异步的，producer给broker发送一次就不再发送了，不管本条消息是否在leader和follower都写入成功。可靠性低，吞吐量当然高。<br>==ack=1==：producer等待leader这个主分区成功写入了消息，producer才会认为消息发送成功，这是默认值，显然是吞吐量与可靠性的一个折中方案<br>==ack=-1==：当所有的follower都同步消息成功后，leader再向producer发送ack，producer才认为此消息发送成功，显然牺牲了吞吐量，因为如果leader有多个followers，同步需要一定时间，producer当然要等待一段时间后，再能继续向leader发送新消息。</p>
<p>当然ack=1的情况下，消息也可能会丢失，这是因为：<br>producer只要收到分区leader成功写入的通知就会认为消息发送成功了，但是也有这样的情况：leader成功写入后，还没来得及把数据同步到follower节点超时等，这时候消息会丢失。</p>
<p>清楚了kafka的ack机制后，来看看ack=-1的同步复制过程：<br>1）producer首先取zookeeper找到指定topic的主分区leader，向leader发送消息<br>2）leader收到消息写入到本地segment文件<br>3）所有follower从leader pull消息并写入自己segment文件<br>4）所有follower向leader发送ack消息<br>5）leader收到所有follower的ack消息<br>6）leader向producer发送ack<br>7）producer收到成功写入的响应</p>
<h4 id="5、kafka-消息索引机制"><a href="#5、kafka-消息索引机制" class="headerlink" title="5、kafka 消息索引机制"></a>5、kafka 消息索引机制</h4><p>&#8195;&#8195;前面的四节内容更多是kafka集群本身的一些机制，其实对于consumer侧，当consumer去broker pull一条的消息时，broker是如何快速找出相应的消息呢？<br>&#8195;&#8195;在前面部署Kafka集群已经知道每个partition都是一个文件目录，每个目录下有成index文件和log文件，它们是成对出现，后缀 “.index” 和 “.log” 分表表示 segment 索引文件和数据文件（存放消息的地方），segment的大小以及相关配置可在server.properties进行设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#segment文件的大小，默认为 1G</span><br><span class="line">log.segment.bytes&#x3D;1024*1024*1024</span><br><span class="line">#滚动生成新的segment文件的最大时长</span><br><span class="line">log.roll.hours&#x3D;24*7</span><br><span class="line">#segment文件保留的最大时长，超过7天将被删除</span><br><span class="line">log.retention.hours&#x3D;24*7</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;Segment 是 Kafka 存储消息的最小单位，Segment 文件命名规则：对于某个partition，例如partition0，全局的第一个 Segment 从 0 开始，后续每个 Segment 文件名为上一个 Segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用 0 填充。如 00000000000000368769.index 和 00000000000000368769.log。<br>对于parition1，它全局的第一个 Segment 也是从 0 开始，切勿认为parition0的最后一个segment的offset会顺延到partition1！<br>以下图为例，分析costumer如何根据offset拿到消息数据：<br>（因为集群测试环境还没有太多segment数据，所以这里参考这篇<a href="https://mp.weixin.qq.com/s/fX26tCdYSMgwM54_2CpVrw">文章的内容</a>：）<br><img src="https://img-blog.csdnimg.cn/20191130215819665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;假设现在研究partition0，在它.index文件中，存储的是key-value格式的值：&lt;n,m&gt;，n代表在.log中按顺序开始第n条消息，m代表该消息的位置偏移m，这里以索引文件中元数据 <code>&lt;3, 497&gt;</code> 为例，表示该368769.log文件中第 3 条Message，该消息所在物理位置为 497。<br>现在consumer要取offset为368773的消息，以下为查找过程：<br>1）在partition0下，有多个segment的index文件，根据二分法，可以快速地位到368772条消息在368769.index上<br>2）在368769.index索引文件，找出&lt;n,m&gt;的具体值，n=368774-368769=4（一般称为base offset），因为索引文件是稀疏结构，4这个值不在索引文件上<br>3）再根据二分法，很快找到3这个base offset&lt;3,497&gt;，因为索引值4没有，只能用不大于4的索引值3。<br>4）再回到368769.log上，从物理位置497开始按顺序查找，当物理位置到达830时，offset为368773的消息被找到。</p>
<p>&#8195;&#8195;从上图可以知道 Index 文件也不是每次递增 1 的，这是因为 Kafka 采取稀疏索引存储的方式，每隔一定字节的数据建立一条索引。它减少了索引文件大小，使得能够把 Index 映射到内存，降低了查询时的磁盘 IO 开销，同时也并没有给查询带来太多的时间消耗。<br>&#8195;&#8195;要满足以上的搜索策略：Kafka为在 Partition 中的每一条 Message 都定义了以下三个属性：</p>
<ul>
<li>Offset：表示 Message 在当前 Partition 中的偏移量，是一个逻辑上的值，唯一确定了在当前Partition 中的一条 Message</li>
<li>MessageSize：表示 Message 内容 Data 的大小。</li>
<li>Data：Message 的具体内容。<br>因此只要消费者只需要订阅的topic后，一旦拿到消息的offset，broker就会按以上检索策略将消息取出。</li>
</ul>
<h4 id="6、consumer-group的工作机制"><a href="#6、consumer-group的工作机制" class="headerlink" title="6、consumer group的工作机制"></a>6、consumer group的工作机制</h4><p>&#8195;&#8195;在图1可看到有多个consumer group，本章节主要探讨为何kafka使用consumer group的设计。<br>&#8195;&#8195;consumer group是kafka实现单播和广播两种消息模型的方式：<br>同一个topic的消息，可以广播给不同的group；<br>同一个topic的消息，每个group里面只能被其中的一个consumer消费。group内的consumer可以使用多线程或多进程来实现，consumer数量建议与partition成整数倍关系，因为kafka设计一个partition只能被group内一个consumer消费，也即是单播模式。</p>
<h5 id="6-1-一个topic为何需要被多个consumer消费？"><a href="#6-1-一个topic为何需要被多个consumer消费？" class="headerlink" title="6.1  一个topic为何需要被多个consumer消费？"></a>6.1  一个topic为何需要被多个consumer消费？</h5><p>&#8195;&#8195;以图1的sparkapp这个topic为例，假设没有consumer group，假设只有一个consumer去消费kafka集群的sparkapp，考虑到consumer只能从leader分区消费，相当于以下架构图：<br><img src="https://img-blog.csdnimg.cn/20191201103918787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">从单个consumer视角观察sparkapp，集群为consumer提供三个分区消费：（leader）partition0、（leader）partition1、（leader）partition2，所以上图简化成下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20191201104854685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">但如果当producer写入消息的速度比consumer读取的速度快呢？结果是：消息堆积越来越严重，对于这种情况，需要增加多个消费者来进行水平扩展消息的读取。<br>可以用实际案例说明：<br>例如这个sparkapp的消息是待发送邮箱的内容和用户邮箱地址，如果仅有一个consumer去读取消息再发邮箱通知用户，那么随消息堆积越来越严重，将会有大量用户不能及时收到邮件通知。<br>解决办法：增加多个consumer，一般是跟leader分区的数目一致，例如本例3个leader分区，对应3个consumer进行消费，每个consumer消费分别对应一个分区进行消费，如下图所示：<br><img src="https://img-blog.csdnimg.cn/2019120111063329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这就保证了生产的消息能够及时被consumer消费处理掉，表现在实际应用场景的效果：大量用户能够及时收到邮箱通知。<br>==注意：consumer数量建议与partition成整数倍关系，例如上面的1倍关系，因为kafka设计一个partition只能被group内一个consumer消费，也即是单播模式。<br>consumer数量由客户端自己通过多线程方式或者多进程方式实现。==</p>
<h5 id="6-2-同一个partition能否被多个consumer同时消费？"><a href="#6-2-同一个partition能否被多个consumer同时消费？" class="headerlink" title="6.2 同一个partition能否被多个consumer同时消费？"></a>6.2 同一个partition能否被多个consumer同时消费？</h5><p>例如（leader）partition0同时被两个consumer消费，如下所示：<br><img src="https://img-blog.csdnimg.cn/20191201111807824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">以邮箱通知这个为例子：<br>两个consumer将拿到重复的消息，在用户侧的效果就是：所有用户都会重复收到同一内容邮箱通知，显然不能接受。<br>kafka设计早已考虑到这些情况，所有kafka不允许同一个consumer group中的两个consumer读取同一个partition。</p>
<h5 id="6-3-kafka为何设计多个consumer-group这样的模型？"><a href="#6-3-kafka为何设计多个consumer-group这样的模型？" class="headerlink" title="6.3 kafka为何设计多个consumer group这样的模型？"></a>6.3 kafka为何设计多个consumer group这样的模型？</h5><p>以邮箱通知这个为例子：<br>这个sparkapp的消息是待发送邮箱的内容和用户邮箱地址，现在有两个应用需要拉取sparkapp这个topic的消息，一个是邮箱通知应用A，另外一个是存储邮箱内容和用户邮箱地址的应用B。</p>
<h6 id="6-3-1-无consumer-group，应用A和应用B会出现什么情况？"><a href="#6-3-1-无consumer-group，应用A和应用B会出现什么情况？" class="headerlink" title="6.3.1 无consumer group，应用A和应用B会出现什么情况？"></a>6.3.1 无consumer group，应用A和应用B会出现什么情况？</h6><p><img src="https://img-blog.csdnimg.cn/20191201114330792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">因为kafka设计每个consumer只能消费一个partition，如上图示，<br>==对于应用A，它开了2个线程==，消费(leader)partition0和(leader)partition1，在用户侧的出现情况：有一部分用户根本没收到邮件通知，漏了(leader)partition2这部分的数据。<br>==对于应用B，它只能开一个线程==，也即是一个consumer，而且只能拿到(leader)partition2的消息，最终出现的情况：数据库里面，根本没有存储到一部分用户的邮件记录。显然是漏了(leader)partition0和(leader)partition1的数据。<br>如何解决上述问题？</p>
<h6 id="6-3-2-为应用建立consumer-group，观测应用A和应用B的情况。"><a href="#6-3-2-为应用建立consumer-group，观测应用A和应用B的情况。" class="headerlink" title="6.3.2 为应用建立consumer group，观测应用A和应用B的情况。"></a>6.3.2 为应用建立consumer group，观测应用A和应用B的情况。</h6><p>考虑到两个应用，这里对应两个group，如下图示：<br><img src="https://img-blog.csdnimg.cn/20191201120617771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">group A 对应应用A，group B 对应应用B<br>kafka限定：<br>group内的consumer只能消费一个分区<br>同一分区（的一条信息）可以被不同group消费，广播模式。<br>从上图的结构可以看出：<br>应用A可以把三个分区的邮箱通知内容都发送到所有用户，不会出现像6.3.1 的情况：遗漏部分数据。<br>应用B可以把三个分区的邮箱通知内容都存储到数据库，不会出现像6.3.1 的情况：遗漏部分数据。<br>这就是kafka的consumer group的设计逻辑。<br>小结：<br>1）如果一个应用需要读取全量消息，那么可为该应用设置一个消费组；<br>如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。<br>2） kafka支持写入的一条消息能够被若干个应用读取这条消息。也就是说：<br>每个应用都可以读到全量的消息，通过为每个应用设置自己的消费组。</p>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka原理</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Spark</title>
    <url>/blog/2019/12/22/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Spark/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面博客文章里，已经把大数据实时分析项目在spark组件之前的各个组件原理、部署和测试都给出相关讨论，接下来是项目最核心的内容：实时计算部分，因为项目将使用spark streaming做微批计算（准实时计算），因此接下的文章内容将深入spark以及spark streaming架构原理，为后面实际计算编程做铺垫。</p>
<h4 id="1、Spark-是什么？"><a href="#1、Spark-是什么？" class="headerlink" title="1、Spark 是什么？"></a>1、Spark 是什么？</h4><p>&#8195;&#8195;Spark是一种分布式的并行计算框架，什么是计算框架？所谓的计算（在数据层面理解）其实是用于数据处理和分析的一套解决方案，例如Python的Pandas，相信用过Pandas都很容易理解Pandas擅长做什么，加载数据、对数据进行各类加工、分析数据等，只不过Pandas只适合在单机上的、数据量百万到千万级的计算组件，而Spark则是分布式的、超大型多节点可并行处理数据的计算组件。</p>
<p>&#8195;&#8195;Spark通常会跟MapReduce做对比，它与MapReduce 的最大不同之处在于Spark是基于内存的迭代式计算——Spark的Job处理的中间（排序和shuffling）输出结果可以保存在内存中，而不是在HDFS磁盘上反复IO浪费时间。除此之外，一个MapReduce 在计算过程中只有Map 和Reduce 两个阶段。而在Spark的计算模型中，它会根据rdd依赖关系预选设计出DAG计算图，把job分为n个计算阶段（Stage），因为它内存迭代式的，在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。<br>&#8195;&#8195;Spark提供了超过80种不同的Transformation和Action算子，如map，reduce，filter，reduceByKey，groupByKey，sortByKey，foreach等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小（尤其用Scala和Python写计算业务代码方面）。正是基于使用易用性，因此Spark能更好地用于基于分布式大数据的数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark生态如下：<br><img src="https://img-blog.csdnimg.cn/20191222113158864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>

<h4 id="2、Spark-运行模式"><a href="#2、Spark-运行模式" class="headerlink" title="2、Spark 运行模式"></a>2、Spark 运行模式</h4><p>目前最为常用的Spark运行模式有：</p>
<ul>
<li>Local：本地进程运行，例如启动一个pyspark交互式shell，一般用于开发调试Spark应用程序</li>
<li>Standalone：利用Spark自带的资源管理与调度器运行Spark集群，采用Master/Slave结构，可引入ZooKeeper实现spark集群HA</li>
<li>Hadoop YARN : 集群运行在YARN资源管理器上，资源管理交给YARN，Spark只负责进行任务调度和计算，参考本博客<a href="https://blog.csdn.net/pysense/article/details/103434832">《基于YARN HA集群的Spark HA集群》</a></li>
<li>Apache Mesos ：运行在著名的Mesos资源管理框架基础之上，该集群运行模式将资源管理交给Mesos，Spark只负责进行任务调度和计算<br>==<strong>Mesos和YARN两种资源有什么区别：</strong>==<br>之前看一个视频，对其给出的解释印象深刻：<br>Mesos：细腻度资源管控<br>YARN：粗粒度资源管控<br>例如有个老师要给45个学生上课，向教务处申请课室资源，若教务处以Mesos模式发放资源，那么它会发放只能容纳45个学生的课室，典型的按需分配；若教务处以YARN模式发放资源，那么它会发放能容200个学生的大教室，但实际上还有155个人位置资源空闲。这就是资源的细腻度和粗粒度的区别。</li>
</ul>
<h4 id="3、适合Spark的场景"><a href="#3、适合Spark的场景" class="headerlink" title="3、适合Spark的场景"></a>3、适合Spark的场景</h4><ul>
<li><p>Spark适用场景：<br>Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合，基于大数据的机器学习再适合不过，例如梯度下降法，需要不断迭代找到全局或局部最优解。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。<br>准实时计算场合：实时接收用户行为原始数据，并通过Spark Streaming计算（转换+加工），例如在广告、报表、推荐系统等业务上，在广告业务方面需要大数据做应用分析、效果分析、定向优化等，在推荐系统方面则需要大数据优化相关排名、个性化推荐以及热点点击分析等，这些业务天生适合大型的互联网巨头。</p>
</li>
<li><p>Spark不适用场景：</p>
<p>  内存消耗极大，在内存不足的情况下，Spark会下放到磁盘，会降低应有的性能<br>  有高实时性要求的流式计算业务，例如实时性要求毫秒级，对每一条数据都触发实时计算的，这种场合已经被Flink称霸。<br>  流线长或文件流量非常大的数据集不适合，这是因为这种场合rdd消耗极大的内存集群压力大时，一旦一个task失败会导致它前面一条线所有的前置任务全部重跑（尤其对于RDD 血缘关系链长且有多个宽依赖的情况），JVM的GC不够及时，内存不能及时释放，将会出现恶性循环导致更多的task失败，导致整个Application效率极低。所以为什么说Spark是适合“微批”处理，意味着每隔一段时间（1秒或者几秒不等）来一批次数据，Spark适合“一小口一小口准实时地吃数据”。</p>
</li>
</ul>
<h4 id="4、Spark相关术语"><a href="#4、Spark相关术语" class="headerlink" title="4、Spark相关术语"></a>4、Spark相关术语</h4><p><img src="https://img-blog.csdnimg.cn/20191221131426758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">一个完整的Spark应用程序，例如wordcount，在提交集群运行时:<br><code>./bin/spark-submit  --name word_count_app --master yarn  --deploy-mode cluster  --py-files word_count.py</code><br>它涉及到上图流程的相关术语： </p>
<ul>
<li><p>SparkContext:整个应用的上下文，控制应用的生命周期。</p>
</li>
<li><p>RDD：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型（鉴于RDD是Spark的核心概念，后面的有一篇博客给出相关讨论）。无法快速理解RDD？这里有两种方式可助于理解：<br>A、把它设想为分布式的Pandas dataframe，df也是一个数据集，也是被加载到内存上，然后利用pandas各个算子对df反复迭代最后得出计算结果。<br>B、<code>rdd = sc.parallelize(list(range(1000)), 10)</code>，这就创建了一个“轻量”的rdd数据集，设想下：这个数组有10亿项，分10个分区，有10个计算节点，每个节点负责1个分区的计算，也就是说从计算节点来看，每个节点负责1亿行的“小块rdd”；而从用户逻辑层面来看：就一个大rdd，包含10亿项数据</p>
</li>
<li><p>RDD的窄依赖和宽依赖</p>
<p>   A、窄依赖NarrowDependency（一对一）：不会产生分区之间的shuffle，所有的父RDD的partition会一一映射到子RDD的partition中，例如Map、FlatMap、Filter算子等</p>
<p>  B、宽依赖ShuffleDependency（一对多）：会引起多个分区之间的shuffle，父RDD中的partition会根据key的不同进行切分，划分到子RDD中对应的partition中，例如reduceByKey的任务。</p>
</li>
</ul>
<ul>
<li><p>DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系（关系链）。</p>
</li>
<li><p>Driver Program：Application中运行main函数并创建的SparkContext，创建SparkContext的目的是和集群的ClusterManager通信，进行计算资源的申请、任务的分配和监控等。因此也可认为SparkContext代表Driver控制程序。Driver负责对Application构建DAG图。</p>
</li>
<li><p>Cluster Manager：集群资源管理中心，例如YARN里面的ResourceManage，负责分配计算资源分配和回收。</p>
</li>
<li><p>Worker Node：启动Executor或Driver负责完成具体计算，在YARN模式中 Worker Node就是NodeManager节点。</p>
</li>
<li><p>Executor：是Application在Worker上面的一个进程，该进程会启动线程池方式去跑task，并负责把数据存在内存或者磁盘上。每个Application都有属于自己的一组Executors。在Spark on YARN模式下，Executor进程名为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor实例，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task。</p>
</li>
</ul>
<ul>
<li><p>Application：用户编写的Spark应用程序，例如下面启动一个名字为word_count_app的Application（简称app），该app的业务逻辑在word_count.py实现。一个Application包含多个Job。<br><code>./bin/spark-submit  --name word_count_app --master yarn  --deploy-mode cluster  --py-files word_count.py</code></p>
</li>
<li><p>Job：包含多个Task组成的并行计算，由Spark Action算子（collect、groupByKey、ReduceByKey、count、takeOrdered等）触发产生。一个action产生一个job，如果一个Application里面的业务代码有多少个action算子，就产生多少个Job。一个Job包含多个RDD及作用于相应RDD上的各种操作。</p>
</li>
</ul>
<ul>
<li><p>Task：任务，运行在Executor上的工作单元，是Executor中的一个线程（Executor是JVM进程，在YARN模式下，就是一个跑在container上JVM进程），与Hadoop MapReduce中的MapTask和ReduceTask一样，是运行Application的基本单位。多个Task组成一个Stage，而Task的调度和管理由TaskScheduler负责。</p>
</li>
<li><p>Stage：DAGScheduler将一个Job划分为若干Stages，每个Stage打包成一组Tasks，又称TaskSet。Stage的调度和划分由DAGScheduler负责。Stage又分为Shuffle Map Stage和Result Stage两种。Stage的边界就在发生Shuffle（rdd宽依赖）的地方。 </p>
</li>
<li><p>DAGScheduler：根据Job构建基于Stage的DAG（有向无环任务图），DAGScheduler会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，并提交Stage给TaskScheduler。</p>
</li>
<li><p>TaskScheduler：将任务(Task)分发给Executor，每个Executor负责运行什么Task由TaskScheduler分配。 </p>
</li>
<li><p>Shared Variables共享变量：Application在整个运行过程中，可能需要一些变量在每个Task中都使用，用于节省计算时间和IO。Spark有两种共享变量：一种缓存到各个节点的广播变量：broadcast；一种只支持加法操作：accumulator，一般用于对rdd求和sum以及累加计数counter。 这个概念需结合实际用例说明，否则难以理解。</p>
</li>
</ul>
<p>&#8195;&#8195;一句话：一个Application(其实就是你设计的Spark业务程序)由多个Job组成，一个Job由多个Stage组成，一个Stage由多个Task组成一个TaskSet。Stage是作业调度的基本单位，Task是执行计算和操作rdd的最小工作单元，以下面的wordcount语句对于的关系图说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20191221150736129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;这里为何画了2个Job，因为该计算任务包含两个Action：reduceByKey和sortBy。Application里面有多少个Action算子，Driver就给Application分配多少个Job。</p>
<p>&#8195;&#8195;Spark计算涉及的相关部件比较多而且相对抽象，需要在实际spark集群上跑几个测试application结合理解，本博客在前面的文章已经在测试项目中结合spark UI的截图就Job、Stage、Task等给出较为详细的说明，参考<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>第7章内容。</p>
<h4 id="5、Spark程序执行流程"><a href="#5、Spark程序执行流程" class="headerlink" title="5、Spark程序执行流程"></a>5、Spark程序执行流程</h4><p><img src="https://img-blog.csdnimg.cn/20191221131426758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;Spark 程序执行流程基于不同资源管理器其有不同的执行流程，以下以Standalone模式说明执行流程。其他资源管理模式的执行流程可以参考这篇文章：<a href="https://www.cnblogs.com/frankdeng/p/9301485.html">《Spark任务提交方式和执行流程》</a></p>
<ul>
<li><p>1)构建Spark Application的运行环境，启动SparkContext也即启动Driver，Driver向资源管理器注册并申请运行Executor资源；</p>
</li>
<li><p>2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；</p>
</li>
<li><p>3)Driver根据算子链预先构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task。</p>
</li>
<li><p>4)TaskScheduler将Task发送给Executor运行，同时Driver将应用程序代码传给Executor。</p>
</li>
<li><p>5)Task在Executor上运行，运行完毕释放所有资源。</p>
</li>
</ul>
<p>其实不管在哪种种资源模式下，Spark的程序执行流程一定离不开三条主线：</p>
<ul>
<li>Driver 申请资源用于启动Executor</li>
<li>Driver 构建DAG图，切分Stage，最终生产出多组TaskSet</li>
<li>Executor 领取Task和业务代码并执行</li>
</ul>
<h4 id="6、理解Spark-Stage的划分"><a href="#6、理解Spark-Stage的划分" class="headerlink" title="6、理解Spark Stage的划分"></a>6、理解Spark Stage的划分</h4><p>&#8195;&#8195;本部分内容比较重要，也是理解spark任务调度原理的关键，本章节内容参考<a href="http://sharkdtu.com/posts/spark-scheduler.html">《Spark Scheduler内部原理剖析》</a>其中任务调度内容</p>
<h5 id="6-1-Spark-Stage的划分"><a href="#6-1-Spark-Stage的划分" class="headerlink" title="6.1 Spark Stage的划分"></a>6.1 Spark Stage的划分</h5><p>&#8195;&#8195;这里以wordcount的Scala语句分析Spark对于一个application是如何划分stage的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;result&quot;)</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;以上就是一个Job，由rdd和Action方法封装而成，SparkContext将Job交给DAGScheduler提交，它会根据rdd的血缘关系的DAG图（你可以理解为预先规划的计算流程）进行切分，将一个Job划分为若干Stages，具体划分策略是，由处于末端的RDD不断通过依赖回溯判断父依赖是否为宽依赖，即以Shuffle为界，划分Stage。<br>划分的Stages分两类，一类叫做ResultStage，由Action方法决定，是DAG计算流程图最下游的Stage，这个Stage会最先被划分出来（因为是从末端回溯，因此首先遇到宽依赖reduceByKey，因此ResultStage最先被划出）。另一类叫做ShuffleMapStage，为下游ResultStage准备数据。<br>以下面的DAG流程作为说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;result&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20191221172609660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;Job由Action算子saveAsFile触发，该Job由rdd3和saveAsTextFile方法组成，根据rdd之间的依赖关系：</p>
<ul>
<li>首先从rdd3开始回溯搜索，在回溯搜索过程中，rdd3依赖rdd2，遇到reduceByKey需要宽依赖，所以在rdd3和rdd2之间划分Stage，该Stage为ResultStage</li>
<li>继续回溯，rdd2依赖rdd1，遇到Map窄依赖，不划分stage</li>
<li>rdd1依赖rdd0，遇到flapMap窄依赖，不划分Stage</li>
<li>最终回溯到源头rdd0，rdd0无父依赖，因此rdd2、rdd1和rdd0都划分到同一个Stage，即ShuffleMapStage。</li>
</ul>
<p>小结：一个Spark应用程序包括Job、Stage以及Task三个概念：</p>
<ul>
<li>Job是以Action方法(算子)为界，遇到一个Action方法则触发一个Job</li>
<li>Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次Stage划分</li>
<li>Task是Stage的子集，以并行度(分区数)来衡量，一个Stage有多少个partition就有多个task</li>
</ul>
<h5 id="6-2-Spark-DAG的可视化"><a href="#6-2-Spark-DAG的可视化" class="headerlink" title="6.2 Spark DAG的可视化"></a>6.2 Spark DAG的可视化</h5><p>&#8195;&#8195;为了更直观理解6.1 stage的划分，以及DAG可视化，这里用另外一个名为word-count的application语句跑一个测试，并在spark web UI查看相关划分情况以及执行计划：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br></pre></td></tr></table></figure>
<p>从提交语句的逻辑即可直接看出分为两个Stage。在web端可直观看到Application有2个executor分别位于两个不同spark节点上。<br>==对于Stage 0：==<br>Stage 0 的Taskset有3个task，其中1个task在节点5上的executor0进程跑，另外2个task在节点6的executor1进程上跑。executor进程会使用多线程方式运行自己管辖的tasks<br><img src="https://img-blog.csdnimg.cn/20191222095742807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">==对于Stage 1：==<br>Stage 1 的Taskset有3个task，其中1个task在节点5上的executor0进程跑，另外2个task在节点6的executor1进程上跑。executor进程会使用多线程方式运行自己管辖的tasks<br><img src="https://img-blog.csdnimg.cn/20191222101000161.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这从两张图也可以看到Application的运行机制：引用官网文档<a href="http://spark.apache.org/docs/latest/job-scheduling.html">Scheduling Across Applications</a>的一句话：</p>
<blockquote>
<p>When running on a cluster, each Spark application gets an independent set of executor JVMs that only run tasks and store data for that application<br>当提交的application是在集群上跑时，每个application包含多个executor JVMs运行进程，这些executors（JVM进程）只负责为当前的application运行它的tasks任务和存储数据</p>
</blockquote>
<h4 id="7、Spark调度过程"><a href="#7、Spark调度过程" class="headerlink" title="7、Spark调度过程"></a>7、Spark调度过程</h4><p>&#8195;&#8195;有了前面内容铺垫后，本章节内容才能比较好理解，本章节内容参考<a href="http://sharkdtu.com/posts/spark-scheduler.html">《Spark Scheduler内部原理剖析》</a>其中的”Spark任务调度总览“，这篇文章质量不错。</p>
<h5 id="7-1-Spark的两级调度模型"><a href="#7-1-Spark的两级调度模型" class="headerlink" title="7.1 Spark的两级调度模型"></a>7.1 Spark的两级调度模型</h5><p>&#8195;&#8195;Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度。Stage级别的调度前面第6章节已经给出详细说明，至于Task级的调度相对复杂，原文作者给出了非常专业的、从源代码执行流程的说明，但这里不再重复累赘，毕竟重心还是以大数据项目在应用层的开发为主。<br>总体调度流程如下图所示。<br><img src="https://img-blog.csdnimg.cn/20191222111758872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>1)Spark RDD通过其Transactions和Action，形成了RDD的计划执行流程图，即DAG，最后通过Action的调用，触发Job并调度执行。</li>
<li>2)DAGScheduler负责Stage级的调度，主要是将DAG按照RDD的宽依赖切分成若干Stages，并将每个Stage里面的多个task打包成一个TaskSet。多个Stage就有多个TaskSets，这些TaskSets由DAGScheduler交给TaskScheduler调度。</li>
<li>3)TaskScheduler负责Task级的调度，TaskSets按照指定的调度策略分发到不同的Executor上执行。</li>
<li>4)调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多个实现API，分别对接不同的资源管理器YARN/Mesos/Standalone。你可以看成SchedulerBackend就是资源的代理，这个代理不断询问TaskScheduler是否需要资源去运行task。</li>
</ul>
<p>==这里会有一个疑问：每个Stage里面的task的数量怎么确定？==<br>每个Stage里面的task的数量是由该Stage中最后一个RDD的Partition数量所决定！</p>
<h5 id="7-2-以Spark-On-Yarn说明调度过程"><a href="#7-2-以Spark-On-Yarn说明调度过程" class="headerlink" title="7.2 以Spark On Yarn说明调度过程"></a>7.2 以Spark On Yarn说明调度过程</h5><p>&#8195;&#8195;Spark-On-Yarn模式下在任务调度期间，ApplicationMaster、Driver、DAGScheduler、TaskScheduler、Executor等内部模块的交互过程，以进一步巩固理解7.1章节的内容。<br>（从下图中有无发现一个有趣的现象：大数据多个组件理解难，其实不是那种像操作系统底层原理的难，而是在于：大数据的每个组件内部有很多个角色模块，每个角色负责的”工作“不一样，整个hadoop生态圈，每个组件和组件之间联系，这就会涉及到几十个实现模块，你要记忆和理解这几十个不同名字模块及其具体能做什么，以及他们之间的逻辑联系关系。<br>举个栗子：</p>
<ul>
<li>如果你是部门经理，部门只有9个人，3个岗位，如果现在接到一个开发项目，你当然很清楚和也容易安排每个岗位的人负责项目哪部分开发工作，并指定谁跟谁如何协调某部分内容。</li>
<li>如果你是市长，假设现在你接到一个上级大型项目，需要你亲自统筹和设计出如何让20个局完成该大型项目的方案。首先需要设计出10个局单位之间怎么配合，10个局单位共有100个岗位，每个局的每个岗位具体负责哪部分工作，并且你要设计出项目某部分内容是由哪个岗位和哪个岗位直接如何调度完成，够崩溃的。这就是大数据生态圈，概念多、杂，概念之间有一定逻辑关系，这些都需要你去理解和记忆，这就是难点所在。<br>）<br><img src="https://img-blog.csdnimg.cn/2019122211433139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li> Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver（这四部分是程序里面类或者模块，不是线程），并启动SchedulerBackend线程以及HeartbeatReceiver线程。</li>
<li>当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。</li>
<li>SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。</li>
<li>DAGScheduler负责的Stage调度</li>
<li>TaskScheduler负责的Task调度。</li>
<li>work node上Executor进程负责运行Task和把数据存在内存或者磁盘上</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本章内容对于博客接下来有关Spark以及Spark Streaming项目的文章理解起着关键的基础作用，下一篇文章，重点细讲RDDs。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>深入解析Python元类作用</title>
    <url>/blog/2019/12/18/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Python%E5%85%83%E7%B1%BB%E4%BD%9C%E7%94%A8/</url>
    <content><![CDATA[<p>&#8195;&#8195;python的元类使用场景一般在大型框架里面，例如Django的ORM框架、基于python实现的高级设计模式，元类的这部分内容相对晦涩，但也是作为python非常核心的知识点，通过解析其机制，有利于阅读和学习优秀中间件源代码的设计逻辑，在面向对象设计的重要性不言而喻。本博客后面的内容将会给出较为复杂的设计模式的文章，里面会出现较多的元类编程，因此有必要单独开一篇文章讨论python元类，相关内容将参考Stack Overflow上一篇很受欢迎的关于python metaclasses的文章：<a href="https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python">what-are-metaclasses-in-python</a></p>
<a id="more"></a>

<h4 id="1、python的class对象"><a href="#1、python的class对象" class="headerlink" title="1、python的class对象"></a>1、python的class对象</h4><h5 id="1-1-python的class也是一种object"><a href="#1-1-python的class也是一种object" class="headerlink" title="1.1 python的class也是一种object"></a>1.1 python的class也是一种object</h5><p>”在python的世界里，一切皆对象（object）“，这句话经常出现在很多python书籍中有关”面向对象或者类“文章里。如果你要深入python，首先面向对象的思维和面向对象的编程经历较为丰富。掌握对类的理解和运用，是理解元类的重要基础。<br>1.1 类即对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [1]: class Foo(object):</span><br><span class="line">   ...:     pass</span><br><span class="line">   ...:</span><br><span class="line">In [2]: my_instance&#x3D;Foo()</span><br><span class="line">In [3]: print(my_instance)</span><br><span class="line">&lt;__main__.Foo object at 0x108561b00&gt;</span><br></pre></td></tr></table></figure>
<p>这里创建了一个名为Foo的类，打印它的实例，可以看到该实例是一个Foo object 存放在内存地址：0x108561b00<br>这里只是说明Foo类的实例是object，怎么确认Foo是一个object呢？<br>两种方式可以回答：<br>方式一：在定义阶段：Foo(object)，Foo这个类继承object，所以Foo是object<br>方式二：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In [8]: isinstance(Foo,object)</span><br><span class="line">Out[8]: True</span><br></pre></td></tr></table></figure>
<p>既然Foo是一个object，那么对于该object则可以扩展其功能：</p>
<ul>
<li><p>可赋值给变量</p>
</li>
<li><p>可被复制</p>
<ul>
<li>可添加属性</li>
<li>可将其当作函数参数传递</li>
<li>当然也可绑定新的类或者对象</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">13</span>]: NewFoo=Foo</span><br><span class="line">In [<span class="number">14</span>]: print(NewFoo)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Foo</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">In</span> [16]:</span> CloneFoo=copy.deepcopy(Foo)</span><br><span class="line">In [<span class="number">17</span>]: print(CloneFoo)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Foo</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">In</span> [20]:</span> Foo.new_attr=<span class="string">&#x27;bar&#x27;</span></span><br><span class="line">In [<span class="number">21</span>]: Foo.new_attr</span><br><span class="line">Out[<span class="number">21</span>]: <span class="string">&#x27;bar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [22]: def myfunc(obj):</span></span><br><span class="line"><span class="string">    ...:     print(obj.__name__)</span></span><br><span class="line"><span class="string">    ...:</span></span><br><span class="line"><span class="string">In [23]: myfunc(Foo)</span></span><br><span class="line"><span class="string">Foo</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [24]: class  NewFoo(object):</span></span><br><span class="line"><span class="string">    ...:     pass</span></span><br><span class="line"><span class="string">    ...:</span></span><br><span class="line"><span class="string">In [25]: Foo.x=NewFoo</span></span><br><span class="line"><span class="string">In [26]: Foo.x</span></span><br><span class="line"><span class="string">Out[26]: __main__.NewFoo</span></span><br></pre></td></tr></table></figure>
<p>总之，只要拿到一个object，你可以对其扩展任意你想得到效果</p>
</li>
</ul>
<h5 id="1-2-动态创建类"><a href="#1-2-动态创建类" class="headerlink" title="1.2 动态创建类"></a>1.2 动态创建类</h5><p>什么是动态创建类？只有运行这个程序后，通过判断给定参数来决定创建的是类A还是类B，而不是给程序写为固定生产类A。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">28</span>]: <span class="function"><span class="keyword">def</span> <span class="title">choose_class</span>(<span class="params">which</span>):</span></span><br><span class="line">    ...:     <span class="keyword">if</span> which ==<span class="string">&#x27;Foo&#x27;</span>:</span><br><span class="line">    ...:         <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:             <span class="keyword">pass</span></span><br><span class="line">    ...:         <span class="keyword">return</span> Foo</span><br><span class="line">    ...:     <span class="keyword">elif</span> which == <span class="string">&#x27;Bar&#x27;</span>:</span><br><span class="line">    ...:         <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:             <span class="keyword">pass</span></span><br><span class="line">    ...:         <span class="keyword">return</span> Bar</span><br><span class="line">    ...:</span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: myclass=choose_class(<span class="string">&#x27;Bar&#x27;</span>)</span><br><span class="line">In [<span class="number">32</span>]: print(myclass.__name__)</span><br><span class="line">Bar</span><br></pre></td></tr></table></figure>
<p>前面提到，既然Foo创建一个实例就是一个对象，把这个逻辑放在Foo身上：既然（某某某）创建一个对象就是一个Foo类，这个某某某是什么？可以做什么？<br>这个某某某就是type这个内建函数（函数也是一个对象），用type也可以像上面一样动态的创建一个类，用法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">type(要创建的类名，该类的所有父类名字组成的元组（若无父类，则为空元组），要创建该类需要用到入参：属性的字典)</span><br><span class="line">一般写成：</span><br><span class="line">type(class_name,class_bases,class_dict)</span><br></pre></td></tr></table></figure>
<p>经典方式一般如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    car=<span class="string">&#x27;Model 3&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=age</span><br><span class="line">        self.age=age</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;my name is &#123;&#125; and &#123;&#125; years old&#x27;</span>.<span class="built_in">format</span>(self.name,self.age))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用type函数动态创建以上Person类的过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 因为Person继承object，所以type的第二个位置参数为(object,)，Person类有三个属性因此class_dict为&#123;&#x27;car&#x27;:car,&#x27;__init__&#x27;:__init__,&#x27;info&#x27;:info&#125;)</span></span><br><span class="line">car = <span class="string">&#x27;Model 3&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">    self.name = name</span><br><span class="line">    self.age = age</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self</span>):</span></span><br><span class="line">    print(self.name,self.age)</span><br><span class="line"></span><br><span class="line">Person = <span class="built_in">type</span>(<span class="string">&#x27;Person&#x27;</span>,(<span class="built_in">object</span>,),&#123;<span class="string">&#x27;car&#x27;</span>:car,<span class="string">&#x27;__init__&#x27;</span>:__init__,<span class="string">&#x27;info&#x27;</span>:info&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: Person.__dict__</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">mappingproxy(&#123;<span class="string">&#x27;car&#x27;</span>: <span class="string">&#x27;Model 3&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__init__&#x27;</span>: &lt;function __main__.__init__(self, name, age)&gt;,</span><br><span class="line">              <span class="string">&#x27;info&#x27;</span>: &lt;function __main__.info(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;__module__&#x27;</span>: <span class="string">&#x27;__main__&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__dict__&#x27;</span>: &lt;attribute <span class="string">&#x27;__dict__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__weakref__&#x27;</span>: &lt;attribute <span class="string">&#x27;__weakref__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__doc__&#x27;</span>: <span class="literal">None</span>&#125;)</span><br><span class="line">In [<span class="number">5</span>]: person = Person(<span class="string">&#x27;Watt&#x27;</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: person</span><br><span class="line">Out[<span class="number">6</span>]: &lt;__main__.Person at <span class="number">0x10896f9e8</span>&gt;</span><br></pre></td></tr></table></figure>
<p>type创建完整Person类！本章内容主要通过类的创建，因此type这个函数并用其实现动态创建类，为元类这个话题做了铺垫，通过以上type创建的实例推出，python创建类必须要具备以下三个参数：</p>
<ul>
<li>1、类名class_name</li>
<li>2、继承关系class_bases</li>
<li>3、类的名称空间class_dict<br>这三个参数是揭开元类是如何改变类的秘密。</li>
</ul>
<h4 id="2-、Python的metaclass元类"><a href="#2-、Python的metaclass元类" class="headerlink" title="2 、Python的metaclass元类"></a>2 、Python的metaclass元类</h4><h5 id="2-1-认识type"><a href="#2-1-认识type" class="headerlink" title="2.1 认识type"></a>2.1 认识type</h5><p>前面的内容已经说明Python中的类也是对象，那么metaclass元类（元类自己也是对象）就是用来创建这些类的类，例如可以这样理解：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MyClass &#x3D; MetaClass()    #元类创建了类</span><br><span class="line">MyObject &#x3D; MyClass()     #被元类创建的类后，用它创建了实例</span><br></pre></td></tr></table></figure>
<p>在上一节内容，type可创建MyClass类：<br><code>MyClass = type(&#39;MyClass&#39;, (), &#123;&#125;)</code><br>MyClass是type()这个特殊类的一个实例，只不过这个实例直接就是类。<br>以上的逻辑主要说明一件事：type这个特殊类，就是python的一个元类，type是Python在背后用来创建所有类的元类，这句话如何理解？<br>首先，还是那句熟悉的话：在python的世界里，一切皆对象（object），包括各类数据结构、函数、类以及元类，它们都来源于一个“创物者”，这个强大的创物者这就是type元类。<br>查看每种对象的<code>__class__</code>属性：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">55</span>]: num=<span class="number">10</span></span><br><span class="line">In [<span class="number">56</span>]: num.__class__</span><br><span class="line">Out[<span class="number">56</span>]: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: alist=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">In [<span class="number">59</span>]: alist.__class__</span><br><span class="line">Out[<span class="number">59</span>]: <span class="built_in">list</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">60</span>]: <span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    ...:     <span class="keyword">pass</span></span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: foo.__class__</span><br><span class="line">Out[<span class="number">61</span>]: function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:     <span class="keyword">pass</span></span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: b=Bar()</span><br><span class="line">In [<span class="number">65</span>]: b.__class__</span><br><span class="line">Out[<span class="number">65</span>]: __main__.Bar</span><br></pre></td></tr></table></figure>
<p>说明每个对象都是某种类，<br>那么，一个<code>__class__</code>.<code>__class__</code>又是属于哪种类呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">69</span>]: num.__class__.__class__</span><br><span class="line">Out[<span class="number">69</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">70</span>]: foo.__class__.__class__</span><br><span class="line">Out[<span class="number">70</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: alist.__class__.__class__</span><br><span class="line">Out[<span class="number">71</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">72</span>]: b.__class__.__class__</span><br><span class="line">Out[<span class="number">72</span>]: <span class="built_in">type</span></span><br></pre></td></tr></table></figure>
<p>在继续往“创物者”方向靠近，发现最后都是type：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">74</span>]: foo.__class__.__class__.__class__</span><br><span class="line">Out[<span class="number">74</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: foo.__class__.__class__.__class__.__class__</span><br><span class="line">Out[<span class="number">75</span>]: <span class="built_in">type</span></span><br></pre></td></tr></table></figure>
<p>以上说明：python的各类数据结构、函数、类以及元类，它们都来源于一个“创物者”，这个强大的创物者这就是type元类。</p>
<h5 id="2-2-认识-metaclass-属性"><a href="#2-2-认识-metaclass-属性" class="headerlink" title="2.2 认识__metaclass__属性"></a>2.2 认识<code>__metaclass__属性</code></h5><p>在python的元类的设计中，通常会出现<code>__metaclass__</code>属性，一般用法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span>   <span class="comment">#python2版本的写法</span></span><br><span class="line">    __metaclass__ = something…</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params">metaclass=something</span>):</span>   <span class="comment">#python3版本的写法</span></span><br><span class="line">    __metaclass__ = something…</span><br></pre></td></tr></table></figure>
<p>当一个类的内部属性定义了<code>__metaclass__</code>属性，说明这个类将由某个元类来创建，当Foo类一旦被调用，因为设计类时可能有继承关系，因此会出现属性搜索过程：<br>1）Foo的定义里面有<code>__metaclass__</code>这个属性？如果有，解释器在内存中通过<code>something...</code>这个元类创建一个名字为Foo的类（对象）<br>2）如果在Foo的作用域内未找到<code>__metaclass__</code>属性，则继续在父类中寻找，若在父类找到，则用<code>something...</code>这个元类创建一个名字为Foo的类（对象）。<br>3）如果任何父类中都找不到<code>__metaclass__</code>属性，它就会在模块层次中去寻找<code>__metaclass__</code>，若找到，则用<code>something...</code>这个元类创建一个名字为Foo的类（对象）。<br>4）如果还是找不到<code>__metaclass__</code>,解释器最终使用内置的type来创建这个Foo类对象。</p>
<p>从上面过程可知，既然找到<code>something...</code>这个元类后它就可以创建类，说明它与type这个终极元类作用一样：都是用来创建类。<br>所以可推出：<code>__metaclass__</code>指向某个跟type功能相仿的元类———任何封装type的元类、继承type的子类、type本身</p>
<p>下面用元类实现的redis连接单例来感受下以上的逻辑：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisMetaSingleton</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;元类做初始化&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#super(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict) python2写法</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(class_name, class_bases, class_dict) <span class="comment"># python3写法</span></span><br><span class="line">        cls._instance =<span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">cls,host,port,db</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;call调用即完成类的实例化，用类的入参创建redis连接实例&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> cls._instance:</span><br><span class="line">            <span class="comment"># cls._instance = super(RedisMetaSingleton, cls).__call__(host,port,db) python2写法</span></span><br><span class="line">            cls._instance = <span class="built_in">super</span>().__call__(host,port,db)<span class="comment"># python3写法</span></span><br><span class="line">        <span class="keyword">return</span> cls._instance</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisSingleton</span>(<span class="params">metaclass=RedisMetaSingleton</span>):</span></span><br><span class="line">    <span class="string">&quot;redis操作专用类&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span>(<span class="params">self,host,port,db</span>):</span></span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.db=db</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conn</span>(<span class="params">self</span>):</span></span><br><span class="line">    	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>测试其实例是否为单例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">37</span>]: r1=RedisSingleton(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="string">&#x27;6379&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">In [<span class="number">38</span>]: r1</span><br><span class="line">Out[<span class="number">38</span>]: &lt;__main__.RedisSingleton at <span class="number">0x10fdc6080</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: r2=RedisSingleton(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="string">&#x27;6379&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">In [<span class="number">40</span>]: r1 <span class="keyword">is</span> r2</span><br><span class="line">Out[<span class="number">40</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>将单例逻辑放在定义元类这里，其他redis常用方法则放在子类实现。此外，该测试用例需要注意的两点：<br>1）RedisMetaSingleton的<code>__init__</code>和<code>__call__</code>第一个参数为cls，表示元类要创建的”类对象“，因此用cls而不是self。元类至于类对象（mataclass==&gt;class object），就像类至于实例(class==&gt;instance)，反复理解该句。<br>2）<code>__init__(cls,class_name,class_bases,class_dict)</code>，第2个到4个参数，其实就是type元类创建类的所需参数：<br>type（类名，父类元组（若无父类，则为空元组），类属性或内部方法的字典）<br>3）由于RedisMetaSingleton继承type，那么super(RedisMetaSingleton, cls)经过搜索后，父类就是type，因此<br>A: <code>super(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict)</code>的初始化就等价于<br><code>type.__init__(class_name, class_bases, class_dict)</code>的初始化<br>B:<code>super(RedisMetaSingleton, cls).__call__(host,port,db)</code>创建类对象就等价于<code>type.__call__(host,port,db)创建类对象</code><br>这就说明RedisSingleton指定由RedisMetaSingleton来创建，在RedisMetaSingleton内部最后交由<code>type.__init__</code>初始化，证明过程如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">119</span>]: <span class="class"><span class="keyword">class</span> <span class="title">RedisMetaSingleton</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">     ...:     <span class="string">&quot;&quot;&quot;在元类层面实现单例&quot;&quot;&quot;</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">     ...:         <span class="built_in">super</span>(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict)</span><br><span class="line">     ...:         print(<span class="string">&#x27;class_name:&#123;&#125; class_bases:&#123;&#125; class_dict:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(class_name,class_bases,class_dict))</span><br><span class="line">     ...:         cls.cls_object =<span class="literal">None</span></span><br><span class="line">     ...:</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">cls,host,port,db</span>):</span></span><br><span class="line">     ...:         <span class="keyword">if</span> <span class="keyword">not</span> cls.cls_object:</span><br><span class="line">     ...:             cls.cls_object = <span class="built_in">super</span>(RedisMetaSingleton, cls).__call__(host,port,db)</span><br><span class="line">     ...:         <span class="keyword">return</span> cls.cls_object</span><br><span class="line">     ...:</span><br><span class="line">     ...: <span class="class"><span class="keyword">class</span> <span class="title">RedisSingleton</span>(<span class="params">metaclass=RedisMetaSingleton</span>):</span></span><br><span class="line">     ...:     <span class="string">&quot;redis操作专用类&quot;</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span>(<span class="params">self,host,port,db</span>):</span></span><br><span class="line">     ...:         self.host=host</span><br><span class="line">     ...:         self.port=port</span><br><span class="line">     ...:         self.db=db</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">conn</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         <span class="keyword">pass</span></span><br><span class="line">     ...:</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>当以上代码在ipython解释器敲下去后，解释器对RedisMetaSingleton做了<code>__init__</code>初始化工作，故可得到以下打印信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class_name:RedisSingleton  # 类名</span><br><span class="line">class_bases:() # 父类元组</span><br><span class="line">class_dict:&#123;&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;RedisSingleton&#39;, &#39;__doc__&#39;: &#39;redis操作专用类&#39;, &#39;__init__&#39;: &lt;function RedisSingleton.__init__ at 0x10ffa0158&gt;, &#39;conn&#39;: &lt;function RedisSingleton.conn at 0x10ffa00d0&gt;&#125; # 要创建类的所有属性字典</span><br></pre></td></tr></table></figure>
<p>这三个参数就是type创建类的所需的参数:<br><code>type（类名，父类元组（若无父类，则为空元组），类属性或内部方法的字典）</code></p>
<p>以上内容略显复杂：归结起来，只要一个普通类指定需要元类创建，那么最终一定是由type这个终极元类来创建。</p>
<h5 id="2-3-自定义元类"><a href="#2-3-自定义元类" class="headerlink" title="2.3 自定义元类"></a>2.3 自定义元类</h5><p>对元类的构建和原理有一定认识后，那么可通过元类定制普通类，真正站在创物者的上帝视野来创建普通类。<br>现在有这样一个需求，要求创建的普通类的属性满足以下条件：<br>对于开头不是<code>__</code>的属性，都要大写，例如get_name(self)，在元类创建该普通类后都会被改为GET_NAME(self)<br>开头为<code>__</code>的属性，大小写保持不变。<br>从type创建普通类的“公式“可知：type（class_name,class_bases,class_dict）,class_dict就是放置了普通类属性或内部方法的字典），故只需要对其修改后，再重新传入type即可实现，需要基于type的<code>__new__</code>方法实现，type当然有<code>__new__</code>方法，因为type是元类，也是类。（元类必然有<code>__new__</code>方法，它创建的普通类例如Person、RedisConn才有这个内建的<code>__new__</code>方法。）<br>具体实现:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="comment"># 在这里，被创建的对象是类，因此第一个参数为cls，而不是类实例化的self，且需重写__new__方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        uppercase_attr = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, val <span class="keyword">in</span> class_dict.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>):</span><br><span class="line">                uppercase_attr[name.upper()] = val</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                uppercase_attr[name] = val</span><br><span class="line">        <span class="comment"># 用uppercase_attr替换了原class_dict，再传入到type，由type创建类，实现了自定义创建类的目标      </span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(class_name,class_bases, uppercase_attr)</span><br></pre></td></tr></table></figure>
<p>考虑到<code>return type(class_name,class_bases, uppercase_attr)</code>的写法不是pythone的OOP的写法（不够高级、抽象），因此又转化为以下OOP写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="comment"># 在这里，被创建的对象是类，因此第一个参数为cls，而不是实例self，且需重写__new__方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        attrs = ((name, value) <span class="keyword">for</span> name, value <span class="keyword">in</span> class_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>))</span><br><span class="line">        uppercase_attr  = <span class="built_in">dict</span>((name.upper(), value) <span class="keyword">for</span> name, value <span class="keyword">in</span> attrs)</span><br><span class="line">        <span class="comment"># 用uppercase_attr替换了原class_dict，再传入type，由type创建类，实现了自定义创建类的目标      </span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases, uppercase_attr)</span><br></pre></td></tr></table></figure>
<p>以上OOP风格在知名的python框架中到处可见！此外，我们知道通过super(UpperAttrMetaclass,cls)可以搜索到父类type，因此开发者会习惯写成以下形式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        attrs = ((name, value) <span class="keyword">for</span> name, value <span class="keyword">in</span> class_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>))</span><br><span class="line">        uppercase_attr  = <span class="built_in">dict</span>((name.upper(), value) <span class="keyword">for</span> name, value <span class="keyword">in</span> attrs)  </span><br><span class="line">        <span class="comment"># return super(UpperAttrMetaclass,cls).__new__(cls,class_name,class_bases, uppercase_attr)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().__new__(cls,class_name,class_bases, uppercase_attr) <span class="comment"># python3的写法</span></span><br></pre></td></tr></table></figure>
<p>定义一个普通类测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">124</span>]: <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">metaclass=UpperAttrMetaclass</span>):</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">     ...:         self.name=name</span><br><span class="line">     ...:         self.age=age</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">get_name</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         print(<span class="string">&#x27;name is:&#x27;</span>,self.name)</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         print(<span class="string">&#x27;age is:&#x27;</span>,self.age)</span><br><span class="line">     ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">125</span>]: Person.__dict__</span><br><span class="line">Out[<span class="number">125</span>]:</span><br><span class="line">mappingproxy(&#123;<span class="string">&#x27;GET_NAME&#x27;</span>: &lt;function __main__.Person.get_name(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;GET_AGE&#x27;</span>: &lt;function __main__.Person.get_age(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;__module__&#x27;</span>: <span class="string">&#x27;__main__&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__dict__&#x27;</span>: &lt;attribute <span class="string">&#x27;__dict__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__weakref__&#x27;</span>: &lt;attribute <span class="string">&#x27;__weakref__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__doc__&#x27;</span>: <span class="literal">None</span>&#125;)</span><br><span class="line">	</span><br></pre></td></tr></table></figure>
<p>当普通类Person定义后，解释器已经用元类UpperAttrMetaclass创建了Person普通类，其get_name和get_age方法名都被改为大写：GET_NAME和GET_AGE，其他双下划线的的方法名字保持不变。</p>
<p>此外，metaclass不局限于类的调用，也可以在任何对象内部调用，例如函数内部调用，例如以下一个模块upper_attr_by_func.py：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper_attr</span>(<span class="params">class_name,class_bases,class_dict</span>):</span></span><br><span class="line">    uppercase_attr = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name, val <span class="keyword">in</span> class_dict.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>):</span><br><span class="line">            uppercase_attr[name.upper()] = val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            uppercase_attr[name] = val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">type</span>(class_name,class_bases,class_dict)</span><br><span class="line"></span><br><span class="line">__metaclass__ = upper_attr  <span class="comment"># 该元类只能作用在本模块的所有类，对其他模块a.py、b.py无影响。</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">metaclass=UpperAttrMetaclass</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;name is:&#x27;</span>,self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;age is:&#x27;</span>,self.age)</span><br></pre></td></tr></table></figure>
<p>但需要注意的是：这种方式，元类的作用域将受到限制，仅能影响本模块upper_attr_by_func.py的所有类，对其他模块的类不产生作用。</p>
<p>综上，可总结元类定制普通类的创建一般如下过程：</p>
<ul>
<li><p>拦截一个普通类，一般在会使用<code>__new__，__init__ 和 __call__</code>，这些方法的内部可以放入对普通类进行不同定制的代码逻辑，其中：<br>A、<code>__new__</code>和<code>__init__</code>方法用于控制类的行为<br>B、 <code>__call__</code>方法用于控制类实例化的行为（</p>
</li>
<li><p>修改普通类，一般是指修改type(class_name,class_bases,class_dict）里面的三个参数，尤其对class_dict修改频繁，例如要求class_dict里面的必须要有<code>__doc__</code>属性，甚至针对父类元组class_bases来操作其继承关系。</p>
</li>
<li><p>通过<code>return super().__new__(cls,class_name,custom_bases, custom_class_dict)</code>创建并返回普通类</p>
</li>
</ul>
<p>元类一般用于复杂的框架上改变类的行为，对于普通简单的类，还有其他两种手段用来改变类：</p>
<ul>
<li>monkey patching</li>
<li>类装饰器<br>按Stack Overflow上的”建议“：<br>如果需要改变类，99%的情况下使用这两种方法，但其实98%的情况你根本不需要改变类。所以你看到很多较为简单的python轮子，一般是几个普通类就可以完成，根本无需动用元类来构建普通类。</li>
</ul>
<h4 id="3、元类定制普通类的示例——myORM"><a href="#3、元类定制普通类的示例——myORM" class="headerlink" title="3、元类定制普通类的示例——myORM"></a>3、元类定制普通类的示例——myORM</h4><p>本节内容参考了廖雪峰的<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072">文章</a>，但其文章有很多关键的语句并无做更细致的说明，本节内容会在重要的元类实现逻辑上给出更详细的文字说明。<br>这里将实现一个轻量ORM——myORM：<br>在架构层面（不面向用户）</p>
<ul>
<li>架构定义了一个元类ModelMetaClass，用于拦截和修改普通类User定义阶段的class_dict属性字典，并用改造后的class_dict传入type来创建普通类User对象。</li>
<li>架构定义了一个Model类，用于把字段属性名和字段值封装在拼接的SQL语句，主要负责与数据库的增删查改。</li>
<li>架构定义了一个基本字段类Field：包含字段名和字段类型</li>
</ul>
<p>在用户层面（面向用户，用户可自行定义各种模型）</p>
<ul>
<li>用户定义一个整数字段类IntField，用于存放整型类型数据，例如id号，age</li>
<li>用户定义一个字符串字段类CharField，用于存放字符类型数据，例如name，email</li>
<li>创建一个User模型的一条行记录</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelMetaClass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    例如定义User普通类,如下：</span></span><br><span class="line"><span class="string">    class User(Model):</span></span><br><span class="line"><span class="string">        id=IntField(&#x27;user_id&#x27;)</span></span><br><span class="line"><span class="string">        name=CharField(&#x27;user_name&#x27;)</span></span><br><span class="line"><span class="string">        email=CharField(&#x27;email&#x27;)</span></span><br><span class="line"><span class="string">        password=CharField(&#x27;password&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    那么元类ModelMetaClass捕获到的属性字典为： </span></span><br><span class="line"><span class="string">    class_dict=&#123;</span></span><br><span class="line"><span class="string">        &#x27;__module__&#x27;: &#x27;__main__&#x27;, </span></span><br><span class="line"><span class="string">        &#x27;__qualname__&#x27;: &#x27;User&#x27;,</span></span><br><span class="line"><span class="string">        &#x27;id&#x27;:IntField&lt;user_id,bigint&gt;,</span></span><br><span class="line"><span class="string">        &#x27;name&#x27;:CharField&lt;&#x27;user_name&#x27;,varchar(100)&gt;,</span></span><br><span class="line"><span class="string">        &#x27;email&#x27;:CharField&lt;&#x27;email&#x27;,varchar(100)),</span></span><br><span class="line"><span class="string">        &#x27;password&#x27;:CharField&lt;&#x27;password&#x27;,varchar(100)&gt;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        <span class="keyword">if</span> class_name == <span class="string">&quot;Model&quot;</span>:</span><br><span class="line">            <span class="comment"># 如果创建的普通类为Model，不修改该类，直接创建即可</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases,class_dict)</span><br><span class="line">        print(<span class="string">&#x27;在ModelMetaClass元类捕获到普通类的属性字典:\n&#x27;</span>,class_dict)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 用于存放字段类型的属性</span></span><br><span class="line">        fields_dict=<span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> attr_name,attr <span class="keyword">in</span> class_dict.items():</span><br><span class="line">            <span class="comment"># 因为普通类属性字典还有__doc__,__qualname__等属性，因此要过滤出属于Field类型属性</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(attr,Field):</span><br><span class="line">                <span class="comment"># 打印结果：attr_name is &quot;id&quot;,field object is &quot;&lt;class &#x27;__main__.IntField&#x27;&gt;&quot;等字段信息</span></span><br><span class="line">                print(<span class="string">&#x27;attr_name is &quot;&#123;&#125;&quot;,field object is &quot;&#123;&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(attr_name,<span class="built_in">type</span>(attr)))</span><br><span class="line">                fields_dict[attr_name]=attr</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> field_name <span class="keyword">in</span> fields_dict.keys():</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            把Field类型的属性在原属性字典剔除：</span></span><br><span class="line"><span class="string">            u=User(name=&#x27;Wott&#x27;)</span></span><br><span class="line"><span class="string">            print(u.name)# 打印结果Wott</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            若不在原字典删除，那么u.name的值为：CharField&lt;email,varchar(100)&gt;</span></span><br><span class="line"><span class="string">            这个值是元类创建User类的属性值，它会覆盖了值为&#x27;Wott&#x27;的u.name实例属性，显然不符合实际情况。</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            class_dict.pop(field_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在原属性字典里增加一个私有属性（字典类型），这个私有属性保存了要创建字段的信息</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__fields_dict__&#x27;</span>]=fields_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取自定义模型中，指定Meta信息的数据库表名 </span></span><br><span class="line"><span class="string">        class User(Model):</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">            class Meta:</span></span><br><span class="line"><span class="string">                db_table=USER_T</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        attr_meta=class_dict.get(<span class="string">&#x27;Meta&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">        print(<span class="string">&#x27;User模型定义的Meta属性:&#x27;</span>,attr_meta) <span class="comment"># Meta: &lt;class &#x27;__main__.User.Meta&#x27;&gt;</span></span><br><span class="line">        meta_table_name=<span class="built_in">getattr</span>(attr_meta,<span class="string">&#x27;db_table&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">        print(<span class="string">&#x27;User模型在Meta指定的数据库表名为：&#x27;</span>,meta_table_name) <span class="comment"># User模型在Meta指定的数据库表名为：： USER_T</span></span><br><span class="line">        <span class="keyword">if</span> attr_meta <span class="keyword">and</span> meta_table_name:</span><br><span class="line">                table=meta_table_name</span><br><span class="line">                <span class="keyword">del</span> class_dict[<span class="string">&#x27;Meta&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">                table=class_name <span class="comment"># 若User模型没指定Meta中的数据库表名，则默认用User模型类名作为数据库表名</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__table_name__&#x27;</span>]=table </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把原Meta属性变成私有属性，这样创建出来的类更具OOP风格</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__Meta__&#x27;</span>]=attr_meta</span><br><span class="line">        <span class="comment"># 以上完成对普通User模型的属性字典改造后，再重新把它传入到type，从而元类ModelMetaClass完成拦截=&gt;定制=&gt;创建普通类的过程。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases,class_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params"><span class="built_in">dict</span>,metaclass=ModelMetaClass</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    1、Model类继承dict，目的是为了满足ORM中使用字典赋值和取值的方式例如</span></span><br><span class="line"><span class="string">    创建u=User(id=1,name=&#x27;Wott&#x27;,email=&#x27;11@11.com&#x27;,password=&#x27;1213&#x27;)</span></span><br><span class="line"><span class="string">	2、Model内部通过拼接普通类的字段属性信息，封装了原生sql语句，例如save(),filter(),update()等方法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self,attr_name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重写内建getattr方法，可实现类似u.name这种点号获取属性值得方式,用起来更具ORM风格</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self[attr_name]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">raise</span> AttributeError(<span class="string">&quot;Model object has no attribute &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attr_name))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self,col_name,col_value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重写setattr方法，可实现类似u.name=&quot;Foo&quot;这种通过点号设置属性值的方式，用起来更具ORM风格</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self[col_name]=col_value</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 字段名列表</span></span><br><span class="line">        column_name_list=[]</span><br><span class="line">        place_holder_list=[]</span><br><span class="line">        <span class="comment"># 字段值列表</span></span><br><span class="line">        column_value_list=[]</span><br><span class="line">        fields_dict=self.__fields_dict__</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 存放字段的字典，key就是字段名，放在字段名列表，value就是字段值，放在字段值列表，两个列表用于拼接sql语句</span></span><br><span class="line">        <span class="keyword">for</span> attr_name,attr <span class="keyword">in</span> fields_dict.items():</span><br><span class="line">            <span class="comment"># 打印为：attr_name==&gt;id,attr==&gt;&lt;class &#x27;__main__.IntField&#x27;&gt;,attr.col_name==&gt;user_id</span></span><br><span class="line">            column_name_list.append(attr.col_name)</span><br><span class="line">            place_holder_list.append(<span class="string">&#x27;%s&#x27;</span>)</span><br><span class="line">            print(self) <span class="comment"># Model Dict:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125; </span></span><br><span class="line">            column_value_list.append(self[attr_name])</span><br><span class="line">            <span class="comment"># 或者column_value_list.append(getattr(self,attr_name))</span></span><br><span class="line"></span><br><span class="line">            sql = <span class="string">&#x27;insert into %s (%s) values (%s)&#x27;</span> % (self.__table_name__, <span class="string">&#x27;,&#x27;</span>.join(column_name_list), <span class="string">&#x27;,&#x27;</span>.join(place_holder_list))</span><br><span class="line">        print(<span class="string">&#x27;SQL语句:&#x27;</span>,sql)</span><br><span class="line">        print(<span class="string">&#x27;SQL的入参值列表:&#x27;</span>,column_value_list)</span><br><span class="line">            <span class="comment"># 连接mysql数据库后，使用cur.execute(sql,column_value_list)即可存入数据</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Model type:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">super</span>().__str__())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Field</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,col_name,col_type</span>):</span></span><br><span class="line">        self.col_name=col_name <span class="comment"># 字段名</span></span><br><span class="line">        self.col_type=col_type <span class="comment"># 字段类型</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        例如将会打印以下格式：</span></span><br><span class="line"><span class="string">        &#x27;id&#x27;: IntField&lt;user_id,bigint&gt;等</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&#123;&#125;&lt;&#123;&#125;,&#123;&#125;&gt;&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__,self.col_name,self.col_type)</span><br><span class="line">    </span><br><span class="line">    __repr__=__str__</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharField</span>(<span class="params">Field</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义字符型字段,默认可变字符类型长度为100</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, col_name, max_length=<span class="number">100</span></span>):</span></span><br><span class="line">        varchar_type=<span class="string">&quot;varchar(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(max_length)</span><br><span class="line">        <span class="built_in">super</span>().__init__(col_name, varchar_type)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IntField</span>(<span class="params">Field</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义整型字段</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, col_name, col_type=<span class="string">&quot;bigint&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(col_name, col_type)</span><br></pre></td></tr></table></figure>
<p>ModelMetaClass负责顶层设计（改造），用户创建所有的普通类如User、Article、Department等，都会被该元类重设设计（改造它们的class_dict）后再创建出这些普通类。</p>
<p>用户定义了一个User模型，有四个字段，并指定创建为表名为USER_T</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="built_in">id</span>=IntField(<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">    name=CharField(<span class="string">&#x27;user_name&#x27;</span>)</span><br><span class="line">    email=CharField(<span class="string">&#x27;email&#x27;</span>,max_length=<span class="number">200</span>)</span><br><span class="line">    password=CharField(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        自定义数据库表名，这里虽然Meta定义为类，</span></span><br><span class="line"><span class="string">        但在元类ModelMetaClass的视角来看，它是一个属性，放在class_dict里面</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        db_table=<span class="string">&#x27;USER_T&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>当用户定义完以上的普通类User后，Python解释器首先在当前类User的定义中查找metaclass，显然当前上下文环境没有找到，则继续在父类Model中查找metaclass，发现Model定义了metaclass=ModelMetaClass，故直接交由ModelMetaclass来创建该普通的User类。</p>
<p>用户创建了User实例并尝试向db插入该条数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u=User(<span class="built_in">id</span>=<span class="number">1</span>,name=<span class="string">&#x27;Wott&#x27;</span>,email=<span class="string">&#x27;11@11.com&#x27;</span>,password=<span class="string">&#x27;1213&#x27;</span>) <span class="comment"># Model继承dict，因此Model子类User当然可用字典创建方式来创建实例</span></span><br><span class="line">u[<span class="string">&#x27;name&#x27;</span>]=<span class="string">&#x27;Foo&#x27;</span><span class="comment"># Model继承dict，因此Model子类User当然可使用字典方式赋值</span></span><br><span class="line">u.password=<span class="string">&#x27;Pa33Wood&#x27;</span><span class="comment"># Model内部定义__setattr__方法，故可用点号给属性赋值</span></span><br><span class="line">print(u.email) <span class="comment"># Model内部定义__getattr__方法，故可用点号取属性值</span></span><br><span class="line">u.save()</span><br></pre></td></tr></table></figure>

<p>以上代码各个位置上的print输出结果如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">第一句print输出：</span><br><span class="line">在ModelMetaClass元类捕获到普通类的属性字典:</span><br><span class="line"> &#123;&#x27;__module__&#x27;: &#x27;__main__&#x27;, &#x27;__qualname__&#x27;: &#x27;User&#x27;, &#x27;id&#x27;: IntField&lt;user_id,bigint&gt;, &#x27;name&#x27;: CharField&lt;user_name,varchar(100)&gt;, &#x27;email&#x27;: CharField&lt;email,varchar(200)&gt;, &#x27;password&#x27;: CharField&lt;password,varchar(100)&gt;, &#x27;Meta&#x27;: &lt;class &#x27;__main__.User.Meta&#x27;&gt;&#125;</span><br><span class="line"></span><br><span class="line">第二句print输出：</span><br><span class="line">attr_name is &quot;id&quot;,field object is &quot;&lt;class &#x27;__main__.IntField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;name&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;email&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;password&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line"></span><br><span class="line">第三句print输出：</span><br><span class="line">User模型定义的Meta属性: &lt;class &#x27;__main__.User.Meta&#x27;&gt;</span><br><span class="line"></span><br><span class="line">第四句print输出：</span><br><span class="line">User模型在Meta指定的数据库表名为： USER_T</span><br><span class="line"></span><br><span class="line">第五句print输出：</span><br><span class="line">11@11.com</span><br><span class="line"></span><br><span class="line">第六句print输出：</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line"></span><br><span class="line">第七句print输出：</span><br><span class="line">SQL语句: insert into USER_T (user_id,user_name,email,password) values (%s,%s,%s,%s)</span><br><span class="line"></span><br><span class="line">第八句print输出：</span><br><span class="line">SQL的入参值列表: [1, &#x27;Foo&#x27;, &#x27;11@11.com&#x27;, &#x27;Pa33Wood&#x27;]</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>python元类</tag>
      </tags>
  </entry>
  <entry>
    <title>理解HDFS文件系统架构和原理</title>
    <url>/blog/2019/10/15/%E7%90%86%E8%A7%A3HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%92%8C%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h4 id="1、Hadoop是一种具体的技术吗？"><a href="#1、Hadoop是一种具体的技术吗？" class="headerlink" title="1、Hadoop是一种具体的技术吗？"></a>1、Hadoop是一种具体的技术吗？</h4><p>&#8195;&#8195;准确的说，Hadoop是一套大数据的解决方案或者技术栈，不仅仅特指某种大数据技术，由Apache基金会上多个与大数据有关的明星组件构成，包括HDFS（分布式文件系统），YARN（分布式资源调度系统），MapReduce（分布式计算系统）、Spark、Hive、Hbase、Mahout、Zookeeper、Flume等，如下图所示。<img src="https://img-blog.csdnimg.cn/20191014221453428.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>

<p>&#8195;&#8195;本文将重点讨论HDFS、YARN以及MapReduce，为何？<br>设想这么一个场景：<br>1）、如果T企业每天产生10T的数据，一年为3650T，显然单台服务器是无法存储的，必须分别存放在多台服务器上。HDFS就是负责存放这些超大数据文件的核心组件。<br>2）、对于初始数据价值密度低的数据文件，T企业需要对其进行数据挖掘，提取高价值密度数据，用以协助调整自身的商业策略，那么这环节会涉及到从多台服务器上读数据、并执行相应的计算、统计逻辑任务（业务代码），这里就涉及如何从多台服务器上的数据计算出一份有价值数据出来呢？这是MapReduce要负责的活。<br>3）、在MapReduce这一场景中，例如T企业同时运行多个主题的计算任务，如果没有对于多台服务器的cpu+内存+网络吞吐资源进行统一分配、有效管理、回收，那么有可能计算非核心任务a占据集群资源的80%，而核心任务确只能分配到20%计算资源，显然无法达到计算资源最优化，这就是YARN要做的事情。<br>当然三个组件要负责的具体工作远不止以上的描述，博客将分别发布三篇文章讨论对应以上三个组件。<br>此外，从Hadoop的演进历史来看：<br>Hadoop1.0版本为两个核心（分布式存储+计算）：HDFS+MapReduce<br>Hadoop2.0版本，引入了Yarn：HDFS+Yarn+Mapreduce<br>Yarn是资源调度框架。能够细粒度的管理和调度任务。此外，还能够支持其他的计算框架，比如Spark等。</p>
<h4 id="2、HDFS"><a href="#2、HDFS" class="headerlink" title="2、HDFS"></a>2、HDFS</h4><p>&#8195;&#8195;一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。<br>HDFS 采用Master/Slave的架构来存储数据，这里以下图所示作为说明：<br><img src="https://img-blog.csdnimg.cn/20191014231209135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="2-1-Hadoop-block块概念"><a href="#2-1-Hadoop-block块概念" class="headerlink" title="2.1 Hadoop block块概念"></a>2.1 Hadoop block块概念</h5><p>&#8195;&#8195;block是物理的文件，真正存储的位置在磁盘中例如目录：{hadoop.tmp.dir}/data，将客户端上传的完整数据文件切分多个block后存储的块文件。Hadoop1.0是按64MB切，BlockSize=64MB。Hadoop2.0 BlockSize=128MB<br>&#8195;&#8195;文件分块存储，DateNode中存储以数字编号的方块用于备份，每个块都会复制到其他节点上（默认3个节点）,如果一个块不可用，可从其它节点读取副本，副本默认为3份，如果配置文件中副本设置为 4 ，而仅有2台Datanode，最后block副本还是2</p>
<p>&#8195;&#8195;对存储小文件，1000个1M的小文件会占用1024个块和1024个 inode，每个1M文件只是占用1个物理文件块中的1M，不会占用整个128M的完整block，但因为inode存储在NameNode的内存里，如果NameNode内存不足以存储更多的inode，那么磁盘也无法存储更多数据block文件，白白浪费存储资源，因此HDFS并不适合存储小文件，务必考虑将小文件合并为大文件，再扔到HDFS上。</p>
<p>&#8195;&#8195;补充：inode的内容：文件（这里指代Linux一切皆文件的文件）索引数据结构，inode中主要存储以下这些元数据：</p>
<pre><code>- inode编号</code></pre>
<ul>
<li>文件大小</li>
<li>占用的块数目与块大小</li>
<li>文件类型（普通文件、目录、管道，etc.）</li>
<li>存储该文件的设备号</li>
<li>链接数目</li>
<li>读、写、执行权限</li>
<li>拥有者的用户ID和组ID</li>
<li>文件的最近访问、数据最近修改时间<ul>
<li>inode最近修改时间<br>其中，inode编号相当于这个结构中的“主键”，说明Linux使用inode编号唯一标识一个文件，通过stat命令可以查看元数据信息，如下图所示。<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# stat /opt/hadoop-3.1.2/ </span><br><span class="line">  File: ‘/opt/hadoop-3.1.2/’</span><br><span class="line">  Size: 204             Blocks: 0          IO Block: 4096   directory</span><br><span class="line">Device: fd00h/64768d    Inode: 4216030   Links: 13</span><br><span class="line">Access: (0755/drwxr-xr-x)  Uid: ( 1001/ UNKNOWN)   Gid: ( 1002/ UNKNOWN)</span><br><span class="line">Context: unconfined_u:object_r:usr_t:s0</span><br><span class="line">Access: 2019-1**51.243402848 +0800</span><br><span class="line">Modify: 2019-**:40.433267611 +0800</span><br><span class="line">Change: 2019-**:40.433267611 +0800</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure>
显示块信息命令：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 显示hdfs目录&#x2F;app下TJKL.txt数据文件，大小为536M，到底被切分为多少文件块，以及这些文件块的详细情况及其所在位置</span><br><span class="line">[root@nn ~]# hdfs fsck &#x2F;app -files -blocks -locations -racks</span><br><span class="line">&#x2F;app&#x2F;TJKL.txt 562227059 bytes, replicated: replication&#x3D;3, 5 block(s):  OK</span><br><span class="line">0. BP-1004123743-192.188.0.4-15**1724:blk_1073741912_1096 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">1. BP-1004123743-192.188.0.4-15**1724:blk_1073741913_1097 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">2. BP-1004123743-192.188.0.4-15**1724:blk_1073741914_1098 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">3. BP-1004123743-192.188.0.4-15**1724:blk_1073741915_1099 len&#x3D;134217728 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br><span class="line">4. BP-1004123743-192.188.0.4-15**1724:blk_1073741916_1100 len&#x3D;25356147 Live_repl&#x3D;3  [&#x2F;default-rack&#x2F;192.188.0.6:9866, &#x2F;default-rack&#x2F;192.188.0.5:9866, &#x2F;default-rack&#x2F;192.188.0.4:9866]</span><br></pre></td></tr></table></figure>
以上信息说明：TJKL.txt被切分为5个文件块（前4个block都是128M，第5个block为24M），每个文件块都有3个副本，每个副本分别放在同一Rack（机架）上三台服务器上</li>
</ul>
</li>
</ul>
<h5 id="2-2-Client"><a href="#2-2-Client" class="headerlink" title="2.2 Client"></a>2.2 Client</h5><p>&#8195;&#8195;连接到hadoop集群的客户端， 或者说使用API或指令操作的一端都可以看做是客户端。<br>文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储<br>与 NameNode 交互，获取文件的位置信息<br>与 DataNode 交互，读取或者写入数据，例如使用管道把文件块从上游节点写到下游节点<br>Client 提供一些命令来管理 HDFS，比如启动或者关闭HDFS<br>Client 可以通过一些命令来访问 HDFS</p>
<h5 id="2-3-NameNode：Master角色，名字节点"><a href="#2-3-NameNode：Master角色，名字节点" class="headerlink" title="2.3 NameNode：Master角色，名字节点"></a>2.3 NameNode：Master角色，名字节点</h5><p>&#8195;&#8195;管理元数据信息（Metadata），只存储元数据信息，存放在内存中，会通过fsimage和edits文件，将元数据信息持久化到磁盘上<br>管理数据块（Block）映射信息<br>配置副本策略<br>处理客户端读写请求<br>注意：Hadoop1.0版本使用SecondaryNamenode做fsimage和edits文件的合并，但是这种机制达不到热备的效果，即存在单点故障问题，Hadoop2.0解决了该缺点。 </p>
<h5 id="2-4-fsimage、edits"><a href="#2-4-fsimage、edits" class="headerlink" title="2.4 fsimage、edits"></a>2.4 fsimage、edits</h5><p>&#8195;&#8195;fsimage 文件，记录元数据信息的文件edits文件，记录元数据信息改动的文件。只要元数据发生变化，这个edits文件就会有对应记录。<br>fsimage和edits文件会定期做合并，这个周期默认是3600s。fsimage根据edits里改动记录进行元数据更新。<br>元数据信息如果丢失，HDFS就不能正常工作了，因此在生产环境中，元数据是需要做备份的。<br>hadoop集群部署中，命令hadoop namenode -format 执行时，创建了初始的fsimage文件和edits文件，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn current]# pwd</span><br><span class="line">&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;namenode&#x2F;current</span><br><span class="line">......</span><br><span class="line">edits_0000000000000000556-0000000000000000557  fsimage_0000000000000001186</span><br><span class="line">edits_0000000000000000558-0000000000000000655  fsimage_0000000000000001186.md5</span><br></pre></td></tr></table></figure>

<h5 id="2-5-Secondary-Namenode"><a href="#2-5-Secondary-Namenode" class="headerlink" title="2.5 Secondary Namenode"></a>2.5 Secondary Namenode</h5><p>辅助 NameNode，分担其工作量。<br>定期合并 fsimage和fsedits，并推送给NameNode。<br>在紧急情况下，可辅助恢复 NameNode。<br>Hadoop集群最开始启动的时候，创建Fsimage和edits文件，这个namenode负责，此外，namenode会做一次文件合并工作，这么做的目的是确保元数据信息是最新的，之后的合并工作，就交给SN去做了。这种SN机制是Hadoop1.0的机制，该机制达不到元数据的实时更新，若NN宕机，元数据信息可能还会丢失。</p>
<h5 id="2-6-DataNode"><a href="#2-6-DataNode" class="headerlink" title="2.6 DataNode"></a>2.6 DataNode</h5><p>Slave角色，具体干活者，NameNode下发指令操作，DataNode 执行实际的操作<br>存储实际的物理数据块block。<br>执行数据块的读/写操作。<br>为了防止datanode挂掉造成的数据丢失，对于文件块要有备份，一个文件块有三个副本。这里体现出hdfs是一种高容错的文件系统。<br>在服务器上的构成：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 subdir0]# pwd</span><br><span class="line">/opt/hadoop-3.1.2/datanode/current/BP-1004123743-192.188.0.4-***current/finalized/subdir0/subdir0</span><br><span class="line">......</span><br><span class="line">blk_1073741876_1052.meta  blk_1073741901_1078.meta  blk_1073741916_1100.meta</span><br><span class="line">blk_1073741877            blk_1073741902</span><br><span class="line">blk_1073741877_1053.meta  blk_1073741902_1079.meta</span><br></pre></td></tr></table></figure>


<h5 id="2-7-HDFS写文件流程图"><a href="#2-7-HDFS写文件流程图" class="headerlink" title="2.7 HDFS写文件流程图"></a>2.7 HDFS写文件流程图</h5><p><img src="https://img-blog.csdnimg.cn/20191015000223140.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>1）、发起一个写数据请求，并指定上传文件的路径，然后去找NN。NN首先会判断路径合法性以及客户端是否有写权限，若符合，则NN会给客户端返回一个输出流。此外，NN会为文件分配块存储信息（理解为记录block文件位置大小等信息的索引文件）。注意NN也是分配块的存储信息，但不做物理切块工作。</p>
<p>2）、客户端拿到输出流以及块存储信息之后，就开始向DN节点写数据。因为一个块数据，有三个副本，所以图里有三个NN节点。<br>3）、数据块的发送（管道发送或者接力棒传递方式），先发给第一台DN节点，数据再从第一台DN发往第二台DN，……，此方式用到了pipeLine 数据流管道的机制，就像redis发送管道命令，然后批量返回结果，从而减少网络来回RTT时间消耗。<br>pipeLine:[bl1,datanode01-datanode02-datanode-03]<br>数据流管道充分利用每台机器的带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时，提高传输效率。<br>packet 默认为64kb大小的数据包</p>
<p>4）、通过ack确认机制，向上游节点发送确认，例如DN3向DN2ack说我已写好数据，DN2继续向DN1说我已写好数据，DN1再跟客户端说:数据块已经在DN1、DN2、DN3都存储好，这么做的目的是确保块数据复制的完整性。</p>
<p> 5）、通过最上游节点DN1，向客户端发送ack，如果块数据没有发送完，客户端会就继续发送下一块，直到所有块数据都已发完，关闭文件关流。</p>
<p>6）、所有块数据都写完后，关闭文件流。</p>
<h5 id="2-8-hadoop读取文件流程图"><a href="#2-8-hadoop读取文件流程图" class="headerlink" title="2.8 hadoop读取文件流程图"></a>2.8 hadoop读取文件流程图</h5><ul>
<li><img src="https://img-blog.csdnimg.cn/20191015001052278.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<p>  1.客户端发出读数据请求，Open File指定读取的文件路径，在NN节点获取元数据信息。<br>    2.NN将目标文件的元数据信息返回给客户端。<br>    3.客户端根据返回的元数据信息，去对应的DN去读块数据。<br>    4.假如一个文件特别大，比如1TB，会分成好多块，此时，NN并是不一次性把所有的元数据信息返回给客户端，而是分节点返回。<br>    5.客户端读完此部分后，再向NN节点要下一部分的元数据信息，再接着读。<br>    6.读完之后，通知namenode关闭流</p>
<h4 id="3、再讨fsimage和editlog"><a href="#3、再讨fsimage和editlog" class="headerlink" title="3、再讨fsimage和editlog"></a>3、再讨fsimage和editlog</h4><ul>
<li><p>NameNode通过组织两个核心数据结构：“FSImage”和“EditLog”文件，实现==维护文件系统树内所有文件和目录，记录每个文件在哪个DateNode的位置和副本信息，来通知客户端应该去哪个节点访问文件blocks。==</p>
</li>
<li><p>fsImage_*：元数据镜像文件，即系统的目录树，包括文件目录和inodes元信息（文件名，文件大小，创建时间，备份级别，访问权限，block size，所有block的构成)，每个inode是hdfs的一个代表文件或者目录的元数据。这个镜像文件相当于hdfs的元数据额数据库文件。</p>
</li>
<li><p>edits_<em>：编辑日志文件，也就是事务日志文件，也就是针对文件系统做的修改操作记录，记录元数据的变化，相当于操作日志文件。一个文件的创建，追加，移动等。 NameNode内存中存储的是=fsimage+edits 检查点：NameNode启动时，从磁盘中读取上面两种文件，然后把edits_</em>里面记录的事务全部刷新到 fsimage_<em>中，这样就截去了旧的edits_</em>事务日志，这个过程叫checkpoint。</p>
</li>
<li><p>==为何使用一大一小两种数据结构的文件？ 因为fsimage本身是大文件，试想你要向大文件每时每刻open file–&gt;append new line–&gt; close file，也太累了吧。因此可把新来的操作记录放到小的EditLog里， 再设定每隔一段时间，把一个FsImage和一个Editlog 进行合并会得到一个新的FsImage。==</p>
</li>
<li><p>==fsimage中存储的信息就相当于整个hdfs在某一时刻的一个快照==，既然这个快照可以管理整个hdfs集群的文件信息，那么对其备份则非常重要，于是<br>引入一个叫SendaryNamenode的节点用来做备份fsimage，它会定期的和namenode就行通信来完成整个的备份操作，具体工作原理见第4点：</p>
</li>
</ul>
<h4 id="4、SecondaryNameNode合并FSImage"><a href="#4、SecondaryNameNode合并FSImage" class="headerlink" title="4、SecondaryNameNode合并FSImage"></a>4、SecondaryNameNode合并FSImage</h4><p><img src="https://img-blog.csdnimg.cn/20191015220557202.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;为何NN节点不负责合并工作而是由其他节点服务器来运行此任务？<br>因为这两个文件合并过程需要消耗内存、磁盘io以及cpu，因此将这些“蓝领”工作交给其他节点做，否则NN主节点消耗大量资源。<br>具体流程：<br>1）、SN定时和NN通信chectpoint，在什么时候进行checkpoint？由两个参数dfs.namenode.checkpoint.preiod(默认值是3600，即1小时)和dfs.namenode.checkpoint.txns(默认值是1000000)来决定），通过请求NN其停止使用edits文件，暂时将新的操作写到一个新的edits.new文件<br>2）、SN通过HTTP GET方式从NN上获取到fsimge和edits文件，并下载到本地的相应目录下；<br>3）、SN将下载下来的fsimage载入到内存，然后一条一条地执行edits文件中的各项更新操作，使得内存中的fsimge保持最新；这个过程就是edits和fsimage文件合并，同时SN节点也会在磁盘上存放一份fsimage（不就实现了fsimage的备份吗）<br>4）、SN执行完（3）操作之后，会通过HTTP POST方式将新的fsimage文件发送到NN节点上<br>5）、 NN将从SN接收到的新的fsimage替换掉旧fsimage文件，同时将edits.new替换edits文件，通过这个过程edit内容就变小而且都是最新的操作日志。</p>
<h4 id="5、文件blocks副本存放策略"><a href="#5、文件blocks副本存放策略" class="headerlink" title="5、文件blocks副本存放策略"></a>5、文件blocks副本存放策略</h4><p>&#8195;&#8195;NN节点如何选择在哪个datanode 存储block副本？<br>这里需要对可靠性、写入带宽和读取带宽进行权衡。Hadoop对DN存储副本有自己的副本策略，在其发展过程中一共有两个版本的副本策略，分别如下所示：<br><img src="https://img-blog.csdnimg.cn/20191015223215262.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS原理</tag>
      </tags>
  </entry>
  <entry>
    <title>gevent与协程</title>
    <url>/blog/2019/12/28/gevent%E4%B8%8E%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<h4 id="1、-yield-实现协程"><a href="#1、-yield-实现协程" class="headerlink" title="1、 yield 实现协程"></a>1、 yield 实现协程</h4><h5 id="1-1-yield-同步执行"><a href="#1-1-yield-同步执行" class="headerlink" title="1.1 yield 同步执行"></a>1.1 yield 同步执行</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>():</span></span><br><span class="line">    send_msg=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer</span></span><br><span class="line">        output=<span class="keyword">yield</span> send_msg</span><br><span class="line">        print(<span class="string">&#x27;[consumer] consuming &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(output))</span><br><span class="line">        send_msg=<span class="string">&#x27;ok&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">consumer_obj,num</span>):</span></span><br><span class="line">    <span class="comment"># 1、启动consumer()生成器</span></span><br><span class="line">    <span class="built_in">next</span>(consumer_obj)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,num+<span class="number">1</span>):</span><br><span class="line">        print(<span class="string">&#x27;[producer] producing &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、通过send()切换到consumer()执行</span></span><br><span class="line">        receive_msg=consumer_obj.send(i)</span><br><span class="line">        print(<span class="string">&#x27;[producer] received a message &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(receive_msg))</span><br><span class="line">    consumer_obj.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    c=consumer()</span><br><span class="line">    producer(c,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[producer] producing <span class="number">1</span></span><br><span class="line">[consumer] consuming <span class="number">1</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">2</span></span><br><span class="line">[consumer] consuming <span class="number">2</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">3</span></span><br><span class="line">[consumer] consuming <span class="number">3</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">4</span></span><br><span class="line">[consumer] consuming <span class="number">4</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">5</span></span><br><span class="line">[consumer] consuming <span class="number">5</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>整个过程无锁，由一个线程执行，producer和consumer协作完成任务，但是以上无法实现并发，生产1个，消费1个，也即1个生产者对应1个消费者</p>
<h5 id="1-2-启动多个yield模拟consumer并发"><a href="#1-2-启动多个yield模拟consumer并发" class="headerlink" title="1.2 启动多个yield模拟consumer并发"></a>1.2 启动多个yield模拟consumer并发</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>(<span class="params">consumer_index</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;consumer-&#123;&#125; started at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(consumer_index,get_time()))</span><br><span class="line">    send_msg=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 消费者保持监听producer的发来的信息</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer</span></span><br><span class="line">        output=<span class="keyword">yield</span> send_msg</span><br><span class="line">        print(<span class="string">&#x27;[consumer-&#123;&#125;] consuming &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(consumer_index,output,get_time()))</span><br><span class="line">        time.sleep(<span class="number">1</span>) <span class="comment"># 模拟IO耗时操作</span></span><br><span class="line">        send_msg=<span class="string">&#x27;ack&#x27;</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">consumer_obj,consumer_num,count</span>):</span></span><br><span class="line">    <span class="comment"># 1、启动n个consumer()生成器，相当于用协程方式模拟并发</span></span><br><span class="line">    consumers=[consumer_obj(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(consumer_num) ]</span><br><span class="line">    <span class="keyword">for</span> each_cons <span class="keyword">in</span> consumers:</span><br><span class="line">        <span class="built_in">next</span>(each_cons)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count):</span><br><span class="line">        print(<span class="string">&#x27;[producer] producing &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i,get_time()))</span><br><span class="line">        <span class="comment"># 2、对每个consumer_obj使用send()切换到consumer()执行</span></span><br><span class="line">        <span class="keyword">for</span> index,each_cons <span class="keyword">in</span> <span class="built_in">enumerate</span>(consumers):</span><br><span class="line">            receive_msg=each_cons.send(i)</span><br><span class="line">            print(<span class="string">&#x27;[producer] received &#123;&#125; from consumer-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(receive_msg,index,get_time()))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">for</span> each_cons <span class="keyword">in</span> consumers:</span><br><span class="line">        each_cons.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    producer(consumer,<span class="number">5</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>1个producer，5个consumer</p>
<p>输出</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">consumer-0 started at 21:12:45</span><br><span class="line">consumer-1 started at 21:12:45</span><br><span class="line">consumer-2 started at 21:12:45</span><br><span class="line">consumer-3 started at 21:12:45</span><br><span class="line">consumer-4 started at 21:12:45</span><br><span class="line">[producer] producing 0 at 21:12:45</span><br><span class="line">[consumer-0] consuming 0 at 21:12:45</span><br><span class="line">[producer] received ack from consumer-0</span><br><span class="line">[consumer-1] consuming 0 at 21:12:46</span><br><span class="line">[producer] received ack from consumer-1</span><br><span class="line">[consumer-2] consuming 0 at 21:12:47</span><br><span class="line">[producer] received ack from consumer-2</span><br><span class="line">[consumer-3] consuming 0 at 21:12:48</span><br><span class="line">[producer] received ack from consumer-3</span><br><span class="line">[consumer-4] consuming 0 at 21:12:49</span><br><span class="line">[producer] received ack from consumer-4</span><br><span class="line">[producer] producing 1 at 21:12:50</span><br><span class="line">[consumer-0] consuming 1 at 21:12:50</span><br><span class="line">[producer] received ack from consumer-0</span><br><span class="line">[consumer-1] consuming 1 at 21:12:51</span><br><span class="line">[producer] received ack from consumer-1</span><br><span class="line">[consumer-2] consuming 1 at 21:12:52</span><br><span class="line">[producer] received ack from consumer-2</span><br><span class="line">[consumer-3] consuming 1 at 21:12:53</span><br><span class="line">[producer] received ack from consumer-3</span><br><span class="line">[consumer-4] consuming 1 at 21:12:54</span><br><span class="line">[producer] received ack from consumer-4</span><br></pre></td></tr></table></figure>
<p>以上运行过程确实是协程运行，但yield无法自动切换协程，上面的运行过程打印出的实际可以发现代码同步执行：<br>5个consumer同时启动，当producer生产1个数据，consumer-0消费数据，而consumer内部有IO耗时操作（time.sleep(1)模拟IO），此时代码逻辑没有把线程当前控制权从consumer-0自动切换到consumer-1，consumer-1等待前面1秒后，才能接着干活。</p>
<h4 id="2-、greenlet实现的协程"><a href="#2-、greenlet实现的协程" class="headerlink" title="2 、greenlet实现的协程"></a>2 、greenlet实现的协程</h4><h5 id="2-1-简单gevent协程例子"><a href="#2-1-简单gevent协程例子" class="headerlink" title="2.1 简单gevent协程例子"></a>2.1 简单gevent协程例子</h5><p>&#8195;&#8195;greenlet是一个用C实现的协程模块，相比于上面使用python的yield实现协程，greenlet可以无需将函数声明为generator的前提下，用手动方式在任意函数之间切换。但在实际使用，往往不会直接使用greenlet，因为它遇到有IO地方是不会自动切换，而Gevent库可以实现这个需求，gevent是对greenlet的封装，实现自动切换，大体的设计逻辑如下：<br>&#8195;&#8195;当一个greenlet（你可以认为这个greenlet是一个协程对象，类比于线程对象thread）遇到IO操作时，比如访问读取文件或者网络socket连接，它会自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换原来位置继续执行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent,datetime</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f1 started at&#x27;</span>,get_time())</span><br><span class="line">    <span class="comment"># gevent模拟IO耗时操作，并且gevent会在此保留现场后自动切换到其它函数</span></span><br><span class="line">    gevent.sleep(<span class="number">4</span>) </span><br><span class="line">    print(<span class="string">&#x27;f1 done at&#x27;</span>,get_time())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f2 started at&#x27;</span>,get_time())</span><br><span class="line">    gevent.sleep(<span class="number">2</span>) </span><br><span class="line">    print(<span class="string">&#x27;f2 done at&#x27;</span>,get_time())</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f3</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f3 started at&#x27;</span>,get_time())</span><br><span class="line">    gevent.sleep(<span class="number">3</span>) </span><br><span class="line">    print(<span class="string">&#x27;f3 done at&#x27;</span>,get_time())</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gevent.joinall([gevent.spawn(f1),gevent.spawn(f2),gevent.spawn(f3)])</span><br></pre></td></tr></table></figure>

<p>打印结果</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">f1 started at  09:21:17.066251</span><br><span class="line">f2 started at  09:21:17.066355</span><br><span class="line">f3 started at  09:21:17.066388</span><br><span class="line">f2 done at  09:21:19.067741</span><br><span class="line">f3 done at  09:21:20.067747</span><br><span class="line">f1 done at  09:21:21.067812</span><br></pre></td></tr></table></figure>

<p>可以看到，gevent在同一时刻运行3个函数，并且，f2先完成，接着f3完成，最后IO耗时最长的f1完成，三个函数共同完成耗时为4秒，说明三个函数并发执行了。如果是同步运行，整个过程耗时为4+2+3=9秒耗时，协程优势凸显。</p>
<h5 id="2-2-gevent-高并发测试"><a href="#2-2-gevent-高并发测试" class="headerlink" title="2.2 gevent 高并发测试"></a>2.2 gevent 高并发测试</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>(<span class="params">task_index</span>):</span></span><br><span class="line">    gevent.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at &#123;&#125; &#x27;</span>.<span class="built_in">format</span>(task_index,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">syn</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        task(i)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    syn(<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同步情况下，耗时4秒</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-0 done at  09:41:14.075001 </span><br><span class="line">task-1 done at  09:41:15.076049 </span><br><span class="line">task-2 done at  09:41:16.077101 </span><br><span class="line">task-3 done at  09:41:17.078055 </span><br></pre></td></tr></table></figure>
<p>gevent实现的协程异步</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">    coroutines=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    gevent.joinall(coroutines)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    asyn(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>原本需要4秒的执行流，现在只需1秒完成所有任务。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-0 done at  09:44:30.535495 </span><br><span class="line">task-1 done at  09:44:30.535749 </span><br><span class="line">task-2 done at  09:44:30.535801 </span><br><span class="line">task-3 done at  09:44:30.535833 </span><br></pre></td></tr></table></figure>

<p>尝试启动10万个任务，用line_profiler 查看函数中耗时操作（line_profiler 目前不兼容3.7，最好用pyenv 切换到3.6进行测试）。只需要在asyn函数上加@profile装饰器即可</p>
<p>创建asyn.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent,datetime,time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>(<span class="params">task_index</span>):</span></span><br><span class="line">    gevent.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(&#x27;task-&#123;&#125; done at &#123;&#125; &#x27;.format(task_index,datetime.datetime.now()))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@profile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">    threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    gevent.joinall(threads)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start=time.time()</span><br><span class="line">    asyn(<span class="number">100000</span>)</span><br><span class="line">    cost=time.time()-start</span><br><span class="line">    print(cost)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(spk) [root@nn spv]# kernprof -l -v asyn.py </span><br><span class="line">5.805598974227905</span><br><span class="line">Wrote profile results to asyn.py.lprof</span><br><span class="line">Timer unit: 1e-06 s</span><br><span class="line"></span><br><span class="line">Total time: 5.73735 s</span><br><span class="line">File: asyn.py</span><br><span class="line">Function: asyn at line 6</span><br><span class="line"></span><br><span class="line">Line #      Hits         Time  Per Hit   % Time  Line Contents</span><br><span class="line">==============================================================</span><br><span class="line">     6                                           @profile</span><br><span class="line">     7                                           def asyn(n):</span><br><span class="line">     8         1    1204525.0 1204525.0     21.0      threads=[gevent.spawn(task,i) for i in range(n)]</span><br><span class="line">     9         1    4532823.0 4532823.0     79.0      gevent.joinall(threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>非常清晰看到，理论上：10万个任务使用协程实现并发运行，总耗时1秒，但实际上，因为需要创建大量greenlet对象，列表创建10万个项耗时1.2秒，gevent joinall 10万个greenlet对象耗时4.5秒，所以整个程序完成总耗时实际为5.7秒左右。</p>
<p>使用memory_profiler库查看asyn.py内存使用情况，使用也简单与line_profiler相似，使用@profile装饰器来标识需要追踪的函数即可。使用协程，10万个对象消耗300多M，鉴于其并发效率高，而且所有的执行都只在一个线程实现 了，因此内存消耗可接受。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(spk) [root@nn spv]<span class="comment"># python -m memory_profiler asyn.py</span></span><br><span class="line">Filename: asyn.py</span><br><span class="line"></span><br><span class="line">Line <span class="comment">#    Mem usage    Increment   Line Contents</span></span><br><span class="line">================================================</span><br><span class="line">     <span class="number">6</span>   <span class="number">36.137</span> MiB   <span class="number">36.137</span> MiB   @profile</span><br><span class="line">     <span class="number">7</span>                             <span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">     <span class="number">8</span>  <span class="number">229.531</span> MiB    <span class="number">0.773</span> MiB       threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">     <span class="number">9</span>  <span class="number">366.270</span> MiB  <span class="number">136.738</span> MiB       gevent.joinall(threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：内存的单位MiB，表示的mebibyte，</p>
<p>MB/s的意思是每秒中传输10^6 byte的数据，以10为底数的指数</p>
<p>MiB/s的意思是每秒中传输2^20 byte的数据，以2为底数的指数</p>
<p>1 MiB =0.9765625 MB</p>
<p>创建100万个task，再看看kernprof -l -v asyn.py ，内存方面使用top可以直观看到asyn.py 占用了2G*0.816=1632 MiB</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">KiB Swap:  2097148 total,  1039272 free,  1057876 used.    14348 avail Mem </span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                </span><br><span class="line">    30 root      20   0       0      0      0 S 45.1  0.0   0:47.13 kswapd0                                </span><br><span class="line"> 29380 root      20   0 2319772   1.4g     56 R 42.8 81.6   1:04.53 kernprof</span><br></pre></td></tr></table></figure>

<p>将协程并发数设为100万，总共耗时为534秒，时间略长，使用gevent在单台服务器上，并发数不要设太离谱，1000个并发足以应付普通项目的需求，例如爬虫，例如做服务端接收客户端发来的socket流数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(spk) [root@nn spv]<span class="comment"># kernprof -l -v asyn.py           </span></span><br><span class="line"><span class="number">534.7752296924591</span></span><br><span class="line">Wrote profile results to asyn.py.lprof</span><br><span class="line">Timer unit: <span class="number">1e-06</span> s</span><br><span class="line"></span><br><span class="line">Total time: <span class="number">532.165</span> s</span><br><span class="line">File: asyn.py</span><br><span class="line">Function: asyn at line <span class="number">6</span></span><br><span class="line"></span><br><span class="line">Line <span class="comment">#      Hits         Time  Per Hit   % Time  Line Contents</span></span><br><span class="line">==============================================================</span><br><span class="line">     <span class="number">6</span>                                           @profile</span><br><span class="line">     <span class="number">7</span>                                           <span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">     <span class="number">8</span>         <span class="number">1</span>   <span class="number">82927089.0</span> <span class="number">82927089.0</span>     <span class="number">15.6</span>      threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">     <span class="number">9</span>         <span class="number">1</span>  <span class="number">449238172.0</span> <span class="number">449238172.0</span>     <span class="number">84.4</span>      gevent.joinall(threads)</span><br></pre></td></tr></table></figure>

<h5 id="2-3-理解gevent的monkey-patch-all"><a href="#2-3-理解gevent的monkey-patch-all" class="headerlink" title="2.3  理解gevent的monkey.patch_all()"></a>2.3  理解gevent的monkey.patch_all()</h5><p>&#8195;&#8195;在接下有关gevent的实际项目中，py程序都会引用monkey.patch_all()这个方法，它的作用是用非阻塞模块替换python自带的阻塞模块，这就是所谓”猴子补丁”，原理是运行时用非阻塞的对象属性替换对应阻塞对象的属性，或者用自己实现的同名非阻塞模块，替换对应的阻塞模块。<br>&#8195;&#8195;注意：这里说的模块就是“有完整功能的一个.py文件“或者”由多个py文件组成的一个完整功能的模块“<br>&#8195;&#8195;例如下面要实现这么一个需求：server.py运行时，将thread.py模块的synfoo函数替换为自定义的mythread.py模块里面asynfoo函数</p>
<p>thread.py 模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;builtin method synfoo of thread.py&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>自定义的mythread.py模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;builtin method asynfoo of mythread.py&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>server.py程序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> thread</span><br><span class="line"><span class="comment"># 在本程序的modules字典里面删除原thread模块</span></span><br><span class="line"><span class="keyword">del</span> sys.modules[<span class="string">&#x27;thread&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用异步的模块替换当前thread模块</span></span><br><span class="line">sys.modules[<span class="string">&#x27;thread&#x27;</span>] = <span class="built_in">__import__</span>(<span class="string">&#x27;mythread&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载thread</span></span><br><span class="line"><span class="keyword">import</span> thread</span><br><span class="line">thread.foo() <span class="comment">#这里的thread已经是mythread模块</span></span><br><span class="line">print(thread)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line"># server.py运行时已经成功将内建同步模块替换为异步的模块</span><br><span class="line">builtin method asynfoo of mythread.py</span><br><span class="line"># thread的指向自定义模块mythread</span><br><span class="line">&lt;module &#x27;mythread&#x27; from &#x27;/opt/spv/mythread.py&#x27;&gt;</span><br></pre></td></tr></table></figure>

<p>以上就是monkey.patch_all()大致逻辑，gevent可以把python内建的多个模块在程序运行时替换为它写的异步模块，默认是把内建的socket、thread、queue等模块替换为非阻塞同名模块</p>
<p>（site-packages/gevent/monkey.py）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_all</span>(<span class="params">socket=<span class="literal">True</span>, dns=<span class="literal">True</span>, time=<span class="literal">True</span>, select=<span class="literal">True</span>, thread=<span class="literal">True</span>, os=<span class="literal">True</span>, ssl=<span class="literal">True</span>, httplib=<span class="literal">False</span></span></span></span><br><span class="line"><span class="function"><span class="params">              subprocess=<span class="literal">True</span>, sys=<span class="literal">False</span>, aggressive=<span class="literal">True</span>, Event=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              builtins=<span class="literal">True</span>, signal=<span class="literal">True</span></span>):</span></span><br><span class="line">    _warnings, first_time = _check_repatching(**<span class="built_in">locals</span>())</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _warnings <span class="keyword">and</span> <span class="keyword">not</span> first_time:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> os:</span><br><span class="line">        patch_os()</span><br><span class="line">    <span class="keyword">if</span> time:</span><br><span class="line">        patch_time()</span><br><span class="line">    <span class="keyword">if</span> thread:</span><br><span class="line">        patch_thread(Event=Event, _warnings=_warnings)</span><br><span class="line">    <span class="comment"># sys must be patched after thread. in other cases threading._shutdown will be</span></span><br><span class="line">    <span class="comment"># initiated to _MainThread with real thread ident</span></span><br><span class="line">    <span class="keyword">if</span> sys:</span><br><span class="line">        patch_sys()</span><br><span class="line">    <span class="keyword">if</span> socket:</span><br><span class="line">        patch_socket(dns=dns, aggressive=aggressive)</span><br><span class="line">    <span class="keyword">if</span> select:</span><br><span class="line">        patch_select(aggressive=aggressive)</span><br><span class="line">    <span class="keyword">if</span> ssl:</span><br><span class="line">        patch_ssl()</span><br><span class="line">    <span class="keyword">if</span> httplib:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;gevent.httplib is no longer provided, httplib must be False&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> subprocess:</span><br><span class="line">        patch_subprocess()</span><br><span class="line">    <span class="keyword">if</span> builtins:</span><br><span class="line">        patch_builtins()</span><br></pre></td></tr></table></figure>

<p>如果不想gevent对某个内建模块覆盖为非阻塞，可以将该模块设为False：monkey.patch_all(thread=False)<br>或者在monkey.patch_all(thread=False) 语句后面追加import threading<br>目的是内建同步模块再次覆盖前面的gevent异步模块，例如server.py文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">monkey.patch_all(thread=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 或者 import threading</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>():</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">syn</span>():</span></span><br><span class="line">	t=threading.thread(target=task,args=())</span><br><span class="line">	t.start()</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span></span><br><span class="line">	gevent.joinall...</span><br></pre></td></tr></table></figure>

<p>由此可知，gevent的patch_all针对模块的覆盖是有顺序的，因为当使用gevent时，import的模块顺序很重要，内建模块在patch_all前面或者在patch_all后面，对应是同步还是异步模块导入。</p>
<h6 id="2-2-1-locals-方法"><a href="#2-2-1-locals-方法" class="headerlink" title="2.2.1 locals()方法"></a>2.2.1 locals()方法</h6><p>&#8195;&#8195;locals()返回一个字典，它可以获取当前模块所有的局部变量以及当前模块引入的其他模块，例如myFoo.py模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">local_module_dict=<span class="built_in">locals</span>()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>():</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line">print(local_module_dict[<span class="string">&#x27;threading&#x27;</span>])</span><br><span class="line">print(local_module_dict[<span class="string">&#x27;socket&#x27;</span>])</span><br><span class="line">输出</span><br><span class="line"><span class="comment"># &lt;module &#x27;threading&#x27; from &#x27;/root/.pyenv/versions/3.7.5/lib/python3.7/threading.py&#x27;&gt;</span></span><br><span class="line"><span class="comment"># &lt;module &#x27;socket&#x27; from &#x27;/root/.pyenv/versions/3.7.5/lib/python3.7/socket.py&#x27;&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>gevent通过locals()获取这些模块后，将需要替换的模块都替换gevent自己实现的非阻塞模块</p>
<h6 id="2-2-2-gevent-替换非阻塞的模块的思路"><a href="#2-2-2-gevent-替换非阻塞的模块的思路" class="headerlink" title="2.2.2 gevent 替换非阻塞的模块的思路"></a>2.2.2 gevent 替换非阻塞的模块的思路</h6><p>&#8195;&#8195;在前面的例子中，使用gevent.sleep()可以让协程自动切换实现异步方式执行，如果使用内建的time.sleep()，则变成同步执行，下面看看gevent如何使用patch_time()方法为sleep打补丁：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_time</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Replace :func:`time.sleep` with :func:`gevent.sleep`.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> gevent.hub <span class="keyword">import</span> sleep</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="comment"># 用gevent.hub.sleep方法替换内建的sleep方法</span></span><br><span class="line">    patch_item(time, <span class="string">&#x27;sleep&#x27;</span>, sleep)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_item</span>(<span class="params">module, attr, newitem</span>):</span></span><br><span class="line">    olditem = <span class="built_in">getattr</span>(module, attr, _NONE)</span><br><span class="line">    <span class="keyword">if</span> olditem <span class="keyword">is</span> <span class="keyword">not</span> _NONE:</span><br><span class="line">        saved.setdefault(module.__name__, &#123;&#125;).setdefault(attr, olditem)</span><br><span class="line">    <span class="built_in">setattr</span>(module, attr, newitem)</span><br></pre></td></tr></table></figure>

<p>实现原理跟2.2提到monke.patch_all()一样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent.hub <span class="keyword">import</span> sleep <span class="comment"># 先导入gevent的sleep</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment"># 再导入内建time模块</span></span><br><span class="line">print(<span class="built_in">getattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>)) <span class="comment"># 获取原内建sleep</span></span><br><span class="line"><span class="comment"># &lt;function time.sleep&gt;</span></span><br><span class="line"><span class="built_in">setattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>,sleep) <span class="comment"># 将gevent的异步sleep方法替换原sleep方法</span></span><br><span class="line">print(<span class="built_in">getattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>)) <span class="comment"># 打印运行是sleep方法看看是内建的还是gevent实现的</span></span><br><span class="line"><span class="comment"># &lt;function gevent.hub.sleep(seconds=0, ref=True)&gt;</span></span><br></pre></td></tr></table></figure>

<p>对于模块的导入，则需要使用<code>getattr自省模式创建一个对象，然后通过__import__引入</code></p>
<p>以gevent替换os为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_os</span>():</span></span><br><span class="line">    patch_module(<span class="string">&#x27;os&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>具体实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用gevent的os替换内建的os模块，这里的name就是&#x27;os&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_module</span>(<span class="params">name, items=<span class="literal">None</span></span>):</span></span><br><span class="line">	<span class="comment"># 通过__import__方法导入用gevent的os</span></span><br><span class="line">    gevent_module = <span class="built_in">getattr</span>(<span class="built_in">__import__</span>(<span class="string">&#x27;gevent.&#x27;</span> + name), name)</span><br><span class="line">    module_name = <span class="built_in">getattr</span>(gevent_module, <span class="string">&#x27;__target__&#x27;</span>, name)</span><br><span class="line">    module = <span class="built_in">__import__</span>(module_name)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    	<span class="comment"># 获取gevent_module里面跟os相关的方法</span></span><br><span class="line">        items = <span class="built_in">getattr</span>(gevent_module, <span class="string">&#x27;__implements__&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> AttributeError(<span class="string">&#x27;%r does not have __implements__&#x27;</span> % gevent_module)</span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> items:</span><br><span class="line">    	<span class="comment"># 用 gevent自己实现的os里面方法替换内建os指定方法</span></span><br><span class="line">        patch_item(module, attr, <span class="built_in">getattr</span>(gevent_module, attr))</span><br><span class="line">    <span class="keyword">return</span> module</span><br></pre></td></tr></table></figure>


<p>相信到了这里，已经可以理解 monkey.patch_all()为何要在gevent的程序头部引入，常见“模板”如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">monkey.patch_all()</span><br></pre></td></tr></table></figure>

<p>很多文章在讨论gevent的协程时，基本都是一句话“这是打补丁，将阻塞模块替换为非阻塞模块”简单带过，对于大部分人来说，这种说明一般会感到疑惑。</p>
<h4 id="3、gevent-examples"><a href="#3、gevent-examples" class="headerlink" title="3、gevent examples"></a>3、gevent examples</h4><p>&#8195;&#8195;本章主要结合一些场景给出gevent用法，参考了gevent官网给出的examples：<a href="http://www.gevent.org/examples/index.html">地址</a></p>
<h5 id="3-1-使用协程高并发爬网页"><a href="#3-1-使用协程高并发爬网页" class="headerlink" title="3.1 使用协程高并发爬网页"></a>3.1 使用协程高并发爬网页</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> random,datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line"></span><br><span class="line"><span class="comment"># patches stdlib (including socket and ssl modules) to cooperate with other greenlets</span></span><br><span class="line"><span class="comment"># 将标准lib打补丁，例如下面的https请求需要用到ssl模块，将该内建的ssl模块替换为gevent的ssl</span></span><br><span class="line">monkey.patch_all()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里给出的https协议来说明gevent可进行SSL的相关任务处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span> (<span class="params">workers=<span class="number">1000</span></span>):</span></span><br><span class="line">    start=time.time()</span><br><span class="line">    url_pool = [</span><br><span class="line">        <span class="string">&#x27;https://www.baidu.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.apple.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.qq.com/&#x27;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    urls=[ random.choice(url_pool) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(workers)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_head</span>(<span class="params">url</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;Starting &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(url,datetime.datetime.now()))</span><br><span class="line">        data = requests.get(url).text <span class="comment"># gevent会在发生IO的位置实现协程自动切换</span></span><br><span class="line">        <span class="comment">#print(&#x27;%s: %s bytes: %r&#x27; % (url, len(data), data[:2]))</span></span><br><span class="line">    jobs = [gevent.spawn(print_head, _url) <span class="keyword">for</span> _url <span class="keyword">in</span> urls]</span><br><span class="line">    gevent.wait(jobs) <span class="comment"># 阻塞主线程，让所有协程得以持续运行</span></span><br><span class="line">    cost=time.time()-start</span><br><span class="line">    print(<span class="string">&#x27;cost:&#x27;</span>,cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>

<p>以上1000个请求，只需运行一个线程，非常轻量且“低功耗”，而多线程方式，则需创建1000个线程，这就是协程的优势。</p>
<h5 id="3-2-gevent实现的socket高并发"><a href="#3-2-gevent实现的socket高并发" class="headerlink" title="3.2 gevent实现的socket高并发"></a>3.2 gevent实现的socket高并发</h5><p><strong>server.py端逻辑</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket,datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecodeErr</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Server</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host=<span class="string">&#x27;0.0.0.0&#x27;</span>,port=<span class="number">8090</span>,conns=<span class="number">100</span></span>):</span></span><br><span class="line">        self._s=socket.socket()</span><br><span class="line">        self._s.bind((host,port))</span><br><span class="line">        self._s.listen(conns)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_request</span>(<span class="params">self,conn</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 接收客户端发送的是比特字节，需要decode为str类型</span></span><br><span class="line">            msg=conn.recv(<span class="number">1024</span>).decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> msg:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            print(<span class="string">&#x27;got the msg:&#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(msg,self.recv_time()))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 发送给client需要byte类型</span></span><br><span class="line">            conn.send(<span class="built_in">bytes</span>(msg,encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">            <span class="keyword">if</span> msg ==<span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">                conn.shutdown(socket.SHUT_RDWR)</span><br><span class="line">                conn.close()</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line"><span class="meta">    @staticmethod            </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recv_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">serve_forever</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                client_conn,client_ip=self._s.accept()</span><br><span class="line">                <span class="comment"># 创建一个新的Greenlet服务新的请求</span></span><br><span class="line">                g=gevent.spawn(self.parse_request,client_conn)</span><br><span class="line">                print(<span class="string">&#x27;new client connected:&#123;&#125; &#123;&#125; serving...&#x27;</span>.<span class="built_in">format</span>(client_ip,g.name))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conns=<span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    server=Server(conns=conns)</span><br><span class="line">    server.serve_forever()                   </span><br></pre></td></tr></table></figure>

<p>另外打开2个终端使用 telnet 188.0.0.10 8090</p>
<p>输出：<br>可以看到每个client请求都是由新的greenlet来服务，这个Greenlet就是协程对象<code>&lt;Greenlet at 0x7f7dc87efa70: parse_request(&lt;gevent._socket3.socket object, fd=7, family=2, ty)&gt;</code>，而非多线程对象。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python server.py 100</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21042) Greenlet-0 serving...</span><br><span class="line">got the msg:foo</span><br><span class="line"> at 10:8:3</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21044) Greenlet-1 serving...</span><br><span class="line">got the msg:bar</span><br><span class="line"> at 10:8:37</span><br></pre></td></tr></table></figure>
<p>这里需要注意：<br>在parse_request里面关闭client的连接用conn.shutdown(socket.SHUT_WR)<br>shutdown 方法的 how 参数接受如下参数值： </p>
<ul>
<li> SHUT_RD：关闭 socket 的输入部分，程序还可通过该 socket 输出数据。(tcp半开状态)</li>
<li> SHUT_WR： 关闭该 socket 的输出部分，程序还可通过该 socket 读取数据。（(tcp半开)状态）</li>
<li> SHUT_RDWR：全关闭。该 socket 既不能读取数据，也不能写入数据。</li>
</ul>
<p>conn.close():关闭完整tcp连接通道<br>close方法不是立即释放，如果想立即释放，需在close之前使用shutdown方法<br>server.py 使用gevent实现可接受高并发连接，下面给出gevent版的client，高并发socket请求</p>
<p>*<em>client.py端代码**</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,server_ip,port,workers=<span class="number">10</span></span>):</span></span><br><span class="line">        self.server_ip=server_ip</span><br><span class="line">        self.port=port</span><br><span class="line">        self.workers=workers</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @staticmethod            </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recv_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">asyn_sock</span>(<span class="params">self,msg</span>):</span></span><br><span class="line">        client=socket.socket()</span><br><span class="line">        client.connect((self.server_ip,self.port))</span><br><span class="line">        bmsg=<span class="built_in">bytes</span>(msg,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        client.sendall(bmsg)</span><br><span class="line">        recv_data=client.recv(<span class="number">1024</span>).decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;gevent object:&#123;&#125; data:&#123;&#125; at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gevent.getcurrent(),recv_data,self.recv_time()))</span><br><span class="line">        client.close()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        threads=[gevent.spawn(self.asyn_sock,<span class="built_in">str</span>(i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.workers)]</span><br><span class="line">        gevent.joinall(threads)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    workers=<span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    c=Client(server_ip=<span class="string">&#x27;188.0.0.10&#x27;</span>,port=<span class="number">8090</span>,workers=workers)</span><br><span class="line">    c.start()</span><br></pre></td></tr></table></figure>

<p>服务器端启动100个连接数，客户端并发10个请求。</p>
<p>服务端打印如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python server.py 100</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21246) Greenlet-0 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21248) Greenlet-1 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21250) Greenlet-2 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21252) Greenlet-3 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21254) Greenlet-4 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21256) Greenlet-5 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21258) Greenlet-6 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21260) Greenlet-7 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21262) Greenlet-8 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21264) Greenlet-9 serving...</span><br><span class="line">got the msg:0 at 10:11:5</span><br><span class="line">got the msg:1 at 10:11:5</span><br><span class="line">got the msg:2 at 10:11:5</span><br><span class="line">got the msg:3 at 10:11:5</span><br><span class="line">got the msg:4 at 10:11:5</span><br><span class="line">got the msg:5 at 10:11:5</span><br><span class="line">got the msg:6 at 10:11:5</span><br><span class="line">got the msg:7 at 10:11:5</span><br><span class="line">got the msg:8 at 10:11:5</span><br><span class="line">got the msg:9 at 10:11:5</span><br></pre></td></tr></table></figure>

<p>客户端打印如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python asyn.py 10</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a85f0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;9&#x27;)&gt; data:9 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a84d0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;8&#x27;)&gt; data:8 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a83b0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;7&#x27;)&gt; data:7 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8290: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;6&#x27;)&gt; data:6 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8170: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;5&#x27;)&gt; data:5 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8050: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;4&#x27;)&gt; data:4 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586ef0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;3&#x27;)&gt; data:3 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586b90: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;2&#x27;)&gt; data:2 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586cb0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;1&#x27;)&gt; data:1 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586a70: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;0&#x27;)&gt; data:0 at:10:11:5</span><br></pre></td></tr></table></figure>

<p>可以看到不管是服务器和客户端，都是由多个greenlet协程对象负责请求或者负责服务。<br>如果server.py端并发数设为10000，client.py并发也设为10000，那么会出现以下情况：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">init__</span><br><span class="line">OSError: [Errno 24] Too many open files</span><br><span class="line">During handling of the above exception, another exception occurred:</span><br></pre></td></tr></table></figure>
<p>这里因为centos限制用户级别在打开文件描述符的数量，可查看默认值：限制至多打开1024个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# ulimit -n</span><br><span class="line">1024</span><br></pre></td></tr></table></figure>

<p>linux 一般会在以下几个文件对系统资源做限制，例如用户级别（此外还有系统级别）的限制： /etc/security/limits.conf，和/etc/security/limits.d/目录，/etc/security/limits.d/里面配置会覆盖/etc/security/limits.conf的配置：</p>
<blockquote>
<p>系统限制用户的资源有：所创建的内核文件的大小、进程数据块的大小、Shell<br>进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell<br>进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。</p>
</blockquote>
<p>提升并发性能：临时修改：ulimit -n 100000；永久性修改：root权限下，在/etc/security/limits.conf中添加如下两行，*表示所有用户，重启/或者注销重登陆生效</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">* soft nofile 102400</span><br><span class="line">* hard nofile 104800</span><br></pre></td></tr></table></figure>

<p>注意hard limit必须大于soft limit</p>
<p>这里将linux设为ulimit -n 100000，10万个描述符！ python server.py 20000个并发，python client.py 10000并发请求打过去，3秒内完成，而且这是因为程序加入print打印语句影响性能，去掉所有print语句，2万个客户端并发不到2秒内完成，gevent或者说底层Greenlet的并发性能非常强。</p>
<h5 id="3-3-gevent数据库操作"><a href="#3-3-gevent数据库操作" class="headerlink" title="3.3 gevent数据库操作"></a>3.3 gevent数据库操作</h5><p>&#8195;&#8195;这里将给出协程方式、多线程方式连接mysql数据库某实际项目备份表，15个字段，2万多条数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">        start=time.time()</span><br><span class="line">        func(*args,**kwargs)</span><br><span class="line">        cost=time.time()-start</span><br><span class="line">        print(<span class="string">&#x27;&#123;&#125; cost:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func.__name__,cost))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_db</span>(<span class="params">index</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;负责读数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(&#x27;start:&#x27;,index)</span></span><br><span class="line">    db = pymysql.connect(host = <span class="string">&#x27;****&#x27;</span>, user = <span class="string">&#x27;****&#x27;</span>, passwd = <span class="string">&#x27;****&#x27;</span>, db= <span class="string">&#x27;****&#x27;</span>)</span><br><span class="line">    cursor = db.cursor()</span><br><span class="line">    sql=<span class="string">&#x27;select count(1) from `article &#x27;</span></span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    nums = cursor.fetchall()</span><br><span class="line">    <span class="comment">#print(&#x27;total itmes:&#x27;,nums)</span></span><br><span class="line">    cursor.close()</span><br><span class="line">    db.close()</span><br><span class="line">    <span class="comment">#print(&#x27;end:&#x27;,index)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gevent_read</span>(<span class="params">workers</span>):</span></span><br><span class="line">    <span class="comment"># 创建多个greenlets协程对象</span></span><br><span class="line">    greenlets = [gevent.spawn(read_db,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(workers)]</span><br><span class="line">    gevent.joinall(greenlets)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	<span class="comment"># 5次测试。这里每次间隔1秒，让客户端连接mysql的connections及时关闭，避免释放不及时导致超过数据库端的允许连接数</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        gevent_read(<span class="number">100</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<p>从代码逻辑可以看出，gevent使用协程非常简单，在头部引入相关模块，再使用gevent.spawn创建多个greenlets对象，最后joinall。以下是测试结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python asyn_mysql.py </span><br><span class="line">gevent_read cost:2.702486276626587</span><br><span class="line">gevent_read cost:2.120276689529419</span><br><span class="line">gevent_read cost:2.1487138271331787</span><br><span class="line">gevent_read cost:2.61714243888855</span><br><span class="line">gevent_read cost:2.1180896759033203</span><br></pre></td></tr></table></figure>

<h5 id="3-4-gevent-多线程"><a href="#3-4-gevent-多线程" class="headerlink" title="3.4 gevent 多线程"></a>3.4 gevent 多线程</h5><p>gevent也有自己线程池，使用的python的thread，两者没区别，如果用了多线程，那么gevent其实就没多大意义了，因为不是协程模式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent.threadpool <span class="keyword">import</span> ThreadPool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">        start=time.time()</span><br><span class="line">        func(*args,**kwargs)</span><br><span class="line">        cost=time.time()-start</span><br><span class="line">        print(<span class="string">&#x27;&#123;&#125; cost:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func.__name__,cost))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpool</span>(<span class="params">workers</span>):</span></span><br><span class="line">    pool = ThreadPool(workers)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        pool.spawn(time.sleep, <span class="number">1</span>)</span><br><span class="line">    gevent.wait()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gpool(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>



<p>输出3秒，10个任务，线程池只有4个worker，因此需分三轮工作，因为耗时3秒</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn spv]# python gpool.py </span><br><span class="line">gpool cost:3.006455183029175</span><br></pre></td></tr></table></figure>

<h5 id="3-5-gevent-其他examples"><a href="#3-5-gevent-其他examples" class="headerlink" title="3.5 gevent 其他examples"></a>3.5 gevent 其他examples</h5><p>这里不再一一列出，可以参考gevent github的<a href="https://github.com/gevent/gevent/tree/master/examples">example目录</a></p>
<p>不过建议看看<strong>geventsendfile.py</strong>和<strong>wsgiserver_ssl.py</strong><br>第一是零拷贝技术的协程，第二个是基于https的协程webserver</p>
<h4 id="4、greenlet-eventlet-gevent的关系"><a href="#4、greenlet-eventlet-gevent的关系" class="headerlink" title="4、greenlet/eventlet/gevent的关系"></a>4、greenlet/eventlet/gevent的关系</h4><p>&#8195;&#8195;Greelent实现了一个比较易用(相比yeild)的协程切换的库。但是greenlet没有自己的调度过程，所以一般不会直接使用。<br>&#8195;&#8195;Eventlet在Greenlet的基础上实现了自己的GreenThread，实际上就是greenlet类的扩展封装，而与Greenlet的不同是，Eventlet实现了自己调度器称为Hub，Hub类似于Tornado的IOLoop，是单实例的。在Hub中有一个event loop，根据不同的事件来切换到对应的GreenThread。同时Eventlet还实现了一系列的补丁来使Python标准库中的socket等等module来支持GreenThread的切换。Eventlet的Hub可以被定制来实现自己调度过程。<br>&#8195;&#8195;Gevent基于libev和Greenlet。不同于Eventlet的用python实现的hub调度，Gevent通过Cython调用libev来实现一个高效的event loop调度循环。同时类似于Eventlet，Gevent也有自己的monkey_patch，在打了补丁后，完全可以使用python线程的方式来无感知的使用协程，减少了开发成本。<br>&#8195;&#8195;这里也顺便给出greenlet/eventlet/gevent和其他可以实现协程模式库的对比表格，该表来自Gruvi作者的项目介绍页。Gruvi是一个轻量且特别的协程库，项目作者因为不太认同常见python协程库的实现方式，而且也不认同不推荐使用monkey patch方式，所有他写了Gruvi，专注green thread：<a href="https://gruvi.readthedocs.io/en/latest/rationale.html">项目地址</a></p>
<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Gruvi</th>
<th align="left">Asyncio</th>
<th align="left">Gevent</th>
<th align="left">Eventlet</th>
</tr>
</thead>
<tbody><tr>
<td align="left">IO library</td>
<td align="left"><a href="https://github.com/joyent/libuv">libuv</a></td>
<td align="left">stdlib</td>
<td align="left"><a href="http://libev.schmorp.de/">libev</a></td>
<td align="left">stdlib / <a href="http://libevent.org/">libevent</a></td>
</tr>
<tr>
<td align="left">IO abstraction</td>
<td align="left">Transports / Protocols</td>
<td align="left">Transports / Protocols</td>
<td align="left">Green sockets</td>
<td align="left">Green sockets</td>
</tr>
<tr>
<td align="left">Threading</td>
<td align="left"><a href="https://pypi.python.org/pypi/fibers">fibers</a></td>
<td align="left"><code>yield from</code></td>
<td align="left"><a href="https://pypi.python.org/pypi/greenlet">greenlet</a></td>
<td align="left"><a href="https://pypi.python.org/pypi/greenlet">greenlet</a></td>
</tr>
<tr>
<td align="left">Resolver</td>
<td align="left">threadpool</td>
<td align="left">threadpool</td>
<td align="left">threadpool / <a href="http://c-ares.haxx.se/">c-ares</a></td>
<td align="left">blocking / <a href="http://www.dnspython.org/">dnspython</a></td>
</tr>
<tr>
<td align="left">Python: 2.x</td>
<td align="left">YES (2.7)</td>
<td align="left">YES (2.6+, via <a href="https://bitbucket.org/enovance/trollius">Trollius</a>)</td>
<td align="left">YES</td>
<td align="left">YES</td>
</tr>
<tr>
<td align="left">Python: 3.x</td>
<td align="left">YES (3.3+)</td>
<td align="left">YES</td>
<td align="left">YES</td>
<td align="left">NO</td>
</tr>
<tr>
<td align="left">Python: PyPy</td>
<td align="left">NO</td>
<td align="left">NO</td>
<td align="left">YES</td>
<td align="left">YES</td>
</tr>
<tr>
<td align="left">Platform: Linux</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
</tr>
<tr>
<td align="left">Platform: Mac OSX</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
</tr>
<tr>
<td align="left">Platform: Windows</td>
<td align="left">FAST (IOCP)</td>
<td align="left">FAST (IOCP)</td>
<td align="left">SLOW (select)</td>
<td align="left">SLOW (select)</td>
</tr>
<tr>
<td align="left">SSL: Posix</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
<td align="left">FAST</td>
</tr>
<tr>
<td align="left">SSL: Windows</td>
<td align="left">FAST (IOCP)</td>
<td align="left">FAST (IOCP 3.5+)</td>
<td align="left">SLOW (select)</td>
<td align="left">SLOW (select)</td>
</tr>
<tr>
<td align="left">SSL: Contexts</td>
<td align="left">YES (also Py2.7)</td>
<td align="left">YES (also Py2.6+)</td>
<td align="left">NO</td>
<td align="left">NO</td>
</tr>
<tr>
<td align="left">HTTP</td>
<td align="left">FAST (via <a href="https://github.com/joyent/http-parser">http-parser</a>)</td>
<td align="left">NO (external)</td>
<td align="left">SLOW (stdlib)</td>
<td align="left">SLOW (stdlib)</td>
</tr>
<tr>
<td align="left">Monkey Patching</td>
<td align="left">NO</td>
<td align="left">NO</td>
<td align="left">YES</td>
<td align="left">YES</td>
</tr>
</tbody></table>
<p>本博客也会为Gruvi写一篇文章，主要是欣赏作者阐述的设计理念。从对比表格来看，Asyncio各方面都出色，而且完全由Python标准库实现，后面也有关于Asyncio深入讨论的文章。</p>
<h4 id="5、gevent-不适用的场合"><a href="#5、gevent-不适用的场合" class="headerlink" title="5、gevent 不适用的场合"></a>5、gevent 不适用的场合</h4><p>这里参考Stack Overflow的文章<a href="https://stackoverflow.com/questions/54254252/asyncio-vs-gevent">《Asyncio vs. Gevent 》</a></p>
<p>it wasn’t perfect:</p>
<ul>
<li>Back then, it didn’t work well on Windows (and it still has some limitations today). gevent在Windows 表现不佳</li>
<li>It couldn’t monkey-patch C extensions, so we coudn’t use MySQLdb,  for example. Luckily, there were many pure Python alternatives, like  PyMySQL. 由于gevent的 monkey-patch替换原理，参考上面2.2，它只支持对存python库打补丁，对于C语言实现的python库，例如MySQLdb，则不支持。</li>
</ul>
<p>这里篇文章大致意思是建议用asyncio，因为它是标准库，有着非常详细的文档以及稳定的python官方维护。gevent也可以用，但是自己要清楚项目演进的后续维护情况。</p>
<p>Supported Platforms</p>
<blockquote>
<p><a href="https://pypi.org/project/gevent/whatsnew_1_3.html">gevent 1.3</a> runs on Python 2.7 and Python 3. Releases 3.4, 3.5 and<br>3.6 of Python 3 are supported. (Users of older versions of Python 2<br>need to install gevent 1.0.x (2.5), 1.1.x (2.6) or 1.2.x (&lt;=2.7.8);<br>gevent 1.2 can be installed on Python 3.3.) gevent requires the<br><a href="https://greenlet.readthedocs.io/">greenlet</a> library and will install<br>the <a href="https://cffi.readthedocs.io/">cffi</a> library by default on Windows.</p>
</blockquote>
<h4 id="6、协程原理解析"><a href="#6、协程原理解析" class="headerlink" title="6、协程原理解析"></a>6、协程原理解析</h4><p>&#8195;&#8195;前面具体的gevent代码示例，对深入理解协程有一定帮助，因为在本文中，把原理性的讨论放在最后一节显得更为合理。谈到协程又不得不把进程、线程以及堆、栈相关概念抛出，以便从全局把握协程、线程和进程。</p>
<h5 id="6-1-进程与内存分配"><a href="#6-1-进程与内存分配" class="headerlink" title="6.1 进程与内存分配"></a>6.1 进程与内存分配</h5><p>&#8195;&#8195;进程是系统资源分配的最小单位，Linux系统由一个个在后台运行process提供所有功能的组成，你可以用<code>ll /proc |wc -l</code>或者<code>ps aus|less</code>查看系统运行的进程。进程自己是需要占用系统资源的，例如cpu、内存、网络，这里我们关注其<br>程序的内存分配。<br>这里以一个由C /C++编译的程序占用的内存分为以下几个部分为例说明，这段内容参考文章<a href="https://blog.csdn.net/ZXR_LJ/article/details/79440577">《堆栈的区别》</a>：</p>
<ul>
<li><p>栈区（stack）： 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p>
</li>
<li><p>堆区（heap）：一般由程序员（在代码里面自行申请内存）分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。</p>
</li>
<li><p>全局区（静态区）（static）：全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放</p>
</li>
<li><p>文字常量区 ：常量字符串就是放在这里的。 程序结束后由系统释放</p>
</li>
<li><p>程序代码区：存放函数体的二进制代码</p>
</li>
</ul>
<p>&#8195;&#8195;可以想象，系统创建一个新的进程都进行以上的复杂内存分配工作，而进程结束后系统还得进行大量内存回收清理工作，如果系统有成千上万个进程创建、切换以及销毁，可想而知，非常消耗资源，”疲于奔命，顾不上其他重要请求“（这就是Apache服务器的并发性的劣势，看看Nginx有多强大）。所以多进程做并发业务，显然不是一个理想方案。</p>
<h5 id="6-2-线程"><a href="#6-2-线程" class="headerlink" title="6.2 线程"></a>6.2 线程</h5><p>&#8195;&#8195;关于进程的描述，其实很多文章可以找到相关讨论，这里以线程和进程的区别作为说明：</p>
<ul>
<li><p>本质区别：进程是操作系统资源分配（分配CPU、内存、网络）的基本单位，而线程是任务（进行某种代码逻辑）调度和执行的基本单位</p>
</li>
<li><p>资源占用区别：每个进程都有独立的代码和程序上下文环境，进程之间的切换消耗较大系统资源（投入大，代价较高）；这里顺便说明为何代价高？因为进程之间切换涉及到用户空间（用户态）和内核空间（内核态）的切换。一个进程里面可以有多个线程运行，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的消耗的是当前进程占有的资源，代价较小，但也不低。</p>
</li>
<li><p>内存分配方面：系统在运行的时候会为每个进程分配不同的内存空间；而对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源。</p>
</li>
<li><p>所处环境：在操作系统中能同时运行多个进程（程序）；而在同一个进程中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行）</p>
</li>
</ul>
<h5 id="6-3-协程"><a href="#6-3-协程" class="headerlink" title="6.3 协程"></a>6.3 协程</h5><p>&#8195;&#8195;终于谈到本章的主角：协程，英文coroutine，它比线程更加轻量，你可以这样认为：一个进程可以拥有多个线程一样，而一个线程也可以拥有多个协程。<br>==<strong>协程与进程的区别</strong>==：</p>
<ul>
<li>执行流的调度者不同，进程是内核调度，而协程是在用户态调度，也就是说进程的上下文是在内核态保存恢复的，而协程是在用户态保存恢复的，很显然用户态的代价更低</li>
<li>进程会被强占，而协程不会，也就是说协程如果不主动让出CPU，那么其他的协程，就没有执行的机会。</li>
<li>对内存的占用不同，实际上协程可以只需要4K的栈就足够了，而进程占用的内存要大的多</li>
<li>从操作系统的角度讲，多协程的程序是单进程，单协程</li>
</ul>
<p>==<strong>协程与线程的区别</strong>==<br>&#8195;&#8195;一个线程里面可以包含多个协程，线程之间需要上下文切换成本相对协程来说是比较高的，尤其在开启线程较多时，线程的切换更多的是靠操作系统来控制，而协程之间的切换和运行由用户程序代码自行控制或者类似gevent这种自动切换，因此协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行），这将为用户可以设计出非常高性能的并发编程模式。如下图所示一个主线程负责使用gevent自动调度（自动切换运行）2个协程，大致逻辑如下：</p>
<ul>
<li>主线程（MainThread，也是根协程或者当前线程）创建（spawn）两个协程，只有有协程遇到IO event时候就把控制权交给当前线程，直到这个协程的IO event已经完成，主线程将控制权给这个协程。<br><img src="https://img-blog.csdnimg.cn/20191228112520473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h4 id="7、小结"><a href="#7、小结" class="headerlink" title="7、小结"></a>7、小结</h4>&#8195;&#8195;本文开启了Python的异步编程文章讨论篇章，算是比较进阶的内容，因为异步模式可让实际项目确实受益不少，在本博客之后有关异步的内容有：asyncio、文件描述符与IO多路复用。</li>
</ul>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>gevent</tag>
        <tag>协程</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Hadoop HA集群部署HBase HA集群（详细版）</title>
    <url>/blog/2019/10/28/%E5%9F%BA%E4%BA%8EHadoop%20HA%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2HBase%20HA%E9%9B%86%E7%BE%A4%EF%BC%88%E8%AF%A6%E7%BB%86%E7%89%88%EF%BC%89/</url>
    <content><![CDATA[<h4 id="1、前言"><a href="#1、前言" class="headerlink" title="1、前言"></a>1、前言</h4><p>&#8195;&#8195;前面的博客中<a href="https://blog.csdn.net/pysense/article/details/102490212">链接</a>已经给出Hadoop3.1.2和yarn的完整部署（但还不是高可用），此篇博客将给出Hadoop的高可用部署，以及HBase高可用，为之后应用数据层开发提供底层的BigTable支持。前面的文章，我们已经深入讨论的ZooKeeper这个中间件的原理以及分布式锁的实现，事实上zookeeper使用最广泛的场景是“选举”主从角色，Hadoop以及Hbase的高可用（主从架构）正是通过ZooKeeper的临时节点机制实现。<br>以下的配置会跳过Hadoop3.1.2的部署，仅给出ZooKeeper分布式物理方式部署、以及HBase的部署过程、测试结果。</p>
<h4 id="2、ZooKeeper与Hadoop、HBase的关系"><a href="#2、ZooKeeper与Hadoop、HBase的关系" class="headerlink" title="2、ZooKeeper与Hadoop、HBase的关系"></a>2、ZooKeeper与Hadoop、HBase的关系</h4><p>&#8195;&#8195;ZooKeeper作为协调器，在大数据组件中提供：管理Hadoop集群中的NameNode、HBase中HBaseMaster的选举，节点之间状态同步等。例如在HBase中，存储HBase的Schema，实时监控HRegionServer，存储所有Region的寻址入口，当然还有最常见的功能就是保证HBase集群中只有一个Master。</p>
<h4 id="3、Hadoop与HBase的关系"><a href="#3、Hadoop与HBase的关系" class="headerlink" title="3、Hadoop与HBase的关系"></a>3、Hadoop与HBase的关系</h4><p>&#8195;&#8195;完整的hadoop组件环境架构图<br><img src="https://img-blog.csdnimg.cn/20191019105428256.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>

<p>&#8195;&#8195;首先HBase是一个分布式的、面向列的开源数据库，正是业务数据需要列数据库的支持以及该数据库能够支持业务超大数据集扩展存储的需求，HBase当然作为中间件选型的首选。上图描述Hadoop组件生态中的各层系统。其中，HBase位于结构化存储层，Hadoop的HDFS为HBase提供了高可靠性、分布式的底层存储支持，Hadoop MapReduce、Spark为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定选举服务和failover机制。<br>此外，Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。<br>==（当然本blog也会针对Hbase的架构原理做出一篇文章讨论）==<br>&#8195;&#8195;其实，本博客有关大数据的多篇文章的内容，都是为了能够梳理出大数据多个组件全流程部署、架构原理、业务数据应用开发到BI的呈现的完整技术内容，以实现大数据库开发工程师必备的项目经历。</p>
<h4 id="4、架构资源规划"><a href="#4、架构资源规划" class="headerlink" title="4、架构资源规划"></a>4、架构资源规划</h4><table>
<thead>
<tr>
<th>nn</th>
<th>dn1</th>
<th>dn2</th>
</tr>
</thead>
<tbody><tr>
<td>1vcpu，2G内存</td>
<td>1vcpu，1G内存</td>
<td>1vcpu，1G内存</td>
</tr>
<tr>
<td>NameNode</td>
<td></td>
<td>NameNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DFSZKFailoverController</td>
<td>DFSZKFailoverController</td>
<td>DFSZKFailoverController</td>
</tr>
<tr>
<td>ResourceManager</td>
<td></td>
<td>ResourceManager</td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
<tr>
<td>JobHistoryServer</td>
<td></td>
<td>JobHistoryServer</td>
</tr>
<tr>
<td>ZooKeeper</td>
<td>ZooKeeper</td>
<td>ZooKeeper</td>
</tr>
<tr>
<td>HBase master</td>
<td></td>
<td>HBase master</td>
</tr>
<tr>
<td>RegionServer</td>
<td>RegionServer</td>
<td>RegionServer</td>
</tr>
<tr>
<td>Hadoop版本、JDK版本、HBase版本、ZooKeeper版本参考如下：</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">hadoop-3.1.2   jdk1.8.0_161   </span><br><span class="line">hbase-2.1.7    zookeeper-3.4.14</span><br></pre></td></tr></table></figure>
<p>关于Hadoop与HBase的兼容性，官方已经给出<a href="http://hbase.apache.org/book.html#_configuration_files">Hadoop version support matrix</a>，目前HBase2.1.x、HBase2.2.x支持Hadoop 3.1.1+（Tested to be fully-functional）。关于版本兼容分析，很多类似文章也有提到，但他们所提到的兼容比对情况已过时，故最佳途径应及时查阅官网最新发布的内容。</p>
<h4 id="5、ZooKeeper集群设置"><a href="#5、ZooKeeper集群设置" class="headerlink" title="5、ZooKeeper集群设置"></a>5、ZooKeeper集群设置</h4><h5 id="5-1-设置nn节点的zoo-conf"><a href="#5-1-设置nn节点的zoo-conf" class="headerlink" title="5.1 设置nn节点的zoo.conf"></a>5.1 设置nn节点的zoo.conf</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;conf</span><br><span class="line"># 拷贝zoo_sample.cfg并重命名为zoo.cfg ：</span><br><span class="line">[root@nn conf]# cp zoo_sample.cfg zoo.conf</span><br><span class="line"></span><br><span class="line"># 修改 zoo.cfg</span><br><span class="line">[root@nn conf] vi zoo.cfg</span><br><span class="line"># data目录需自行创建，添加：</span><br><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;data</span><br><span class="line"># 在该文件最后添加，指定zookeeper集群主机及端口，节点数必须为奇数</span><br><span class="line">server.1&#x3D;nn:2888:3888</span><br><span class="line">server.2&#x3D;dn1:2888:3888</span><br><span class="line">server.3&#x3D;dn2:2888:3888</span><br><span class="line"></span><br><span class="line"># 在&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;data目录下创建myid文件</span><br><span class="line">[root@nn data]# pwd</span><br><span class="line">&#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;data</span><br><span class="line">[root@nn data]# touch myid</span><br><span class="line">[root@nn data]# ls</span><br><span class="line">myid</span><br><span class="line"># 文件内容为1，即表示当前节点为在zoo.cfg中指定的server.1</span><br></pre></td></tr></table></figure>
<h5 id="5-2-将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid"><a href="#5-2-将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid" class="headerlink" title="5.2 将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid"></a>5.2 将zookeeper目录拷贝到dn1、dn2节点上，并更改对于的myid</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# scp -r zookeeper-3.4.14&#x2F; dn1:&#x2F;opt</span><br><span class="line">[root@nn opt]# scp -r zookeeper-3.4.14&#x2F; dn2:&#x2F;opt</span><br><span class="line"># 更改dn1 myid内容为2，dn2 myid内容为3</span><br></pre></td></tr></table></figure>
<h5 id="5-3-设置zk的全局环境变量"><a href="#5-3-设置zk的全局环境变量" class="headerlink" title="5.3 设置zk的全局环境变量"></a>5.3 设置zk的全局环境变量</h5><p>在三个节点上都要配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt] vi  &#x2F;etc&#x2F;profile</span><br><span class="line"># 新增</span><br><span class="line">ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeper-3.4.14 </span><br><span class="line">export   PATH&#x3D;$ZOOKEEPER_HOME&#x2F;bin:$PATH  </span><br></pre></td></tr></table></figure>
<p>source ~/.bash_profile</p>
<h5 id="5-4-启动zk集群"><a href="#5-4-启动zk集群" class="headerlink" title="5.4 启动zk集群"></a>5.4 启动zk集群</h5><p>在每个节点上运行<code>zkServer.sh start</code><br>查看三个节点的zk角色</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># nn节点</span><br><span class="line">[root@nn ~]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"># dn1节点</span><br><span class="line">[root@dn1 opt]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br><span class="line"># dn2节点</span><br><span class="line">[root@dn2 opt]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper-3.4.14&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure>
<p>使用jps查看zk的进程QuorumPeerMain<br>QuorumPeerMain是zookeeper集群的启动入口类，用来加载配置启动QuorumPeer线程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# jps</span><br><span class="line">1907 Jps</span><br><span class="line">1775 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">[root@dn1 opt]# jps</span><br><span class="line">16226 Jps</span><br><span class="line">16201 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">[root@dn2 opt]# jps</span><br><span class="line">5824 QuorumPeerMain</span><br><span class="line">5861 Jps</span><br></pre></td></tr></table></figure>
<p>注意：如果某个节点使用jps命令后，没有QuorumPeerMain进程，一般是因为zk的端口号2181被占用，在<code>/opt/zookeeper-3.4.14</code>目录中，zookeeper.out执行日志会给相应的提示。<br>[root@nn zookeeper-3.4.14]# ls<br>bin              ivy.xml      README.md                 zookeeper-3.4.14.jar.sha1  zookeeper.out<br>….<br>以下为之前docker方式部署zk时占用了2181端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn2 zookeeper-3.4.14]# ss -tnlp |grep 2181</span><br><span class="line">LISTEN     0      128         :::2181                    :::*                   users:((&quot;docker-proxy&quot;,pid&#x3D;1499,fd&#x3D;4))</span><br><span class="line"># kill docker占用的2181进程，再重新启动zk即可。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="6、Hadoop-HA配置详细说明"><a href="#6、Hadoop-HA配置详细说明" class="headerlink" title="6、Hadoop HA配置详细说明"></a>6、Hadoop HA配置详细说明</h4><p>首先配置hadoop的jdk路径：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi hadoop-env.sh</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br></pre></td></tr></table></figure>
<h5 id="6-1-core-site-xml-加入zk服务"><a href="#6-1-core-site-xml-加入zk服务" class="headerlink" title="6.1 core-site.xml 加入zk服务"></a>6.1 core-site.xml 加入zk服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- hdfs地址，单点模式值为namenode主节点名，本测试为HA模式，需设置为nameservice  的名字--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hdapp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">  &lt;!-- 这里的路径默认是NameNode、DataNode、JournalNode等存放数据的公共目录，也可以单独指定 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/hadoop-3.1.2/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--加入zk服务，不少于三个节点--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--设置web访问用户，否则web端浏览hdfs文件目录会提权限不足--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="6-2-hdfs-site-xml"><a href="#6-2-hdfs-site-xml" class="headerlink" title="6.2  hdfs-site.xml"></a>6.2  hdfs-site.xml</h5><p>因为要配置hadoop HA，因此这部分的属性项比较多，这部分内容参考<br><a href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">Apache官网HA配置</a>，官网已经给出非常详细且易懂的描述。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- hadoop HA 配置开始 --&gt;</span><br><span class="line">  &lt;!-- 为namenode集群起一个services name，名字和core-site.xml的fs.defaultFS指定的一致 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.nameservices&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdapp&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">    &lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.hdapp&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nn,dn2&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定nn的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.hdapp.nn&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nn:9000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">      &lt;!-- 指定dn2的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.hdapp.dn2&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;dn2:9000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">    &lt;!--名为nn的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.hdapp.nn&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;nn:50070&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 名为dn2的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.hdapp.dn2&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;dn2:50070&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- namenode间用于共享编辑日志的journal节点列表&#x2F;hdapp是表示日志存储的在hdfs上根路径，为多个HA可公用服务器进行数据存储，节约服务器成本 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;qjournal:&#x2F;&#x2F;nn:8485;dn1:8485;dn2:8485&#x2F;hdapp&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- journalnode 上用于存放edits日志的目录 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.journalnode.edits.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;tmp&#x2F;dfs&#x2F;journalnode&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.automatic-failover.enabled.hdapp&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.hdapp&lt;&#x2F;name&gt;  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">    </span><br><span class="line">  &lt;!-- 一旦需要NameNode切换，使用两方式进行操作，优先使用sshfence --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.methods&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;sshfence</span><br><span class="line">shell(&#x2F;bin&#x2F;true)</span><br><span class="line">&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 如果使用ssh进行故障切换，使用ssh通信时指定私钥所在位置 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;root&#x2F;.ssh&#x2F;id_rsa&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- ssh连接超时超时时间，30s --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;30000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"> &lt;!-- HA配置结束 --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 设置 hdfs 副本数量，这里跟节点数量一致 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt; </span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<p>注意在sshfence设置中，若ssh用户名不是root，且ssh端口不是默认22，则需改为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;sshfence([[my_hadoop][:31900]])&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br></pre></td></tr></table></figure>
<p>==以上设置非常重要，涉及sshfence能否正常切换主备hadoop服务==</p>
<p>官网给出自定义shell脚本去切换namenode进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">shell - run an arbitrary shell <span class="built_in">command</span> to fence the Active NameNode</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">The shell fencing method runs an arbitrary shell <span class="built_in">command</span>. It may be configured like so:</span></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;shell(/path/to/my/script.sh arg1 arg2 ...)&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h5 id="6-3-mapred-site-xml"><a href="#6-3-mapred-site-xml" class="headerlink" title="6.3 mapred-site.xml"></a>6.3 mapred-site.xml</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;!-- 采用yarn作为mapreduce的资源调度框架 --&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定nn作为jobhistory服务器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放已完成job的历史日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放正在运行job的历史日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done_intermediate&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放yarn stage的日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/history/staging&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> web上默认最多显示20000个历史的作业记录信息，这里设为1000个。</span></span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.jobhistory.joblist.cache.size&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;mapreduce.jobhistory.cleaner.enable&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 一天清理一次 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.jobhistory.cleaner.interval-ms&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;86400000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 仅保留最近1周的job日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.jobhistory.max-age-ms&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;432000000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>注意，这里加入yarn执行application（job）的日志记录进程，因为nn和dn2做了HA，所以nn、dn2节点都配上该jobhistory服务，dn1节点不需要。</p>
<h5 id="6-4-yarn-site-xml配置"><a href="#6-4-yarn-site-xml配置" class="headerlink" title="6.4  yarn-site.xml配置"></a>6.4  yarn-site.xml配置</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 启用yarn HA高可用性 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定resourcemanager的名字，自行命名，跟服务器hostname无关 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hayarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定nn节点为rm1 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定dn2节点为rm2  --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;dn2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定当前机器nn作为主rm1 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 指定zookeeper集群机器 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 禁止启动一个线程检查每个任务正使用的物理内存量、虚拟内存量是否可用 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">   	&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">   	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">   	&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">   	&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="6-5-指定worker"><a href="#6-5-指定worker" class="headerlink" title="6.5 指定worker"></a>6.5 指定worker</h5><p>三个节点都设为datanode，在生产环境中，DD不要跟DN放在同一台服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn hadoop-3.1.2]# vi etc&#x2F;hadoop&#x2F;workers </span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>6.5 将/opt/hadoop-3.1.2/目录拷贝到dn1、dn2节点<br>(因为dn1不作为resourcemanager standby角色，因此在其yarn-site.xml删除)<br>nn节点作为resourcemanager 角色，因此在其yarn-site.xml，RM设为自己</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>而dn2节点作为resourcemanager standby角色，因此在其yarn-site.xml，RM设为自己</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h5 id="6-6-修改start-dfs-sh和-stop-dfs-sh文件"><a href="#6-6-修改start-dfs-sh和-stop-dfs-sh文件" class="headerlink" title="6.6  修改start-dfs.sh和 stop-dfs.sh文件"></a>6.6  修改start-dfs.sh和 stop-dfs.sh文件</h5><p>在/opt/hadoop-3.1.2/sbin/中，分别在 start-dfs.sh 和 stop-dfs.sh文件开始处添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">HDFS_ZKFC_USER=root</span><br><span class="line">HDFS_JOURNALNODE_USER=root</span><br><span class="line"><span class="meta">#</span><span class="bash"> 以上内容在三个节点上配置</span></span><br><span class="line"></span><br><span class="line">在start-yarn.sh和stop-yarn.sh文件头部添加如下命令</span><br><span class="line">​```shell</span><br><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br><span class="line"><span class="meta">#</span><span class="bash"> 以上内容在三个节点上配置</span></span><br></pre></td></tr></table></figure>
<p>至此，以及完成hadoop层面的HA的配置文件，因为属性项很多，配置过程务必仔细核对，否则启动各种出错。下面将逐步验证各项组件启动情况</p>
<h4 id="7、集群启动"><a href="#7、集群启动" class="headerlink" title="7、集群启动"></a>7、集群启动</h4><p>在第5节内容中，三个节点zk的QuorumPeerMain进程已正常启动</p>
<h5 id="7-1-启动JournalNode进程"><a href="#7-1-启动JournalNode进程" class="headerlink" title="7.1 启动JournalNode进程"></a>7.1 启动JournalNode进程</h5><p>JournalNode服务三个节点启动都启动，因此，需在每个节点上单独运行启动命令，建议使用全局命令hdfs，否则得去每个节点的sbin目录下使用hadoop-daemon.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ]# hdfs --daemon start journalnode</span><br><span class="line">[root@nn opt]# jps</span><br><span class="line">4889 JournalNode</span><br><span class="line">4987 Jps</span><br><span class="line">1775 QuorumPeerMain</span><br></pre></td></tr></table></figure>
<p>若启动正常，jps可以看三个节点都启动了相应进程</p>
<h5 id="7-2-格式化-NameNode和zkfc"><a href="#7-2-格式化-NameNode和zkfc" class="headerlink" title="7.2 格式化 NameNode和zkfc"></a>7.2 格式化 NameNode和zkfc</h5><p>这里的zkfc指：ZK Failover Controller daemon<br>==在NameNode的nn主节点上进行==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# hdfs namenode -format</span><br><span class="line"><span class="meta">#</span><span class="bash"> 成功提示</span></span><br><span class="line">Storage directory /opt/hadoop-3.1.2/tmp/dfs/name has been successfully formatted.</span><br><span class="line"></span><br><span class="line">[root@nn opt]# hdfs zkfc -formatZK</span><br><span class="line"><span class="meta">#</span><span class="bash"> 成功提示</span></span><br><span class="line">: Successfully created /hadoop-ha/hdapp in ZK.</span><br></pre></td></tr></table></figure>
<p>==重要==<br>在备机dn2节点上执行fsimge元数据同步命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 sbin]# hdfs namenode -bootstrapStandby</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到namenode主从服务信息</span></span><br><span class="line">=====================================================</span><br><span class="line">About to bootstrap Standby ID dn2 from:</span><br><span class="line">           Nameservice ID: hdapp</span><br><span class="line">        Other Namenode ID: nn</span><br><span class="line">  Other NN&#x27;s HTTP address: http://nn:50070</span><br><span class="line">  Other NN&#x27;s IPC  address: nn/192.188.0.4:9000</span><br><span class="line">             Namespace ID: 778809532</span><br><span class="line">            Block pool ID: **</span><br><span class="line">               Cluster ID:**</span><br><span class="line">           Layout version: -64</span><br><span class="line">       isUpgradeFinalized: true</span><br><span class="line">=====================================================</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dn2节点通过http get去nn节点下载FsImage以便实现元数据同步</span></span><br><span class="line">namenode.TransferFsImage: Opening connection to http://nn:50070/imagetransfer?getimage=1&amp;txid=0&amp;storageInfo=-64:778809532:***:CID-594d1106-a909-4e60-8a2d-d54e264beee2&amp;bootstrapstandby=true</span><br><span class="line">***</span><br><span class="line">namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 391 bytes.</span><br></pre></td></tr></table></figure>

<h5 id="7-3-启动ZookeeperFailoverController、HDFS、YARN"><a href="#7-3-启动ZookeeperFailoverController、HDFS、YARN" class="headerlink" title="7.3 启动ZookeeperFailoverController、HDFS、YARN"></a>7.3 启动ZookeeperFailoverController、HDFS、YARN</h5><p>启动HA服务是有顺序的，需先启动ZKFC再启动HDFS，该服务管理hadoop的namenode主备切换；若先启动HDFS，则在未启动ZKFC进程之前，两个namenode都是standby模式，直到ZKFC启动后，HDFS才会正常进入HA主备模式。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、启动主备切换服务，在nn、dn2分别执行</span></span><br><span class="line">[root@nn sbin]# hdfs --daemon start zkfc</span><br><span class="line">[root@dn2 sbin]# hdfs --daemon start zkfc</span><br><span class="line"><span class="meta">#</span><span class="bash"> DFSZKFailoverController进程</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、只需在主节点nn上操作执行</span></span><br><span class="line">[root@nn sbin]# ./start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 验证:nn,dn2显示NN、DN、JN，dn1显示DN、JN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意namenode节点的tmp/dfs目录必须具有以下三个目录，否则namenode启动失败，提示相关目录不存在</span></span><br><span class="line">[root@dn2 dfs]# pwd</span><br><span class="line">/opt/hadoop-3.1.2/tmp/dfs</span><br><span class="line">[root@dn2 dfs]# ls</span><br><span class="line">data  journalnode  name</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、只需在主节点nn上执行</span></span><br><span class="line">[root@nn sbin]# /start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 验证：nn,dn2显示RN、NM，dn1显示NM</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="7-4-启动Application（job）History进程服务"><a href="#7-4-启动Application（job）History进程服务" class="headerlink" title="7.4  启动Application（job）History进程服务"></a>7.4  启动Application（job）History进程服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动JobHistoryServer</span></span><br><span class="line">mapred --daemon start  historyserver</span><br></pre></td></tr></table></figure>
<p>JobHistoryServer的作用：<br>可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。</p>
<h5 id="7-5-使用命令或者web页面查看集群组件服务情况"><a href="#7-5-使用命令或者web页面查看集群组件服务情况" class="headerlink" title="7.5 使用命令或者web页面查看集群组件服务情况"></a>7.5 使用命令或者web页面查看集群组件服务情况</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在NameNode主节点nn上，用命令查看Namenode</span></span><br><span class="line">[root@nn opt]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn opt]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure>
<p>在<code>http://nn:50070</code>查看nn状态（最好使用Chrome查看，用Firefox查看Utilities栏目-Browse the file system没响应）<br><img src="https://img-blog.csdnimg.cn/20191020121813446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>http://dn2:50070</code>查看dn2状态<br><img src="https://img-blog.csdnimg.cn/20191020122008421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在NameNode主节点nn上，用命令查看RM节点主备状态</span><br><span class="line">[root@nn opt]# yarn rmadmin -getServiceState rm1 </span><br><span class="line">standby</span><br><span class="line">[root@nn opt]# yarn rmadmin -getServiceState rm2</span><br><span class="line">active</span><br></pre></td></tr></table></figure>
<p>注意：这里显示是dn2（rm2）节点为active状态，当在浏览器输入<code>http://nn:8088</code>时，会自动被重定向到dn2的web服务：<code>http://dn2:8088/cluster</code><br><img src="https://img-blog.csdnimg.cn/2019102012262488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="7-6-hadoop主备切换测试"><a href="#7-6-hadoop主备切换测试" class="headerlink" title="7.6 hadoop主备切换测试"></a>7.6 hadoop主备切换测试</h5><p><strong>1）主备切换失败情况：</strong><br>在切换测试之前，请先检查Linux系统上有无安装一个fuser的工具<br>fuser：fuser可用于查询文件、目录、socket端口和文件系统的使用进程，并且可以使用fuser关闭进程，当文件系统umount报device busy时，常用到fuser查询并关闭使用相应文件系统的进程。<br>在6.2章节hdfs-site.xml配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &lt;!-- 一旦需要NameNode切换，使用ssh方式或者shell进行操作 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.ha.fencing.methods&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">sshfence</span><br><span class="line">        shell(&#x2F;bin&#x2F;true)&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>以上表示fencing的方法目前有两种，sshfence和shell<br>sshfence方法是指通过ssh登陆到active namenode节点并kill了该namenode进程，因此需设置ssh免密登陆，还要保证有杀掉namenode进程的权限，以保证hadoop集群在任何时候只有一个namenode节点处于active状态。<br>如果Linux系统没有fuser工具，那么sshfence执行会提示提示<br>==fuser: command not found==<br>==Fencing method org.apache.hadoop.ha.SshFenceByTcpPort(null) was unsuccessful.==<br>该日志路径是在dn2作为standby节点的日志目录下：<br><code>/opt/hadoop-3.1.2/logs/hadoop-root-zkfc-dn2.log</code><br>导致主备无法正常切换，可能出现脑裂（例如两个namenode都是active模式或者standby模式）</p>
<p><strong>2）解决方案：</strong><br>只需安装fuser工具即可<br><code>yum install psmisc -y</code>，安装之后，查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@dn2 logs]# ls &#x2F;usr&#x2F;sbin&#x2F;fuser </span><br><span class="line">&#x2F;usr&#x2F;sbin&#x2F;fuser</span><br></pre></td></tr></table></figure>

<p><strong>3）主备切换nn，dn2状态变更：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在主节点nn上，手动kill掉namenode进程，可以看到dn2立即变为active状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn sbin]# jps</span><br><span class="line">7424 NameNode</span><br><span class="line">5585 JournalNode</span><br><span class="line">5347 DataNode</span><br><span class="line">6024 ResourceManager</span><br><span class="line">5001 QuorumPeerMain</span><br><span class="line">25322 JobHistoryServer</span><br><span class="line">8922 Jps</span><br><span class="line">6652 DFSZKFailoverController</span><br><span class="line">6157 NodeManager</span><br><span class="line">[root@nn sbin]# kill -9 7424</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">active</span><br></pre></td></tr></table></figure>
<p>==以上kill了nn的namenode进程，再启动该进程，看看nn能否变为standby模式==</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# hdfs --daemon start namenode</span><br><span class="line">[root@nn ~]# hdfs haadmin -getServiceState nn</span><br><span class="line">standby</span><br><span class="line"># 或者使用强制转换主备命测试，注意因为是是测试环境，所以可以强制测试，如果已经在生产环境，请做好fsimage备份，否则可能主备的元数据不同步导致数据丢失。</span><br><span class="line">[root@nn ~]# hdfs haadmin -getAllServiceState                   </span><br><span class="line">nn:9000                                            active    </span><br><span class="line">dn2:9000                                           standby   </span><br><span class="line">[root@nn ~]# hdfs haadmin -transitionToStandby --forcemanual  nn</span><br><span class="line">[root@nn ~]# hdfs haadmin -getAllServiceState                   </span><br><span class="line">nn:9000                                            standby   </span><br><span class="line">dn2:9000                                           active   </span><br></pre></td></tr></table></figure>
<p>可以看到启动NN服务后，nn自身成功转为standby模式。<br>同理，RM的主备切换和恢复的过程跟上述一致，这里不再赘述。</p>
<p>==4）在zookeeper目录下查看hadoop HA建立的znode及其内容==<br>注意：以下说的zk节点是指znode，是一种类似目录的路径，不是指hadoop节点（服务器），注意区分。<br>[root@nn opt]# zkCli.sh </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看zk的根节点/有哪些节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /</span><br><span class="line">[zookeeper, yarn-leader-election, hadoop-ha]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看hadoop-ha节点,可以看到子节点hdapp就是我们在hdfs-site.xml里配置的集群nameservices名称，若有多个集群，那么/hadoop-ha节点将有多个子节点</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 9] ls /hadoop-ha</span><br><span class="line">[hdapp]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 继续查看子节点hdapp是否还有子节点:可以看到这是都active状态节点的信息</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 11] ls /hadoop-ha/hdapp</span><br><span class="line">[ActiveBreadCrumb, ActiveStandbyElectorLock]</span><br><span class="line"><span class="meta">#</span><span class="bash"> ABC是持久节点，ASE是临时节点，nn、dn2都在ASE注册监听临时节点删除事件</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看主备选举的锁节点存放在哪个节点地址</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 7] get /hadoop-ha/hdapp/ActiveStandbyElectorLock</span><br><span class="line"></span><br><span class="line">hdappdn2dn2 �F(�&gt;</span><br><span class="line">***</span><br></pre></td></tr></table></figure>
<p>这里可以看到dn2节点抢到了ActiveStandbyElectorLock，因此作为active节点。<br>ActiveBreadCrumb持久节点用来防止脑裂设计，通过注册事件回调sshfence方法在另外一个节点上kill 掉NN进程<br>具体逻辑参考这篇文章的讨论：<a href="https://www.jianshu.com/p/8a6cc2d72062">文章链接</a>，内容还不错。</p>
<p>同样的yarn-leader-election选举处理逻辑也是借用zk节点特性和注册事件回调方法来实现，大体差不多。</p>
<p>至此负责底层分布式存储的Hadoop HA高可用已经完整实现，这部分是重点和难点，因此占了较大篇幅。此外这里还没给出HA管理员命令的使用以及理解：hdfs haadmin，不过这部分内容一般是hadoop集群运维部负责的工作，作为开发者的我们，也需要了解其中一部分内容。<br>接下来的关于HBase的主备配置则相对简单。</p>
<h4 id="8、HBase的HA配置"><a href="#8、HBase的HA配置" class="headerlink" title="8、HBase的HA配置"></a>8、HBase的HA配置</h4><h5 id="8-1-配置conf"><a href="#8-1-配置conf" class="headerlink" title="8.1  配置conf"></a>8.1  配置conf</h5><p>进入hbase-1.3.1/conf/目录，修改配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/hbase-2.1.7/conf</span><br><span class="line">[root@nn conf]# pwd vi hbase-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> The java implementation to use.  Java 1.8+ required.要求1.8以上的JDK</span></span><br><span class="line">export JAVA_HOME=/opt/jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 禁用HBase自带的Zookeeper，使用独立部署Zookeeper</span></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>
<p>以上配置在三个节点上配置（其实只需在nn和dn2 HMaster节点配置），为了避免以后需将dn1作为主节点时因之前漏了配置导致启动服务各种报错。</p>
<h5 id="8-2-配置hbase-site-xml"><a href="#8-2-配置hbase-site-xml" class="headerlink" title="8.2 配置hbase-site.xml"></a>8.2 配置hbase-site.xml</h5><p>关于hbase-site的详细内容，可以参考:<br>Apache HBase <a href="http://hbase.apache.org/book.html">Getting Started</a>里面内容。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 设置HRegionServers共享的HDFS目录，必须设为在hdfs-site中dfs.nameservices的值：hdapp，而且不能有端口号，该属性会让hmaster在hdfs集群上建一个/hbase的目录 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hdapp/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 启用分布式模式 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 启用分布式模式时，以下的流能力加强需设为false --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定Zookeeper集群位置，值可以是hostname或者hostname:port --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn,dn1,dn2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定独立Zookeeper安装路径 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/zookeeper-3.4.14&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定ZooKeeper集群端口 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>以上配置在三个节点配上</p>
<p>==注意：有部分有关HBaseHA配置技术博客文章中，有人会把hbase.rootdir配成以下形式==：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 直接指定hdfs的主节点</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;nn:9000&#x2F;hbase&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;!-- 直接指定主的HMaster服务 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.master&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;nn:60000&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>接着他们还会在<code>/opt/hbase-2.1.7/conf</code>创建一个文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi backup-master</span><br><span class="line"># 内容为hbase的备机服务器，例如本文的dn2节点</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>这种配法不是hbase HA的方式，是<a href="http://hbase.apache.org/book.html#standalone_dist">官方配置</a>给出的单机模式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">5.1.1. Standalone HBase over HDFS</span><br><span class="line"></span><br><span class="line">A sometimes useful variation on standalone hbase has all daemons running inside the one JVM but rather than persist to the local filesystem, instead they persist to an HDFS instance.</span><br><span class="line">....</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;namenode.example.org:8020&#x2F;hbase&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这种配法容易导致HBase服务退出：一旦nn节点从active状态切换为standby或者宕机，即使dn2对外提供hdfs服务，但hbase只认nn为active状态，并且会提示出错：<br> Operation category READ is not supported in state standby，也即没有可用的hdfs文件服务提供给HMaster进程去读，最后导致hbase异常退出。</p>
<h5 id="8-3-编辑regionservers"><a href="#8-3-编辑regionservers" class="headerlink" title="8.3 编辑regionservers"></a>8.3 编辑regionservers</h5><p>修改regionservers文件，因为当前是使用独立的Zookeeper集群，所以要指定RegionServers所在机器，按规划，三个节点都是RS角色：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/hbase-2.1.7/conf</span><br><span class="line">[root@nn conf]# vi regionservers </span><br><span class="line">nn</span><br><span class="line">dn1</span><br><span class="line">dn2</span><br></pre></td></tr></table></figure>
<p>以上配置在三个节点配上</p>
<h5 id="8-4-创建hdfs-site-xml的软链到hbase的conf目录下"><a href="#8-4-创建hdfs-site-xml的软链到hbase的conf目录下" class="headerlink" title="8.4 创建hdfs-site.xml的软链到hbase的conf目录下"></a>8.4 创建hdfs-site.xml的软链到hbase的conf目录下</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# ln -s /opt/hadoop-3.1.2/etc/hadoop/hdfs-site.xml /opt/hbase-2.1.7/conf/hdfs-site.xml</span><br><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/hbase-2.1.7/conf</span><br><span class="line">[root@nn conf]# ll hdfs-site.xml </span><br><span class="line">lrwxrwxrwx. 1 root root 42 ** hdfs-site.xml -&gt; /opt/hadoop-3.1.2/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>
<p>该操作在三个节点上都要执行，这一环节的配置非常关键，HBase团队也给出相关解释：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Procedure: HDFS Client Configuration</span><br><span class="line">    Of note, if you have made HDFS client configuration changes on your Hadoop cluster, such as configuration directives for HDFS clients, as opposed to server-side configurations, you must use one of the following methods to enable HBase to see and use these configuration changes:</span><br><span class="line">        Add a pointer to your HADOOP_CONF_DIR to the HBASE_CLASSPATH environment variable in hbase-env.sh.</span><br><span class="line">        Add a copy of hdfs-site.xml (or hadoop-site.xml) or, better, symlinks, under $&#123;HBASE_HOME&#125;&#x2F;conf, or</span><br><span class="line">        if only a small set of HDFS client configurations, add them to hbase-site.xml.</span><br><span class="line">An example of such an HDFS client configuration is dfs.replication. If for example, you want to run with a replication factor of 5, HBase will create files with the default of 3 unless you do the above to make the configuration available to HBase.</span><br></pre></td></tr></table></figure>
<p>目的是为了HBase能够同步hdfs配置变化，例如上面提到当hdfs副本数改为5时，如果不创建这种配置映射，那么HBase还是按默认的3份去执行。</p>
<p>若缺少这个软链接，HBase启动集群服务有问题，部分RS无法启动！</p>
<h5 id="8-5-启动HBase集群遇到的问题"><a href="#8-5-启动HBase集群遇到的问题" class="headerlink" title="8.5 启动HBase集群遇到的问题"></a>8.5 启动HBase集群遇到的问题</h5><p>1） HMaster是否有顺序<br>首先，hbase的HA模式是工作在hdfs HA模式下，因此首先保证hdfs HA为正常状态。其次，HMaster无需在hdfs主节点上先启动，在standby节点也可以先启动，但每个HMaster的节点需独立运行start-hbase.sh。</p>
<p>2） 在启动HBase期间，相关出错的解决</p>
<p>A、HMaster进程启动正常，但是提示slf4j jar包存在多重绑定<br><code>SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/opt/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/hbase-2.1.7/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</code><br>解决：其实该提示不影响HMaster和HRegionServer进程，可以选择忽略</p>
<p>B、启动HMaster短暂几秒后异常退出，日志提示找不到相关class：java.lang.NoClassDefFoundError: org/apache/htrace/SamplerBuilder<br>==解决办法==，将<code>htrace-core-3.1.0-incubating.jar </code>拷到lib目录下，<br> <code>$HBASE_HOME/ </code>为环境变量配置HBase路径:<br><code>cp $HBASE_HOME/lib/client-facing-thirdparty/htrace-core-3.1.0-incubating.jar $HBASE_HOME/lib/</code></p>
<p>C、两个节点的HMaster进程都正常运行，但所有HRegionServer进程会自动退出<br>原因：集群服务器之间的时间不同步导致，<br>解决办法：时间做同步</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta"> #</span><span class="bash"> 将硬件时间写到系统时间</span></span><br><span class="line">[root@dn1 ~]# hwclock -s </span><br><span class="line">保存时钟</span><br><span class="line">[root@dn1 ~]# clock -w</span><br></pre></td></tr></table></figure>
<p>或者增加与master之间的时钟误差宽容度（不建议）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;</span><br><span class="line">&lt;value&gt;150000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h5 id="8-6-查看HBase集群信息"><a href="#8-6-查看HBase集群信息" class="headerlink" title="8.6  查看HBase集群信息"></a>8.6  查看HBase集群信息</h5><p>1）首先查看各个节点已经启动的服务<br>nn节点(HMaster、HRegionServer)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# jps</span><br><span class="line">5585 JournalNode</span><br><span class="line">5347 DataNode</span><br><span class="line">23844 NameNode</span><br><span class="line">26839 Jps</span><br><span class="line">6024 ResourceManager</span><br><span class="line">25322 JobHistoryServer</span><br><span class="line">5001 QuorumPeerMain</span><br><span class="line">26026 HMaster</span><br><span class="line">6652 DFSZKFailoverController</span><br><span class="line">6157 NodeManager</span><br><span class="line">26191 HRegionServer</span><br></pre></td></tr></table></figure>
<p>dn1节点(HRegionServer)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn1 opt]# jps</span><br><span class="line">12917 HRegionServer</span><br><span class="line">8074 JournalNode</span><br><span class="line">7979 DataNode</span><br><span class="line">7886 QuorumPeerMain</span><br><span class="line">8191 NodeManager</span><br><span class="line">13183 Jps</span><br></pre></td></tr></table></figure>
<p>dn2节点(HMaster、HRegionServer)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@dn2 conf]# jps</span><br><span class="line">3200 NodeManager</span><br><span class="line">18416 Jps</span><br><span class="line">16339 NameNode</span><br><span class="line">25322 JobHistoryServer</span><br><span class="line">17827 HMaster</span><br><span class="line">2869 DataNode</span><br><span class="line">3973 DFSZKFailoverController</span><br><span class="line">17686 HRegionServer</span><br><span class="line">2698 QuorumPeerMain</span><br><span class="line">2970 JournalNode</span><br></pre></td></tr></table></figure>

<p>2）可通过web页面查看hbase主备情况<br>A、<code>http://nn:16010</code>显示nn为master，dn2为backup master，拥有3个RS<br><img src="https://img-blog.csdnimg.cn/20191020231354724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">B、<code>http://dn2:16010</code>显示dn2为backup master，当前active master为nn节点<br><img src="https://img-blog.csdnimg.cn/20191020231738392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">C、在zookeeper上查看</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# zkCli.sh</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[zookeeper, yarn-leader-election, hadoop-ha, hbase]</span><br></pre></td></tr></table></figure>
<p>以上可以看到hbase znode以及其他znode</p>
<p>继续查看/hbase子路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;hbase    </span><br><span class="line">[meta-region-server, rs, splitWAL,</span><br><span class="line">backup-masters, table-lock, flush-table-proc,</span><br><span class="line">master-maintenance, online-snapshot,</span><br><span class="line">switch, master, running, draining,</span><br><span class="line">namespace, hbaseid, table]</span><br></pre></td></tr></table></figure>
<p>以上可以看出hbase的集群服务器非常依赖zookeeper组件！！</p>
<p>查看hbase节点的RS路径列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] ls &#x2F;hbase&#x2F;rs</span><br><span class="line">[dn2,16020,1571571550293, nn,16020,1571571549066, dn1,16020,1571571535046]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] get &#x2F;hbase&#x2F;rs&#x2F;dn1,16020,1571571535046</span><br><span class="line">�regionserver:16020&amp;�Q��PBU�&#125;�</span><br></pre></td></tr></table></figure>
<p>注意：在zkCli客户端get 相关path的内容因编码问题查看时会显示乱码，可通过hbase web端口查看zk内容<img src="https://img-blog.csdnimg.cn/20191020233357702.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20191020233644573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h5 id="8-7-HBase-主备切换测试"><a href="#8-7-HBase-主备切换测试" class="headerlink" title="8.7 HBase 主备切换测试"></a>8.7 HBase 主备切换测试</h5><p>1）kill掉 nn节点的HMaster，查看dn2是否转为active master</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# jps</span><br><span class="line">26026 HMaster</span><br><span class="line">[root@nn ~]# kill -9 26026</span><br></pre></td></tr></table></figure>
<p>在<code>http://dn2:16010</code>查看主备情况，可以看到nn节点down后，dn2成为master hbase节点<br><img src="https://img-blog.csdnimg.cn/20191021000759842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">2）将nn恢复Hbase服务，查看nn的HMaster是否为backup状态<br>在nn节点上执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn bin]# pwd</span><br><span class="line">/opt/hbase-2.1.7/bin</span><br><span class="line">[root@nn bin]# ./hbase-daemon.sh start master</span><br></pre></td></tr></table></figure>
<p>查看<code>http://nn:16010</code>，可以看到nn节点已经作为backup master，dn2节点为active master<br><img src="https://img-blog.csdnimg.cn/20191021001137709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>http://dn2:16010</code>也可查看。</p>
<p>3）强制转换底层hdfs主备状态，查看hbase HA状态<br>把nn强制变为standby，hbase 主：nn，hbase：dn2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# hdfs haadmin -transitionToStandby --forcemanual  nn</span><br></pre></td></tr></table></figure>
<p>把dn2强制变为standby，hbase 主：nn，hbase：dn2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# hdfs haadmin -transitionToStandby --forcemanual  dn2</span><br></pre></td></tr></table></figure>
<p>hbase的主从状态不受底层hdfs主从变化影响，因为对于hbase来说，它只知道集群hdfs服务： hdfs://hdapp/hbase并没有改变。</p>
<p>至此，本文已经成功搭建了hadoop HA、yarn HA以及HBase HA服务，过程详细，积累不少经验，为之后本人给出大数据应用最核心内容之一——“数据应用开发”铺了很好的基础。</p>
<p>小结：所有服务的启动顺序如下所示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">完整启动顺序</span><br><span class="line">1、分别在三台服务器上启动zookeeper</span><br><span class="line">[root@nn sbin]# zkServer.sh start</span><br><span class="line"># 三个服务器均可看到QuorumPeerMain进程</span><br><span class="line"></span><br><span class="line">2、在nn和dn2主节点上，启动zkfc</span><br><span class="line">[root@nn sbin]# hdfs --daemon start zkfc</span><br><span class="line"># 在nn和dn2主节点上，均可看到DFSZKFailoverController进程</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3、在主nn节点上，运行start-dfs.sh，无需在其他节点再运行该命令</span><br><span class="line">[root@nn sbin]# .&#x2F;start-dfs.sh </span><br><span class="line"># 可以看到NameNode、DataNode、journalnode服务</span><br><span class="line"></span><br><span class="line">4、在主nn节点上，运行start-yarn.sh，无需在其他节点再运行该命令</span><br><span class="line">[root@nn sbin]# .&#x2F;start-yarn.sh </span><br><span class="line"># 可以看到ResourceManager、NodeManager服务</span><br><span class="line"></span><br><span class="line">5、在nn、dn2主节点上，启动JobHistoryServer服务</span><br><span class="line">[root@nn bin]#  mapred --daemon start  historyserver</span><br><span class="line"># jps看到JobHistoryServer服务</span><br><span class="line"></span><br><span class="line">6、在nn、dn2主节点上，启动hbase服务</span><br><span class="line"></span><br><span class="line"># nn节点启动hbase</span><br><span class="line">[root@nn bin]# .&#x2F;start-hbase.sh</span><br><span class="line"></span><br><span class="line"># 在dn2节点上启动hbase，</span><br><span class="line">[root@dn2 bin]# .&#x2F;start-hbase.sh</span><br><span class="line"># HMaster\HRegionServer</span><br></pre></td></tr></table></figure>

<h4 id="9、HBase在hdfs创建的目录"><a href="#9、HBase在hdfs创建的目录" class="headerlink" title="9、HBase在hdfs创建的目录"></a>9、HBase在hdfs创建的目录</h4><p><img src="https://img-blog.csdnimg.cn/201910202357587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">/hbase目录下的内容<br><img src="https://img-blog.csdnimg.cn/20191021000048443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">关于这些文件的解释以及作用，将在下一篇HBase架构原理给出。</p>
<h4 id="10、在HBase创建table测试"><a href="#10、在HBase创建table测试" class="headerlink" title="10、在HBase创建table测试"></a>10、在HBase创建table测试</h4><p>这部分内容回到大家相对熟悉的数据库知识领域，本节内容仅提供基础demo用法，关于HBase的数据结构以及架构原理，本博客将在另外一篇文章进行深入讨论。<br>以下company表为例进行基本操作，该表包含staff_info和depart_info两个列簇，表结构如下所示：<br><img src="https://img-blog.csdnimg.cn/20191023223724743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">以下为基本的hbase使用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入HBase进行交互式操作</span></span><br><span class="line">[root@nn ~] hbase shell</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建company表，从这里即可看出列数据库的优势，无需预先定义列，表结构是松散灵活的，之后想加多少列都行。</span></span><br><span class="line">hbase(main):&gt; create &#x27;company&#x27;,&#x27;staff_info&#x27;,&#x27;depart_info&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前HBase有哪些表</span></span><br><span class="line">hbase(main):&gt; list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">获得表company的描述信息</span></span><br><span class="line">hbase(main):&gt; describe &#x27;company&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看全表数据,相当于selec * from company</span></span><br><span class="line">hbase(main):&gt; scan &#x27;t_user&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 往表插入数据，语法 put  <span class="string">&#x27;t&#x27;</span> ,<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;v&#x27;</span>  (表名，行rowkey，列簇：具体列，值)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第1行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:name&#x27;,&#x27;Aery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:age&#x27;,&#x27;25&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:sex&#x27;,&#x27;Male&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:name&#x27;,&#x27;Develop&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:level&#x27;,&#x27;10&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;109&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第2行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:name&#x27;,&#x27;Bery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:age&#x27;,&#x27;23&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;staff_info:sex&#x27;,&#x27;Female&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:name&#x27;,&#x27;HR&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:level&#x27;,&#x27;9&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R1&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;108&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第3行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:name&#x27;,&#x27;Cery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:age&#x27;,&#x27;26&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:name&#x27;,&#x27;Market&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:level&#x27;,&#x27;9&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;107&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 第4行记录</span></span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:name&#x27;,&#x27;Dery&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:age&#x27;,&#x27;27&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;staff_info:sex&#x27;,&#x27;Male&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:name&#x27;,&#x27;Finance&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:level&#x27;,&#x27;9&#x27;</span><br><span class="line">hbase(main):&gt; put &#x27;company&#x27; ,&#x27;R2&#x27;,&#x27;depart_info:inner_tel&#x27;,&#x27;106&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取数据，有以下几种方式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里cell已经包含timestamp和value</span></span><br><span class="line">hbase(main):038:0&gt; get &#x27;company&#x27;,&#x27;R1&#x27;</span><br><span class="line">COLUMN              cell                                                         </span><br><span class="line"> depart_info:inner_tel  timestamp=**, value=108                              </span><br><span class="line"> depart_info:level      timestamp=**, value=9                                </span><br><span class="line"> depart_info:name       timestamp=**, value=HR                               </span><br><span class="line"> staff_info:age         timestamp=**, value=23                               </span><br><span class="line"> staff_info:name        timestamp=**, value=Bery                             </span><br><span class="line"> staff_info:sex         timestamp=**, value=Female                           </span><br><span class="line">1 row(s)</span><br><span class="line">Took 0.0427 seconds    </span><br><span class="line">                        </span><br><span class="line"></span><br><span class="line">hbase(main):039:0&gt; get &#x27;company&#x27;,&#x27;R2&#x27;,&#x27;staff_info&#x27;</span><br><span class="line">COLUMN                  CELL  </span><br><span class="line"> staff_info:age         timestamp=**  value=27                               </span><br><span class="line"> staff_info:name        timestamp=**, value=Dery                             </span><br><span class="line"> staff_info:sex         timestamp=**, value=Male                             </span><br><span class="line">1 row(s)</span><br><span class="line"></span><br><span class="line">hbase(main):&gt; get &#x27;company&#x27;,&#x27;R1&#x27;,&#x27;staff_info:age&#x27;</span><br><span class="line"></span><br><span class="line">COLUMN                  CELL                                                          </span><br><span class="line"> staff_info:age         timestamp=**, value=23                               </span><br><span class="line">1 row(s)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 计算company rowkey数目</span></span><br><span class="line">hbase(main):041:0&gt; count &#x27;company&#x27;</span><br><span class="line">2 row(s)</span><br><span class="line">Took 0.1696 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除R2中的部门信息的level列                                                                    hbase(main):072:0&gt; delete <span class="string">&#x27;company&#x27;</span>,<span class="string">&#x27;R2&#x27;</span>,<span class="string">&#x27;depart_info:level&#x27;</span></span> </span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空表内容</span></span><br><span class="line"><span class="meta">hbase(main)&gt;</span><span class="bash"> truncate <span class="string">&#x27;company&#x27;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 禁用表，之后drop表才有效</span></span><br><span class="line">hbase(main):&gt; disable &#x27;company&#x27;</span><br><span class="line">hbase(main):&gt; drop &#x27;company&#x27;</span><br><span class="line">hbase(main):&gt; exists &#x27;company&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>company表的信息也可以在HMaster web端查看<br><img src="https://img-blog.csdnimg.cn/20191024003258472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">     &#8195;&#8195;以上为基本的hbase表操作，如果用shell的方式开发hbase数据应用，效率是非常低的，就像直接在mysql的shell写复杂sql、在Oracle的shell写sql那样，虽然很raw，但不友好。而对于mysql的sql开发，大家会用workbench或者DBeaver；对于Oracle的开发，大家会用PL/SQL Developer；在程序业务逻辑开发层面，大家会引入DB-API 第三方库实现业务逻辑开发。<br>那么对于HBase分布式数据库的开发，需要用到Hive工具。<br>&#8195;&#8195;Hive是建立在 Hadoop 上的数据仓库基础构架，它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），通过Hive，用户可以类 SQL 查询语言（又称 HQL）去“操作Hbase上的数据”，省去独自开发mapper 和 reducer 来处理计算任务（当然复杂的业务逻辑还是需要开发mapper和reducer）。<br>&#8195;&#8195;本博客将在后面的文章中，引入Hive组件，配合HBase进行某个主题的大数据实际项目开发。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>Hadoop集群</tag>
        <tag>HBase集群</tag>
      </tags>
  </entry>
  <entry>
    <title>基于PySpark和ALS算法实现基本的电影推荐流程</title>
    <url>/blog/2020/01/11/%E5%9F%BA%E4%BA%8EPySpark%E5%92%8CALS%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;本文内容第一部分给出Pyspark常见算子的用法，第二部分则参考书籍《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。本文内容为大数据实时分析项目提供基本的入门知识。</p>
<h4 id="1、PySpark简介"><a href="#1、PySpark简介" class="headerlink" title="1、PySpark简介"></a>1、PySpark简介</h4><p>&#8195;&#8195;本节内容的图文一部分参考了这篇文章<a href="http://sharkdtu.com/posts/pyspark-internal.html">《PySpark 的背后原理 》</a>，个人欣赏此博客作者，博文质量高，看完受益匪浅！Spark的内容不再累赘，可参考本博客<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》</a>。PySpark的工作原理图示如下：<br><img src="https://img-blog.csdnimg.cn/20200108220627968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<a id="more"></a>

<p>&#8195;&#8195;在这里，Py4J 是一个用 Python 和 Java 编写的库，它可以让Python代码实现动态访问JVM的Java对象，同时JVM也能够回调 Python对象。因此PySpark就是在Spark外围包装一层Python API，借助Py4j实现Python和Java的交互（这里的交互就是通过socket实现，传字节码），进而实现通过Python编写Spark应用程序。<br><img src="https://img-blog.csdnimg.cn/20200108215814813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;在Driver端，PySparkContext通过Py4J启动一个JVM并产生一个JavaSparkContext；在Executor端，则不需要借助Py4j，因为Executor端运行的是由Driver传过来的Task业务逻辑（其实就是java的字节码）。</p>
<h4 id="2、Pyspark接口用法"><a href="#2、Pyspark接口用法" class="headerlink" title="2、Pyspark接口用法"></a>2、Pyspark接口用法</h4><h5 id="读取数据源"><a href="#读取数据源" class="headerlink" title="读取数据源"></a>读取数据源</h5><p>PySpark支持多种数据源读取，常见接口如下： </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc.pickleFile() <span class="comment"># &lt;class &#x27;pyspark.rdd.RDD&#x27;&gt;</span></span><br><span class="line">sc.textFile() <span class="comment"># &lt;class &#x27;pyspark.rdd.RDD&#x27;&gt;</span></span><br><span class="line">spark.read.json() <span class="comment"># &lt;class &#x27;pyspark.sql.dataframe.DataFrame&#x27;&gt;</span></span><br><span class="line">spark.read.text() <span class="comment"># &lt;class &#x27;pyspark.sql.dataframe.DataFrame&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>例如读取本地要注意，格式为<code>file://+文件绝对路径</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;file:///home/mparsian/dna_seq.txt&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取hdfs上文件数据</span></span><br><span class="line">sc.textFile(<span class="string">&quot;your_hadoop/data/moves.txt&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h5><p>Spark的算子分为两类：Transformation和Action。<br>Transformation仅仅是定义逻辑，并不会立即执行，有lazy特性，目的是将一个RDD转为新的RDD，可以基于RDDs形成lineage（DAG图）；<br>Action：触发Job运行，真正触发driver运行job；</p>
<p><strong>第一类算子：Transformation</strong></p>
<ul>
<li>map(func): 返回一个新的RDD，func会作用于每个map的key，例如在wordcount例子要<code>rdd.map(lambda a, (a, 1))</code>将数据转换成(a, 1)的形式以便之后做reduce<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">   )</span><br><span class="line">word_map_rdd = word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w, <span class="number">1</span>))</span><br><span class="line">mapping = word_map_rdd.collect()</span><br><span class="line">print(mapping)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;pyspark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li> mappartitions(func, partition):  Return a new RDD by applying a function to each partition of this RDD.和map不同的地方在于map的func应用于每个元素，而这里的func会应用于每个分区，能够有效减少调用开销，减少func初始化次数。减少了初始化的内存开销。<br>例如将一个数据集合分成2个区，再对每个区进行累加，该方法适合对超大数据集合的分区累加处理，例如有1亿个item，分成100个分区，有10台服务器，那么每台服务器就可以负责自己10个分区的数据累加处理。<br>官方也提到mappartitions中如果一个分区太大，一次计算的话可能直接导致内存溢出。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">10</span>, <span class="number">22</span>, <span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">each_partition</span>):</span> </span><br><span class="line"><span class="keyword">yield</span> <span class="built_in">sum</span>(each_partition)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[[<span class="number">10</span>, <span class="number">22</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">rdd.mapPartitions(f).glom().collect()</span><br><span class="line">[[<span class="number">32</span>], [<span class="number">7</span>]]</span><br></pre></td></tr></table></figure>




<ul>
<li><p>filter(func): 返回一个新的RDD，func会作用于每个map的key，用于筛选数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize ([<span class="string">&quot;fooo&quot;</span>, <span class="string">&quot;bbbar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;Aoo&quot;</span>])</span><br><span class="line">rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="string">&#x27;foo&#x27;</span> <span class="keyword">in</span> x).collect()</span><br><span class="line"><span class="comment"># [&#x27;fooo&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li>flatMap(func): 返回一个新的RDD，func用在每个item，并把item切分为多个元素返回，例如wordcount例子的分类<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize ([<span class="string">&quot;this is pyspark&quot;</span>, <span class="string">&quot;this is spark&quot;</span>])</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27; &#x27;</span>)).collect()</span><br><span class="line"><span class="comment">#可以看到每个item为一句话，经过func后，分解为多个单词（多个元素）</span></span><br><span class="line"><span class="comment"># [&#x27;this&#x27;, &#x27;is&#x27;, &#x27;pyspark&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;spark&#x27;]</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize ((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> x:(<span class="number">2</span>*x,<span class="number">3</span>*x)).collect()</span><br><span class="line"><span class="comment"># 对原来每个item分别乘2乘3，func返回两个item</span></span><br><span class="line"><span class="comment"># [2, 3, 4, 6, 6, 9]</span></span><br></pre></td></tr></table></figure>


<ul>
<li>flatMapValues(func)：flatMapValues类似于mapValues，不同的在于flatMapValues应用于元素为key-value对的RDD中Value。每个一kv对的Value被输入函数映射为一系列的值，然后这些值再与原RDD中的Key组成一系列新的KV对。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;aoo&quot;</span>]), (<span class="string">&quot;age&quot;</span>, [<span class="string">&quot;12&quot;</span>, <span class="string">&quot;20&quot;</span>])])</span><br><span class="line">rdd.flatMapValues(<span class="keyword">lambda</span> x:x).collect()</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;aoo&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;12&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;20&#x27;</span>)]</span><br></pre></td></tr></table></figure>



<ul>
<li><p>mapValues(func): 返回一个新的RDD，对RDD中的每一个value应用函数func。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;aoo&quot;</span>]), (<span class="string">&quot;age&quot;</span>, [<span class="string">&quot;12&quot;</span>, <span class="string">&quot;20&quot;</span>])])</span><br><span class="line">rdd.mapValues(<span class="keyword">lambda</span> value:<span class="built_in">len</span>(value)).collect()</span><br><span class="line"><span class="comment"># [(&#x27;name&#x27;, 3), (&#x27;age&#x27;, 2)]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>distinct(): 去除重复的元素</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.distinct().collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 10), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>subtractByKey(other): 删除在RDD1与RDD2的key相同的项</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.subtractByKey(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>

<ul>
<li>subtract(other): 取差集<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.subtract(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li>intersection(other): 交集运算，保留在两个RDD中都有的元素</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.intersection(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 10)]</span></span><br></pre></td></tr></table></figure>

<p>有关key-value类型的处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="comment"># 取出所有item的key</span></span><br><span class="line">rdd.keys().collect() <span class="comment"># [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;]</span></span><br><span class="line"><span class="comment"># 取出所有的values</span></span><br><span class="line">rdd.values().collect() <span class="comment"># [1, 10, 1, 1]</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>foldByKey</code>(<em>zeroValue</em>, <em>func</em>, <em>numPartitions=None</em>)</p>
<p>Merge the values for each  key using an associative function “func” and a neutral “zeroValue” which  may be added to the result an arbitrary number of times, and must not  change the result (e.g., 0 for addition, or  1 for multiplication.).<br>其实foldByKey也像reduceBykey，对同一key中的value进行合并，例如对相同key进行value累加，zeroValue=0表示累加：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.foldByKey(<span class="number">0</span>, <span class="keyword">lambda</span> x,y:x+y).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对相同key进行value累乘，注意zeroValue=1代表累乘：</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line">rdd.foldByKey(<span class="number">1</span>, <span class="keyword">lambda</span> x,y:x*y).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 4), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>


<ul>
<li>groupByKey(numPartitions=None): 将(K, V)数据集上所有Key相同的数据聚合到一起，得到的结果是(K, (V1, V2…))<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">len</span>).collect())</span><br><span class="line"><span class="comment"># 统计数据集每个key的个数总和</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 3), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">list</span>).collect())</span><br><span class="line"><span class="comment"># 将每个key的v聚合到一个list里面</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, [1, 10, 1]), (&#x27;b&#x27;, [1])]</span></span><br></pre></td></tr></table></figure>

<ul>
<li>reduceByKey(func, numPartitions=None):此算子最常用， 将(K,  V)数据集上所有Key相同的数据聚合到一起，func的参数即是每两个K-V中的V。可以使用这个函数来进行计数，例如reduceByKey(lambda  a,b:a+b)就是将key相同数据的Value进行相加。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;foo&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;bar&quot;</span>, <span class="number">3</span>)])</span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> x, y : x + y).collect() <span class="comment"># [(&#x27;foo&#x27;, 3), (&#x27;bar&#x27;, 3)]  </span></span><br><span class="line">x.reduceByKey(<span class="built_in">max</span>).collect() <span class="comment">#  [(&#x27;foo&#x27;, 2), (&#x27;bar&#x27;, 3)]</span></span><br></pre></td></tr></table></figure>



<ul>
<li>join(other, numPartitions=None): 将(K, V)和(K, W)类型的数据进行JOIN操作，得到的结果是这样(K, (V, W))</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;bar&quot;</span>, <span class="number">10</span>) , (<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;bar&quot;</span>, <span class="number">12</span>) , (<span class="string">&quot;foo&quot;</span>, <span class="number">12</span>)])</span><br><span class="line">rdd1.join(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;bar&#x27;, (10, 12)), (&#x27;foo&#x27;, (1, 12))]</span></span><br></pre></td></tr></table></figure>


<ul>
<li>union(other): 并集运算，合并两个RDD</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rdd1 &#x3D; sc.parallelize([(&quot;a&quot;, 10) ,(&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">rdd2 &#x3D; sc.parallelize([(&quot;a&quot;, 10) ,(&quot;c&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">rdd1.union(rdd2).collect()</span><br><span class="line"># [(&#39;a&#39;, 10), (&#39;b&#39;, 1), (&#39;a&#39;, 1), (&#39;a&#39;, 10), (&#39;c&#39;, 1), (&#39;a&#39;, 1)]</span><br></pre></td></tr></table></figure>
<p>还有更多的transmission算子这里不再一一列举，可以参考官网PySpark API文档。</p>
<p>第二类算子：Action</p>
<ul>
<li><p>collect(): 以数组的形式，返回数据集中所有的元素。在数据探索阶段常用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line">word_map_rdd = word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w, <span class="number">1</span>))</span><br><span class="line">word_map_rdd.collect()</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[(<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;pyspark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>collectAsMap将k-v数据rdd集合转为python字典类型，同一key的项，只取第一项，其他的项被忽略</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.collectAsMap() <span class="comment"># &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure></li>
<li><p>count(): 返回数据集中元素的个数</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line">word_rdd.count() <span class="comment"># 8</span></span><br></pre></td></tr></table></figure>

<ul>
<li>take(n): 返回数据集的前N个元素</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">word_rdd.take(<span class="number">3</span>) <span class="comment"># [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure>

<ul>
<li>takeOrdered(n): 升序排列，取出前N个元素<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;zoo&quot;</span>, <span class="string">&quot;aoo&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>) <span class="comment"># [&#x27;aoo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li>takeOrdered(n, key=lambda num: -num): 降序排列，取出前N个元素<br>key=lambda num: -num只适用数值型的rdd，其实就将每项数值变为负数再排列</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>).takeOrdered(<span class="number">3</span>,key=<span class="keyword">lambda</span> num:-num)</span><br><span class="line">print(rdd)</span><br></pre></td></tr></table></figure>
<p>字符串的rdd排序，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;fooo&quot;</span>, <span class="string">&quot;bbbar&quot;</span>, <span class="string">&quot;ffoo&quot;</span>, <span class="string">&quot;zoo&quot;</span>, <span class="string">&quot;aoo&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按字符长度降序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>,key=<span class="keyword">lambda</span> item:-<span class="built_in">len</span>(item))</span><br><span class="line"><span class="comment"># 按字符长度升序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>,key=<span class="built_in">len</span>)</span><br><span class="line"><span class="comment">#按字母升序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ul>
<li><p>countByKey(): 对同一key值累计其计数，例如wordcount</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;bar&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.countByKey().items()</span><br><span class="line"><span class="comment"># dict_items([(&#x27;foo&#x27;, 2), (&#x27;bar&#x27;, 1)])以元组的方式返回</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>countByValue():对值分组统计</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">9</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">rdd.countByValue().items()</span><br><span class="line"><span class="comment"># dict_items([(9, 2), (10, 3)])</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li><p>Persistence(持久化)<br>persist(): 将数据按默认的方式进行持久化<br> unpersist(): 取消持久化<br>saveAsTextFile(path): 将数据集保存至文件</p>
</li>
<li><p>创建rdd对象时指定分区，<br><code>parallelize(c, numSlices=None)</code><br>对每个元素都分区</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc.parallelize([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>], <span class="number">5</span>).glom().collect()</span><br><span class="line"><span class="comment"># [[0], [2], [3], [4], [6]]</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>glom方法：Return an RDD created by coalescing all elements within each partition into a list<br>指定两个分区</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line">[[<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]]</span><br></pre></td></tr></table></figure>



<ul>
<li>广播rdd<br>给定一个key为id的字段数据集合，给定其id，求字段对应的value</li>
</ul>
<p>非广播方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apples = sc.parallelize([(<span class="number">1</span>, <span class="string">&#x27;iPhone X&#x27;</span>),(<span class="number">2</span>, <span class="string">&#x27;iPhone 8&#x27;</span>),(<span class="number">5</span>, <span class="string">&#x27;iPhone 11&#x27;</span>)])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将该数据集合转为字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apples_dict=apples.collectAsMap()</span><br><span class="line"><span class="comment"># &#123;1: &#x27;iPhone X&#x27;, 2: &#x27;iPhone 8&#x27;, 5: &#x27;iPhone 11&#x27;&#125;</span></span><br></pre></td></tr></table></figure>

<p>给定id集合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids = sc.parallelize([<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>通过map方法取出ids对应的value</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:apples_dict[x]).collect()</span><br><span class="line"><span class="comment"># [&#x27;iPhone 8&#x27;, &#x27;iPhone X&#x27;, &#x27;iPhone 11&#x27;]</span></span><br></pre></td></tr></table></figure>

<p>这种方式，在ids与apples_dict之间的映射转换，每一个id查找映射，都需要将ids和apples_dict传到worker节点上计算，如果有100万个id，而且apples_dict是个超大字典，那么就需要进行100万次上传worker再计算结果，显然效率极低，也不合理。</p>
<p>使用广播方式可避免这种情况<br>将apples_dict转为广播变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">apples_dict_bc=sc.broadcast(apples_dict)</span><br><span class="line">print(<span class="built_in">type</span>(apples_dict_bc))</span><br><span class="line"><span class="comment"># &lt;class &#x27;pyspark.broadcast.Broadcast&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>给定id集合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids = sc.parallelize([<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>id对应的value，使用apples_dict_bc.value[x]这个广播变量，获取id对应的value</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ids.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:apples_dict_bc.value[x]).collect()</span><br><span class="line"><span class="comment"># [&#x27;iPhone 8&#x27;, &#x27;iPhone X&#x27;, &#x27;iPhone 11&#x27;]</span></span><br></pre></td></tr></table></figure>

<p>在开始计算时，apples_dict_bc会传到worker node的内存上（如果数据集合太大，有部分数据则存在磁盘）。之后worker 可以一直使用这个“常驻内存广播变量”处理映射任务，即使有100万个id，客户端只需要把id传到worker即可，这个大apples_dict_bc数据集合则无需再传送到worker，大大减少时间。</p>
<ul>
<li>累加器accumulator：</li>
</ul>
<p>创建测试数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>创建accumulator累加器total，用于累加数集合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total=sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>创建accumulator累加器counter，用于计数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">counter=sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>使用foreach，对每一项都使用total累计该元素的值，counter累加已处理的元素个数，注意：counter这个accumulator变量是自增1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd.foreach(<span class="keyword">lambda</span> item:[total.add(item),counter.add(<span class="number">1</span>)])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">total.value # 15.0</span><br><span class="line">counter.value 5</span><br></pre></td></tr></table></figure>

<h5 id="完整的wordcount示例"><a href="#完整的wordcount示例" class="headerlink" title="完整的wordcount示例"></a>完整的wordcount示例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="keyword">as</span> sc</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_spark_context</span>()</span></span><br><span class="line">    conf=SparkConf().setAppName(&quot;word_count&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    spark_context=sc.getOrCreate(conf)    </span><br><span class="line">    <span class="keyword">return</span> spark_context</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_count</span>(<span class="params">spark_sc,input_file,output_dir,delimiter=<span class="string">&#x27; &#x27;</span></span>):</span></span><br><span class="line">    data_rdd=spark_sc.textFile(input_file) <span class="comment"># </span></span><br><span class="line">    word_rdd=text_rdd.flatMap(<span class="keyword">lambda</span> line:line.split(delimiter))</span><br><span class="line">    count_rdd=word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> v1,v2:v1+v2)</span><br><span class="line">    count_rdd.saveAsTextFile(output_dir) <span class="comment">#注意这里参数为文件夹 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    sc_obj=create_spark_context()</span><br><span class="line">    word_count(sc_obj,<span class="string">&quot;file:///opt/data.txt&quot;</span>,<span class="string">&quot;file:///opt/word_count_output&quot;</span>)</span><br></pre></td></tr></table></figure>


<p>查看存放的输出结果，计算结果的输出文件放在part-00000这个文件，而_SUCCESS文件是无内容的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[root@nn opt]<span class="comment"># ls word_count_output/</span></span><br><span class="line">part-<span class="number">00000</span>  _SUCCESS</span><br><span class="line"></span><br><span class="line">[root@nn word_count_output]<span class="comment"># cat part-00000 </span></span><br><span class="line">(<span class="string">&#x27;linux&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;is&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;the&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;best&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;centos&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;macos&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;redhat&#x27;</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<h4 id="3、基于PySpark和ALS的电影推荐流程"><a href="#3、基于PySpark和ALS的电影推荐流程" class="headerlink" title="3、基于PySpark和ALS的电影推荐流程"></a>3、基于PySpark和ALS的电影推荐流程</h4><p>&#8195;&#8195;本节内容参考书籍pdf版本《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。<br>&#8195;&#8195;(有一点需要指出的是：该书的作者似乎为出书而出书，在前面十来章内容，冗长且基础，大量截图以及table，其实大部分内容可言简意赅。但他们似乎为了出书为了销量，需把这本书打造“很厚，页数多，专业技术书籍”的印象，但其精华只有后面关于pyspark.mllib机器学习示例的内容。)</p>
<h5 id="数据集背景"><a href="#数据集背景" class="headerlink" title="数据集背景"></a>数据集背景</h5><p>数据源：<code>https://grouplens.org/datasets/movielens/</code><br>这里有非常详细的电影训练数据，适合项目练手<br>数据信息：<br>MovieLens 100K<br>movie ratings.<br>Stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies</p>
<p>数据样例结构：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ml-100k]# ls</span><br><span class="line">allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base  u.genre  u.occupation</span><br><span class="line">mku.sh     u1.test  u3.base  u4.test  ua.base  ub.test  u.info   u.user</span><br><span class="line">README     u2.base  u3.test  u5.base  ua.test  u.data   u.item</span><br></pre></td></tr></table></figure>

<p>有关数据结构的说明，可以查看README文件，例如u.data:4个字段，user id | item id | rating | timestamp.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">196     242     3       881250949</span><br><span class="line">186     302     3       891717742</span><br></pre></td></tr></table></figure>

<h5 id="读取用户数据"><a href="#读取用户数据" class="headerlink" title="读取用户数据"></a>读取用户数据</h5><p>探索基本数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_rdd=sc.textFile(<span class="string">&quot;file:///opt/ml-100k/u.data&quot;</span>)</span><br><span class="line">user_rdd.count()<span class="comment"># 100000</span></span><br><span class="line">user_rdd.first() <span class="comment"># &#x27;196\t242\t3\t881250949&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因ALS入参为3个字段，故只需取出user_rdd前3个字段的:用户id，产品id以及评分:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_rating_rdd=user_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27;\t&#x27;</span>)[:<span class="number">3</span>]) <span class="comment"># 每行分割后为一个包含4个元素的列表，取前3项即可</span></span><br><span class="line">raw_rating_rdd.take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[[<span class="string">&#x27;196&#x27;</span>, <span class="string">&#x27;242&#x27;</span>, <span class="string">&#x27;3&#x27;</span>],[<span class="string">&#x27;186&#x27;</span>, <span class="string">&#x27;302&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]] <span class="comment"># 注意，每个item是列表</span></span><br></pre></td></tr></table></figure>

<p>ALS训练数据格式的入参为一组元组类型的数据：Rating(user,product,rating)，过还需做以下转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rating_rdd=raw_rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]))<span class="comment"># x[0],x[1],x[2]对应用户id，电影id，评分</span></span><br><span class="line">rating_rdd.take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;196&#x27;</span>, <span class="string">&#x27;242&#x27;</span>, <span class="string">&#x27;3&#x27;</span>), (<span class="string">&#x27;186&#x27;</span>, <span class="string">&#x27;302&#x27;</span>, <span class="string">&#x27;3&#x27;</span>)]<span class="comment"># rdd的每个item为元组类型</span></span><br></pre></td></tr></table></figure>

<p>查看不重复的用户总量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_users=rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>]).distinct().count()</span><br><span class="line">total_users <span class="comment"># 943</span></span><br></pre></td></tr></table></figure>

<p>查看不重复的电影总量（同上）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">total_moves&#x3D;rating_rdd.map(lambda x:x[1]).distinct().count()</span><br><span class="line">total_moves # 1682</span><br></pre></td></tr></table></figure>

<h5 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h5><p>大致处理流程：读取文件=&gt;user_rdd=&gt;raw_rating_rdd=&gt;rating_rdd，这里rating_rdd的格式就是ALS训练数据的格式Rating(user,product,rating)，然后再用ALS.train，训练结束后，就会创建模型对象MatrixFactorizationModel</p>
<p><strong>这里简单介绍ALS算法</strong>：Alternating Least Squares matrix factorization，其实就是（交替）最小二乘法，这里为何使用ALS？因为它同时考虑了User和Item两个方面，即即可基于用户进行推荐又可基于物品，所以适合推荐型的场景，模型一般如下：<br><img src="https://img-blog.csdnimg.cn/20200109205253156.png" alt="Am×n=Um×k×Vk×n"><br>原始协同矩阵是一个<code>m*n</code>的矩阵，是由m<em>k和k</em>n两个矩阵相乘得到的，其中k&lt;&lt;m,n，U表示用户矩阵，V表示商品矩阵，k为U、V矩阵的的秩。学过线性代数应该知道<code>A*B=C</code>，两个矩阵相乘的结果，这就是所谓协同矩阵。<br><img src="https://img-blog.csdnimg.cn/20200109204624664.png" alt="在这里插入图片描述"><br>协同推荐就等同于<code>C=A*B</code>矩阵分解，矩阵分解（协同推荐矩阵是一个稀疏矩阵，因为不是所有的用户都对产品评分）最终又可以转换成了一个优化问题。将用户u对商品V的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。在这个矩阵分解的训练过程中，评分缺失项得到了填充，那么这个填充的项就可以根据用户ID进行推荐。<br>更详细内容可以参考这两篇文章：<a href="https://blog.csdn.net/YMPzUELX3AIAp7Q/article/details/85241209">文章1</a>、<a href="https://www.cnblogs.com/xiguage119/p/10813393.html">文章2</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"><span class="comment"># 注意ALS算法是基于矩阵运算，因此需要环境安装numpy库</span></span><br></pre></td></tr></table></figure>

<p><code>ALS.train(ratings,rank,iterations=5,lambda_=0.01)</code><br>ratings:训练数据集合，就是上面提到的Rating(user,product,rating)，也即是rating_rdd这个经过预处理的数据集</p>
<p>一句完成训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=ALS.train(rating_rdd,<span class="number">10</span>,<span class="number">10</span>,<span class="number">0.01</span>)</span><br><span class="line">model<span class="comment"># &lt;pyspark.mllib.recommendation.MatrixFactorizationModel at 0x7f3159bc8048&gt;</span></span><br></pre></td></tr></table></figure>

<p>该模型对象有几个属性：<br>model.rank # 10 分解为稀疏矩阵的秩<br>userFeatures 为分解后的用户矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.userFeatures().take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="number">1</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.7229161262512207</span>, <span class="number">0.036963045597076416</span>, <span class="number">0.23517486453056335</span>, -<span class="number">0.18118669092655182</span>, -<span class="number">1.4776617288589478</span>, -<span class="number">1.0425325632095337</span>, <span class="number">0.3823653757572174</span>, -<span class="number">0.3569445312023163</span>, -<span class="number">0.2874303162097931</span>, <span class="number">0.0020452593453228474</span>])),</span><br><span class="line"> (<span class="number">2</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.3199065327644348</span>, <span class="number">0.41293472051620483</span>, <span class="number">0.12430011481046677</span>, -<span class="number">0.42582616209983826</span>, -<span class="number">0.4546814560890198</span>, -<span class="number">1.496929407119751</span>, <span class="number">0.6246935725212097</span>, <span class="number">0.49794384837150574</span>, -<span class="number">0.3813674747943878</span>, <span class="number">0.7599969506263733</span>]))]</span><br></pre></td></tr></table></figure>

<p>productFeatures为分解后的电影（产品）矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.productFeatures().take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="number">1</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.9663546681404114</span>, <span class="number">0.0724567249417305</span>, <span class="number">0.22562265396118164</span>, -<span class="number">0.14772379398345947</span>, -<span class="number">1.3601692914962769</span>, -<span class="number">1.1434344053268433</span>, <span class="number">1.0299423933029175</span>, -<span class="number">0.17817920446395874</span>, -<span class="number">1.0483288764953613</span>, <span class="number">0.4326847195625305</span>])),</span><br><span class="line"> (<span class="number">2</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.701686441898346</span>, -<span class="number">0.44971194863319397</span>, <span class="number">0.36079081892967224</span>, -<span class="number">0.1727607101202011</span>, -<span class="number">0.4821830689907074</span>, -<span class="number">1.1037342548370361</span>, <span class="number">0.8413264155387878</span>, -<span class="number">0.08249323815107346</span>, -<span class="number">1.0539320707321167</span>, <span class="number">0.6040329337120056</span>]))]</span><br></pre></td></tr></table></figure>

<h5 id="调用已训练的模型"><a href="#调用已训练的模型" class="headerlink" title="调用已训练的模型"></a>调用已训练的模型</h5><p>model已经封装好几个常用的方法，api使用简便</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Signature: model.recommendProducts(user, num)</span><br><span class="line">Docstring:</span><br><span class="line">Recommends the top &quot;num&quot; number of products for a given user and</span><br><span class="line">returns a list of Rating objects sorted by the predicted rating in</span><br><span class="line">descending order.</span><br></pre></td></tr></table></figure>

<p>例如给用户199推荐前5部电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.recommendProducts(<span class="number">199</span>,<span class="number">5</span>)</span><br><span class="line">[Rating(user=<span class="number">199</span>, product=<span class="number">854</span>, rating=<span class="number">10.774026140227157</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">962</span>, rating=<span class="number">9.30074590770409</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">1176</span>, rating=<span class="number">8.813180359193545</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">1280</span>, rating=<span class="number">8.11317788460314</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">718</span>, rating=<span class="number">7.8722593701756995</span>)]</span><br></pre></td></tr></table></figure>

<p>这个结果表示，rating值越大，越排在越前面，代表更为优先推荐，首先推荐给用户199的为854这部电影<br>根据用户ID:199和电影ID:854，查询预测评分:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict(<span class="number">199</span>,<span class="number">854</span>) <span class="comment"># 10.774026140227157</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用用得更多的场合是：将某部电影推荐给感兴趣的用户，可通过model.recommendUsers得出这些用户，例如，将电影ID为154，推荐给前10个用户</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.recommendUsers(<span class="number">154</span>,<span class="number">10</span>)</span><br><span class="line">输出：</span><br><span class="line">[Rating(user=<span class="number">133</span>, product=<span class="number">154</span>, rating=<span class="number">6.346890714591231</span>),</span><br><span class="line"> Rating(user=<span class="number">866</span>, product=<span class="number">154</span>, rating=<span class="number">6.10978058348641</span>),</span><br><span class="line"> Rating(user=<span class="number">50</span>, product=<span class="number">154</span>, rating=<span class="number">6.018355541192427</span>),</span><br><span class="line"> Rating(user=<span class="number">783</span>, product=<span class="number">154</span>, rating=<span class="number">5.991043569104054</span>),</span><br><span class="line"> Rating(user=<span class="number">310</span>, product=<span class="number">154</span>, rating=<span class="number">5.658875199814674</span>),</span><br><span class="line"> Rating(user=<span class="number">809</span>, product=<span class="number">154</span>, rating=<span class="number">5.636975519395109</span>),</span><br><span class="line"> Rating(user=<span class="number">78</span>, product=<span class="number">154</span>, rating=<span class="number">5.4898250475467725</span>),</span><br><span class="line"> Rating(user=<span class="number">762</span>, product=<span class="number">154</span>, rating=<span class="number">5.47223950904501</span>),</span><br><span class="line"> Rating(user=<span class="number">273</span>, product=<span class="number">154</span>, rating=<span class="number">5.318862413529849</span>),</span><br><span class="line"> Rating(user=<span class="number">264</span>, product=<span class="number">154</span>, rating=<span class="number">5.295430734770273</span>)]</span><br></pre></td></tr></table></figure>

<p>可以快速得出对电影ID为154最感兴趣的前10个用户，不过在推荐的信息里面，看不到电影名称，还需关联电影名的数据，从而形成完整的推荐信息。</p>
<p>加载电影详情数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">move_info_rdd=sc.textFile(<span class="string">&quot;file:///opt/ml-100k/u.item&quot;</span>)</span><br><span class="line">move_info_rdd.take(<span class="number">3</span>)</span><br><span class="line">输出：</span><br><span class="line">[<span class="string">&#x27;1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>查看u.item电影详情表的字段说明，总共有19个字段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u.item     -- Information about the items (movies); this <span class="keyword">is</span> a tab separated</span><br><span class="line">              <span class="built_in">list</span> of</span><br><span class="line">              movie <span class="built_in">id</span> | movie title | release date | video release date |</span><br><span class="line">              IMDb URL | unknown | Action | Adventure | Animation |</span><br><span class="line">              Children<span class="string">&#x27;s | Comedy | Crime | Documentary | Drama | Fantasy |</span></span><br><span class="line"><span class="string">              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |</span></span><br><span class="line"><span class="string">              Thriller | War | Western |</span></span><br></pre></td></tr></table></figure>

<p>作为测试，无需使用全部字段，只需挑出感兴趣的字段即可：电影id，电影名，url</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">move_splited_rdd=move_info_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取3个字段，将转为map类型，name:电影名，url：电影ur </span></span><br><span class="line">func=<span class="keyword">lambda</span> a_list:(<span class="built_in">int</span>(a_list[<span class="number">0</span>]),<span class="string">&#x27;name:%s,url:%s&#x27;</span>%(a_list[<span class="number">1</span>],a_list[<span class="number">4</span>]))</span><br><span class="line">move_map_info_rdd=move_splited_rdd.<span class="built_in">map</span>(func).collectAsMap() <span class="comment">#move_map_info_rdd 已经是字典类</span></span><br><span class="line">print(move_map_info_rdd)</span><br><span class="line"><span class="comment"># python字典类型的电影信息</span></span><br><span class="line">&#123;<span class="number">1</span>: <span class="string">&#x27;name:Toy Story (1995) url:http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)&#x27;</span>,</span><br><span class="line"> <span class="number">2</span>: <span class="string">&#x27;name:GoldenEye (1995) url:http://us.imdb.com/M/title-exact?GoldenEye%20(1995)&#x27;</span>,</span><br><span class="line"> ......</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>move_map_info_rdd的key就是电影ID，因此只需要关联model.recommendUsers(154,10)输出的<code>Rating(user=133, product=154, rating=6.346890714591231),</code> product id，即可输出完整的推荐信息如下：<br>给用户id为199的用户推荐3部电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result=model.recommendProducts(<span class="number">199</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">f&#x27;user:<span class="subst">&#123;r.user&#125;</span>,moveid:<span class="subst">&#123;r.product&#125;</span>,move_info:<span class="subst">&#123;move_map_info_rdd[r.product]&#125;</span>,rating:<span class="subst">&#123;r.rating&#125;</span>&#x27;</span>)</span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user:199,moveid:854,move_info:name:Bad Taste (1987) url:http://us.imdb.com/M/title-exact?Bad%20Taste%20(1987),rating:10.774026140227157</span><br><span class="line"></span><br><span class="line">user:199,moveid:962,move_info:name:Ruby in Paradise (1993) url:http://us.imdb.com/M/title-exact?Ruby%20in%20Paradise%20(1993),rating:9.30074590770409</span><br><span class="line"></span><br><span class="line">user:199,moveid:1176,move_info:name:Welcome To Sarajevo (1997) url:http://us.imdb.com/M/title-exact?Welcome+To+Sarajevo+(1997),rating:8.813180359193545</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将model持久化到本地后，再封装为完整的逻辑，方便重新使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(sc,<span class="string">&#x27;/opt/ml-100k/asl_model&#x27;</span>) <span class="comment"># sc为spark程序开头的spark context</span></span><br><span class="line"><span class="comment"># 若再次存储再会提示出错，所以一般是这么用：</span></span><br><span class="line"><span class="keyword">try</span>：</span><br><span class="line">    model.save(sc,path)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>model以一个目录的形式保存，而且还保存了user和product的数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ml-100k]# tree asl_model/</span><br><span class="line">asl_model/</span><br><span class="line">├── data</span><br><span class="line">│   ├── product</span><br><span class="line">│   │   ├── part-00000-bf34d65a-81e8-4124-a254-6e6044b8da2d-c000.snappy.parquet</span><br><span class="line">│   │   └── _SUCCESS</span><br><span class="line">│   └── user</span><br><span class="line">│       ├── part-00000-3953175d-e560-42a5-8de3-fcc86a4b625c-c000.snappy.parquet</span><br><span class="line">│       └── _SUCCESS</span><br><span class="line">└── metadata</span><br><span class="line">    ├── part-00000</span><br><span class="line">    └── _SUCCESS</span><br></pre></td></tr></table></figure>

<p>如何加载已训练好的本地模型？使用load方法即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load(sc,<span class="string">&#x27;/opt/ml-100k/asl_model&#x27;</span>) <span class="comment"># path为</span></span><br></pre></td></tr></table></figure>
<h5 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h5><p>将以上的处理流程封装类，便于调用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="keyword">as</span> sc</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"><span class="keyword">import</span> os,datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MoveRecommend</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,model_path,user_path,move_path,app_name=<span class="string">&quot;move_recommend&quot;</span>,master=<span class="string">&quot;local[*]&quot;</span></span>):</span></span><br><span class="line">        self.app_name=app_name</span><br><span class="line">        self.master=master</span><br><span class="line">        self.sc=self.create_spark_context()</span><br><span class="line">        self.train_rank=<span class="number">10</span> <span class="comment"># 稀疏矩阵分解的秩</span></span><br><span class="line">        self.train_iter=<span class="number">10</span> <span class="comment"># 迭代次数</span></span><br><span class="line">        self.train_lambda=<span class="number">0.01</span> <span class="comment"># 正则化参数(惩罚因子)        </span></span><br><span class="line">        self.user_path=user_path </span><br><span class="line">        self.move_path=move_path</span><br><span class="line">        self.model_path=model_path</span><br><span class="line">        self.model=self.get_model()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_spark_context</span>(<span class="params">self</span>):</span></span><br><span class="line">        conf=SparkConf().setAppName(self.app_name).setMaster(self.master)</span><br><span class="line">        spark_context=sc.getOrCreate(conf)    </span><br><span class="line">        <span class="keyword">return</span> spark_context</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;如果给定的目录没有model，则重新训练model，如果已有model，则直接加载使用&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(self.model_path):</span><br><span class="line">            print(<span class="string">f&#x27;model not found,start traing at <span class="subst">&#123;self.get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> self.train_and_save()</span><br><span class="line">        <span class="keyword">return</span> model.load(self.sc,self.model_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_and_save</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;只用训练集，训练model并持久化到本地目录&quot;&quot;&quot;</span></span><br><span class="line">        user_rdd=self.sc.textFile(<span class="string">&quot;file://&quot;</span>+self.user_path)</span><br><span class="line">        raw_rating_rdd=user_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27;\t&#x27;</span>)[:<span class="number">3</span>]) <span class="comment"># 每行分割后为一个包含4个元素的列表，取前3项即可</span></span><br><span class="line">        rating_rdd=raw_rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]))<span class="comment"># x[0],x[1],x[2]对应用户id，电影id，评分</span></span><br><span class="line">        model=ALS.train(rating_rdd,self.train_rank,self.train_iter,self.train_lambda)</span><br><span class="line">        model.save(self.sc,self.model_path)</span><br><span class="line">        print(<span class="string">f&#x27;model training done at <span class="subst">&#123;self.get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> model </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_move_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个字典列表，每个字典存放3个电影详情字段&quot;&quot;&quot;</span>        </span><br><span class="line">        move_info_rdd=self.sc.textFile(<span class="string">&quot;file://&quot;</span>+self.move_path)</span><br><span class="line">        move_splited_rdd=move_info_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line">        <span class="comment"># 提取3个字段，将转为map类型，name:电影名，url：电影ur </span></span><br><span class="line">        func=<span class="keyword">lambda</span> a_list:(<span class="built_in">int</span>(a_list[<span class="number">0</span>]),<span class="string">&#x27;name:%s,url:%s&#x27;</span>%(a_list[<span class="number">1</span>],a_list[<span class="number">4</span>]))</span><br><span class="line">        move_map_info_rdd=move_splited_rdd.<span class="built_in">map</span>(func).collectAsMap() <span class="comment">#move_map_info_rdd 已经是字典类 </span></span><br><span class="line">        <span class="keyword">return</span> move_map_info_rdd</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend_product_by_userid</span>(<span class="params">self,user_id,num=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据给定用户id，向其推荐top N部电影&quot;&quot;&quot;</span>                </span><br><span class="line">        result= self.model.recommendProducts(user_id,num)</span><br><span class="line">        move_dict=self.get_move_dict()</span><br><span class="line">        <span class="keyword">return</span> [(r.user,r.product,move_dict[r.product],r.rating) <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend_user_by_moveid</span>(<span class="params">self,move_id,num=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据给定电影ID，推荐对该电影感兴趣的top N 个用户&quot;&quot;&quot;</span>     </span><br><span class="line">        result=self.model.recommendUsers(move_id,num)</span><br><span class="line">        move_dict=self.get_move_dict()</span><br><span class="line">        <span class="keyword">return</span> [(r.user,r.product,move_dict[r.product],r.rating) <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>调用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m=MoveRecom(model_path=<span class="string">&#x27;/opt/ml-100k/costom_model&#x27;</span>,user_path=<span class="string">&#x27;/opt/ml-100k/u.data&#x27;</span>,move_path=<span class="string">&#x27;/opt/ml-100k/u.item&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>输出训练时间：<br>model not found,start traing at 26:45<br>model training done at 27:06</p>
<h5 id="项目难点说明"><a href="#项目难点说明" class="headerlink" title="项目难点说明"></a>项目难点说明</h5><p>&#8195;&#8195;上面的例子只是给出demo流程，而且数据已准备，但如果针对实际项目，则需要你处理以下两个主要难点：<br>（1） 训练数据的获取、整理和加工，并将这一流程自动化。<br>（2）模型的训练，以及根据新数据重新训练模型，以保证模型推荐效果最优，并将这一流程自动化。<br>&#8195;&#8195;至于其他工作，例如web 层面的开发，以及Apps或者说底层数据的存储，对于全栈开发者来说，并无大碍，只是需要耗费更多精力而已。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本文给出了较为入门的基于PySpark实现的推荐类的业务流程，该逻辑其实是离线的模式：训练数据已经加工好，模型训练也没有进行深度调优。事实上，如果将其作为一个生产可用项目来实施，需将大数据生态圈相关技术栈以及web 开发进行整合，此类项目的架构设计一般有下面三部分：</p>
<ul>
<li>需推荐的业务数据（包括训练集和测试集）收集、计算、存储：大数据生态圈相关技术栈实现</li>
<li>模型训练方面：离线存储PySpark计算后生成的训练模型，而且需要定时训练和更新该模型文件，以便保持最优模型。</li>
<li>以web api的方式提供推荐数据：为BI或者其他应用以get、post的方式提供推荐数据，例如post一个用户ID，返回相应的推荐条目</li>
</ul>
<p>以下简要说明两种基本架构图：<br><strong>第一种：适合数据量不大，几个节点组成的小型“大数据”服务</strong><br><img src="https://img-blog.csdnimg.cn/20200111095444386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;这种架构较为简单，数据源本身已经存储在各个业务的原有数据库中或者日志文件，开发者无需借助hadoop存储组件，自行实现数据源抽取模块，接着只需PySpark读取这些数据并训练成模型文件即可，模型文件管理可以通过定时训练更新，最后通过web API的形式为上层应用提供推荐或者匹配记录。<br>需要注意的是：构建web API方式这里用了Python栈，当然可用Java栈或者Go栈</p>
<p><strong>第二种：适合数据量大的中大型大数据服务</strong><br><img src="https://img-blog.csdnimg.cn/20200111100116354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;此类架构适合那些几十GB到几百GB级别甚至是TB级别的分布式大数据节点集群，此类场景需引入hadoop相关生态圈的技术栈，用于处理大量属鸡的存储和计算：Flume、Kafka、HBase、Hive，在计算层提供分布式的Spark组件支撑离线模型计算。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>PySpark推荐</tag>
      </tags>
  </entry>
  <entry>
    <title>构建高可用Hive HA和整合HBase开发环境（一）</title>
    <url>/blog/2019/11/14/%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hive%20HA%E5%92%8C%E6%95%B4%E5%90%88HBase%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;前面的项目中，已经实现了HadoopHA、HBaseHA，本文将加入Hive数据仓库工作，并整合HBase，实现完整的大数据开发项目所具备的开发环境，为后面博客关于数据应用层开发提供支撑。</p>
<h3 id="1、Hive-Requirements"><a href="#1、Hive-Requirements" class="headerlink" title="1、Hive Requirements"></a>1、Hive Requirements</h3><p>​    按官网给出的基本环境</p>
<ul>
<li>Java 1.7：  <a href="https://issues.apache.org/jira/browse/HIVE/fixforversion/12329345/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel">Hive versions1.2</a> onward require Java 1.7 or newer. java1.7或更高版本</li>
<li>Hadoop 2.x (preferred)：推荐hadoop2.x版本</li>
</ul>
<a id="more"></a>

<p>hive安装包可在清华镜像源拉取：<code>https://mirrors.tuna.tsinghua.edu.cn/apache/hive/</code></p>
<p>目前stable版本为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apache-hive-2.3.6-bin.tar.gz 2019-08-23 02:53  221M </span><br></pre></td></tr></table></figure>

<p>最新版为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apache-hive-3.1.2-bin.tar.gz 2019-08-27 04:20  266M  </span><br></pre></td></tr></table></figure>

<p>如何定位自身Hadoop版本与hive版本的兼容呢?</p>
<p>例如本blog前面部署hadoop3.1.2，可通过在hive官网查看其对应的版本</p>
<p><code>http://hive.apache.org/downloads.html</code>，官网给出的news：</p>
<blockquote>
<p> 26 August 2019: release 3.1.2 available</p>
<p>This release works with Hadoop 3.x.y.</p>
</blockquote>
<p> hive3.1.2版本支持hadoop3.x.y版本，结合本blog内容，这里使用hive3.1.2：</p>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/">安装包下载地址</a></p>
<p>从hive官网给出的hadoop版本兼容可以看出hive2.x.y一般是兼容hadoop2.x.y</p>
<h3 id="2、Hive-环境部署"><a href="#2、Hive-环境部署" class="headerlink" title="2、Hive 环境部署"></a>2、Hive 环境部署</h3><h4 id="2-1-配置环境变量"><a href="#2-1-配置环境变量" class="headerlink" title="2.1 配置环境变量"></a>2.1 配置环境变量</h4><p>hive安装包所在路径，个人习惯将所有大数据组件放在/opt目录下，方便管理和配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# pwd</span><br><span class="line">/opt/hive-3.1.2</span><br><span class="line"></span><br><span class="line">[root@nn hive-3.1.2]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 追加到文件后面</span></span><br><span class="line">export HIVE_HOME=/opt/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@nn hive-3.1.2]# source /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看hive版本</span></span><br><span class="line">[root@nn hive-3.1.2] hive --version</span><br><span class="line">Hive 3.1.2</span><br><span class="line">Git git://HW13934/Users/gates/tmp/hive-branch-3.1/hive -r 8190d2be7b7165effa62bd21b7d60ef81fb0e4af</span><br><span class="line">Compiled by gates on ** PDT 2019</span><br><span class="line">From source with checksum 0492c08f784b188c349f6afb1d8d9847</span><br></pre></td></tr></table></figure>



<h4 id="2-2-配置hive-env-sh和hive-site-xml"><a href="#2-2-配置hive-env-sh和hive-site-xml" class="headerlink" title="2.2 配置hive-env.sh和hive-site.xml"></a>2.2 配置hive-env.sh和hive-site.xml</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# cp hive-default.xml.template  hive-site.xml</span><br><span class="line">[root@nn conf]# cp hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">[root@nn conf]# vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在文件最后修改</span></span><br><span class="line">HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export HIVE_CONF_DIR=/opt/hive-3.1.2/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/opt/hive-3.1.2/lib</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Hive-site.xm的配置项比较多，自带模板文件内容长达6900多行，仅给出重要的设置项，其他属性的设置以及描述可参考<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2">官网</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!--元数据库的mysql的配置项--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://nn:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;py_ab2018&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.readOnlyDatastore&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateTables&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateColumns&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line">    &lt;!--zookeeper的有关设置--&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">      &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hiveserver2_zk&lt;/value&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.zookeeper.publish.configs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--hiveserver2配置，可使得外部客户端使用thrift RPC协议连接远程hive--&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.client.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.client.password&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;py_ab2018&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--binary对应TCP协议，也可配成http协议--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.transport.mode&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;binary&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--thriftserver对外限制最大最小连接数--&gt;  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.min.worker.threads&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--有关日志文件--&gt;    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/scratchdir&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/resources&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.querylog.location&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/querylog&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/operation-log&lt;/value&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>注意因为hadoop做了HA配置，因此以上的配置需要在主nn节点和backup dn2节点配置上，==在第7.1章节内容将会给出hiveserver2的相关内容==。</p>
<h4 id="2-3-配置Hive的运行日志"><a href="#2-3-配置Hive的运行日志" class="headerlink" title="2.3 配置Hive的运行日志"></a>2.3 配置Hive的运行日志</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">[root@nn hive-3.1.2]# mkdir logs</span><br><span class="line"></span><br><span class="line">[root@nn conf]# cp hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line">[root@nn conf]# vi hive-log4j2.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">property.hive.log.dir = /root/hive-3.1.2/logs</span><br></pre></td></tr></table></figure>


<h4 id="2-4-加入mysql-connector"><a href="#2-4-加入mysql-connector" class="headerlink" title="2.4  加入mysql connector"></a>2.4  加入mysql connector</h4><p>hive需用通过jdbc连接mysql，该jar需自行下载，并将其拷贝至以下目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# cp mysql-connector-java-5.1.32-bin.jar /opt/hive-3.1.2/lib/</span><br></pre></td></tr></table></figure>


<h4 id="2-5-在mysql建表"><a href="#2-5-在mysql建表" class="headerlink" title="2.5 在mysql建表"></a>2.5 在mysql建表</h4><p>其实这里无需在msyql建表，因为hive-site.xml文件里面已经配置为自动创建元数据库表，hive做初始化时会自动创建。也即本节内容可以忽略。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; create database hive default character set utf8 collate utf8_general_ci</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| hive               |</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> grant all on hive.* to <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;py_ab2018&#x27;</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 容许本地访问,否则hive的schema初始化将无法访问msyql</span></span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;nn&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;localhost&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;127.0.0.1&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt;  select host,user,authentication_string from mysql.user;  +-----------+--------+-----------------------+</span><br><span class="line">| host      | user   | authentication_string |</span><br><span class="line">+-----------+--------+-----------------------+</span><br><span class="line">| localhost | root   |                       |</span><br><span class="line">| nn        | root   |                       |</span><br><span class="line">| 127.0.0.1 | root   |                       |</span><br><span class="line">| ::1       | root   |                       |</span><br><span class="line">| nn        | hive   |                       |</span><br><span class="line">| %         | hadoop |                       |</span><br><span class="line">| %         | hive   |                       |</span><br><span class="line">| localhost | hive   |                       |</span><br><span class="line">| 127.0.0.1 | hive   |                       |</span><br><span class="line">+-----------+--------+-----------------------+</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> <span class="built_in">exit</span>;(quit;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="2-6-初始化hive-schema"><a href="#2-6-初始化hive-schema" class="headerlink" title="2.6 初始化hive schema"></a>2.6 初始化hive schema</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# schematool  -initSchema -dbType mysql </span><br><span class="line">Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">[root@nn hive-3.1.2]# </span><br></pre></td></tr></table></figure>

<h4 id="2-7-在mysql上查看hive创建的元表"><a href="#2-7-在mysql上查看hive创建的元表" class="headerlink" title="2.7 在mysql上查看hive创建的元表"></a>2.7 在mysql上查看hive创建的元表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; use hive</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">MariaDB [hive]&gt; show tables;</span><br><span class="line">| AUX_TABLE                     |</span><br><span class="line">| BUCKETING_COLS                |</span><br><span class="line">| CDS                           |</span><br><span class="line">| COLUMNS_V2                    |</span><br><span class="line">| COMPACTION_QUEUE              |</span><br><span class="line">| COMPLETED_COMPACTIONS         |</span><br><span class="line">| COMPLETED_TXN_COMPONENTS      |</span><br><span class="line">| CTLGS                         |</span><br><span class="line">| DATABASE_PARAMS               |</span><br><span class="line">| DBS                           |</span><br><span class="line">| DB_PRIVS                      |</span><br><span class="line">| DELEGATION_TOKENS             |</span><br><span class="line">| FUNCS                         |</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>


<h4 id="2-8-启动hive"><a href="#2-8-启动hive" class="headerlink" title="2.8 启动hive"></a>2.8 启动hive</h4><p>启动hive之前，务必hadoop服务已经启动，若hadoop为HA结构，必须其中一个namenode节点为active节点，例如本项目中，hadoopHA为：nn和dn2都作为namenode节点。</p>
<p>除此之外，还需手动在hdfs上创建hive的工作目录：这里官方的说明如下</p>
<p>In addition, you must use below HDFS commands to create <code>/tmp</code> and <code>/user/hive/warehouse</code> (aka <code>hive.metastore.warehouse.dir</code>) and set them <code>chmod g+w</code> before you can create a table in Hive.</p>
<p>以下就是对/tmp加入group写权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir -p &#x2F;tmp&#x2F;hive</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse</span><br></pre></td></tr></table></figure>
<p>warehouse目录下放置的就是表对应的数据文件，在后面的章节会提供说明</p>
<p>启动hive，该命令是指启动hive cli，就像mysql shell</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# hive</span><br><span class="line">Hive Session ID = 627577c0-2560-4318-92af-bc2512f91d3b</span><br><span class="line"><span class="meta">hive&gt;</span></span><br></pre></td></tr></table></figure>

<p>以上说明hive部署成功，jps可以看到多了一个RunJar进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# jps</span><br><span class="line">13042 QuorumPeerMain</span><br><span class="line">20163 JournalNode</span><br><span class="line">19780 NameNode</span><br><span class="line">20709 Jps</span><br><span class="line">19499 DFSZKFailoverController</span><br><span class="line">20299 RunJar</span><br><span class="line">19918 DataNode</span><br></pre></td></tr></table></figure>

<p>==启动过程可能遇到问题==：</p>
<p>1）启动hive会有一个多重绑定的提示</p>
<p>SLF4J: Class path contains multiple SLF4J bindings.<br>SLF4J: Found binding in [jar:file:/opt/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: Found binding in [jar:file:/opt/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p>
<p>原因：</p>
<p>hadoop/common/lib有个slf4j-log4j的jar包，hive的lib下也有一个slf4j-log4j</p>
<p>那么在环境变量/etc/profile都配置两者的环境，hive启动后，会找到两个slf4j-log4j，因此提示多重绑定</p>
<p>解决办法：</p>
<p>保留hadoop/common/lib有个slf4j-log4j的jar包，将hive lib目录下的slf4j-log4j重命名即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn lib]# mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak</span><br></pre></td></tr></table></figure>
<p>==注意==：<br>当这个hive的日志jar包去掉后，hive日志模式将默认使用hadoop的日志配置，启动hive cli或者在hive cli上执行任何命令时都会不断打印出日志，如果需要进程在hive cli操作数据，那么建议保留hive的log4j包。如果使用外部可视化数据库管理客户端连接hive，那么可删除之。</p>
<p>2） hive在hdfs的/tmp/hive不具有写权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rwxrwxr-x</span><br></pre></td></tr></table></figure>

<p>将用户组以及其他用户加入可读可写可执行权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -chmod -R 777 &#x2F;tmp</span><br></pre></td></tr></table></figure>

<h3 id="3、hive建表测试"><a href="#3、hive建表测试" class="headerlink" title="3、hive建表测试"></a>3、hive建表测试</h3><p>HQL语句跟SQL差别不大，若对sql非常熟悉，HQL拿来即用。相关用法参考官网：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-DDLOperations">DDL语句</a>、<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SQLOperations">HQL查询用法</a></p>
<h4 id="3-1-创建一个员工表"><a href="#3-1-创建一个员工表" class="headerlink" title="3.1 创建一个员工表"></a>3.1 创建一个员工表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create table if not exists emp(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">sexual string,</span><br><span class="line">depart_id int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by&#x27;\t&#x27;</span><br><span class="line">stored as textfile;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc emp;</span></span><br><span class="line">OK</span><br><span class="line">id                      int                                         </span><br><span class="line">name                    string                                      </span><br><span class="line">age                     int                                         </span><br><span class="line">sexual                  string                                      </span><br><span class="line">depart_id               int                                         </span><br><span class="line">Time taken: 0.263 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure>

<p>员工表的本地数据emp.txt</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1 	Aery	25	Male    1</span><br><span class="line">2 	Bery	23	Female	2</span><br><span class="line">3	Cery	26	Female	3</span><br><span class="line">4	Dery	27	Male		2</span><br></pre></td></tr></table></figure>

<h4 id="3-2-hive-cli导入测试文本数据"><a href="#3-2-hive-cli导入测试文本数据" class="headerlink" title="3.2 hive cli导入测试文本数据"></a>3.2 hive cli导入测试文本数据</h4><p>上面创建一个emp.txt文本数据，若要使用hive将其映射为一张表，需要将数据文件上传到hdfs，hive已经提供相关命令进行此类文件数据的上传操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/opt/hive-3.1.2/test_data/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line"></span><br><span class="line">Loading data to table default.emp</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.768 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from emp;</span></span><br><span class="line">OK</span><br><span class="line">1       Aery    25      Male    1</span><br><span class="line">2       Bery    23      Female  2</span><br><span class="line">3       Cery    26      Femalei 3</span><br><span class="line">4       Dery    27      Male    2</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from emp a <span class="built_in">where</span> a.name=<span class="string">&#x27;Dery&#x27;</span>;</span></span><br><span class="line">OK</span><br><span class="line">4       Dery    27      Male    2</span><br><span class="line">Time taken: 0.327 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>


<p>hive会把本地数据上传到hdfs文件系统上具体路径如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# hdfs dfs -ls /user/hive/warehouse/emp</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root root         73 ** /user/hive/warehouse/emp/emp.txt</span><br></pre></td></tr></table></figure>

<p>从上面可知，hive建的表默认放在hdfs的warehouse目录下，而且上传的用户数据文件放在相应的表名字目录下。</p>
<h4 id="3-3-加载hdfs上的数据"><a href="#3-3-加载hdfs上的数据" class="headerlink" title="3.3  加载hdfs上的数据"></a>3.3  加载hdfs上的数据</h4><p>除了可以直接在hive cli里加载本地数据，也可先把本地数据上传到hdfs上，再通过hive加载</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn test_data]# hdfs dfs -put emp.txt /tmp</span><br><span class="line">[root@nn test_data]# hdfs dfs -ls /tmp</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   3 root supergroup         73 ** /tmp/emp.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先清空之前的数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> truncate table emp;</span></span><br><span class="line">OK</span><br><span class="line">Time taken: 0.957 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> hive导入hdfs的数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">&#x27;/tmp/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">&#x27;/tmp/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line">Loading data to table default.emp</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.593 seconds</span><br><span class="line"></span><br><span class="line">hive导入本地文件所需的实际为：1.768 s，是hdfs导入的3倍。</span><br></pre></td></tr></table></figure>

<p>==todo==<br>hive 按分区上传，上传的数据会指定在相应的分区上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">hive按分区删除数据：</span><br><span class="line">alter table table_name drop partition (partition_name=&#x27;分区名&#x27;)</span><br></pre></td></tr></table></figure>


<h3 id="4、为何使用Hive？"><a href="#4、为何使用Hive？" class="headerlink" title="4、为何使用Hive？"></a>4、为何使用Hive？</h3><p>&#8195;&#8195;前面的内容为hive环境构建及其测试，那么在大数据开发项目中，为何要引入Hive组件？</p>
<h4 id="4-1-无Hive组件的大数据处理"><a href="#4-1-无Hive组件的大数据处理" class="headerlink" title="4.1 无Hive组件的大数据处理"></a>4.1 无Hive组件的大数据处理</h4><p>&#8195;&#8195;从本人博客前面几篇关于大数据组件部署和技术架构解析的blog可以了解到，若没有Hive这样的组件，<br>&#8195;&#8195;当需要从hdfs的原始数据做高级数据分析时，首先肯定需要使用java写MapReduce程序，如果再加入Spark分布式内存计算引擎，那么还需使用Scala语言写spark程序（或者使用python写pyspark）。事实上，MapReduce的程序写起来比较繁琐（注意：不是难），占据大量工作和时间。对于大部分数据开发人员（含数据分析），其实更关心的是把这些海量数据“统一处理”后，最终的呈现的数据是否有价值或者提供商业决策。若无Hive这样的组件，整个项目组将耗费大量的人力去开发更低层MapReduce程序，无论业务逻辑简单与否（虽然极其复杂的业务数据需要可能还是得写MP程序才能完成）。</p>
<h4 id="4-2-Hive组件在大数据分析与处理上的优势"><a href="#4-2-Hive组件在大数据分析与处理上的优势" class="headerlink" title="4.2 Hive组件在大数据分析与处理上的优势"></a>4.2 Hive组件在大数据分析与处理上的优势</h4><p>&#8195;&#8195;在大数据处理和分析中，能否有个更高层更抽象的语言层来描述算法和数据处理流程，就像传统数据库的SQL语句。Apache项目大神早已考虑到：传统数据库的数据分析与处理，每个人都在用SQL即可完成各自分析任务，这种方式在大数据hadoop生态必须给予引入。于是就有了Pig和Hive。Pig是接近脚本方式去描述MapReduce，Hive则用的是SQL，它们把脚本和SQL语言翻译成MapReduce程序，然后再丢给底层的MapReduce或者spark计算引擎去计算。也就是说，大数据开发人员只需要用更直观易懂、大家都熟悉的SQL语言去写大数据job即可完成绝大部分MapReduce任务，而且项目组的非计算机背景工作人员也可直接通过写SQL完成相应的大数据分析任务，简直不要太爽！</p>
<p>正因为Hive如此易用和SQL的通用性，Hive逐渐成长成了大数据仓库的核心组件，甚至很多公司的流水线作业集完全是用SQL描述，因为易写易改，一看就懂，容易维护。</p>
<h4 id="4-3-Hive在hadoop项目中的作用"><a href="#4-3-Hive在hadoop项目中的作用" class="headerlink" title="4.3 Hive在hadoop项目中的作用"></a>4.3 Hive在hadoop项目中的作用</h4><ul>
<li><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张关系型数据库的表，并提供类似传统数据库的SQL查询功能</p>
<p>这里如何理解？以本文第3章节内容说明：</p>
<p>这里说的结构化的数据文件，例如emp.txt数据文件，里面的数据是结构化的，每行的字段值用tab键空格隔开，用换行符’\n’进行换行，该数据文件直接存在hdfs上映射为一张关系型数据库的表：因为是结构化数据，一行记录分4列，有即每行都有4个字段，当然可以把该数据文件emp.txt看成是一张数据库表。</p>
</li>
</ul>
<ul>
<li><p>Hive的查询效率取决于使用第一代的MapReduce计算框架还是内存Spark/Tez框架</p>
<p>这句表述如何理解？</p>
</li>
</ul>
<p>==&#8195;&#8195;4.2 章节提到，数据应用开发或者数据分析人员开始用Hive分析数据之后，虽然写SQL即可实现MP任务，但Hive在MapReduce处理任务的速度实在太慢，这是底层默认采用MapReduce计算架构。Spark/Tez作为新一代的内存计算框架既然比MP计算效率更高，当然可以引入到Hive里面，于是就有了Hive on  Spark/Hive on Tez，到此，基本完成一个数据仓库的架构了，有了Hive on  Spark/Hive on Tez，基本解决了中低速数据处理的需求，这里的中低速是指（批数据分析）：例如查询某个栏目截止到昨天的访问量，时效性滞后比较长。==</p>
<p>&#8195;&#8195;而高速数据处理的需求（流数据分析）：例如要查询截止到1小时前，某个栏目的访问量，时效性要求高，近乎实时效果。</p>
<ul>
<li> Hive只适合用来做批量数据统计分析</li>
</ul>
<h3 id="5、Hive与HBase的关系"><a href="#5、Hive与HBase的关系" class="headerlink" title="5、Hive与HBase的关系"></a>5、Hive与HBase的关系</h3><p>&#8195;&#8195;在前面的blog，给出了非常详细的HBase高可用的部署测试的描述，那么在本文中，HBase跟Hive是怎么样结合使用呢？或者他们之间有什么关系吗？</p>
<p>&#8195;&#8195;首先：Hive与HBase是没有联系的，也就是说，在大数据项目中，有Hive+Spark/MapReduce+HDFS+结构化数据，也可以独立完成大数据分析任务，同样，有HBase+HDFS+数据，也可以独立完成大数据分析任务。因为Hbase和Hive在大数据架构中处在不同位置，Hbase主要解决实时高效查询的需求，尤其是Key-Value形式的查询；而Hive主要解决数据处理和计算问题，例如联合查询、统计、汇总等。这两个组件可以独立使用，也可以配合一起使用。</p>
<h4 id="5-1-两者之间的区别"><a href="#5-1-两者之间的区别" class="headerlink" title="5.1 两者之间的区别"></a>5.1 两者之间的区别</h4><ul>
<li><p>Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的随机实时查询，如日志明细、交易清单、轨迹行为等。</p>
</li>
<li><p>Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。</p>
</li>
<li><p>通过元数据来描述Hdfs上的结构化文本数据，通俗点来说，就是定义一张表来描述HDFS上的结构化文本，包括各列数据名称，数据类型是什么等，方便我们处理数据，当前很多SQL ON Hadoop的计算引擎均用的是hive的元数据，如Spark SQL、Impala等；</p>
</li>
<li><p>基于第一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据；<br>也可参考以下两者的各自优点对比图：<br><img src="https://img-blog.csdnimg.cn/2019110915233826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
</ul>
<h4 id="5-2-两者配合使用时的大数据处理流程"><a href="#5-2-两者配合使用时的大数据处理流程" class="headerlink" title="5.2 两者配合使用时的大数据处理流程"></a>5.2 两者配合使用时的大数据处理流程</h4><p>在大数据架构中，Hive和HBase是协作关系，处理流程一般如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191109153101866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>&#8195;&#8195;1）通过ETL工具将数据源抽取到HDFS存储，这里的数据源包括mysql等关系型数据库的数据、ftp、api接口、txt、excel、日志文件等，这里说的抽取有两种意思：一种为脚本式的自动化抽取，例如写个定时任务把ftp的数据定时导入到HDFS中，另外一种抽取则是使用Apache组件Flume，能够实时抽取日志记录到kafka消息组件中，再由消费端（例如存入hbase或者mysql等）消费kafka的日志消息，这部分内容也会在本blog给出。</p>
<p>&#8195;&#8195;2）通过Hive清洗、处理原始数据；</p>
<p>&#8195;&#8195;3）HIve清洗处理后的数据，若面向海量数据随机查询场景，例如key-value，则可存入Hbase；若其他查询场景则可导入到mysql等其他数据库</p>
<p>&#8195;&#8195;4）大数据BI分析、应用的数据接口API开发，可从HBase获得查询数据。</p>
<h4 id="5-3-如果Hbase不需要Hive组件，如何实现易用的查询？"><a href="#5-3-如果Hbase不需要Hive组件，如何实现易用的查询？" class="headerlink" title="5.3 如果Hbase不需要Hive组件，如何实现易用的查询？"></a>5.3 如果Hbase不需要Hive组件，如何实现易用的查询？</h4><p>&#8195;&#8195;在文章<a href="https://blog.csdn.net/pysense/article/details/102635656">基于HadoopHA服务部署HBaseHA分布式服务（详细版）</a>的第10章节内容，提到操作HBase 表的示例，例如要查询company表的R2行记录，首先启动hbase shell，使用以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase(main):&gt; get &#39;company&#39;,&#39;R1&#39;,&#39;staff_info:age&#39;</span><br><span class="line"></span><br><span class="line">COLUMN                  CELL                                                          </span><br><span class="line"> staff_info:age         timestamp&#x3D;**, value&#x3D;23  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>&#8195;&#8195;可以看到，这种查询方式适合开发人员或者hbase管理员，而对于已经非常熟悉SQL查询的分析人员来说，无疑非常不友好。Hive正好能提供一种叫“外部表”的机制实现以SQL的形式对HBase的数据进行查询操作，内容在以下章节给出。</p>
<h3 id="6、为HBase引入Hive组件"><a href="#6、为HBase引入Hive组件" class="headerlink" title="6、为HBase引入Hive组件"></a>6、为HBase引入Hive组件</h3><p>&#8195;&#8195;前面提到，引入Hive就是为了能够使用SQL语句轻松完成对于HBase上的数据进行查询任务。<br>==&#8195;&#8195;Hive连接HBase的原理：==<br>&#8195;&#8195;让hive加载到连接hbase的jar包，通过hbase提供的java api即可实现Hive对Hbase的操作，此时可以吧Hive看成是HBase的客户端，类似navicat客户至于mysql，只不过navicat提供UI操作界面，hive是通过cli shell操作，当然我们也可以使用Hive的UI操作工具来实现UI操作（后面会给出基于DBeaver来实现）</p>
<h4 id="6-1-hive-env-sh"><a href="#6-1-hive-env-sh" class="headerlink" title="6.1  hive-env.sh"></a>6.1  hive-env.sh</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn conf]# vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件最后添加</span></span><br><span class="line">export HBASE_HOME=/opt/hbase-2.1.7</span><br></pre></td></tr></table></figure>
<h4 id="6-2-在hive-site-xml添加zookeeper集群"><a href="#6-2-在hive-site-xml添加zookeeper集群" class="headerlink" title="6.2 在hive-site.xml添加zookeeper集群"></a>6.2 在hive-site.xml添加zookeeper集群</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">&lt;!--zookeeper的有关设置--&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hiveserver2_zk&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;    </span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.zookeeper.publish.configs&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>以上两个配置实现了Hive连接至Hbase</p>
<h4 id="6-3-测试hive操作hbase"><a href="#6-3-测试hive操作hbase" class="headerlink" title="6.3 测试hive操作hbase"></a>6.3 测试hive操作hbase</h4><p>&#8195;&#8195;首先hbase有测试数据，之前创建的company table，里面有两个列簇，这里不再赘述。<br>&#8195;&#8195;在hive创建外部表，用于映射Hbase的列簇，这里以staff_info列簇作为测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> CREATE EXTERNAL TABLE staff_info(</span></span><br><span class="line">rowkey string,</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">sexual string</span><br><span class="line">) </span><br><span class="line">STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; </span><br><span class="line">WITH SERDEPROPERTIES </span><br><span class="line">(&quot;hbase.columns.mapping&quot;=&quot;:key,staff_info:name,staff_info:age,staff_info:sex&quot;) </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;company&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>外部表创建语法解释：<br>==创建一个外部表，表名为staff_info，字段有4个，（rowkey,name,age,sexual），其中rowkey为对于hbase上的rowkey，该字段不是数据字段，name、age、sexual为数据字段。处理类org.apache.hadoop.hive.hbase.HBaseStorageHandler，hbase到hive的映射关系：:key,列簇:列名1，列簇:列名2…==<br>指定映射HBase的table name</p>
<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">** INFO  [16e10346-1e6d-4bb5-b89b-bd12f3614ec7 main] zookeeper.RecoverableZooKeeper: Process identifier&#x3D;hconnection-0x448892f1 connecting to ZooKeeper ensemble&#x3D;nn:2181,dn1:2181,dn2:2181</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.151 seconds</span><br></pre></td></tr></table></figure>
<p>在hive查询相关hbase的staff_info数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from staff_info;</span><br><span class="line">OK</span><br><span class="line">R1      Bery    23      Female</span><br><span class="line">R2      Dery    27      Male</span><br><span class="line">Time taken: 3.562 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">hive&gt; select * from staff_info a where a.name&#x3D;&#39;Bery&#39;;</span><br><span class="line">OK</span><br><span class="line">R1      Bery    23      Female</span><br><span class="line">Time taken: 1.376 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

<p>以上完成Hive和HBase的开发环境整合配置。</p>
<h3 id="7、使用SQL开发工具连接hive进行高级SQL开发"><a href="#7、使用SQL开发工具连接hive进行高级SQL开发" class="headerlink" title="7、使用SQL开发工具连接hive进行高级SQL开发"></a>7、使用SQL开发工具连接hive进行高级SQL开发</h3><p>&#8195;&#8195;在前面章节内容可以看到，hive的操作直接基于hive服务器上的hive cli上进行，使用hive交互命令式写sql效率会很低，调试也不方便，因此需要外部SQL IDE工具提高开发效率。本文采用DBeaver，也是本人长期使用的数据库管理客户端工具，重点它是开源的，在Mac上用起来流畅、UI有一定设计感！）。</p>
<p>关于DBeaver的介绍（<a href="https://dbeaver.io/">官网下载</a>）：</p>
<blockquote>
<p>DBeaver 是一个开源、跨平台、基于java语言编写的的通用数据库管理工具和 SQL 客户端，支持 MySQL, PostgreSQL, Oracle, Hive、Spark、elasticsearch等以及其他兼容 JDBC 的数据库(DBeaver可以支持的数据库太多了)</p>
<p>DBeaver 提供一个图形界面用来查看<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BB%93%E6%9E%84/5507713">数据库结构</a>、执行SQL查询和脚本，浏览和导出数据，处理<a href="https://baike.baidu.com/item/BLOB/543419">BLOB</a>/CLOB 数据，修改数据库结构等。<br><img src="https://img-blog.csdnimg.cn/20191109154205202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到，DBeaver支持各自类型数据库以及hadoop相关的组件，之后会有专门文章用DBeaver开发spark数据分析项目。</p>
</blockquote>
<p>DBeaver连接hive需要做以下几个配置，否则无法成功连接</p>
<h4 id="7-1-配置hive-site-xml和core-site-xml"><a href="#7-1-配置hive-site-xml和core-site-xml" class="headerlink" title="7.1  配置hive-site.xml和core-site.xml"></a>7.1  配置hive-site.xml和core-site.xml</h4><p>hive服务端启用相应的thrift TCP端口，暴露给客户端连接使用。<br>在2.2章节内容，hive-site.xml已经配置了hive server2服务，端口号按默认的10000，监听host为全网地址0.0.0.0，nn和dn2都需要配置hive server2。此外，还需要hadoop的配置文件core-site.xml放通拥有hdfs文件系统的用户，在本blog里，hadoop的用户为root上，需加入以下property<br>==core-site.xml配置如下==</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">  &lt;!--放通客户端以root用户访问hdfs--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>如果hadoop文件使用者不是’root‘用户，例如‘foo-bar’用户那么对应的name值为<br><code>&lt;name&gt;hadoop.proxyuser.foo-bar.groups&lt;/name&gt;</code>，<br>以上配置需要在nn和dn2同时配置，因为这两个节点做了hadoop HA。</p>
<p>若不配置“放通客户端以root用户访问hdfs”，使用DBeaver或者jdbc api连接hive server2会提示以下出错信息：</p>
<p>==连接错误提示==<br>Required field ‘serverProtocolVersion’ is unset! Struct:TOpenSessionResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*org.apache.hive.service.cli.HiveSQLException:Failed to open new session: java.lang.RuntimeException: ==org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate root:14:13 ==</p>
<h4 id="7-2-在nn主节点上启动hiveserver2服务"><a href="#7-2-在nn主节点上启动hiveserver2服务" class="headerlink" title="7.2 在nn主节点上启动hiveserver2服务"></a>7.2 在nn主节点上启动hiveserver2服务</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以前台进程方式打开</span></span><br><span class="line">[root@nn conf]# hiveserver2</span><br><span class="line">Hive Session ID = 1c92d507-7725-4e57-a7fe-03a9ae0cdf13</span><br></pre></td></tr></table></figure>

<p>使用jps -ml查看所有大数据组件服务的情况，RunJar表示hiveserver2服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# jps -ml</span><br><span class="line">16340 org.apache.hadoop.util.RunJar /opt/hive-3.1.2/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.aux.jars ****</span><br><span class="line">14085 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">14710 org.apache.hadoop.hbase.master.HMaster start</span><br><span class="line">5815 org.apache.hadoop.hdfs.tools.DFSZKFailoverController</span><br><span class="line">13273 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">16666 sun.tools.jps.Jps -ml</span><br><span class="line">5451 org.apache.zookeeper.server.quorum.QuorumPeerMain /opt/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">13547 org.apache.hadoop.hdfs.qjournal.server.JournalNode</span><br><span class="line">14876 org.apache.hadoop.hbase.regionserver.HRegionServer start</span><br><span class="line">13135 org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">13951 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br></pre></td></tr></table></figure>
<p>也可查看是否有10000端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ss -nltp|grep 10000</span><br><span class="line">LISTEN     0      50          :::10000                   :::*                  </span><br><span class="line"> users:((&quot;java&quot;,pid&#x3D;16340,fd&#x3D;506))</span><br></pre></td></tr></table></figure>
<p>至此，hiveserver2已经可以对外提供hive的连接服务。</p>
<h4 id="7-3-配置DBeaver连接hive"><a href="#7-3-配置DBeaver连接hive" class="headerlink" title="7.3 配置DBeaver连接hive"></a>7.3 配置DBeaver连接hive</h4><p>创建新的hive连接<br><img src="https://img-blog.csdnimg.cn/20191109165053150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>编辑驱动设置</code>里面，选择下载驱动，这里DBeaver会自动去拉取相应的jar驱动包<br><img src="https://img-blog.csdnimg.cn/20191109165332355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">驱动为：<code>hive-jdbc-uber-2.6.5.0-292.jar</code> (Uber开发的驱动？)</p>
<p>测试是否可连，以下提示远程hive服务器的版本为hive3.1.2<br><img src="https://img-blog.csdnimg.cn/20191109165734163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">下图可以看到DBeaver已经可以查看hive之前创建的emp表，以及hive的外部表——hbase的staff_info表<br><img src="https://img-blog.csdnimg.cn/20191109170010278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在DBeaver编辑器上对hive上的emp表进行简单的查询：<br><img src="https://img-blog.csdnimg.cn/20191109171320511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">至此，hive的SQL可视化开发环境已经部署完成，配合DBeaver出色的Tab自动补全，写HQL效率有效提升。</p>
<h4 id="7-4-hiveserver2的webUI"><a href="#7-4-hiveserver2的webUI" class="headerlink" title="7.4 hiveserver2的webUI"></a>7.4 hiveserver2的webUI</h4><p>&#8195;&#8195;在上一节内容，通过命令<code>hiveserver2</code>可启动远程连接服务，其实该命令还启动另外一个进程：hiveserver2自己的webUI服务进程，该web页面可看到每个客户端在hive服务器上执行过的查询语句、会话，包括IP、用户名、当前执行的操作（查询）数量、链接总时长、空闲时长等指标，是管理客户端连接和查询的后台页面。<br>在hiveserver2服务器上也即nn节点上查看10002端号：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn ~]# ss -nltp|grep 10002</span><br><span class="line">LISTEN     0      50          :::10002                   :::*                   users:((&quot;java&quot;,pid=16340,fd=511))</span><br></pre></td></tr></table></figure>
<p>web 页面入口：<a href="http://nn:10002/">http://nn:10002/</a><br>当前连接的客户端会话<br><img src="https://img-blog.csdnimg.cn/20191109173709403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">已经完成的查询语句，这里可以看到HQL使用底层计算框架为MapReduce</p>
<p><img src="https://img-blog.csdnimg.cn/20191109173859568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">至此，已经完成使用外部SQL客户端工具DBeaver连接hive的任务，那么接下来：在Hbase导入大数据文件，部署高可用hiveserver2服务。</p>
<h3 id="8-使用beeline连接hiveserver2"><a href="#8-使用beeline连接hiveserver2" class="headerlink" title="8 使用beeline连接hiveserver2"></a>8 使用beeline连接hiveserver2</h3><p>&#8195;&#8195;在以上章节都提到两种方式连接到hiveserver2，此外，还有hive自带的一个客户端工具beeline，也可以连接到hive，按hive的官方规划，beeline将取代之前版本的hive cli。具体为何取代hive cli，参考官网说明：</p>
<blockquote>
<p>HiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline.<br>HiveCLI is now deprecated in favor of Beeline, as it lacks the<br>multi-user, security, and other capabilities of HiveServer2.  To run<br>HiveServer2 and Beeline from shell:</p>
</blockquote>
<p>连接用法hiveserver2的用法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">beeline -u jdbc:hive2:nn:10000 -n root -p ****</span><br></pre></td></tr></table></figure>
<p>可以看出因为beeline在使用jdbc接口连接时要求带入hive-site.xml配置账户和密码，因此官网说提供了 security功能。<br>具体使用方式这里不再</p>
<h3 id="8、部署高可用的Hive服务"><a href="#8、部署高可用的Hive服务" class="headerlink" title="8、部署高可用的Hive服务"></a>8、部署高可用的Hive服务</h3><p> 以上仅在hdfs、hbase的主节点nn配置hive单集服务，hive可以看做是hdfs对外提供的SQL客户端服务，若nn节点不可用，将导致nn节点hive服务也无法使用，因此实际生产环境，需要将hive部署为HA模式，与hdfs和hbaseHA模式一起构成完整的高可用离线分析大数据开发环境。这部分的内容在下一篇文章给出：构建高可用Hive HA和整合HBase开发环境（二）</p>
<p>​         </p>
]]></content>
      <categories>
        <category>Hive</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>Hive集群</tag>
        <tag>HBase开发环境</tag>
      </tags>
  </entry>
  <entry>
    <title>深入解析asyncio与协程</title>
    <url>/blog/2020/01/04/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90asyncio%E4%B8%8E%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面的文章，已经通过gevent实现高并发的协程，本文将详细讨论Python标准库异步IO——asyncio。在Python3.4中引入了协程的概念以及asyncio。asyncio底层调用yield from语法，将任务变成生成器后挂起，这种方式无法实现协程之间的自动切换，在Python3.5中正式确立引入了async和await 的语法，所有的这些工作都使得Python实现异步编程变得更容易上手。</p>
<a id="more"></a>

<h4 id="1、asyncio的基本概念"><a href="#1、asyncio的基本概念" class="headerlink" title="1、asyncio的基本概念"></a>1、asyncio的基本概念</h4><ul>
<li><p>event_loop 事件循环：每一个需要异步执行的任务都要注册到事件循环中，事件循环负责管理和调度这些任务之间的执行流程（遇到IO则自动切换协程等）。</p>
</li>
<li><p>coroutine 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。</p>
</li>
<li><p>task 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含任务的各种状态。</p>
</li>
<li><p>future： 代表将来执行或没有执行的任务的结果。它和task上没有本质的区别</p>
</li>
<li><p>async/await 关键字：在python3.5及以上，用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。</p>
</li>
</ul>
<p>&#8195;&#8195;在异步的模式里，所有代码逻辑都会运行在一个forever事件循环中（你可以把整个事件循环看成一个总控中心，它监听着当前线程创建的多个协程发发生的事件），它可以同时执行多个协程，这些协程异步地执行，直到遇到 await 关键字，事件循环将会挂起该协程，事件循环这个总控再把当前线程控制权分配给其他协程，直到其他的协程也挂起或者执行完毕，再进行下一个协程的执行。</p>
<h4 id="2、使用asyncio"><a href="#2、使用asyncio" class="headerlink" title="2、使用asyncio"></a>2、使用asyncio</h4><h5 id="2-1-使用async关键字和await定义协程"><a href="#2-1-使用async关键字和await定义协程" class="headerlink" title="2.1 使用async关键字和await定义协程"></a>2.1 使用async关键字和await定义协程</h5><p>在Python3.5之前，要实现协程方式的写法一般如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>) </span><br></pre></td></tr></table></figure>
<p>在Python3.5以后，全面使用async关键字和await定义协程，代码显更直观。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># async 定义了mytask为协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="comment"># 这里就像gevent的sleep方法模拟IO，而且该协程会被asyncio自动切换</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>) <span class="comment"># await 要求该行语句的IO是有返回值的例如response=request.get(url)，如果直接使用await time.sleep(2),则无法创建协程对象</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建事件循环对象，该事件循环由当前主线程拥有 </span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks=[mytask(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)] <span class="comment"># 这里mytask()是协程对象，不会离开运行。</span></span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks)) <span class="comment"># 这里实行的逻辑就像gevent.joinall(tasks)一样，表示loop一直运行直到所有的协程tasks都完成</span></span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;代码中通过async关键字定义一个协程（coroutine），不过该协程不能直接运行，需将它注册到事件循环loop里面，由后者在协程内部发生IO时（asyncio.sleep(2)）时候调用协程。asyncio.get_event_loop方法可以创建一个事件循环，然后使用run_until_complete将协程注册到事件循环，并启动事件循环。<br>输出，从结果可以看出，5个协程同一时刻并发运行。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-1 started at:17:12:37</span><br><span class="line">task-0 started at:17:12:37</span><br><span class="line">task-3 started at:17:12:37</span><br><span class="line">task-2 started at:17:12:37</span><br><span class="line">task-4 started at:17:12:37</span><br><span class="line">task-1 done at:17:12:38</span><br><span class="line">task-0 done at:17:12:38</span><br><span class="line">task-3 done at:17:12:38</span><br><span class="line">task-2 done at:17:12:38</span><br><span class="line">task-4 done at:17:12:38</span><br></pre></td></tr></table></figure>
<p>关于await要求该句为返回值：<br>await asyncio.sleep(2)，这里可以看看sleep返回什么</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sleep</span>(<span class="params">delay, result=<span class="literal">None</span>, *, loop=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Coroutine that completes after a given time (in seconds).&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> delay == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">yield</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> loop <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        loop = events.get_event_loop()</span><br><span class="line">    future = loop.create_future()</span><br><span class="line">    h = future._loop.call_later(delay,</span><br><span class="line">                                futures._set_result_unless_cancelled,</span><br><span class="line">                                future, result)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">yield</span> <span class="keyword">from</span> future)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        h.cancel()</span><br></pre></td></tr></table></figure>
<p>如果设为delay值，且loop事件循环已创建（即使代码未创建它也会自动创建），返回的是future对象(yield from future)，而这里可以挂起当前协程，直到future完成</p>
<h5 id="2-2-task对象"><a href="#2-2-task对象" class="headerlink" title="2.2 task对象"></a>2.2 task对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...同上</span><br><span class="line"><span class="comment"># async 定义了mytask为协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="comment"># 这里就像gevent的sleep方法模拟IO，而且该协程会被asyncio自动切换</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;ok&#x27;</span></span><br><span class="line"></span><br><span class="line">coro=mytask(<span class="number">1</span>)</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task=loop.create_task(coro) <span class="comment"># 将协程对象封装为task对象</span></span><br><span class="line">print(<span class="string">&#x27;before register to loop:&#x27;</span>,task)</span><br><span class="line">loop.run_until_complete(future=task)</span><br><span class="line">print(<span class="string">&#x27;after loop completed,task return the result:&#x27;</span>,task.result())</span><br></pre></td></tr></table></figure>

<p>查看打印结果：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">before register to loop: &lt;Task pending coro=&lt;mytask() running at /opt/asyn.py:9&gt;&gt;</span><br><span class="line">task-1 started at:17:39:06</span><br><span class="line">task-1 done at:17:39:07</span><br><span class="line">after loop completed,task return the result: ok</span><br></pre></td></tr></table></figure>
<p>将协程封装为task对象后，task在注册到事件循环之前为pending状态，1秒后，task 结束，并且通过task.result()可以获取协程结果值。<br>task对象也可用asyncio.ensure_future(coro)创建（接收coro协程或者future对象），它内部封装了loop.create_task</p>
<h5 id="2-2-future对象"><a href="#2-2-future对象" class="headerlink" title="2.2 future对象"></a>2.2 future对象</h5><p>前面定义说了future表示将来执行或没有执行的任务的结果，task是future的子类。<br>基本的方法有：<br>• cancel(): 取消future的执行，调度回调函数<br>• result(): 返回future代表的结果<br>• exception(): 返回future中的Exception<br>• add_done_callback(fn): 添加一个回调函数，当future执行的时候会调用这个回调函数。<br>• set_result(result): 将future标为运行完成，并且设置return值，该方法常用<br>使用future，可以在协程结束后自行回调函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line">...同上</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coru_1</span>(<span class="params">future_obj,N</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coru_1 started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    total=<span class="built_in">sum</span>(<span class="built_in">range</span>(N))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future_obj.set_result(<span class="string">&#x27;coru_1 returns:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total))</span><br><span class="line">    print(<span class="string">&#x27;coru_1 done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coru_2</span>(<span class="params">future_obj,N</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coru_2 started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    total=<span class="built_in">sum</span>(<span class="built_in">range</span>(N))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future_obj.set_result(<span class="string">&#x27;coru_2 returns:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total))</span><br><span class="line">    print(<span class="string">&#x27;coru_2 done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call_back</span>(<span class="params">future_obj</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;saved to redis at :&#x27;</span>,get_time(),future_obj,future_obj.result())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    f1=asyncio.Future()</span><br><span class="line">    f2=asyncio.Future()</span><br><span class="line">    tasks=[coru_1(f1,<span class="number">10</span>),coru_2(f2,<span class="number">20</span>)]</span><br><span class="line">    f1.add_done_callback(call_back)</span><br><span class="line">    f2.add_done_callback(call_back)</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coru_1 started at:16:52:07</span><br><span class="line">coru_2 started at:16:52:07</span><br><span class="line">coru_1 done at:16:52:09</span><br><span class="line">coru_2 done at:16:52:09</span><br><span class="line">saved to redis at : 16:52:10 &lt;Future finished result=&#x27;coru_1 returns:45&#x27;&gt; coru_1 returns:45</span><br><span class="line">saved to redis at : 16:52:11 &lt;Future finished result=&#x27;coru_2 returns:190&#x27;&gt; coru_2 returns:190</span><br></pre></td></tr></table></figure>
<p>两个协程同时启动且在同一时间结束运行。之后开始回调，可以看到协程1先回调，1秒完成后，再切换到协程2回调。</p>
<h5 id="2-3-获取协程并发执行后的所有返回值"><a href="#2-3-获取协程并发执行后的所有返回值" class="headerlink" title="2.3 获取协程并发执行后的所有返回值"></a>2.3 获取协程并发执行后的所有返回值</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line">    </span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">coros=[read_file(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)] <span class="comment"># 创建多个协程</span></span><br><span class="line">tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros]<span class="comment"># 将协程封装为task对象 </span></span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"><span class="comment"># 或者loop.run_until_complete(asyncio.gether(*tasks))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重点在这里，当所有的协程结束后，可批量获取所有协程的返回结果</span></span><br><span class="line">get_all_result=[ t.result() <span class="keyword">for</span> t <span class="keyword">in</span> tasks]</span><br><span class="line">print(get_all_result)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">task-0 started at:15:53:08</span><br><span class="line">task-1 started at:15:53:08</span><br><span class="line">task-2 started at:15:53:08</span><br><span class="line">task-3 started at:15:53:08</span><br><span class="line">[&#x27;task-0 done at:15:53:09&#x27;, &#x27;task-1 done at:15:53:09&#x27;, &#x27;task-2 done at:15:53:09&#x27;, &#x27;task-3 done at:15:53:09&#x27;]</span><br></pre></td></tr></table></figure>
<p>以上也无需使用future的回调机制获取协程返回值，直接在loop结束后，从task对象的result方法即可获得协程返回值。<br>需要注意的是：<br>用于等待所有协程完成的方法asyncio.wait和asyncio.gather，都是接受多个future或coro组成的列表，区别：asyncio.gather内边调用ensure_future方法将列表中不是task的coro封装为future对象，而wait则没有。</p>
<h5 id="2-4-asyncio-gather-vs-asyncio-wait"><a href="#2-4-asyncio-gather-vs-asyncio-wait" class="headerlink" title="2.4 asyncio.gather vs asyncio.wait"></a>2.4 asyncio.gather vs asyncio.wait</h5><p>这里再给两个例子说明这两者的区别以及应用场合：<br>asyncio.gather</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">group_id,coro_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;group&#123;&#125;-task&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(group_id,coro_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(coro_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;group&#123;&#125;-task&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(group_id,coro_id,get_time())</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建三组tasks</span></span><br><span class="line">tasks1=[asyncio.ensure_future(coro(<span class="number">1</span>,i))<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">5</span>)]</span><br><span class="line">tasks2=[asyncio.ensure_future(coro(<span class="number">2</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>,<span class="number">8</span>)]</span><br><span class="line">tasks3=[asyncio.ensure_future(coro(<span class="number">3</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">group1=asyncio.gather(*tasks1) <span class="comment"># 对第1组的协程进行分组，group1</span></span><br><span class="line">group2=asyncio.gather(*tasks2) <span class="comment"># 对第2组的协程进行分组，group2</span></span><br><span class="line">group3=asyncio.gather(*tasks3) <span class="comment"># 对第3组的协程进行分组，group3</span></span><br><span class="line"></span><br><span class="line">all_groups=asyncio.gather(group1,group2,group3) <span class="comment"># 把3个group再聚合成一个大组，也是就所有协程对象的被聚合到一个大组</span></span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">all_group_result=loop.run_until_complete(all_groups) </span><br><span class="line"><span class="keyword">for</span> index,group <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_group_result): <span class="comment"># 获取每组协程的输出</span></span><br><span class="line">    print(<span class="string">&#x27;group &#123;&#125; result:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index+<span class="number">1</span>,group))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">group1-task1 started at:35:19</span><br><span class="line">group1-task2 started at:35:19</span><br><span class="line">group1-task3 started at:35:19</span><br><span class="line">group1-task4 started at:35:19</span><br><span class="line">group2-task6 started at:35:19</span><br><span class="line">group2-task7 started at:35:19</span><br><span class="line">group3-task8 started at:35:19</span><br><span class="line">group3-task9 started at:35:19</span><br><span class="line">group 1 result:[&#x27;group1-task1 done at:35:21&#x27;, &#x27;group1-task2 done at:35:21&#x27;, &#x27;group1-task3 done at:35:21&#x27;, &#x27;group1-task4 done at:35:21&#x27;]</span><br><span class="line">group 2 result:[&#x27;group2-task6 done at:35:21&#x27;, &#x27;group2-task7 done at:35:21&#x27;]</span><br><span class="line">group 3 result:[&#x27;group3-task8 done at:35:21&#x27;, &#x27;group3-task9 done at:35:21&#x27;]</span><br></pre></td></tr></table></figure>
<p>从打印结果可知，每组协程都在同一时刻开始以及同一时刻结束，asyncio.gather就是用于在更高层面对task进行分组，以不同的组管理不同的协程，你可以看出gather是一个粗粒度组织协程，自动收集所有协程结束后的返回值。</p>
<p>asyncio.wait</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line">tasks=[coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">first_complete,unfinished1=loop.run_until_complete(asyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED))</span><br><span class="line"><span class="comment"># 获取首个已结束的协程返回值,注意这里firt_complete是一个set()</span></span><br><span class="line"></span><br><span class="line">first_done_task=first_complete.pop()</span><br><span class="line">print(<span class="string">&#x27;首个完成的协程返回值：&#x27;</span>,first_done_task.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished1))</span><br><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第二阶段已完成的协程返回值</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished2:</span><br><span class="line">    print(t.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第二阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished3,unfinished3=loop.run_until_complete(asyncio.wait(unfinished2))</span><br><span class="line"><span class="comment"># 获取第三阶段已完成的协程返回值</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished3:</span><br><span class="line">    print(t.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished3))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-5 started at:23:40</span><br><span class="line">coro-1 started at:23:40</span><br><span class="line">coro-6 started at:23:40</span><br><span class="line">coro-7 started at:23:40</span><br><span class="line">coro-2 started at:23:40</span><br><span class="line">coro-8 started at:23:40</span><br><span class="line">coro-3 started at:23:40</span><br><span class="line">coro-9 started at:23:40</span><br><span class="line">coro-4 started at:23:40</span><br><span class="line">首个完成的协程返回值： coro-1 done at:23:41</span><br><span class="line">还未结束的协程数量： 8</span><br><span class="line">coro-2 done at:23:42</span><br><span class="line">coro-3 done at:23:43</span><br><span class="line">coro-4 done at:23:44</span><br><span class="line">还未结束的协程数量： 5</span><br><span class="line">coro-6 done at:23:46</span><br><span class="line">coro-7 done at:23:47</span><br><span class="line">coro-9 done at:23:49</span><br><span class="line">coro-5 done at:23:45</span><br><span class="line">coro-8 done at:23:48</span><br><span class="line">还未结束的协程数量： 0</span><br></pre></td></tr></table></figure>
<p>从输出结果可以很清看出<code>asyncio.wait</code>很精确的控制协程运行过程，通过<code>wait(return_when=asyncio.FIRST_COMPLETED)</code>可拿到运行完成的协程，通过<code>wait(timeout)</code>控制指定时间后放回已完成的协程。</p>
<h5 id="2-5-嵌套协程的实现（协程内调用协程）"><a href="#2-5-嵌套协程的实现（协程内调用协程）" class="headerlink" title="2.5 嵌套协程的实现（协程内调用协程）"></a>2.5 嵌套协程的实现（协程内调用协程）</h5><h6 id="一种易于理解的调用异步函数的方式"><a href="#一种易于理解的调用异步函数的方式" class="headerlink" title="一种易于理解的调用异步函数的方式"></a>一种易于理解的调用异步函数的方式</h6><p>&#8195;&#8195;在介绍嵌套协程或者闭包协程、协程内调用协程的概念前，先看看普通函数内部调用普通函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func1</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func2</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func3</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gel_all_data</span>():</span></span><br><span class="line">    result1=func1(<span class="string">&#x27;foo&#x27;</span>)</span><br><span class="line">    result2=func2(result1)</span><br><span class="line">    result3=func3(result2)</span><br><span class="line">    <span class="keyword">return</span> (result1,result2,result3)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;在同步的编程思维下，大家很容易理解get_all_data函数内部调用func1等三个外部函数来获取相应返回值，其实将同步改为异步的过程很简单：<br>==在每个函数前面使用关键字async向python解释器声明这是异步函数，如果需要调用外部异步函数，需使用await关键字==，将上面的同步编程改成异步编程的模式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func1</span>(<span class="params">start_data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>) <span class="comment"># 要使用asyncio的异步sleep方法，它会让出线程控制权给其他协程，而内建的sleep为同步性质</span></span><br><span class="line">    <span class="keyword">return</span> start_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func2</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func3</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_all_data</span>():</span></span><br><span class="line">    result1=<span class="keyword">await</span> func1(<span class="string">&#x27;foo&#x27;</span>) <span class="comment"># 在异步函数内部，使用await关键字调用其他异步函数，并获取该异步函数的返回值。执行流会在此将当前线程控权让出</span></span><br><span class="line">    result2=<span class="keyword">await</span> func2(result1)<span class="comment"># 同上</span></span><br><span class="line">    result3=<span class="keyword">await</span> func3(result2) <span class="comment"># 同上</span></span><br><span class="line">    <span class="keyword">return</span>(result1,result2,result3)</span><br></pre></td></tr></table></figure>
<p>该异步的get_all_data其实要实现的需求为：一个协程内部调用其他协程，而且可以将返回值放置在不同的协程上，可以实现链式的协程调度，这看起来就是一个协程任务流 。<br>当熟悉了这种异步的编程模式后，可以玩一些更为进阶的例子：</p>
<h6 id="协程嵌套示例"><a href="#协程嵌套示例" class="headerlink" title="协程嵌套示例"></a>协程嵌套示例</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">inner_coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">5</span>) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    coros=[inner_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros] </span><br><span class="line">    inner_tasks,pendings=<span class="keyword">await</span> asyncio.wait(tasks) <span class="comment"># 这句实现了协程中再调用协程</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    <span class="comment"># 使用asyncio.wait(tasks)可以在外层协程里面获取嵌套协程的运行返回值</span></span><br><span class="line">    <span class="keyword">for</span> task <span class="keyword">in</span> inner_tasks:</span><br><span class="line">        print(task.result())</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">	loop.close()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">outter_coro started at:14:52:59</span><br><span class="line">coro-0 started at:14:52:59</span><br><span class="line">coro-1 started at:14:52:59</span><br><span class="line">coro-2 started at:14:52:59</span><br><span class="line">coro-3 started at:14:52:59</span><br><span class="line"></span><br><span class="line">outter_coro done at:14:53:04</span><br><span class="line">coro-1 done at:14:53:04</span><br><span class="line">coro-3 done at:14:53:04</span><br><span class="line">coro-2 done at:14:53:04</span><br><span class="line">coro-0 done at:14:53:04</span><br></pre></td></tr></table></figure>
<p>可以看到外层协程和内层协程同时启动（当然外层协程函数最先执行），而且都在同一个时刻结束。<br>内层协程返回值只能在外程协程内部获取，能否在<br>loop.run_until_complete(outter_coro()) 之后，一次性获取协程返回值？ 需要改用asyncio.gather方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    coros=[inner_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros] </span><br><span class="line">    print(<span class="string">&#x27;outter_coro done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">all_coro_result=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> all_coro_result:</span><br><span class="line">    print(t)</span><br><span class="line">    </span><br><span class="line">loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">outter_coro started at:58:44</span><br><span class="line">outter_coro done at:58:44</span><br><span class="line">coro-0 started at:58:44</span><br><span class="line">coro-1 started at:58:44</span><br><span class="line">coro-2 started at:58:44</span><br><span class="line">coro-3 started at:58:44</span><br><span class="line">coro-0 done at:58:46</span><br><span class="line">coro-1 done at:58:46</span><br><span class="line">coro-2 done at:58:46</span><br><span class="line">coro-3 done at:58:46</span><br></pre></td></tr></table></figure>
<p>从外层协程outter_coro的启动时刻和结束时刻都一样可以看出，outter_coro和return await asyncio.gather(*tasks)是异步执行的，且在outter_coro结束后，loop事件循环只需管理coro-0到coro-3这4个协程。</p>
<p>从以上两种嵌套协程返回值的写法，可以看到这样逻辑：</p>
<ul>
<li>外层协程直接返回 awaitable对象给loop，loop就可以在最后获取所有协程的返回值；<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>()</span></span><br><span class="line"><span class="function">	<span class="title">return</span> <span class="title">await</span> <span class="title">asyncio</span>.<span class="title">wait</span>(<span class="params">tasks</span>) # 返回<span class="title">awaitable</span>对象给下文<span class="title">loop</span>，这里用<span class="title">asyncio</span>.<span class="title">wait</span>挂起所有协程</span></span><br><span class="line"><span class="function">	</span></span><br><span class="line">done,pending=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">	print(task.result())</span><br></pre></td></tr></table></figure></li>
<li>外层协程没有返回 awaitable对象给loop，loop无法获取所有协程的返回值，只能在外程协程里面获取所有协程返回值<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>()</span></span><br><span class="line">	done,pending=await asyncio.wait(tasks) # 没有返回awaitable对象给下文loop</span><br><span class="line">	<span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">		print(task.result())</span><br><span class="line">		</span><br><span class="line">loop.run_until_complete(outter_coro())</span><br></pre></td></tr></table></figure>
<h5 id="2-6-如何取消运行中协程"><a href="#2-6-如何取消运行中协程" class="headerlink" title="2.6 如何取消运行中协程"></a>2.6 如何取消运行中协程</h5>future（task）对象主要有以下几个状态：<br>pending、running、done、cancelled<br>创建future（task）的时候，task为pending状态:<br><code>tasks=[asyncio.ensure_future(coro) for coro in coros]</code><br>事件循环调用执行的时候且协程未结束时对应tsak为running状态，<br><code>loop.run_until_complete(outter_coro())</code><br>事件循环运行结束后，所有的task为done状态，<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">done,pending=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">	print(task.result())</span><br></pre></td></tr></table></figure>
那么最后一个cancelled状态如何实现呢？例如你想在某些协程未done之前将其cancel掉，如何处理？引用2.4章节的asyncio.wait例子说明：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line">tasks=[coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">first_complete,unfinished1=loop.run_until_complete(asyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED))</span><br><span class="line"><span class="comment"># 获取首个已结束的协程返回值,注意这里firt_complete是一个set()</span></span><br><span class="line"></span><br><span class="line">first_done_task=first_complete.pop()</span><br><span class="line">print(<span class="string">&#x27;首个完成的协程返回值：&#x27;</span>,first_done_task.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished1))</span><br><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished2:</span><br><span class="line">    print(t.result())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> unfinished2: <span class="comment"># 取消剩余未运行的task</span></span><br><span class="line">    print(<span class="string">&#x27;cancell unfinished task:&#x27;</span>,task,<span class="string">&#x27;==&gt;is canceled:&#x27;</span>,task.cancel())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
输出结果：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coro-4 started at:17:47</span><br><span class="line">coro-5 started at:17:47</span><br><span class="line">coro-1 started at:17:47</span><br><span class="line">coro-6 started at:17:47</span><br><span class="line">coro-7 started at:17:47</span><br><span class="line">coro-2 started at:17:47</span><br><span class="line">coro-8 started at:17:47</span><br><span class="line">coro-3 started at:17:47</span><br><span class="line">coro-9 started at:17:47</span><br><span class="line">首个完成的协程返回值： coro-1 done at:17:48</span><br><span class="line">还未结束的协程数量： 8</span><br><span class="line">coro-2 done at:17:49</span><br><span class="line">coro-3 done at:17:50</span><br><span class="line">coro-4 done at:17:51</span><br><span class="line">还未结束的协程数量： 5</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br></pre></td></tr></table></figure>
从输出结果可看到，8个协程task并发运行，最早结束的是coro-1，接着是coro-2、coro-3、coro-4，因为设定asyncio.wait(unfinished1,timeout=3) 3秒超时，只要超过3秒后，loop返回这些未运行的task，接着再逐个取消，可以看到5个协程被取消，True表示当前协程取消成功。</li>
</ul>
<h5 id="2-7-理解loop的相关方法"><a href="#2-7-理解loop的相关方法" class="headerlink" title="2.7  理解loop的相关方法"></a>2.7  理解loop的相关方法</h5><h6 id="loop-run-until-complate-vs-loop-run-forever"><a href="#loop-run-until-complate-vs-loop-run-forever" class="headerlink" title="loop.run_until_complate vs loop.run_forever"></a>loop.run_until_complate vs loop.run_forever</h6><p>&#8195;&#8195;<code>loop.run_until_complate</code>可以在程序的不同位置多次调用，例如在2.4 <code>asyncio.gather vs asyncio.wait</code> 提到的<code>asyncio.wait </code>用法，同一程序中能出现多个<code>loop.run_until_complate</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第二阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished3,unfinished3=loop.run_until_complete(asyncio.wait(unfinished2))</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished3))</span><br></pre></td></tr></table></figure>
<p>而对于 loop.run_forever，在同一程序中，只能有一个，因为该事件是在当前线程后台永久运行:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;start a coro&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;coro done&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro())</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.run_forever() <span class="comment"># 程序不会退出，loop一直挂在这里，等待其他future对象</span></span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro()) <span class="comment">#程序没有报错，但执行流永远不会到达这里，该句永远不会运行</span></span><br><span class="line">loop.run_forever() <span class="comment"># 程序没有报错，但执行流永远不会到达这里,该语句永远不会运行 </span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">start a coro</span><br><span class="line">coro done</span><br><span class="line">,,,,</span><br><span class="line"># 等待程序退出</span><br></pre></td></tr></table></figure>
<h6 id="loop-stop"><a href="#loop-stop" class="headerlink" title="loop.stop()"></a>loop.stop()</h6><p>上面提到如果程序仅有loop.run_forever()，那么当future完成后，程序一直没有退出，若要求实现当future完成后，程序也需要正常退出，可以这样处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span>  datetime</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">n</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;start a coro at &#x27;</span>,get_time())</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(n)</span><br><span class="line">    print(<span class="string">&#x27;coro done at &#x27;</span>,get_time())</span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro(<span class="number">2</span>))</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.stop() <span class="comment"># 在run_forever()前，先stop</span></span><br><span class="line">print(<span class="string">&#x27;stop后，loop事件还在运行？ at &#x27;</span>,get_time())</span><br><span class="line">loop.run_forever() </span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">stop后，loop事件还在运行？ at  43:11</span><br><span class="line">start a coro at  43:11</span><br></pre></td></tr></table></figure>
<p>从输出可以看到，上面的示例代码都是异步并发运行。<br><code>print(&#39;stop后，loop事件还在运行？ at &#39;,get_time())</code>语句跟future任务同时运行<br>==注意：如果把loop.stop()方法放在run_forever后面，可预见，程序不会退出==</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">loop.run_forever() </span><br><span class="line">loop.stop() <span class="comment"># 执行流永远不会到达这一句</span></span><br><span class="line">print(<span class="string">&#x27;stop后，loop事件还在运行？ at &#x27;</span>,get_time()) <span class="comment"># 执行流永远不会到达这一句</span></span><br></pre></td></tr></table></figure>
<h6 id="loop-call-soon-loop-call-later"><a href="#loop-call-soon-loop-call-later" class="headerlink" title="loop.call_soon/loop.call_later"></a>loop.call_soon/loop.call_later</h6><p>这两个方法用于在异步函数里面调用同步函数(普通函数)，且可以实现立刻调用或者稍后调用：<br><code>loop.call_soon(callback, *args, context=None）</code>: 立刻调用，并返回</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">name,stat=<span class="number">1</span></span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>,name,<span class="string">&#x27;keyword args:&#x27;</span>,stat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">loop</span>):</span></span><br><span class="line">    loop.call_soon(callback,<span class="string">&#x27;get first callback&#x27;</span>)</span><br><span class="line">    wrapper_func=functools.partial(callback,stat=<span class="number">2</span>)</span><br><span class="line">    loop.call_soon(wrapper_func,<span class="string">&#x27;get second call back&#x27;</span>)</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    loop.run_until_complete(run(loop))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure>
<p>打印：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">args: get first callback keyword args: 1</span><br><span class="line">args: get second call back keyword args: 2</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;异步调用同步函数其实已经破坏了异步的并发机制，因此很少使用这些非异步的方法。<br>&#8195;&#8195;此外loop.call_soon不支持协程函数传入关键字，因此可以通过偏函数先把关键字参数”传入“callback的kwargs里面，之后在call_soon里面，就可以利用这个被”包装过的callback“再传入位置参数即可(loop.run_until_complete传入关键字参数也一样这么处理)</p>
<p><code>loop.call_later(delay, callback, *args, context=None) </code>： 再给定一个时间之后，再调用callback。context默认当前线程的上下文</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">name,stat=<span class="number">1</span></span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>,name,<span class="string">&#x27;keyword args:&#x27;</span>,stat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">loop</span>):</span></span><br><span class="line">    loop.call_later(<span class="number">2</span>,callback,<span class="string">&#x27;get first callback&#x27;</span>)</span><br><span class="line">    loop.call_soon(callback,<span class="string">&#x27;callback soon&#x27;</span>)    </span><br><span class="line">    wrapper_func=functools.partial(callback,stat=<span class="number">0</span>)</span><br><span class="line">    loop.call_later(<span class="number">1</span>,wrapper_func,<span class="string">&#x27;get second callback&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>) <span class="comment"># 这里如果不设sleep，那么call_soon执行后loop马上退出，导致2个有延时运行的callback也退出了。这里要大于等于delay时间最长的call_later</span></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    loop.run_until_complete(run(loop))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure>
<p>打印：可以看到call_soon最先完成回调，接着才是设为1秒后运行的回调，2秒的回调</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">args: callback soon keyword args: 1</span><br><span class="line">args: get second callback keyword args: 0</span><br><span class="line">args: get first callback keyword args: 1</span><br></pre></td></tr></table></figure>
<p>同样，该loop.call_later不常用。</p>
<h4 id="3、asyncio进阶用法"><a href="#3、asyncio进阶用法" class="headerlink" title="3、asyncio进阶用法"></a>3、asyncio进阶用法</h4><p>&#8195;&#8195;在上面的例子中，一个主线程创建一个永久事件循环（该永久事件不会自动退出，而是run forever，除非主线程运行后没有阻塞或者手动中断程序运行），再把所有的协程注册到该永久事件循环，该方式较为基础用法的异步模式。在这一节，🔚asyncio高级用法，用线程1创建一个forever事件循环，线程2可以向事件循环中动态添加协程，而且不受主线程阻塞。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_another_loop</span>(<span class="params">loop</span>):</span></span><br><span class="line">    asyncio.set_event_loop(loop)</span><br><span class="line">    loop.run_forever()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">new_loop = asyncio.new_event_loop()</span><br><span class="line"><span class="comment"># start一个新的线程1，用于启动一个永久事件循环</span></span><br><span class="line">t = threading.Thread(target=start_another_loop,args=(new_loop,))</span><br><span class="line">t.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前线程向线程1的loop注册tasks</span></span><br><span class="line">asyncio.run_coroutine_threadsafe(read_file(<span class="number">5</span>),new_loop)</span><br><span class="line">asyncio.run_coroutine_threadsafe(read_file(<span class="number">5</span>),new_loop)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-5 started at:09:30</span><br><span class="line">coro-2 started at:09:30</span><br><span class="line">coro-2 done at:09:35</span><br><span class="line">coro-5 done at:09:35</span><br></pre></td></tr></table></figure>
<p>这个动态添加协程task对象有何用？如果task的参数是从redis队列实时取得，然后交由run_coroutine_threadsafe向loop注册协程，那么不就实现基于协程producer-consumer模式。</p>
<h5 id="利用redis-队列实现loop循环事件动态添加协程"><a href="#利用redis-队列实现loop循环事件动态添加协程" class="headerlink" title="利用redis 队列实现loop循环事件动态添加协程"></a>利用redis 队列实现loop循环事件动态添加协程</h5><p>&#8195;&#8195;这种方式，可以实现并发模式，producer：向redis 队列push 数据(这个数据是指协程task需要的参数，例如sleep的)，consumer：使用asyncio.run_coroutine_threadsafe(read_file(msg),new_loop)不断消费producer的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> datetime,time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCoro</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host=<span class="string">&#x27;127.0.0.1&#x27;</span>,port=<span class="number">6379</span>,key=<span class="string">&#x27;coro_queue&#x27;</span>,max_redis_conns=<span class="number">1000</span>,semaphore=<span class="number">2</span></span>):</span></span><br><span class="line">        self.r_pool=redis.ConnectionPool(host=host,port=port,max_connections=max_redis_conns)</span><br><span class="line">        self.r_conn=redis.Redis(connection_pool=self.r_pool)</span><br><span class="line">        self.r_queue_key=key</span><br><span class="line">        self.semaphore=semaphore</span><br><span class="line">        self.new_loop=asyncio.new_event_loop()</span><br><span class="line">        self.start()      </span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">self,task_id</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; 协程的worker，这里模拟IO耗时操作 &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,self.get_time()))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">        print(<span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,self.get_time()))</span><br><span class="line">        <span class="comment">#return &#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;.format(task_id,get_time()</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forever_loop</span>(<span class="params">self,loop_obj</span>):</span></span><br><span class="line">    	<span class="string">&quot;&quot;&quot;用于主线程启动一个永久事件循环，接收来自另外一个线程注册的协程对象&quot;&quot;&quot;</span></span><br><span class="line">        asyncio.set_event_loop(loop_obj)</span><br><span class="line">        loop_obj.run_forever()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_forever_loop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用一个主线程去启动一个永久事件循环&quot;&quot;&quot;</span></span><br><span class="line">        t=threading.Thread(target=self.forever_loop,args=(self.new_loop,))</span><br><span class="line">        t.start()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forever_consumer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;由另外一个子线层启动，该线程不断从redis队列获取数据，并用run_coroutine_threadsafe不断向new_loop注册task对象&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            task_id=self.r_conn.rpop(self.r_queue_key)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> task_id:</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            task_id=task_id.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            asyncio.run_coroutine_threadsafe(self.coro(<span class="built_in">int</span>(task_id)),self.new_loop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_forever_consumer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用一个子线程用于向事件循环注册协程对象 &quot;&quot;&quot;</span></span><br><span class="line">        t=threading.Thread(target=self.forever_consumer)</span><br><span class="line">        t.start()</span><br><span class="line">        t.join()<span class="comment"># 这里要阻塞当前线程，否则就无法实现不但从redis队列获取任务了。若不阻塞，主线程start()后，子线程start（）后，程序立即结束</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;在一个方法里面，同时启动两个线程，简化api&quot;&quot;&quot;</span></span><br><span class="line">        self.start_forever_loop()</span><br><span class="line">        self.forever_consumer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    coro=MyCoro() </span><br><span class="line">    coro.start()</span><br></pre></td></tr></table></figure>
<p>以上代码的逻辑：<br>创建两个线程，</p>
<ul>
<li>线程1负责启动一个forever事件循环，用于接收另外一个线程2注册的协程对象（task对象）</li>
<li>线程2负责不断从redis队列获取任务数据后，再把协程注册到线程1启动的事件循环，从而实现loop循环事件动态添加协程。<br>该例子只需要2个线程，即可实现高并发模式。</li>
</ul>
<p>测试：在redis-cli里面，先lpush一个10，再lpush1个2秒，1个4秒，最终线程2按顺序创建3个线程，都会注册到loop里面，注意对比3个协程的完成时间：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># redis-cli 在coro_queue队列添加数据</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 10</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 2</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 4</span><br><span class="line">(integer) 1</span><br><span class="line"></span><br><span class="line"># 程序输出</span><br><span class="line">coro-10 started at:56:48</span><br><span class="line">coro-2 started at:56:50</span><br><span class="line">coro-4 started at:56:51</span><br><span class="line">coro-2 done at:56:52</span><br><span class="line">coro-4 done at:56:55</span><br><span class="line">coro-10 done at:56:58</span><br></pre></td></tr></table></figure>
<p>从打印的时刻可以很清楚看到，3个协程并发执行，总共10秒完成。若同步模式，则需要2+4+10=16秒才能完成。</p>
<h4 id="4、asyncio最适合的使用场景"><a href="#4、asyncio最适合的使用场景" class="headerlink" title="4、asyncio最适合的使用场景"></a>4、asyncio最适合的使用场景</h4><p>&#8195;&#8195;从redis、Nginx、node.js、Tornado、Twisted这些使用IO多路复用技术的中间件或者框架，可以很明确的推出结论：异步逻辑非常适合处理有Network IO且高并发的socket连接场景，因为这些场景往往需要等待IO，例如：访问web接口数据、访问网页、访问数据库，都是client向server发起网络IO。因此本节给出asyncio的3个场景：异步爬虫，高并发的socket服务、数据库连接。<br>&#8195;&#8195;但是：asyncio的周边库似乎有不少坑，而且距离稳定生产环境有一定距离，参考<a href="https://www.zhihu.com/question/266094857/answer/304655007">知乎文章吐槽的asyncio</a>，所有很多文章介绍asyncio基本使用场合，或者自行开发的小工具，很少文章能给出一个使用asyncio实现的复杂项目。当然，协程肯定不适合CPU计算场景。<br>&#8195;&#8195;目前star较高的几个协程异步库有：<a href="https://github.com/aio-libs/aiohttp">aiohttp</a>、<a href="https://github.com/aio-libs/aioredis">aioredis</a>、<a href="https://github.com/aio-libs/aiomysql">aiomysql</a>、<a href="https://github.com/aio-libs/aiopg">aiopg</a>。aiopg:is a library for accessing a PostgreSQL database from the asyncio 。以上四个协程异步库底层通过封装asyncio实现。本节主要介绍aioredi和aiohttp，其他库可参考官方示例。</p>
<h5 id="4-1-aioredis"><a href="#4-1-aioredis" class="headerlink" title="4.1 aioredis"></a>4.1 aioredis</h5><p>&#8195;&#8195;aioredis实现的api很丰富，支持sentinel连接，运行原生redis命令的接口，但是它不支持cluster集群的连接：<code>Current release (1.3.0) of the library does not support Redis Cluster in a full manner.</code><br>单个连接示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">key,value</span>):</span></span><br><span class="line">    redis = <span class="keyword">await</span> aioredis.create_redis(</span><br><span class="line">        <span class="string">&#x27;redis://127.0.0.1&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> redis.<span class="built_in">set</span>(key,value)</span><br><span class="line">    val = <span class="keyword">await</span> redis.get(key)</span><br><span class="line">    redis.close()</span><br><span class="line">    <span class="keyword">await</span> redis.wait_closed()</span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure>
<p>下面使用单例模式和协程with协议，实现基本的async redis 类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AsynRedis</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    _instance = <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,redis_uri=<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,pool=<span class="literal">False</span>,max_conn=<span class="number">100</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span></span>):</span></span><br><span class="line">        self._redis_uri = redis_uri</span><br><span class="line">        self._encoding = encoding</span><br><span class="line">        self._pool=pool</span><br><span class="line">        self._max_conn=max_conn</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">__aenter__</span>(<span class="params">self</span>):</span> <span class="comment"># with协议入口</span></span><br><span class="line">        <span class="keyword">await</span> self.get_conn()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">__aexit__</span>(<span class="params">self, exc_type, exc, tb</span>):</span> <span class="comment">#with 协议出口</span></span><br><span class="line">        <span class="keyword">await</span> self.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_conn</span>(<span class="params">self</span>):</span> <span class="comment"># 单例模式创建redis连接，可选pool或者单连接</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._instance:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._pool:</span><br><span class="line">                self._instance = <span class="keyword">await</span> aioredis.create_redis(self._redis_uri)</span><br><span class="line">            self._instance=<span class="keyword">await</span> aioredis.create_redis_pool(self._redis_uri,maxsize=self._max_conn)</span><br><span class="line">        <span class="keyword">return</span> self._instance</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">asyn_set</span>(<span class="params">self,*args,**kwargs</span>):</span> </span><br><span class="line">        response=<span class="keyword">await</span> self._instance.<span class="built_in">set</span>(*args,**kwargs)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">asyn_get</span>(<span class="params">self,key</span>):</span></span><br><span class="line">        value=<span class="keyword">await</span> self._instance.get(key,encoding=self._encoding)</span><br><span class="line">        <span class="keyword">return</span> value </span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._instance:</span><br><span class="line">            self._instance.close()</span><br><span class="line">            <span class="keyword">await</span> self._instance.wait_closed()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_coro</span>(<span class="params">index</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsynRedis() <span class="keyword">as</span> f: <span class="comment"># 异步的aenter和aexit实现with协议</span></span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> start at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        key=<span class="string">&#x27;foo-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index)</span><br><span class="line">        result=<span class="keyword">await</span> f.asyn_set(key,get_time()) <span class="comment"># 将时刻作为value，用于观察协程并发,获取set操作返回值，True表示set成功。</span></span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,key is set? <span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    tasks=[redis_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-9 start at 57:15</span><br><span class="line">coro-4 start at 57:15</span><br><span class="line">coro-2 start at 57:15</span><br><span class="line">coro-5 start at 57:15</span><br><span class="line">coro-0 start at 57:15</span><br><span class="line">coro-8 start at 57:15</span><br><span class="line">coro-6 start at 57:15</span><br><span class="line">coro-7 start at 57:15</span><br><span class="line">coro-3 start at 57:15</span><br><span class="line">coro-1 start at 57:15</span><br><span class="line"></span><br><span class="line">coro-9 done at 57:16,key is set? True</span><br><span class="line">coro-4 done at 57:16,key is set? True</span><br><span class="line">coro-2 done at 57:16,key is set? True</span><br><span class="line">coro-5 done at 57:16,key is set? True</span><br><span class="line">coro-0 done at 57:16,key is set? True</span><br><span class="line">coro-8 done at 57:16,key is set? True</span><br><span class="line">coro-6 done at 57:16,key is set? True</span><br><span class="line">coro-7 done at 57:16,key is set? True</span><br><span class="line">coro-3 done at 57:16,key is set? True</span><br><span class="line">coro-1 done at 57:16,key is set? True</span><br></pre></td></tr></table></figure>
<p>可以看到10个redis协程同一时刻并发set key，并且同一时刻完成。<br>在redis-cli查看key，所有的key都value都是同一时刻，说明协程并发运行正确，而在多线程方式，则需要创建<code>10个线程</code>才可以实现<code>单线程+10协程</code>的效果，若当并发量高达1万+时，可以想象多线程将消耗大量系统资源以及线程切换，效率必然不高。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys foo*</span><br><span class="line"> 1) &quot;foo-8&quot;</span><br><span class="line"> 2) &quot;foo-3&quot;</span><br><span class="line"> 3) &quot;foo-9&quot;</span><br><span class="line"> 4) &quot;foo-0&quot;</span><br><span class="line"> 5) &quot;foo-1&quot;</span><br><span class="line"> 6) &quot;foo-7&quot;</span><br><span class="line"> 7) &quot;foo-5&quot;</span><br><span class="line"> 8) &quot;foo-6&quot;</span><br><span class="line"> 9) &quot;foo-2&quot;</span><br><span class="line">10) &quot;foo-4&quot;</span><br><span class="line">127.0.0.1:6379&gt; get foo-8</span><br><span class="line">&quot;57:16&quot;</span><br><span class="line">127.0.0.1:6379&gt; get foo-0</span><br><span class="line">&quot;57:16&quot;</span><br></pre></td></tr></table></figure>
<p>这里用两个魔法方法<code>__aenter__</code>,<code>__aexit__</code>实现with协议，但要注意的是，协程的with方法只能在协程函数内部使用，这个with的上下文就是该协程的上下文。如果写在主线程外部，则提示语法出错：<br><img src="https://img-blog.csdnimg.cn/20200104125027492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这是因为协程上下文代表协程自己的栈等信息，肯定不是主线程的上下文，所以不能把<code>async with</code>写在程序的全局位置<br>注意：aioredis似乎有个bug，当MacOS系统的文件描述最大限制已设为10000，aioredis不管使用单连接或者pool方式，当并发数设为大值例如1000，部分协程完成后，剩余部分协程都会被阻塞（暂未找到原因）。</p>
<p>在项目中，如果要使用aioredis，可以用asyncio的semaphore信号量限制并发数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_coro</span>(<span class="params">semaphore,index</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> semaphore:</span><br><span class="line">        r=<span class="keyword">await</span> aioredis.create_redis(<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,db=<span class="number">0</span>)</span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> start at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">0.05</span>)</span><br><span class="line">        key=<span class="string">&#x27;bar-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index)</span><br><span class="line">        result=<span class="keyword">await</span> r.<span class="built_in">set</span>(key,get_time(),expire=<span class="number">100</span>)</span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,key is set? <span class="subst">&#123;result&#125;</span>&#x27;</span>)    </span><br><span class="line">        r.close()</span><br><span class="line">        <span class="keyword">await</span> r.wait_closed()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">max_task=<span class="number">100</span></span>):</span></span><br><span class="line">    sem=asyncio.Semaphore(<span class="number">10</span>)</span><br><span class="line">    tasks=[redis_coro(sem,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_task)]</span><br><span class="line">    <span class="keyword">await</span> asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(run())</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure>

<h5 id="4-1-aiohttp"><a href="#4-1-aiohttp" class="headerlink" title="4.1 aiohttp"></a>4.1 aiohttp</h5><p>&#8195;&#8195;Asynchronous HTTP client/server framework for asyncio and Python<br>aiohttp应该是除了request库外最强大的HTTP库，而且是异步实现，三个主要功能：</p>
<ul>
<li>Supports both Client and HTTP Server.</li>
<li>Supports both Server WebSockets and Client WebSockets out-of-the-box without the Callback Hell.</li>
<li>Web-server has Middlewares, Signals and pluggable routing.<br>除了基本client和server端服务，还支持websocket、web-server模块还支持可插拔的中间件，官方也给了详细的aiohttp demo代码，<a href="https://github.com/aio-libs/aiohttp-demos">链接</a></li>
</ul>
<p>以下代码逻辑：使用协程并发get html页面，并使用协程方式存储html到文件或者存到redis</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime,time</span><br><span class="line"><span class="keyword">import</span> asyncio,aiohttp,aioredis,aiofiles</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">save_to_file</span>(<span class="params">html,index</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;将html文本存放到本地目录，使用异步aiofiles实现。&quot;&quot;&quot;</span></span><br><span class="line">    file_name=<span class="string">f&#x27;/opt/test_aiohttp/html-<span class="subst">&#123;index&#125;</span>.txt&#x27;</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(file_name,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">await</span> f.write(html)</span><br><span class="line">    print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">save_to_redis</span>(<span class="params">html,index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将html文本存放到redis，采用协程模式 &quot;&quot;&quot;</span></span><br><span class="line">    r=<span class="keyword">await</span> aioredis.create_redis(<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,db=<span class="number">0</span>)</span><br><span class="line">    coro_key=<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span>&#x27;</span></span><br><span class="line">    result=<span class="keyword">await</span> r.<span class="built_in">set</span>(coro_key,html,expire=<span class="number">60</span>)</span><br><span class="line">    print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,is saved? <span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line">    r.close()</span><br><span class="line">    <span class="keyword">await</span> r.wait_closed()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_html</span>(<span class="params">semaphore,session,url,index,container=<span class="string">&#x27;file&#x27;</span></span>):</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;获取url对应的html文本，并调用存放文本的协程，这里就是前面章节提到的嵌套协层，用await调用外部协程 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> semaphore:<span class="comment"># 由外部传入的信号量，控制并发数</span></span><br><span class="line">            print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> started at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">with</span> session.get(url,timeout=<span class="number">5</span>) <span class="keyword">as</span> response:</span><br><span class="line">                    <span class="keyword">if</span> response.status==<span class="number">200</span>:</span><br><span class="line">                        <span class="keyword">if</span> container==<span class="string">&#x27;file&#x27;</span>:</span><br><span class="line">                            container_=save_to_file</span><br><span class="line">                        <span class="keyword">else</span>:container_=save_to_redis</span><br><span class="line">                        html_text=<span class="keyword">await</span> response.read()</span><br><span class="line">                        <span class="keyword">await</span> container_(html_text,index)</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">semaphore,url,max_workers=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        urls=[url]*max_workers</span><br><span class="line">        tasks=[asyncio.ensure_future(get_html(semaphore,session,each_url,index)) <span class="keyword">for</span> index,each_url <span class="keyword">in</span> <span class="built_in">enumerate</span>(urls)]</span><br><span class="line">        <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    semaphore=asyncio.Semaphore(<span class="number">5</span>)</span><br><span class="line">    loop.run_until_complete(run(semaphore,<span class="string">&#x27;http://spark.apachecn.org/#/&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 官方推荐使用Zero-sleep保证底层的一些socket连接完成关闭</span></span><br><span class="line">    loop.run_until_complete(asyncio.sleep(<span class="number">0</span>))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure>
<p>打印结果：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">coro-0 started at 49:26</span><br><span class="line">coro-1 started at 49:26</span><br><span class="line">coro-2 started at 49:26</span><br><span class="line">coro-3 started at 49:26</span><br><span class="line">coro-4 started at 49:26</span><br><span class="line"></span><br><span class="line">coro-0 done at 49:26</span><br><span class="line">coro-5 started at 49:26</span><br><span class="line">coro-2 done at 49:26</span><br><span class="line">coro-6 started at 49:26</span><br><span class="line">coro-4 done at 49:26</span><br><span class="line">coro-7 started at 49:26</span><br><span class="line">coro-1 done at 49:26</span><br><span class="line">coro-8 started at 49:26</span><br><span class="line">coro-3 done at 49:26</span><br><span class="line">coro-9 started at 49:26</span><br><span class="line"></span><br><span class="line">coro-6 done at 49:26</span><br><span class="line">coro-5 done at 49:27</span><br><span class="line">coro-7 done at 49:27</span><br><span class="line">coro-8 done at 49:27</span><br><span class="line">coro-9 done at 49:27</span><br></pre></td></tr></table></figure>
<p>这里使用asyncio.Semaphore(5)控制并发数，从输出可以看出，一开始有5个协程启动，最先5个协程运行结束后，另外5个协程同时启动。因为10个任务，分了2轮进行。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本文内容相对较多且杂，主要是asyncio协程库有较多api，这些api与同步编程的python库有较大的区别，结合asyncio实现的几个第三方协层库来看，可以看到协程在python生态位置较为小众，如果在项目（例如高性能web接口服务）中引入协程异步编程，可以考虑Tornado以及Twisted。（想想Go语言还是相当强大，协程生态成熟）</p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>协程</tag>
        <tag>asyncio</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）</title>
    <url>/blog/2020/01/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%BC%82%E6%AD%A5IO%E7%9A%84%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91%E2%80%94%E2%80%94IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%EF%BC%88select%E3%80%81poll%E3%80%81epoll%EF%BC%89/</url>
    <content><![CDATA[<p>&#8195;&#8195;在前面两篇文章<a href="https://blog.csdn.net/pysense/article/details/103721630">《gevent与协程》</a>和<a href="https://blog.csdn.net/pysense/article/details/103745410">《asyncio与协程》</a>，讨论了有关协程异步编程方面的内容，从代码层面和基本的demo可以大致理解协程的工作方式。如果要深入理解为何单线程基于事件的驱动可以在“低能耗”的条件下达到高性能的IO服务，则要研究Linux底层实现原理——IO多路复用，而理解IO多路复用的前提是对文件描述符有较为深入的理解，因此本文把文件描述符和IO多路复用放在同一篇文章里，形成全局的体系化认知，这就是本文讨论的内容。</p>
<a id="more"></a>



<h4 id="1、理解文件描述符"><a href="#1、理解文件描述符" class="headerlink" title="1、理解文件描述符"></a>1、理解文件描述符</h4><h5 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h5><p>&#8195;&#8195;在Linux中，一切皆文件，而理解文件描述符才能理解“一切皆文件”的真实含义，IO多路复用的select、poll和epoll机制正是通过操作文件描述符集合来处理IO事件。<br>含义，这里引用百度的介绍：</p>
<blockquote>
<p>&#8195;&#8195;文件描述符是一个索引号，是一个非负整数，它指向普通的文件或者I/O设备，它是连接用户空间和内核空间纽带。在linux系统上<a href="https://baike.baidu.com/item/%E5%86%85%E6%A0%B8/108410">内核</a>（kernel）利用文件描述符（file descriptor）来访问文件。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要使用文件描述符来指定待读写的文件。（在Windows系统上，文件描述符被称作文件句柄）</p>
</blockquote>
<p>当你看完本篇内容后，再回它这段解释，总结得真到位！在后面会给出为何文件描述符是一个非负整数，而不是其他更为复杂数据结构呢（例如hash map、list、链表等）？</p>
<h5 id="1-2-打开一个文件"><a href="#1-2-打开一个文件" class="headerlink" title="1.2 打开一个文件"></a>1.2 打开一个文件</h5><p>&#8195;&#8195;当某个进程打开一个已有文件或创建一个新文件时，内核向该进程返回一个文件描述符（一个非负整数）。<br>(在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于<a href="https://baike.baidu.com/item/UNIX">UNIX</a>、<a href="https://baike.baidu.com/item/Linux">Linux</a>这样的操作系统。)<br>这里以打开的iPython shell进程调用os.open为例，OS是Centos7.5</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> os</span><br><span class="line">In [<span class="number">6</span>]: fd = os.<span class="built_in">open</span>( <span class="string">&quot;/opt/test.txt&quot;</span>, os.O_RDWR|os.O_CREAT) <span class="comment"># os.O_RDWR读写模式打开，os.O_CREAT若文件不存在则创建               </span></span><br><span class="line">In [<span class="number">7</span>]: fd                                                                             </span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">17</span> <span class="comment"># 这个17就是file descriptor</span></span><br></pre></td></tr></table></figure>
<p>在Python里面，os.open方法返回文件描述符是更为底层API，而open方法是返回python文件对象，是更贴近用户的API。</p>
<p>在linux系统上查看以上iPython进程打开的所有文件描述符示例：（这里就是一个文件描述表的大致形式，每一个文件描述符指向一个文件或者设备）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ll /proc/11622/fd #11622为ipython的shell进程</span><br><span class="line">total 0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 0 -&gt; /dev/pts/0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 1 -&gt; /dev/pts/0</span><br><span class="line">lr-x------ 1 root root 64 **** 16:43 10 -&gt; pipe:[41268]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 11 -&gt; pipe:[41268]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 12 -&gt; anon_inode:[eventpoll]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 13 -&gt; socket:[41269]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 14 -&gt; socket:[41270]</span><br><span class="line">lr-x------ 1 root root 64 **** 16:43 15 -&gt; pipe:[41271]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 16 -&gt; pipe:[41271]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 17 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 18 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 19 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 2 -&gt; /dev/pts/0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 20 -&gt; anon_inode:[eventpoll]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 3 -&gt; /dev/null</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 4 -&gt; /root/.ipython/profile_default/history.sqlite</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 5 -&gt; /root/.ipython/profile_default/history.sqlite</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 6 -&gt; anon_inode:[eventpoll]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 7 -&gt; socket:[41266]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 8 -&gt; socket:[41267]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 9 -&gt; anon_inode:[eventpoll]</span><br></pre></td></tr></table></figure>

<p>因为在ipython里面，<code>fd = os.open( &quot;/opt/test.txt&quot;, os.O_RDWR)</code> 运行3次，也就文件<code>/opt/test.txt</code>打开3次，所以返回个文件描述符:17、18、19（从这里说明，同一进程可以同一时刻打开同一文件多次）</p>
<p>11622进程号指向当前iPython shell，查看它打开的文件描述符18，指向被打开文件：<code>/opt/test.txt</code>：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@nn opt]# ll /proc/11622/fd/18 </span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 /proc/11622/fd/18 -&gt; /opt/test.txt</span><br></pre></td></tr></table></figure>
<p>关闭文件描述符就关闭了所打开的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">In [14]: os.close(19)                                                                  </span><br><span class="line">In [15]: os.close(18)                                                                  </span><br><span class="line">In [16]: os.close(17)</span><br></pre></td></tr></table></figure>

<h5 id="1-3-对文件描述符进行读写"><a href="#1-3-对文件描述符进行读写" class="headerlink" title="1.3 对文件描述符进行读写"></a>1.3 对文件描述符进行读写</h5><p>读：通过给定文件描述符读文件内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># os.read()方法的docstring</span></span><br><span class="line"><span class="string">os.read()</span></span><br><span class="line"><span class="string">Signature: os.read(fd, length, /)</span></span><br><span class="line"><span class="string">Docstring: Read from a file descriptor.  Returns a bytes object.</span></span><br><span class="line"><span class="string">Type:      builtin_function_or_method</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd=os.<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,os.O_RDWR|os.O_CREAT)</span><br><span class="line">data=os.read(fd,<span class="number">64</span>) <span class="comment">#指定读文件前64byte内容 </span></span><br><span class="line">print(data) <span class="comment"># b&#x27;foo\nbar\n\n&#x27;</span></span><br></pre></td></tr></table></figure>

<p>写：通过给定文件描述符将数据写入到文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># os.read()方法的docstring</span></span><br><span class="line"><span class="string">Signature: os.write(fd, data, /)</span></span><br><span class="line"><span class="string">Docstring: Write a bytes object to a file descriptor.</span></span><br><span class="line"><span class="string">Type:      builtin_function_or_method</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd=os.<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,os.O_RDWR|os.O_CREAT)</span><br><span class="line">byte_nums=os.write(fd,<span class="string">b&#x27;save data by file descriptor directly \n&#x27;</span>) <span class="comment"># 注意要写入byte类型的数据</span></span><br><span class="line">print(byte_nums) <span class="comment"># 返回写入byte字符串长度（字符个数）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>了解基本调用底层的os读写文件描述符的方法，也可以封装出一个类似内建open方法的定制myopen类。</p>
<h5 id="1-4-通过管道打开文件描述符"><a href="#1-4-通过管道打开文件描述符" class="headerlink" title="1.4 通过管道打开文件描述符"></a>1.4 通过管道打开文件描述符</h5><p>也可以通过管道pipe方法（创建一个无名管道）同时打开一个读文件描述符以及一个写文件描述符。（有关管道的定义和理解本文不再累赘，可参考其他博文。）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd_read,fd_write=os.pipe()</span><br><span class="line">print(<span class="string">&#x27;fd_read:&#x27;</span>,fd_read,<span class="string">&#x27;fd_write:&#x27;</span>,fd_write) <span class="comment">#系统返回两个整数3、4， fd_read: 3 fd_write: 4</span></span><br><span class="line">os.write(fd_write,<span class="string">b&#x27;foo&#x27;</span>) <span class="comment"># 向管道的写端写入数据</span></span><br><span class="line">os.read(fd_read,<span class="number">64</span>) <span class="comment"># 从管道的读端读取数据</span></span><br></pre></td></tr></table></figure>
<p>创建管道时总是返回相邻的两个整数，因为stderr为2，故之后创建的文件描述符只能从3开始，示意图如下：<br><img src="https://img-blog.csdnimg.cn/20200105162510770.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如果尝试向管道另外一端的fd_write描述符读取数据，就会报错，所以对于管道，读数据只能在读文件描述符上读操作，写入数据只能在写文件描述符操作。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">os.read(fd_write,64)</span><br><span class="line">OSError: [Errno 9] Bad file descriptor</span><br></pre></td></tr></table></figure>

<p>如果已经把fd_read读取完好后，此时管道为空，若再读取该管道，进程会被阻塞，因为写管道端没有数据写入，这是管道的性质之一——数据一旦被读走，便不在管道中存在，若此时还继续向读端反复读取，则进程会被阻塞。</p>
<p>注意写入管道的字符个数是有限制的，当超过管道容量时，写入操作被阻塞，可以通过以下方法精确策略出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pipe_capacity</span>(<span class="params">size</span>):</span></span><br><span class="line">    fd_read,fd_write=os.pipe()</span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    print(<span class="string">&quot;start to count&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,size+<span class="number">1</span>):</span><br><span class="line">        os.write(fd_write,<span class="string">b&#x27;a&#x27;</span>)</span><br><span class="line">        total=i</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">&quot;end to count,total bytes:&quot;</span>,total)</span><br><span class="line"></span><br><span class="line">get_pipe_capacity(<span class="number">64</span>*<span class="number">1024</span>)</span><br><span class="line">输出：</span><br><span class="line">start to count</span><br><span class="line">end to count,total <span class="built_in">bytes</span>: <span class="number">65536</span></span><br></pre></td></tr></table></figure>

<p>往管道写入<code>64*1024</code> 大小的byte时，管道写端未发生阻塞，当把写入的byte数改为：写入<code>64*1024+1 </code>时，写入操作被阻塞了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_pipe_capacity(<span class="number">64</span>*<span class="number">1024</span>+<span class="number">1</span>)</span><br><span class="line">输出:</span><br><span class="line">start to count <span class="comment"># 执行流被阻塞，无后续输出。</span></span><br></pre></td></tr></table></figure>

<p>通过该方法可以精确测量出pipe默认容量为64KB。<br>看到这部内容，是否有人联想到在使用subprocess执行某些cmd命令后，一直卡在读取输出上？<br>常见用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p= subprocess.Popen(your_cmd, shell= <span class="literal">True</span>, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) <span class="comment"># 问题出现在：标准输出使用了管道，而管道有容量限制，当命令返回的数据大小超过管道64KB时，执行流卡在这里</span></span><br><span class="line">bytes_result, err = p.communicate(timeout=<span class="number">1</span>)</span><br><span class="line">str_result = <span class="built_in">str</span>(bytes_result, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> p.returncode != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> str_result:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>问题出现在：subprocess.Popen标准输出使用了管道，而管道有容量限制，当你的your_cmd返回的数据大小超过管道64KB时（stdout获取返回数据，用了管道存放），执行流卡在subprocess.Popen这里，其实进程阻塞了。<br>既然知道管道有容量限制，那么可以将stdout定向到本地文件系统，那么输出的数据就存放到容量更大的文件，建议使用临时文件作为重定向输出，如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stdout_by_tempfile</span>():</span></span><br><span class="line">    <span class="comment"># SpooledTemporaryFile也是一个普通文件对象，当然支持with协议（它的源码实现了__enter__和__exit__方法）</span></span><br><span class="line">    <span class="keyword">with</span> tempfile.SpooledTemporaryFile(buffering=<span class="number">1</span>*<span class="number">1024</span>) <span class="keyword">as</span> tf:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个临时文件对象，注意这个bufferfing不是限制只能存储1024字节的数据，而是输出内容超过1024字节后，自动将输出的数据缓存到临时文件里&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            fd = tf.fileno() <span class="comment"># 返回文件描述符，这个文件描述符不再指向管道，而是指向某个临时文件</span></span><br><span class="line">            p = subprocess.Popen(your_cmd,stdin=fd,stdout=fd,stderr=fd,shell=<span class="literal">True</span>) <span class="comment"># 将stdout输出的内容定向到文件描述符指向的文件</span></span><br><span class="line">            p.communicate(timeout=<span class="number">5</span>) <span class="comment"># 指定输出超时时间</span></span><br><span class="line">            tf.seek(<span class="number">0</span>) <span class="comment"># 将文件对象指针放置起始位置，以便读取从头到尾的完整的已存数据</span></span><br><span class="line">            output_data = tf.readlines() <span class="comment"># 一次读取临时文件的所有数据（这是读取的是byte类型），也可用迭代器（如果数据上几百M）这里tf就是普通文件对象，因为也有readlines、readline、write、tell等常见文件操作方法</span></span><br><span class="line">            save_to_db(output_data) <span class="comment"># 将your_cmd输出的数据存到db或者其他地方</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>经过对文件描述符的讨论，现在你可以轻松“操作进程的stdin或者stdout”</p>
<h5 id="1-5-常见的文件描述符0、1、2"><a href="#1-5-常见的文件描述符0、1、2" class="headerlink" title="1.5 常见的文件描述符0、1、2"></a>1.5 常见的文件描述符0、1、2</h5><p>&#8195;&#8195;在Linux系统上，每个进程都有属于自己的stdin、stdout、stderr。标准输入（standard input）的文件描述符是 0，标准输出（standard output）是 1，标准错误（standard error）是 2。尽管这种习惯并非<a href="https://baike.baidu.com/item/Unix">Unix</a>内核的特性，但是因为一些 shell 和很多应用程序都使用这种习惯，因此，如果内核不遵循这种习惯的话，很多应用程序将不能使用。</p>
<p>&#8195;&#8195;在上面的iPython例子中，ll /proc/11622/fd 其实是列出属于11622进程的文件描述符表，可以看到，每个进程拥有的fd数值从0到linux限制的最大值，其中每个进程自己的0、1、2就是用于当前进程的标准输入、标准输出和标准错误。</p>
<blockquote>
<p>关于文件描述符表简单介绍：操作系统内核为每个进程在u_block结构中维护文件描述符表，所有属于该进程的文件描述符都在该表中建立索引:数值–&gt;某个文件。你可以把整个文件描述表看成是一个C语言的数组，数组的元素指向文件引用，数组的下表就是它的文件描述符</p>
</blockquote>
<p>下面，已iPython的一个shell进程为例，如何将stdin、stdout、stderr的0、1、2替换为其他文件描述符：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 也可以用sys模块，例如：sys.stdout.fileno() </span></span><br><span class="line">In [<span class="number">4</span>]: stdin_fd=os.sys.stdin.fileno() <span class="comment"># 当前iPython shell进程的标准输入</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: stdin_fd</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: stdout_fd=os.sys.stdout.fileno() <span class="comment">#当前iPython shell进程的标准输出</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: stdout_fd</span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: stderr_fd=os.sys.stderr.fileno() <span class="comment">#当前iPython shell进程的标准错误</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: stderr_fd</span><br><span class="line">Out[<span class="number">9</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>os.sys.stdin等是什么呢？其实这些对象跟open(file_name,mode) 打开文件返回的文件对象是一样的，例如下面：os.sys.stdin是以utf-8编码的只读模式的文件对象，os.sys.stdout以及os.sys.stderr是以utf-8编码的写模式的文件对象，既然是文件对象，那么读对象就支持read、readline等方法，写对象则支持write等方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">21</span>]: os.sys.stdin</span><br><span class="line">Out[<span class="number">21</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stdin&gt;&#x27;</span> mode=<span class="string">&#x27;r&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: os.sys.stdout</span><br><span class="line">Out[<span class="number">22</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stdout&gt;&#x27;</span> mode=<span class="string">&#x27;w&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: os.sys.stderr</span><br><span class="line">Out[<span class="number">23</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stderr&gt;&#x27;</span> mode=<span class="string">&#x27;w&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>将文件描述符2：stdout替换为其他打开某个文件的文件描述符：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">2</span>]: f=<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">In [<span class="number">3</span>]: f.fileno()</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">11</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: os.sys.stdout=f</span><br><span class="line">In [<span class="number">5</span>]: os.sys.stdout.fileno() <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br><span class="line">In [<span class="number">7</span>]: print(<span class="string">&#x27;stdout redirect to file&#x27;</span>) <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br><span class="line">In [<span class="number">8</span>]: os.sys.stdout <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br></pre></td></tr></table></figure>
<p>查看/opt/test.txt文件内容</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line"> % cat test.txt</span><br><span class="line"></span><br><span class="line">11</span><br><span class="line"></span><br><span class="line">stdout redirect to file</span><br><span class="line"></span><br><span class="line">&lt;_io.TextIOWrapper name=&#x27;/opt/test.txt&#x27; mode=&#x27;w&#x27; encoding=&#x27;UTF-8&#x27;&gt;</span><br></pre></td></tr></table></figure>
<p>可以看到当前进程os.sys.stdout标准输出不再是2，而是11，指向某个已打开的文件。<br>当拿到一个已知的文件描述符后（一个非负整数），那么可以调用os.write(fd,bstr)方法向fd指向的文件写入数据，例如向文件描述符为11写入b’foo’字符串</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">11</span>]: os.write(<span class="number">11</span>,<span class="string">b&#x27;foo\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>查看文件/opt/test.txt内容：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">% cat test.txt</span><br><span class="line"></span><br><span class="line">11</span><br><span class="line"></span><br><span class="line">stdout redirect to file</span><br><span class="line"></span><br><span class="line">&lt;_io.TextIOWrapper name=&#x27;/opt/test.txt&#x27; mode=&#x27;w&#x27; encoding=&#x27;UTF-8&#x27;&gt;</span><br><span class="line">foo</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<p>==这一小节内容是想表述这么一个逻辑：如果要进程要对文件写入数据、或者读取数据（这不就是IO吗），底层必须通过文件描述符来实现，这就为讨论IO多路复用提供很好的知识背景，因为IO多路复用就是涉及到client向server写入数据，或者从server读取数据的需求。==</p>
<h5 id="1-6-进程打开文件描述符的个数"><a href="#1-6-进程打开文件描述符的个数" class="headerlink" title="1.6 进程打开文件描述符的个数"></a>1.6 进程打开文件描述符的个数</h5><p>&#8195;&#8195;文件描述符的有效范围是 0 到 OPEN_MAX。centos7.5默认每个进程最多可以打开 1024个文件（0 -1023）。对于 FreeBSD 、Mac OS X  和 Solaris 来说，每个进程最多可以打开文件的多少取决于<a href="https://baike.baidu.com/item/%E7%B3%BB%E7%BB%9F%E5%86%85%E5%AD%98">系统内存</a>的大小，int 的大小，以及系统管理员设定的限制。Linux 2.4.22 强制规定最多不能超过 1,048,576 。</p>
<p>调整文件描述符打开数量的限制：</p>
<p>管理用户可以在etc/security/limits.conf配置文件中设置他们的文件描述符极限，如下例所示。<br><code>softnofile 10240</code><br><code>hardnofile 20480</code><br>系统级文件描述符极限还可以通过将以下三行添加到/etc/rc.d/rc.local启动脚本中来设置：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">\#Increasesystem-widefiledescriptorlimit.</span><br><span class="line">echo4096&gt;/proc/sys/fs/file-max</span><br><span class="line">echo16384&gt;/proc/sys/fs/inode-max</span><br></pre></td></tr></table></figure>
<p>在一些基于IO事件实现的高性能中间件例如redis、nginx、gevent等，在其官方的调优教程，一般会建议将系统打开文件描述符的数量设为大值，以便发挥并发性能。</p>
<h5 id="1-7-文件描述符底层原理"><a href="#1-7-文件描述符底层原理" class="headerlink" title="1.7 文件描述符底层原理"></a>1.7 文件描述符底层原理</h5><p>&#8195;&#8195;之所以将文件描述符的底层原理放在本节最后讨论，是考虑到，当前面的内容你已经理解后，那么再讨论背后原理，将更容易理解。<br>&#8195;&#8195;总结1.2~1.6的内容：</p>
<ul>
<li>进程只有拿到文件描述符才能向它指向的物理文件写入数据或者读取数据，然后再把这些数据用socket方式（通过网卡）远程传输给client。</li>
<li>文件描述符就是操作系统为了高效管理已打开文件所创建的一个索引。给os.wirte传入fd，进程非常迅速通过fd找到已打开的文件，进程高效率了，作为操作系统当然也更高效管理这些进程。</li>
</ul>
<p>&#8195;&#8195;那么不禁会提问：为什么进程只有拿到文件描述符才能向它指向的物理文件写入数据或者读取数据？本节内容回答此问题，相关图或者表述参考这些文章：<a href="https://blog.csdn.net/qq_28114615/article/details/94590598">《Linux中文件描述符的理解(文件描述符、文件表项、i-node)》</a>（推荐这篇文章，作者从源码的角度解析fd的理解）、<a href="https://blog.csdn.net/wan13141/article/details/89433379">《Linux文件描述符到底是什么？》</a></p>
<p>&#8195;&#8195;基本知识背景：理解数组、指针、结构体以及内存，c语言的结构体像Python的类，都是为了封装属性和方法，形成一个“具备多个功能”的object。<br><strong>原理</strong><br>&#8195;&#8195;一个 Linux 进程启动后，它在内核中每一个打开的文件都需要由3种数据结构支撑运行：</p>
<ul>
<li><p>每个进程对应一张打开文件描述符表，属于进程级的数据结构，进程通过调用系统IO方法（传入文件描述符）访问文件数据（用户态切到内核态）；</p>
</li>
<li><p>内核维持一张打开文件表，文件表由多个文件表项组成，属于系统级数据结构，该文件表创建者和管理由内核负责，每个进程可共享；</p>
</li>
<li><p>每个打开的文件对应一个i-node数据结构，系统通过i-node可以取到位于磁盘的数据（用于返回给用户态，内核态切回用户态），存在于内核中。<br>（机智的小伙伴应该联想到这个技术点：为何用户程序读取文件数据，会出现用户态到内核态切换，然后再由内核态转到用户态？上面3个表可以回答这个问题）</p>
</li>
</ul>
<p>三者的关系图如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20200105184452579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>文件描述符表</strong><br>&#8195;&#8195;在Linux中，对于每一个进程，都会分配一个PCB（进程控制块——Processing Control Block），在C代码实现上，这个数据结构名为<code>task_struct</code>，它里面有一个成员变量<code>*files</code>(属于files_struct类型)，<code>files_struct</code>的指针又指向一个<code>指针数组fd_array</code>，数组每一个元素都是一个指向<code>file类型的指针</code>，该进程打开的每个文件都属于file类型。从这里得出：<br>所谓文件描述符，就是fd_array[NR_OPEN_DEFAULT]这个指针数组的索引号，这也回答了为何文件描述符为非负整数。</p>
<p><code>task_struct类型--&gt;*files指针(files_struct类型)--&gt;fd_array(文件描述符表)</code><br>==task_struct类型的定义(省略部分代码)==：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> &#123;</span></span><br><span class="line">	......</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">files_struct</span> *<span class="title">files</span>;</span> </span><br><span class="line">	......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>==files_struct类型的定义(省略部分代码)==：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">files_struct</span> &#123;</span></span><br><span class="line">	......</span><br><span class="line">	<span class="keyword">int</span> next_fd; #进程新打开一个文件对应的文件描述符</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">file</span> __<span class="title">rcu</span> * <span class="title">fd_array</span>[<span class="title">NR_OPEN_DEFAULT</span>];</span> <span class="comment">//进程级打开文件描述符表</span></span><br><span class="line">	......	</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>系统级文件表</strong><br>&#8195;&#8195;每一个打开的文件都对应于一个file结构体（c语言上用结构体，而其他高级语言例如python或者java则称为<code>file类型</code>或者<code>file对象</code>），在该结构体中，f_flags描述了文件标志，f_pos描述了文件的偏移位置，而在<code>f_path中有含有一个指向一个inode结点的指针</code>，因此f_path非常关键，它直接指向物理文件存储的inode节点。<br>文件表指向逻辑大致如下：<br><code>file类型 --&gt; f_path变量（path类型 --&gt; *dentry指针（dentry类型）--&gt; d_inode指针（inode类型）</code></p>
<p>==file类型的定义(省略部分代码)==：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> &#123;</span></span><br><span class="line">	......	</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">path</span>		<span class="title">f_path</span>;</span>     <span class="comment">//属于path类型，包括目录项以及i-node</span></span><br><span class="line">	<span class="keyword">atomic_long_t</span>		f_count;  <span class="comment">//文件打开次数</span></span><br><span class="line">	<span class="keyword">fmode_t</span>			f_mode;   <span class="comment">//文件打开时的mode，对应于open函数的mode参数</span></span><br><span class="line">	......		</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>==path类型的定义(省略部分代码)==：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">path</span> &#123;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">vfsmount</span> *<span class="title">mnt</span>;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">dentry</span> *<span class="title">dentry</span>;</span><span class="comment">//目录项</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>==dentry类型的定义(省略部分代码)==：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">dentry</span> &#123;</span></span><br><span class="line">	......	</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">inode</span> *<span class="title">d_inode</span>;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">super_block</span> *<span class="title">d_sb</span>;</span>	<span class="comment">/* The root of the dentry tree *</span></span><br><span class="line"><span class="comment"> 	......	</span></span><br><span class="line"><span class="comment">&#125;;</span></span><br></pre></td></tr></table></figure>
<p>从以上“file类型嵌套链”可知：进程打开一个文件后，系统给它返回一个文件描述符fd，进程通过fd调用系统io方法，系统（内核）通过f_path再到dentry指针找到物理文件的inode，从而找到相应的数据块。</p>
<p><strong>系统级的文件i-node表</strong><br>&#8195;&#8195;继续上面内容，内核找到i-node节点后，就能获取文件数据块在磁盘上的位置以及文件大小等文件的元数据信息，使得进程能够根据已打开文件对应的文件描述符一路定位到磁盘上相应文件的位置，从而进行文件读写。<br>==inode类型的定义(省略部分代码)==：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">inode</span> &#123;</span></span><br><span class="line">    .......</span><br><span class="line">	<span class="keyword">umode_t</span>			i_mode;     <span class="comment">//权限</span></span><br><span class="line">	<span class="keyword">uid_t</span>			i_uid;      <span class="comment">//用户id</span></span><br><span class="line">	<span class="keyword">gid_t</span>			i_gid;      <span class="comment">//组id</span></span><br><span class="line">    .......</span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">long</span>		i_ino;   <span class="comment">//inode节点号</span></span><br><span class="line">	<span class="keyword">loff_t</span>			i_size;   <span class="comment">//文件大小</span></span><br><span class="line">	.......</span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>		<span class="title">i_atime</span>;</span>  <span class="comment">//最后一次访问(access)的时间</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>		<span class="title">i_mtime</span>;</span>  <span class="comment">//最后一次修改(modify)的时间</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">timespec</span>		<span class="title">i_ctime</span>;</span>  <span class="comment">//最后一次改变(change)的时间</span></span><br><span class="line">    .......	</span><br><span class="line">	<span class="keyword">blkcnt_t</span>		i_blocks;    <span class="comment">//块数</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">address_space</span>	*<span class="title">i_mapping</span>;</span>   <span class="comment">//块地址映射</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面有Linux文件常见的基本属性，例如访问时间、修改时间、权限、属主等，系统级的文件表里每一个文件表项都会指向i-node，这个i-node对应磁盘中的一个物理文件。</p>
<h5 id="本节小结"><a href="#本节小结" class="headerlink" title="本节小结"></a>本节小结</h5><p>&#8195;&#8195;到处，有关文件描述符的底层原理介绍完毕，本节内容也只是抛砖引玉，读者可自行去检索Linux文件系统原理或者VFS虚拟文件系统原理等底层文件系统知识(从这里联想到不得不佩服Apache开发hdfs文件系统的大咖团队，他们对Linux文件系统的底层实现应该且必须是绝对掌握的)。如果你能理解以上全部内容，那么在第2部分的IO多路复用中提到的大部分概念，将不再晦涩难懂。</p>
<h4 id="2、-IO多路复用原理"><a href="#2、-IO多路复用原理" class="headerlink" title="2、 IO多路复用原理"></a>2、 IO多路复用原理</h4><p>&#8195;&#8195;IO：input和output，一般指数据的写入、数据的读取。IO主要分为两类：硬盘 IO和网络IO，本内容主要针对网络 IO。复用的含义？复用当然理解为重复使用某个<code>事物</code>，而在本文，这个<code>事物</code>是一个线程，因此，IO多路复用，是指并发的socket连接复用一个IO线程(换句话说：只需要一个线程，即可为多个client同时提供socket连接请求)。在第1章节中，如果用户程序要将数据写入或者读取数据，那么它在底层必须通过文件描述符才能达到相应操作结果，因此IO多路复用与文件描述符密切相连，这就是为何在第一章节里给出了大量有关文件描述符知识的原因。</p>
<h5 id="2-1-IO触发用户空间与内核空间之间的切换"><a href="#2-1-IO触发用户空间与内核空间之间的切换" class="headerlink" title="2.1 IO触发用户空间与内核空间之间的切换"></a>2.1 IO触发用户空间与内核空间之间的切换</h5><p>在本博客前面有关大数据项目的文章里，其中<a href="https://blog.csdn.net/pysense/article/details/103301847">《深入理解kafka》</a>提到kafka通过通过sendfile（零拷贝机制）提高消费者端的吞吐量，其中就提到用户空间与内核空间之间的切换，结合第1章节内容简要介绍：<br><img src="https://img-blog.csdnimg.cn/2020010622220268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p>
<ul>
<li>用户程序通过系统调用获得网络和文件的数据</li>
<li>内核负责网络和文件数据的读写 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">file_path=<span class="string">&#x27;/opt/test.txt&#x27;</span> <span class="comment"># 上下文为用户空间</span></span><br><span class="line">fd=os.<span class="built_in">open</span>(file_path,os.O_RDWR|os.O_CREAT) <span class="comment"># 用户空间切换到内核空间</span></span><br><span class="line">data=os.read(fd,<span class="number">64</span>) <span class="comment">#指定读文件前64byte内容  # 上下文从用户空间切换到内核空间，数据准备好后，上下文再从内核空间再切换到用户空间</span></span><br><span class="line">print(data) <span class="comment"># 上下文为用户空间</span></span><br></pre></td></tr></table></figure>
对于os.read的过程，用文件描述符的背景也可以理解：read底层用fd读取文件数据的流程：进程级文件描述符，到系统级文件表，再到系统级i-node表。从进程级到系统级，这里从代码层面展示用户空间到内核的空间的上下文切换<br>所以只要有网络IO或者磁盘IO，必然会发生用户空间到内核空间的上下文切换。</li>
</ul>
<p>图的原理参考<br><a href="https://www.cnblogs.com/yanguhung/p/10145755.html">https://www.cnblogs.com/yanguhung/p/10145755.html</a></p>
<h5 id="2-2-IO模型的介绍"><a href="#2-2-IO模型的介绍" class="headerlink" title="2.2 IO模型的介绍"></a>2.2 IO模型的介绍</h5><p><strong>IO模型基本分类</strong>：<br>（1）Blocking I/O（同步阻塞IO）：最常见也最传统IO模型，即代码语句按顺序执行若某一条语句执行需等待那么后面的代码会被阻塞，例如常见顺序步骤：读取文件、等待内核返回数据、拿到数据、处理输出<br>（2）同步非阻塞IO（Non-blocking IO）：默认创建的socket为阻塞型，将socket设置为NONBLOCK，业务流程则变为同步非阻塞IO<br>（3）IO多路复用（IO Multiplexing ）：即经典的Reactor设计模式，有时也称为异步阻塞IO，Java中的Selector和Linux中的epoll都是这种模型。<br>（4）异步IO（Asynchronous IO）：即经典的Proactor设计模式，也称为异步非阻塞IO<br>==这里也给出个人在知乎看到一篇关于IO模型更为形象的回答：<a href="https://www.zhihu.com/question/32163005">链接</a>==，通过购买火车票的场景来介绍5种IO模型（本章节未提到的信号驱动的IO模型）</p>
<p><a href="https://mp.weixin.qq.com/s/E3PYOSCuO4O6JB2FpHyZCg">https://mp.weixin.qq.com/s/E3PYOSCuO4O6JB2FpHyZCg</a></p>
<p><strong>同步和异步</strong><br>&#8195;&#8195;同步是指用户线程发起IO请求后需要等待或者轮询内核IO操作完成后才能继续执行；例如内核读文件需要耗时10秒，那么用户线程发起读取文件IO后，等待内核从磁盘拷贝到内存10秒，接着用户线程才能进行下一步对文件内容进行其他操作，按顺序执行。<br>&#8195;&#8195;而异步是指用户线程发起IO请求后仍继续执行，当内核IO操作完成后会通知用户线程，或者调用用户线程注册的回调函数。</p>
<p><strong>阻塞和非阻塞</strong><br>&#8195;&#8195;阻塞是指内核空间IO操作需要为把数据返回到用户空间；而非阻塞是指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成。</p>
<p>以下为四个模型的内容，图和部分文参考此篇<a href="https://www.cse.huji.ac.il/course/2004/com1/Exercises/Ex4/I.O.models.pdf">《英文原文文章》</a></p>
<h6 id="同步阻塞IO"><a href="#同步阻塞IO" class="headerlink" title="同步阻塞IO"></a>同步阻塞IO</h6><p>&#8195;&#8195;同步阻塞IO模型是最简单的IO模型，如图1所示：<img src="https://img-blog.csdnimg.cn/20200111121353731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;用户线程通过系统调用recvfrom方法向内核发起IO读文件操作（application switch to kernel），后面的代码被阻塞，用于线程处于等待当中，当内核已经从磁盘拿到数据并加载到内核空间，然后将数据拷贝到用户空间（kernel switch to application），用户线程再进行最后的data process数据处理。</p>
<p>同步阻塞IO模型的伪代码描述为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">read(socket, buffer) # 执行流被阻塞，直到buffer有数据可以读或者内核抛给用户程序一个error信号，程序才会往下执行。</span><br><span class="line">process(buffer) </span><br></pre></td></tr></table></figure>
<p>缺点分析：<br>&#8195;&#8195;用在多线程高并发场景（例如10万并发），服务端与客户端一对一连接，对于server端来说，将大量消耗内存和CPU资源（用户态到内核态的上下文切换），并发能力受限。</p>
<h6 id="同步非阻塞IO"><a href="#同步非阻塞IO" class="headerlink" title="同步非阻塞IO"></a>同步非阻塞IO</h6><p>&#8195;&#8195;同步非阻塞IO是在同步阻塞IO的基础上，将socket设置为NONBLOCK。这样做用户线程可以在发起IO请求后可以立即返回，原理图如下：<br><img src="https://img-blog.csdnimg.cn/20200111123715373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;在该图中，用户线程前面3次不断发起调用recvfrom，内核还未准备好数据，因此只能返回error of EWOULDBLOCK，直到最后一次调用recvfrom时，内核已经将数据拷贝到用户buffer端，此次可读取到数据，接下来就是process the data。</p>
<p>同步非阻塞IO模型的伪代码描述为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while true:</span><br><span class="line">        try:</span><br><span class="line">        	streaming_data&#x3D;read(buffer)</span><br><span class="line">    	 	do_someting(streaming_data)</span><br><span class="line">    		do_foo(streaming_data)	    </span><br><span class="line">    		do_bar(streaming_data)     </span><br><span class="line">	    except error of EWOULDBLOCK:</span><br><span class="line">	         print(&#39;kernel not ready for data yet,going to next loop&#39;)</span><br><span class="line">	         pass</span><br><span class="line">    	sleep(0.1)</span><br></pre></td></tr></table></figure>

<p>该模式有两个明显的缺点：</p>
<p>&#8195;&#8195;第一点：即client需要循环system call，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请求的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求。如果有10万个客户端连接，那么将消耗大量的serverCPU资源和占用带宽。</p>
<p>&#8195;&#8195;第二点：虽然设定了一个间隔时间去轮询，但也会发生一定响应延迟，因为每间隔一小段时间去轮询一次read操作，而任务可能在两次轮询之间的任意时间就已经完成，这会导致整体数据吞吐量的降低。</p>
<p>&#8195;&#8195;（以上的流程就像你在Starbucks店点了一杯cappuccino ，付款后，咖啡师正在制作中，而你却每隔0.1秒从座位走到点餐台问咖啡师OK了没，以至于你根本无法腾出时间享受<code>用一台MacBook Pro优雅的coding的下午茶美好时光</code>。当然如果仅有你1个人以这种方式去询问，咖啡师应该还可以接受（假设“客户是上帝这个真理“在Starbucks能够严格实施）。假设有10万个客户，都以这方式去轮询咖啡师，想象下画面…）</p>
<h6 id="IO多路复用模式"><a href="#IO多路复用模式" class="headerlink" title="IO多路复用模式"></a>IO多路复用模式</h6><p>&#8195;&#8195;前面两种模式缺点明显，那么 IO多路复用模式就是为了解决以上两种情况，IO多路复用是指内核一旦发现进程指定的一个或者多个IO事件准备读取，它就通知该进程，原理图如下：<br><img src="https://img-blog.csdnimg.cn/20200111144223563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;前面两种IO模型用户线程直接调用recvfrom来等待内核返回数据，而IO复用则通过调用select（还有poll或者epoll）系统方法，此时用户线程会阻塞在select语句处，等待内核copy数据到用户态，用户再收到内核返回可读的socket文件描述符，伪代码如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">while true:</span><br><span class="line">	all_fds=select()# 执行流在此处阻塞，当之前注册的socket文件描述符集合有其中的fd发生IO事件，内核会放回所有fds（注意：select不会返回具体发生IO事件的fd，需要用户线程自行查找）</span><br><span class="line">    for each_fd in all_fds:</span><br><span class="line">        if can_read(fd):  # 遍历内核返回每个socket文件描述符对象来判断到底是哪个流产生的IO事件。</span><br><span class="line">        	process_data(fd) # 找到了发生IO事件的文件描述符fd</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>此IO模型优点：<br>&#8195;&#8195;用户线程终于可以实现一个线程内同时发起和处理多个socket的IO请求，用户线程注册多个socket，（对于内核就是文件描述符集合），然后不断地调用select读取被激活的socket 文件描述符。（在这里，select看起就像是用户态和内核态之间的一个代理）<br>缺点在下文会谈到。</p>
<h6 id="IO多路复用适用场景："><a href="#IO多路复用适用场景：" class="headerlink" title="IO多路复用适用场景："></a>IO多路复用适用场景：</h6><p>&#8195;&#8195;从Redis、Nginx等这些强大的用于高并发网络访问的中间件可知，IO多路复用目前使用最突出的场景就是：socket连接，也即web服务，一般指高性能网络服务。<br>&#8195;&#8195;与多进程和多线程技术的简单粗暴的业务实现不同，I/O多路复用技术的最大优势是系统开销小，系统不必创建多进程或者多线程，也不必维护这些进程/线程的复杂上下文以及内存管理，从而大大减小了系统的开销，极大提升响应时间。</p>
<h4 id="3、深入理解select、poll"><a href="#3、深入理解select、poll" class="headerlink" title="3、深入理解select、poll"></a>3、深入理解select、poll</h4><p>&#8195;&#8195;上面第2节内容提到了IO多路复用的基本工作原理，目前linux支持I/O多路复用的系统调用常见有 select，poll，epoll（linux2.4内核前主要是select和poll，epoll方法则是从Linux 2.6内核引入），它们都是实现这么一个逻辑：一个进程可以监听多个文件描述符（10k-100k不等，看服务器性能），一旦某个文件描述符就绪（一般是读就绪或者写就绪），内核返回这些可读写的文件描述符给到用户线程，从而让用户线程进行相应的读写操作，这一过程支持并发请求。<br>下面就linux实现IO多路复用三种方式进行详细讨论：</p>
<h5 id="理解select函数"><a href="#理解select函数" class="headerlink" title="理解select函数"></a>理解select函数</h5><p>&#8195;&#8195;select：在一段时间内，监听用户线程感兴趣的文件描述符上面的可读、可写和异常等事件，在这里通过简单介绍其C接口的用法即可理解select功能，API：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/select.h&gt;</span></span></span><br><span class="line"><span class="keyword">int</span> select（<span class="keyword">int</span> nfds, fd_set * readfds, fd_set * writefds, fd_set * exceptfds, <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> * <span class="title">timeout</span>);</span></span><br></pre></td></tr></table></figure>
<p>函数参数解释，<a href="https://my.oschina.net/ijaychen/blog/184647">参考文章</a><br><code>nfds</code>：<br>&#8195;&#8195;非负整数的变量，表示当前线程打开的所有件文件描述符集的总数，nfds=maxfdp+1，计算方法就是当前线程打开的最大文件描述符+1</p>
<p><code>*readfds</code>:<br>&#8195;&#8195;fd_set集合类型的指针变量，表示当前线程接收到内核返回的可读事件文件描述符集合（有数据到了这个状态称之为读事件），如果这个集合中有一个文件可读，内核给select返回一个大于0的值，表示有文件可读，如果没有可读的文件，则根据timeout参数再判断是否超时，若内核阻塞当前线程的时长超出timeout，select返回0，若发生错误返回负值。传入NULL值，表示不关心任何文件的读变化</p>
<p><code>*writefd</code>:<br>&#8195;&#8195;当前有多少个写事件（关心输出缓冲区是否已满）<br>最后一个结构体表示每个几秒钟醒来做其他事件，用来设置select等待时间</p>
<p><code>*exceptfds</code>：<br>&#8195;&#8195;监视文件描述符集合中的有抛出异常的fd</p>
<p><code>timeout</code>：<br>&#8195;&#8195;select()的超时结束时间，它可以使select处于三种状态：<br>（1）若将NULL以形参传入，select置于阻塞状态，当前线程一直等到内核监视文件描述符集合中某个文件描述符发生变化为止；<br>（2）若将时间值设为0秒0毫秒，表示非阻塞，不管文件描述符是否有变化，都立刻返回继续执行，文件无变化返回0，有变化返回一个正值；<br>（3）timeout的值大于0，等待时长，即select在timeout时间内阻塞，超时后返回-1，否则在超时后不管怎样一定返回。</p>
<p><code>select函数返回值</code>：<br>&#8195;&#8195;执行成功则返回绪的文件描述符的总数。如果在超过时间内没有任何文件描述符准备就绪，将返回0；失败则返回-1并设置errno；若在select等待事件内程序接收到信号，则select立即返回-1，并设置errno为EINTER。<br>（从这里可以得出：写C的同学尤其是Unix 网络开发方向，对什么select、poll、epoll早已轻车熟路）</p>
<p><strong>select的优点</strong><br>&#8195;&#8195;select目前几乎在所有的平台上支持，其良好跨平台支持。</p>
<p> <strong>select的缺点</strong></p>
<p>（1）打开的文件描述符有最大值限制<br>&#8195;&#8195;默认1024，当然可自行设为较大值，例如10万，取决于服务器性内存和cpu配置。 </p>
<p>（2）对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。<br>&#8195;&#8195;当一个线程发起socket请求数较大时例如100，用户线程每次select()都会触发server端的内核遍历所有文件描述符，如果有1万个client发起这种IO请求，server的内核要遍历1万*100=100万的文件描述符。可想而知这种时间复杂度为o(n)是非常低效率的。<br>（3）第2点说了，当并发量大时，服务端提供server socket连接的进程需要维护一个用来存放大量fd的数据结构（参考1.7章节的内容：<code>task_struct类型--&gt;*files指针(files_struct类型)--&gt;fd_array(文件描述符表)</code>），会导致用户态和内核态之间在传递该数据结构时复制占用内存开销大。<br><a href="https://www.itnotebooks.com/?p=1106">https://www.itnotebooks.com/?p=1106</a></p>
<h5 id="理解poll函数"><a href="#理解poll函数" class="headerlink" title="理解poll函数"></a>理解poll函数</h5><p>本节内容部分参考<a href="https://blog.csdn.net/skypeng57/article/details/82743681">《poll函数解析》</a>，oll函数的定义：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">poll</span><span class="params">(struct pollfd *fds, <span class="keyword">nfds_t</span> nfds, <span class="keyword">int</span> timeout)</span></span>;</span><br></pre></td></tr></table></figure>
<p>pollfd类型定义：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span>&#123;</span></span><br><span class="line">　　<span class="keyword">int</span> fd;              <span class="comment">//文件描述符：socket或者其他输入设备的对应fd</span></span><br><span class="line">　　<span class="keyword">short</span> events;    <span class="comment">//用户向内核注册感兴趣的事件（读事件、写事件、异常事件）</span></span><br><span class="line">　　<span class="keyword">short</span> revents;   <span class="comment">//内核返回给用户注册的就绪事件</span></span><br><span class="line">　　&#125;;</span><br></pre></td></tr></table></figure>
<p>events有以下三大类：<br><img src="https://img-blog.csdnimg.cn/2020011122281639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">例如fd=10，events=POLLRDNORM<br>revents：返回用户在调用poll注册的感兴趣且已就绪的事件</p>
<p>参数说明：<br>pollfd类型的<code>*fds</code>变量：传入socket的文件描述符，用户线程通过fds[i].events注册感兴趣事件(可读、可写、异常)，<br><code>nfds</code>:<br>跟select的nfds参数相同<br><code>timeout</code>:<br>INFTIM:永远等待<br>0:立即返回，不阻塞<br>大于0:等待给定时长</p>
<p><code>函数返回值</code>：<br>成功时，poll() 返回结构体中 revents事件不为 0 的文件描述符个数；<br>如果在超时前没有任何事件发生，poll()返回 0；</p>
<p><code>工作流程</code><br>（1）pollfd初始化，传入socket的文件描述符，设置感兴趣事件event，以及内核revent。设置时间限制（用户线程通过fds[i].events传入感兴趣事件，内核通过修改fds[i].revents向用户线程返回已经就绪的事件）<br>（2）用户线程调用poll，并阻塞于此处<br>（3）内核返回就绪事件，并处理该事件</p>
<p>==select与poll本质差别不大，只是poll没有最大文件描述符的限制，因为它是基于链表来存储的==</p>
<p><code>poll缺点</code>：<br>（1）大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是否有意义。<br>（2）poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd</p>
<p>这里引用了这篇文章<a href="https://www.cnblogs.com/zhuwbox/p/4222382.html">《linux 下 poll 编程》</a>代码来说明poll流程，程序逻辑并不难理解，能够让poll返回就绪的事件，是内核驱动通过中断信号来判断事件是否发生：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line">......</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OPEN_MAX 1024</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd, sockfd, i, maxi;</span><br><span class="line">    <span class="keyword">char</span> buf[MAXLINE];</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> <span class="title">client</span>[<span class="title">OPEN_MAX</span>];</span><span class="comment">//存放客户端发来的所有socket对应的文件描述符，限定最大可用文件描述符1024</span></span><br><span class="line">    ......</span><br><span class="line">    client[<span class="number">0</span>].fd = listenfd;<span class="comment">// 传入socket对应的文件描述符</span></span><br><span class="line">    client[<span class="number">0</span>].events = POLLRDNORM;<span class="comment">//关心监听套机字的读事件</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(;;)</span><br><span class="line">    &#123;</span><br><span class="line">        nready = poll(client, maxi + <span class="number">1</span>, <span class="number">-1</span>); <span class="meta">#server 进程调用poll，用户线程在此处阻塞，直到内核返回就绪的POLLRDNORM事件的文件描述符集合</span></span><br><span class="line">        <span class="keyword">if</span>(client[<span class="number">0</span>].revents &amp; POLLRDNORM) # 如果收到内核返回client注册的事件</span><br><span class="line">        &#123;</span><br><span class="line">            connfd = accept(listenfd, (SA *) &amp;cliaddr, &amp;clilen); # 获取每个client socket连接对应的文件描述符</span><br><span class="line">            <span class="keyword">if</span>(connfd &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; OPEN_MAX; ++i)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(client[i].fd &lt; <span class="number">0</span>)</span><br><span class="line">                    client[i].fd = connfd; # 将客户端的请求的fd加到polled这个列表</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(i == OPEN_MAX)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;too many clients&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            client[i].events = POLLRDNORM;# 为客户端的fd注册可读事件</span><br><span class="line">            <span class="keyword">if</span>(i &gt; maxi)</span><br><span class="line">            &#123;</span><br><span class="line">                maxi = i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(--nready &lt;=<span class="number">0</span> )</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; OPEN_MAX; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>((sockfd = client[i].fd) &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(client[i].revents &amp; POLLRDNORM | POLLERR) <span class="meta"># server通过轮询所有的文件描述符，如果revents有读事件或者异常事件</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>((n = read(sockfd, buf, MAXLINE)) &lt; <span class="number">0</span>)# 读取数据</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(errno == ECONNRESET)</span><br><span class="line">                    &#123;</span><br><span class="line">                        close(sockfd); # 若该就绪的文件描述符返回的异常事件，则重置</span><br><span class="line">                        client[i].fd = <span class="number">-1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;read error!\n&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(n == <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    close(sockfd);</span><br><span class="line">                    client[i].fd = <span class="number">-1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    write(sockfd, buf,  n);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(--nready &lt;= <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;从上面看，不管select和poll都需要在返回后，都需要通过遍历文件描述符来获取已经就绪的socket（<code>for(i = 1; i &lt; OPEN_MAX; ++i)</code>==&gt;<code>if(client[i].revents &amp; POLLRDNORM | POLLERR)</code>）。事实上，高并发连接中例如10k个连接，在同一时刻可能只有小部分的socket fd处于就绪状态，但server端进程却为此不断的遍历，当注册的描述符数量的增长，其效率也会线性下降。<br>该图为select、poll和Epoll性能对比（还有一个更高性能的Kqueue）<br><img src="https://img-blog.csdnimg.cn/20200111174200198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到，随着socket连接数量增大（对应文件描述符数量也增加），select、poll处理响应更慢，epoll响应速度几乎不受文件描述符数量的影响。</p>
<h4 id="4、深入理解epoll"><a href="#4、深入理解epoll" class="headerlink" title="4、深入理解epoll"></a>4、深入理解epoll</h4><p>&#8195;&#8195;考虑到epoll为本文核心内容，而且知识相对更有深度，因此将其单独作为1个章节讨论。部分内容参考的以下文章（吸收多篇文章内容后，你会赞叹epoll设计的精巧）：<br><a href="https://blog.csdn.net/daaikuaichuan/article/details/83862311">《epoll原理详解及epoll反应堆模型》</a><br><a href="https://www.itnotebooks.com/?p=1106">《Linux下的I/O复用与epoll详解》</a><br><a href="https://blog.csdn.net/mango_song/article/details/42643971">《我读过最好的Epoll模型讲解》</a><br><a href="https://mp.weixin.qq.com/s/FRg_lSHDiZofzTZApU6z9Q">《 彻底搞懂epoll高效运行的原理 》</a></p>
<h5 id="4-1-epoll的c语言接口详解："><a href="#4-1-epoll的c语言接口详解：" class="headerlink" title="4.1 epoll的c语言接口详解："></a>4.1 epoll的c语言接口详解：</h5><p>这里介绍epoll的IO模型完成创建过程：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_create</span><span class="params">(<span class="keyword">int</span> size)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>功能说明：创建一个epoll实例，参数<code>size</code>用来指定该epoll对象可以管理的socket文件描述符的个数。在Linux 2.6.8以后的版本中，参数 size 已被忽略，但是必须大于0。</p>
</li>
<li><p>函数返回值：一个代表新创建的epoll实例的文件描述符epoll_fd，这个描述符由server端持有，用于<code>统管所有client请求的socket连接对应的文件描述符</code></p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_ctl</span><span class="params">(<span class="keyword">int</span> epfd, <span class="keyword">int</span> op, <span class="keyword">int</span> fd, struct epoll_event *event)</span></span>;  </span><br></pre></td></tr></table></figure>
<ul>
<li>epfd：epoll_create创建的epoll对象，以文件描述符形式epoll_fd返回，对于一个server进程，它对于只有一个epoll_fd。</li>
<li>op:监听socket时告诉内核要监听针对这个socket（其实是socket对应的文件描述符fd）什么类型的事件，主要有以下三种要监听的事件类型需要注册到epoll_fd上：</li>
<li><ul>
<li>EPOLL_CTL_ADD：注册新的fd到epfd中；</li>
</ul>
</li>
<li><ul>
<li>EPOLL_CTL_MOD：修改已经注册的fd的监听事件；</li>
</ul>
</li>
<li><ul>
<li>EPOLL_CTL_DEL：从epfd中删除一个fd；</li>
</ul>
</li>
</ul>
<ul>
<li><p>fd：epfd需要监听的fd，主要指server为client的socket请求创建对应文件描述符，来一个socket连接就对应新建一个文件描述符，</p>
</li>
<li><p>epoll_event：告诉内核需要对所注册文件描述符的什么类型事件进行监听，和poll函数支持的事件（读、写、异常）类型基本相同，不同是epoll还可以监听两个额外的事件：EPOLLET和EPOLLONESHOT（水平触发和边缘触发），这是epoll高性能的关键，文章后面会深入讨论。epoll_event结构如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> &#123;</span></span><br><span class="line">  <span class="keyword">__uint32_t</span> events;  <span class="comment">/* Epoll的事件类型 */</span></span><br><span class="line">  <span class="keyword">epoll_data_t</span> data;  <span class="comment">/* User data variable */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>其中events就包括以下事件：</p>
</li>
<li><p>EPOLLIN ： 监听的文件描述符可读（包括client主动向server断开socket连接的事件）</p>
</li>
<li><p>EPOLLOUT：  监听的文件描述符可写；</p>
</li>
<li><p>EPOLLPRI： 监听的文件描述符有紧急的数据可读；</p>
</li>
<li><p>EPOLLERR： 监听的文件描述符有异常事件；</p>
</li>
<li><p>EPOLLRDHUP： 监听的文件描述对应的socket连接被挂断；这里挂断像不像打电话给对方，对方挂断你的电话的意思？没错，这里是指socket连接的client断开了TCP连接（TCP半开），此时epoll监听对应的socket文件描述符会触发一个EPOLLRDHUP事件。==（这里也需要给出一个知识：使用 2.6.17 之后版本内核，对端连接断开触发的 epoll 事件会包含 EPOLLIN | EPOLLRDHUP，即 0x2001。有了这个事件，对端断开连接的异常就可以在TCP层进行处理，无需到应用层处理，提高断开响应速度。）==<br>EPOLLET：将EPOLL设为边缘触发模式Edge Triggered，一般用法如下：<br><code>ev.events = EPOLLIN|EPOLLET; //监听读状态同时设置ET模式</code><br>如果要设为水平触发Level Triggered，只需：<br><code>ev.events = EPOLLIN //默认就是水平触发模式</code></p>
</li>
<li><p>EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket文件描述符，需要再次把这个socket加入到EPOLL队列里。</p>
</li>
<li><p>关于epoll_event类型的data用法如下：<br>定义一个变量ev，类型为struct epoll_event<br>ev.data.fd = 10;//将要监听的文件描述符绑定到ev.data.fd<br>ev.events = EPOLLIN|EPOLLET; //监听读状态同时设置ET模式</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_wait</span> <span class="params">( <span class="keyword">int</span> epfd, struct epoll_event* events, <span class="keyword">int</span> maxevents, <span class="keyword">int</span> timeout )</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>函数返回值：成功时返回有IO（或者异常）事件就绪的文件描述符的数量，如果在timeout时间内没有描述符准备好则返回0。出错时，epoll_wait()返回-1并且把errno设置为对应的值</p>
</li>
<li><p>events：内核检测监听描述符发生了事件，内核将这些描述符的所有就绪事件以events数组返回给用户，。</p>
</li>
<li><p>maxevents：指定最多监听多少个事件类型</p>
</li>
<li><p>timeout：指定epoll的超时时间，单位是毫秒。当timeout为-1是，epoll_wait调用将永远阻塞，直到某个时间发生。当timeout为0时，epoll_wait调用将立即返回。</p>
</li>
</ul>
<p>==为更好理解该epoll_wait，这里给个简单epoll工作流程说明：==</p>
<ul>
<li><p>1、假设socket server进程持有的epoll_fd为3，即<code>epoll_fd=epoll_create(size=1024);</code></p>
</li>
<li><p>2、假设现有2个client向server发起socket连接，server给它分配的文件描述符是4和5，并且注册的事件为：EPOLLIN可读事件，注册过程如下：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">ev.data.fd = 4</span><br><span class="line">ev.events = EPOLLIN</span><br><span class="line">epoll_ctl(epfd=4, op=EPOLL_CTL_ADD, fd=4, &amp;ev);  # 这里不是C语言的写法，只是为了方便说明原理，将关键字参数也列出来，用类似python的参数语法</span><br><span class="line"></span><br><span class="line">ev.data.fd = 5</span><br><span class="line">ev.events = EPOLLIN</span><br><span class="line">epoll_ctl(epfd=4, op=EPOLL_CTL_ADD, fd=5, &amp;ev);  </span><br></pre></td></tr></table></figure>
</li>
<li><p>3、假设现在文件描述符5可读事件触发（例如内核已经完成将data.log拷贝到用户空间）</p>
</li>
<li><p>4、调用epoll_wait(epfd=3, events,maxevents=10, timeout=-1)，返回1，表示当前内核告诉server进程有1个文件描述符发生了事件，这里events存放的是就绪文件描述符及其事件：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">ready_fds=epoll_wait(epfd=3, events,maxevents=10, timeout=-1)</span><br><span class="line">for i in range(ready_fds）：</span><br><span class="line">	ev=events[i] //从内核返回的事件数组中取出epoll_event类型</span><br><span class="line">	print(ev.data.fd) //这里返回的就绪文件描述符是5，对应client的第二个socket连接</span><br><span class="line">	print(ev.evevts）//这里返回EPOLLIN可读事件</span><br><span class="line">	os.read(ev.data.fd)//读取文件描述符5指向的数据</span><br></pre></td></tr></table></figure>
<p>==请重点关注第4点，这里可以初步回答epoll适合的场景以及为何epoll比select和poll更高效的原因：（这里说了是<code>初步</code>，第4.2和4.3节将给出更有深度的内容）<br>&#8195;&#8195;epoll适合的场景：适用于连接数量大且长连接，但活动连接较少的情况。如何解释？在上面的例子中，我们假设了2个client socket连接，现在，我们假设10万个socket连接，而当中”活跃“（就绪）的文件描述符只有100个，调用epoll_wait返回100，接着在for循环里面将非常快速处理完可读事件。==<br>&#8195;&#8195;也就是说，使用epoll的IO效率，不会随着socket连接数（文件描述符connect_fd数量）的增加而降低。因为内核只返回少量活跃就绪的fd才会被回调处理；<br>&#8195;&#8195;换句话说：epoll几乎跟外部连接总数无关，它只管“就绪”的连接，这就是Epoll的效率就会远远高于select和poll的原因。<br>&#8195;&#8195;现在大家应该可以理解第3节最后给出的Libevet Benchmark图：100个active条件下，连接的文件描述符从2500到15000，可以看到epoll高性能几乎保持稳定，因为它只需处理这固定的100个就绪的fds，而select和poll，要处理的是从2500个到15000个，因此处理时长也是线性增长，效率越来越低。</p>
</li>
</ul>
<p>==简单总结epoll3个方法即可完成高效且并发数高的文件描述符监听的基本流程==</p>
<ul>
<li>A、server端持有可统管所有socket文件描述符的唯一epoll_fd=epoll_create()</li>
<li>B 、epoll_ctl(epoll_fd，添加或者删除所有待监听socket文件描述符以及它们的事件类型)</li>
<li>C、返回有事件发生的就绪文件描述符 =epoll_wait(epoll_fd)</li>
</ul>
<h5 id="4-2-epoll用红黑树管理所有监听文件描述符"><a href="#4-2-epoll用红黑树管理所有监听文件描述符" class="headerlink" title="4.2 epoll用红黑树管理所有监听文件描述符"></a>4.2 epoll用红黑树管理所有监听文件描述符</h5><h6 id="用python设计一个伪epoll模型？"><a href="#用python设计一个伪epoll模型？" class="headerlink" title="用python设计一个伪epoll模型？"></a>用python设计一个伪epoll模型？</h6><p>&#8195;&#8195;在4.1介绍epoll的相关函数中，大家应该有这么一个<code>开发者直觉</code>：<br>每次有新的文件描述符，则将其注册到一个”由内核管理的数据结构“，而且还是向该数据结构添加、删除文件描述符感兴趣的事件。ok，既然提到这个数据结构可以<code>添加、删除</code>，这个直觉告诉我们，内核是否用一个类似python列表（或者链表）的方式来管理所有监听文件描述符呢（其中列表中每项用）？不妨假设epoll就是用python列表管理fds，来讨论”原版epoll“的设计：</p>
<ol>
<li><p>epoll_create(size)是内核创建一个python列表<br>epoll_list= epoll_create(size=100) //全局list变量，用户进程持有，可监听100个外部文件描述符。</p>
</li>
<li><p>假设现在有2个新的外部socket连接请求server，3个tcp的socket连接对应文件描述符为4、5、6，注册事件为EPOLLIN可读<br>注册过程如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoll_list= epoll_create(size=<span class="number">100</span>) </span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">	connect_fd=socket_obj.accept().fd</span><br><span class="line">	epoll_ctl(epoll_list, op=<span class="string">&#x27;EPOLL_CTL_ADD&#x27;</span>,&#123;<span class="string">&#x27;fd&#x27;</span>:connect_fd,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;)</span><br><span class="line">	</span><br></pre></td></tr></table></figure>
<p>op都是添加，说明是向epoll_list进行append操作</p>
</li>
<li><p>由于用户空间持有epoll_list，若内核要处理epoll，需将epoll_list拷贝到内核空间，对于内核，它看到的epoll_list如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoll_list=[ </span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">6</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
</li>
<li><p>假设现在文件描述符4、5可读，最终调用回调函数处理业务：<br><code>ready_fds_list=epoll_wait(epoll_list,events,maxevents=10, timeout=-1)</code># epoll_list又从内核空间拷贝到用户空间，当前仅有2个文件描述符，性能还ok，而且假设返回的是就绪文件描述符列表:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ready_fds_list=[ </span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">]</span><br><span class="line"><span class="keyword">for</span>  each_fd <span class="keyword">in</span> ready_fds_list:</span><br><span class="line">	callback(each_fd)</span><br></pre></td></tr></table></figure>
<p>经过上面4个步骤，我们貌似造出了一个可易于理解的、python版本的<code>epoll</code>。下面介绍<code>原版epoll设计</code></p>
<h6 id="真epoll设计"><a href="#真epoll设计" class="headerlink" title="真epoll设计"></a>真epoll设计</h6></li>
</ol>
<p><strong>第一点</strong>：上面简陋python版本epoll，用列表来管理所有监听的文件描述符<br><code>epoll_list= epoll_create(size=100) </code><br>==真实设计==：<code>epoll_fd= epoll_create(size=100) </code>，Linux用一个特殊文件描述符，并创建了eventpoll实例，eventpoll里面指向了一个<code>红黑树</code>，这棵树就是用于存放所有监听的文件描述符及其事件。</p>
<p><strong>第二点</strong>：上面简陋python版本epoll，用列表来管理所有监听的文件描述符，每次有新的socket连接，注册fd以及获取活动fd都会发生用户态到内核态、内核态到用户态切换：拷贝的对象——epoll_list。假设当前服务器有10万个socket连接请求，那么将发生10万次用户态到内核态切换，以及10万次内核态到用户态的切换，显然效率极低。<br>==真实设计==：第一点说了，用epoll_fd指向一颗存放在内核空间的红黑树，如何避免用户态和内核态频繁切换？Linux用mmap()函数解决。mmap将用户空间的一块地址和内核空间的一块地址同时映射到同一块物理内存地址，使得这块物理内存对用户进程和内核均可访问，砍掉用户态和内核态切换的环节（注意区别zero copy）。也就是说内核拿到epoll_fd后，可直接操作epoll_fd指向的红黑树上存放的所有被监听的文件描述符，妙了！<br>==这里说的epoll_fd是如何实现在底层指向一个红黑树呢?==<br>用户进程调用：epoll_fd=epoll_create(size)时，用户进程mmap映射的一块物理地址上就创建一个eventpoll结构体对象，该对象包含一个红黑树的根节点，从而实现epoll_fd由此至终都指向这颗红黑树（内核也可直接访问）：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventpoll</span>&#123;</span>  </span><br><span class="line">    ....  </span><br><span class="line">    <span class="comment">/*红黑树的根节点，这颗树的节点存储着epoll_fd所有要监听的文件描述符及该文件描述符关联的其他属性*/</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rb_root</span>  <span class="title">rbr</span>;</span>  </span><br><span class="line">    <span class="comment">/*双链表中则存放着将要通过epoll_wait返回给用户的满足条件文件描述符、事件类型*/</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">rdlist</span>;</span>  </span><br><span class="line">    <span class="comment">/*还有其他成员变量，这里省略了，因为我们更关注存放就绪事件链表和存放被监听的所有文件描述符的红黑树 */</span></span><br><span class="line">    ....  </span><br><span class="line">&#125;;  </span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;上面的rdllist是一个双向链表，用于存储就绪事件的文描述符。用户进程代码执行epoll_wait时，执行流阻塞（在底层，是内核让进程休眠），直到监听的文件描述符有中断事件发生，内核将这些就绪文件描述符添加到rdlist里面，最后返回用户的是events数组，数组包含就绪的fd及其事件类型。</p>
<p><strong>第三点</strong>：用列表存放大量项，但需要进行增或者删除操作时，列表时间复杂度为<br>&#8195;&#8195;时间复杂度O(N)，想象下10万个连接的时间复杂度<br>==真实设计==：Linux为epoll设计了一个红黑树的数据结构，当调用epoll_ctl函数用于添加或者删除一个文件描述符时，对应内核而已，都是在红黑树的节点去处理，而红黑树本身插入和删除性能比列表高，时间复杂度O(logN)，N为树的高度，太巧妙了。<br>&#8195;&#8195;这个红黑树上的节点存放什么数据呢？每个节点称为epoll item结构，里面的组成如下：<br>fd:被epoll对象监听的文件描述符（client发起的socket连接对应的文件描述符）<br>event：要监听该fd的就绪事件，例如前面说的EPOLLIN可读事件<br>file:在1.6章节提到的系统文件表中，一个文件描述符对应系统文件表中一个文件项，这个文件项就是file类型，用于指向inode）<br>ready_node:双向链表中一个就绪事件的节点，可指回双向链表。</p>
<p><img src="https://img-blog.csdnimg.cn/20200118225411426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;用户进程调用epoll_create函数，对应在内核就已经创建了一个全局唯一eventpoll实例（object），对应背后完整的数据结构如下，示意图来源于<a href="https://zhuanlan.zhihu.com/p/50984245">该文章</a>：<br>&#8195;&#8195;图中的list就是双向链表，可以清楚看出epoll主要用了两个struct类型（当然不止这2个成员变量，还有锁、等待队列）和两种数据结构，完成高并发IO模型的构建，设计巧妙。因此，如果大家提高个人开发能力以及设计能力，数据结构必须要精通！（这里涉及到内核对红黑树、链表的操作是线程安全的，源码用了锁保操作原子性）<br><img src="https://img-blog.csdnimg.cn/20200119210515593.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;关于更深入的epoll红黑树以及事件就绪链表等底层的代码实现和图解析，这里有两篇文章，写得很好，作者根据英文原版内容自行理解后的整理：<br><a href="https://www.cnblogs.com/sduzh/p/6714281.html">《Linux内核笔记：epoll实现原理（一）》</a><br><a href="https://www.cnblogs.com/sduzh/p/6793879.html">《Linux内核笔记：epoll实现原理（二）》</a><br>（英文原版的链接现无法访问，地址<a href="https://idndx.com/2014/09/01/the-implementation-of-epoll-1/">《the-implementation-of-epoll》</a> )</p>
<h5 id="4-3-level-trigger和edge-trigger"><a href="#4-3-level-trigger和edge-trigger" class="headerlink" title="4.3 level trigger和edge trigger"></a>4.3 level trigger和edge trigger</h5><p>&#8195;&#8195;epoll工作模式支持水平触发(level trigger，简称LT，又称普通模式)和边缘触发(edge trigger，简称ET，又称“高速模式”)，而select和poll只支持LT模式。这里说的触发需要通过以下详细的说明来体会其内涵：<br>==level trigger模式的触发条件：==</p>
<ul>
<li>对于读就绪事件，只要用户程序没有读完fd的数据，也即缓冲内容不为空，epoll_wait还会继续返回该fd，让用户程序继续读该fd</li>
<li>对于写就绪事件，只要用户程序未向fd写满数据，也即缓冲区还不满，epoll_wait还会继续返回该fd，让用户程序继续对该fd写操作</li>
</ul>
<p>原理解释：<br>&#8195;&#8195;假设当前用户进程添加监听的文件描述符为4，以下简称为4fd，当该4fd有可读可写就绪事件时，epoll_wait()有返回，于是用户程序去进行读写操作，如果当前这一轮用户程序没有把4fd数据一次性全部读写完，那么下一轮调用 epoll_wait()时，它还会返回4fd这个事件对象，让你继续把4fd的缓存区上读或者写。如果用户程序一直不去读写，它会一直通知返回4fd。<br>参考代码如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">　　<span class="keyword">int</span> epfd,nfds;</span><br><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>,<span class="title">events</span>[10];</span> <span class="comment">//ev用于注册事件，数组用于返回要处理的事件</span></span><br><span class="line">　　epfd = epoll_create(<span class="number">1</span>); <span class="comment">//监听标准输入描述符，用于做测试</span></span><br><span class="line">　　ev.data.fd = STDIN_FILENO; <span class="comment">//标准输入描述符绑定到用户data的fd变量</span></span><br><span class="line">　　ev.events = EPOLLIN; <span class="comment">//监听读事件，且默认为LT水平触发事件</span></span><br><span class="line">　　epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;ev); <span class="comment">//注册epoll事件</span></span><br><span class="line">　　<span class="keyword">for</span>(;;)</span><br><span class="line">　　&#123;</span><br><span class="line">　　　　nfds = epoll_wait(epfd, events, <span class="number">5</span>, <span class="number">-1</span>); <span class="comment">//内核返回就绪事件</span></span><br><span class="line">　　　　<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++)</span><br><span class="line">　　　　&#123;</span><br><span class="line">　　　　　　<span class="keyword">if</span>(events[i].data.fd==STDIN_FILENO) <span class="comment">//如果返回的事件对象的fd为标准输入fd，则打印字符串，注意到，用户程序没有在缓存区读取数据</span></span><br><span class="line">　　　　　　　　<span class="built_in">printf</span>(<span class="string">&quot;epoll LT mode&quot;</span>);</span><br><span class="line"></span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;上面代码最关键的地方：在<code>if(events[i].data.fd==STDIN_FILENO) </code>之后，用户程序没有在标准输入的缓存区读取数据，根据水平触发原理，epoll_wait一直返回STDIN_FILENO这个就绪读事件，该代码最终效果：屏幕标准输出一直打印”epoll LT mode”字符串。<br>&#8195;&#8195;上面的过程可以解释为何LT模式是epoll工作效率较低的模式，具体说明如下：<br>&#8195;&#8195;假设除了感兴趣监听的文件描述符4fd，还有另外100个我不需要读写文件描述符（监听它们不代表一定要处理他们的就绪读写事件），最终会出现这样场景：epoll_wait每次都把这100个fd返回，而我只想对4fd进行读写，因此导致程序必须从101个fd中检索出4fd，若这些100fd以更高的优先级返回，那么用户则更晚才能拿到4fd，最终降低业务处理效率。</p>
<p>==edge trigger模式的触发条件：==</p>
<ul>
<li><p>对于读就绪事件，常见触发条件：<br> 缓冲区由空变为不空的时候（有数据可读时）<br> 当有新增数据到达时，即缓冲区中的待读数据变多时</p>
</li>
<li><p>对于写就绪事件，常见触发条件<br>  缓冲区由满变为空的时候（可写）<br> 当有旧数据被发送走，即缓冲区中的内容变少的时</p>
</li>
</ul>
<p>原理解释：<br>&#8195;&#8195;假设当前用户进程添加监听的文件描述符为4，以下简称为4fd，当该4fd有可读可写就绪事件时，epoll_wait()有返回，于是用户程序去读写操作，如果当前这一轮用户程序没有把4fd数据一次性全部读写完，那么下次调用epoll_wait()时，它不会再返回这个4fd就绪事件，直到在4fd上出现新的可读写事件才会通知你。这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。<br>参考代码如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">　　<span class="keyword">int</span> epfd,nfds;</span><br><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>,<span class="title">events</span>[10];</span> <span class="comment">//ev用于注册事件，数组用于返回要处理的事件</span></span><br><span class="line">　　epfd = epoll_create(<span class="number">1</span>); <span class="comment">//监听标准输入描述符，用于做测试</span></span><br><span class="line">　　ev.data.fd = STDIN_FILENO; <span class="comment">//标准输入描述符绑定到用户data的fd变量</span></span><br><span class="line">　　ev.events = EPOLLIN|EPOLLET; <span class="comment">//监听读事件，而且开启ET触发事件</span></span><br><span class="line">　　epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;ev); <span class="comment">//注册epoll事件</span></span><br><span class="line">　　<span class="keyword">for</span>(;;)</span><br><span class="line">　　&#123;</span><br><span class="line">　　　　nfds = epoll_wait(epfd, events, <span class="number">5</span>, <span class="number">-1</span>); <span class="comment">//内核返回就绪事件</span></span><br><span class="line">　　　　<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++)</span><br><span class="line">　　　　&#123;</span><br><span class="line">　　　　　　<span class="keyword">if</span>(events[i].data.fd==STDIN_FILENO) <span class="comment">//如果返回的事件对象的fd为标准输入fd，则打印字符串，注意到，用户程序没有在缓存区读取数据</span></span><br><span class="line">　　　　　　　　<span class="built_in">printf</span>(<span class="string">&quot;epoll ET mode&quot;</span>);</span><br><span class="line"></span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&#8195;&#8195;程序运行效果：<code>ev.events = EPOLLIN|EPOLLET</code>，将epoll监听的标准输入文件描述符设为ET模式，当向stdin敲入字符串abc时，缓存区由空转为不空，触发epoll_wait返回就绪事件，而之后用户程序并没有把缓冲区读取数据，根据ET原理，程序只打印一次”epoll ET mode”后就被阻塞。因为epoll_wait只通知一次，下次不再通知用户该4fd事件。除非外界再向stdin敲入字符串以至缓存区新增了数据，epoll_wait就会通知用户这个4fd有就绪事件。</p>
<h5 id="4-4-水平触发和边缘触发的小结"><a href="#4-4-水平触发和边缘触发的小结" class="headerlink" title="4.4 水平触发和边缘触发的小结"></a>4.4 水平触发和边缘触发的小结</h5><p>&#8195;&#8195;以某个被监听的文件描述符发生读事件作为示例：</p>
<ul>
<li>a.对于某个监听的文件描述符fd，假设其指向的读缓冲区初始时刻为空</li>
<li>b. 假设内核拷贝了4KB数据到用户进程的读缓冲区</li>
<li>c.不管水平触发还是边缘触发模式，epoll_wait此时都会返回可读就绪事件</li>
<li>d. 若采用水平触发方式，用户读取了2KB的数据，读缓冲区还剩余2KB数据，epoll_wait还会继续返回（通知）用户fd有可读就绪事件，直到读缓冲变为空为止。</li>
<li>f.若采用边缘触发方式，用户读取了2KB的数据，读缓冲区还剩余2KB数据，epoll_wait不再返回（通知）用户fd有可读就绪事件，除非读缓存区被用户进程或者内核写入新增数据例如1KB（此时读取缓冲变为3KB数据)那么epoll_wait才会通知用户有可读就绪事件。</li>
</ul>
<p>&#8195;&#8195;到此，已经完成对epoll深入解析的内容，当你掌握这些底层原理后，再回看当前出色中间件或框架如redis、nginx、node.js、tornado等，真香！</p>
]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>IO多路复用</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
</search>
