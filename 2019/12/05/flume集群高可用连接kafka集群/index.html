<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yield-bytes.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":240,"width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="&amp;#8195;&amp;#8195;在前面blog文章中：《在hadoopHA节点上部署flume高可用组件》和《在hadoopHA节点上部署kafka集群组件》，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示： &amp;#8195;&amp;#8195;flume NG集群前向的source是各类实时的log数据，通过fl">
<meta property="og:type" content="article">
<meta property="og:title" content="flume集群高可用连接kafka集群">
<meta property="og:url" content="https://yield-bytes.github.io/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/index.html">
<meta property="og:site_name" content="yield-bytes">
<meta property="og:description" content="&amp;#8195;&amp;#8195;在前面blog文章中：《在hadoopHA节点上部署flume高可用组件》和《在hadoopHA节点上部署kafka集群组件》，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示： &amp;#8195;&amp;#8195;flume NG集群前向的source是各类实时的log数据，通过fl">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191201225119507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191203235843219.png">
<meta property="article:published_time" content="2019-12-05T13:00:22.000Z">
<meta property="article:modified_time" content="2020-11-21T16:55:06.010Z">
<meta property="article:tag" content="flume连接kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="https://yield-bytes.github.io/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>flume集群高可用连接kafka集群 | yield-bytes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="yield-bytes" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yield-bytes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">分享与沉淀</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th-large fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://gitee.com/yield-bytes" class="github-corner" title="Follow me on Gitee" aria-label="Follow me on Gitee" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yield-bytes.github.io/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个非常专注技术总结与分享的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yield-bytes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          flume集群高可用连接kafka集群
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-05 21:00:22" itemprop="dateCreated datePublished" datetime="2019-12-05T21:00:22+08:00">2019-12-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-22 00:55:06" itemprop="dateModified" datetime="2020-11-22T00:55:06+08:00">2020-11-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index"><span itemprop="name">Flume</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&#8195;&#8195;在前面blog文章中：<a target="_blank" rel="noopener" href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;flume NG集群前向的source是各类实时的log数据，通过flume sink将这些日志实时sink到后向kafka集群，所有flume sink其实是本架构里kafka的producer角色，kafka集群后向连接spark streaming，用于消费kafka的实时消息（log日志数据）流。</p>
<a id="more"></a>

<p>组件版本：<br>flume-1.9.0、kafka-2.12 </p>
<h4 id="1-在kafka集群上创建相应的topic"><a href="#1-在kafka集群上创建相应的topic" class="headerlink" title="1.在kafka集群上创建相应的topic"></a>1.在kafka集群上创建相应的topic</h4><p>&#8195;&#8195;在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是直接sink到hdfs上。<br>创建topic需要做一定规划，考虑到目前有三个broker节点，分别为nn、dn1以及dn2节点，所以创建了3个分区，每个分区有三个replica，topic名为：sparkapp</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181&#x2F;kafka-zk --replication-factor 3 --partitions 3 --topic sparkapp </span><br><span class="line">Created topic sparkapp.</span><br><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181&#x2F;kafka-zk --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>-zookeeper nn:2181/kafka-zk：因为kafka在zk的所有znode都统一放置在/kafka-zk路径下，所以启动时需要注意加上该路径。</p>
<h4 id="2-单节点配置flume的agent-sink"><a href="#2-单节点配置flume的agent-sink" class="headerlink" title="2.单节点配置flume的agent sink"></a>2.单节点配置flume的agent sink</h4><h5 id="2-1-配置flume-文件"><a href="#2-1-配置flume-文件" class="headerlink" title="2.1 配置flume 文件"></a>2.1 配置flume 文件</h5><p>&#8195;&#8195;这里首先给出单节点的flume是如何连接到kafka集群，在nn节点上启动flume进程。在第3章节，将给出flume集群连接kafka集群，实现两组件之间的高可用实时数据流。<br>flume source的数据源为<code>/opt/flume_log/web_log/access.log</code><br>这里拷贝一份新的配置文件，配置过程简单，相关组件的参数说明可以参考flume官网最新文档：<br><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-sink">kafka-sink章节</a><br><code>[root@nn conf]# cp flume-conf.properties flume-kafka.properties</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">vi flume-kafka.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">设置<span class="built_in">source</span>组件<span class="built_in">exec</span>模式用 tail -F实时读取文本新的数据行</span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel组件，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定sinktype 为kafka sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的broker列表，用逗号（英文）隔开</span></span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> 前面创建的topic名称</span></span><br><span class="line">a1.sinks.k1.kafka.topic = sparkapp</span><br><span class="line"><span class="meta">#</span><span class="bash"> How many messages to process <span class="keyword">in</span> one batch. flume一次写入kafka的消息数</span></span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line"><span class="meta">#</span><span class="bash"> ack=1 说明只要求producer写入leader主分区即完成（<span class="built_in">wait</span> <span class="keyword">for</span> leader only)）</span></span><br><span class="line"><span class="meta">#</span><span class="bash">How many replicas must acknowledge a message before its considered successfully written.</span> </span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以设置分区ID，这里使用默认，也即kafka自己分区器</span></span><br><span class="line"><span class="meta">#</span><span class="bash">a1.sinks.k1.kafka.defaultPartitionId</span></span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 5</span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息的压缩类型</span></span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>组件和sinks组件绑定channel组件</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里sink的channel是单数</span></span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<p>这里的linger.ms=5主要是处理下情况：<br>producer是按照batch进行发送，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。<br>这里设置producer请求延时5ms才会被发送。</p>
<h5 id="2-2-测试数据消费情况"><a href="#2-2-测试数据消费情况" class="headerlink" title="2.2 测试数据消费情况"></a>2.2 测试数据消费情况</h5><p>在dn1节点上启动kafka consumer进程，等待flume sink，因为kafka已经集群，所以–bootstrap-server 选任意一个节点都可以，前提所选节点需在线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  saprkapp</span><br></pre></td></tr></table></figure>

<p>在nn节点上启动flume agent，这里不是后台启动，目的是为了实时观测flume agent 实时数据处理情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-kafka.properties --name a1</span><br></pre></td></tr></table></figure>
<p>手动在source文件最加新的文本行<br>[root@nn web_log]# echo “testing flume to kafka” &gt;&gt;access.log<br>在dn1 上可以看到实时输出nn节点上flume sink过来的消息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  sparkapp</span><br><span class="line">testing flume to kafka</span><br></pre></td></tr></table></figure>
<p>以上完成单节点flume实时数据到kafka的配置和测试，下面将使用flume集群模式sink到kafka集群</p>
<h4 id="3-flume-NG集群连接kafka集群"><a href="#3-flume-NG集群连接kafka集群" class="headerlink" title="3.flume NG集群连接kafka集群"></a>3.flume NG集群连接kafka集群</h4><p><img src="https://img-blog.csdnimg.cn/20191201225119507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">图3 为flume NG集群连接kafka集群的示意图<br>配置也相对简单，只需要把<a target="_blank" rel="noopener" href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>第4.2章节的collector配置的sinks部分改为kafkasink，agent1、agent2和agent3用第4.1章节所提的配置文件内容即可，本文不再给出。<br>给个flume节点角色分布表<br>| 节点 |  flume 角色|kafka角色|<br>|–|–|–|<br>| nn | agent1，collector 1|kafka broker<br>| dn1 | agen2 |kafka broker<br>| dn2 | agent3，collector2 |kafka broker</p>
<h5 id="3-1-配置collector"><a href="#3-1-配置collector" class="headerlink" title="3.1 配置collector"></a>3.1 配置collector</h5><p>因为测试环境计算资源有限，每个flume进程和kafka进程都在同一服务器上运行，实际生产环境flume和kafka分别在不同服务器上。<br>配置collector：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume-1.9.0&#x2F;conf</span><br><span class="line">[root@nn conf]# vi avro-collector.properties</span><br><span class="line"># 定义三个组件</span><br><span class="line">collector1.sources &#x3D; r1</span><br><span class="line">collector1.sinks &#x3D; k1</span><br><span class="line">collector1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 定义source：这里的source配成avro，连接flume agent端sink avro</span><br><span class="line">collector1.sources.r1.type &#x3D; avro</span><br><span class="line"># bind的属性：dn2节点对应改为dn2</span><br><span class="line">collector1.sources.r1.bind &#x3D; nn</span><br><span class="line">collector1.sources.r1.port &#x3D; 52020</span><br><span class="line"></span><br><span class="line">#定义channel</span><br><span class="line">collector1.channels.c1.type &#x3D; memory</span><br><span class="line">collector1.channels.c1.capacity &#x3D; 500</span><br><span class="line">collector1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># 指定sinktype 为kafka sink，从而使得flume collector成为kafka的producer</span><br><span class="line">collector1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">collector1.sinks.k1.kafka.bootstrap.servers &#x3D; nn:9092,dn1:9092,dn2:9092</span><br><span class="line">collector1.sinks.k1.kafka.topic &#x3D; sparkapp</span><br><span class="line">collector1.sinks.k1.kafka.flumeBatchSize &#x3D; 20</span><br><span class="line">collector1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line">collector1.sinks.k1.kafka.producer.linger.ms &#x3D; 5</span><br><span class="line">collector1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"></span><br><span class="line"># source组件和sinks组件绑定channel组件</span><br><span class="line">collector1.sources.r1.channels &#x3D; c1</span><br><span class="line">collector1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure>

<h5 id="3-2-启动flume-ng集群服务"><a href="#3-2-启动flume-ng集群服务" class="headerlink" title="3.2 启动flume-ng集群服务"></a>3.2 启动flume-ng集群服务</h5><p>在nn和dn2节点上使用nohup &amp; 后台启动flume collector进程<br>nn节点上collector进程，jps可以看到Application进程<br> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[1] 26895</span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">27168 Jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">23589 Kafka</span><br><span class="line">26895 Application</span><br></pre></td></tr></table></figure><br> dn2同样操作，这里不再给出。</p>
<p>分别在nn、dn1和dn2节点上使用nohup &amp; 后台启动启动flume agent进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-agent.properties --name  agent1 -Dflume.root.logger=INFO,console &amp; </span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">324 Jps</span><br><span class="line">23589 Kafka</span><br><span class="line">30837 Application</span><br><span class="line">32462 Application</span><br></pre></td></tr></table></figure>
<p>可以在nn节点看到两个 Application进程，一个是flume collector进程，另外一个是flume agent进程。<br>dn1和dn2取同样的后台启动方式，这里不再给出。</p>
<h5 id="3-3-测试flume与kafka高可用"><a href="#3-3-测试flume与kafka高可用" class="headerlink" title="3.3 测试flume与kafka高可用"></a>3.3 测试flume与kafka高可用</h5><p>任找一个节点，启动kafka consumer 进程，这里在dn1节点上启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# .&#x2F;bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092,dn1:9092,dn2:9092 --topic sparkapp</span><br></pre></td></tr></table></figure>
<p>因为nn、dn1、dn2三个节点上都有flume agent进程，分别在每个节点下的access.log追加新数据行，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># nn节点追加新数据行</span><br><span class="line">[root@nn web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@nn web_log]# echo &quot;flume&amp;kafka HA--msg from nn&quot; &gt;&gt;access.log</span><br><span class="line"># dn1节点追加新数据行</span><br><span class="line">[root@dn1 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn1 web_log]#echo &quot;flume&amp;kafka HA--msg from dn1&quot; &gt;&gt;access.log</span><br><span class="line"># dn2节点追加新数据行</span><br><span class="line">[root@dn2 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn2 web_log]# echo &quot;flume&amp;kafka HA--msg from dn2&quot; &gt;&gt;access.log</span><br></pre></td></tr></table></figure>
<p>在kafka consumer shell可以看到实时接收三条记录：<br><img src="https://img-blog.csdnimg.cn/20191203235843219.png" alt="在这里插入图片描述"><br>能否关闭其中collector所在服务器中的一台用于同时测试flume和kafka的高可用，例如关闭nn服务器？<br>不能，因为这里只有三个节点，zookeeper集群必须要三个及以上才能保证高可用，因此这里只需要kill nn节点上的collector，此时flume集群只有dn2的collector在工作，按上面的步骤，在三个节点上都给access.log新增数据行，同样可以正常观测到dn1的kafka consumer拿到3条message</p>
<h4 id="4-小结"><a href="#4-小结" class="headerlink" title="4.小结"></a>4.小结</h4><p>本文给出了flume与kafka连接的高可用部署过程，设置相对简单，考虑到测试环境资源限制，这里把flume集群和kafka集群放在同一服务器，生产环境中，flume集群有独立的服务器提供，kafka集群也由独立的服务器提供。后面几篇文章将给出有关spark的深度理解和kafka与spark streaming整合内容，目的为用于实现实时处理批数据的环节。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/flume%E8%BF%9E%E6%8E%A5kafka/" rel="tag"># flume连接kafka</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/12/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/" rel="prev" title="深入理解Kafka">
      <i class="fa fa-chevron-left"></i> 深入理解Kafka
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/12/08/%E5%9F%BA%E4%BA%8EYARN%20HA%E9%9B%86%E7%BE%A4%E7%9A%84Spark%20HA%E9%9B%86%E7%BE%A4/" rel="next" title="基于YARN HA集群的Spark HA集群">
      基于YARN HA集群的Spark HA集群 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%9C%A8kafka%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%88%9B%E5%BB%BA%E7%9B%B8%E5%BA%94%E7%9A%84topic"><span class="nav-number">1.</span> <span class="nav-text">1.在kafka集群上创建相应的topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%8D%95%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AEflume%E7%9A%84agent-sink"><span class="nav-number">2.</span> <span class="nav-text">2.单节点配置flume的agent sink</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-%E9%85%8D%E7%BD%AEflume-%E6%96%87%E4%BB%B6"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 配置flume 文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E6%B6%88%E8%B4%B9%E6%83%85%E5%86%B5"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 测试数据消费情况</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-flume-NG%E9%9B%86%E7%BE%A4%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4"><span class="nav-number">3.</span> <span class="nav-text">3.flume NG集群连接kafka集群</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-%E9%85%8D%E7%BD%AEcollector"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 配置collector</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-%E5%90%AF%E5%8A%A8flume-ng%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 启动flume-ng集群服务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-%E6%B5%8B%E8%AF%95flume%E4%B8%8Ekafka%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 测试flume与kafka高可用</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%B0%8F%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">4.小结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">一个非常专注技术总结与分享的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yield-bytes</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">577k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:44</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
