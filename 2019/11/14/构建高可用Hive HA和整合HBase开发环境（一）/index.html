<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yield-bytes.gitee.io","root":"/blog/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":240,"width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="&amp;#8195;&amp;#8195;前面的项目中，已经实现了HadoopHA、HBaseHA，本文将加入Hive数据仓库工作，并整合HBase，实现完整的大数据开发项目所具备的开发环境，为后面博客关于数据应用层开发提供支撑。 1、Hive Requirements​    按官网给出的基本环境  Java 1.7：  Hive versions1.2 onward require Java 1.7 or">
<meta property="og:type" content="article">
<meta property="og:title" content="构建高可用Hive HA和整合HBase开发环境（一）">
<meta property="og:url" content="https://yield-bytes.gitee.io/blog/2019/11/14/%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hive%20HA%E5%92%8C%E6%95%B4%E5%90%88HBase%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%88%E4%B8%80%EF%BC%89/index.html">
<meta property="og:site_name" content="yield-bytes">
<meta property="og:description" content="&amp;#8195;&amp;#8195;前面的项目中，已经实现了HadoopHA、HBaseHA，本文将加入Hive数据仓库工作，并整合HBase，实现完整的大数据开发项目所具备的开发环境，为后面博客关于数据应用层开发提供支撑。 1、Hive Requirements​    按官网给出的基本环境  Java 1.7：  Hive versions1.2 onward require Java 1.7 or">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2019110915233826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109153101866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109154205202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109165053150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109165332355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109165734163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109170010278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109171320511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109173709403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191109173859568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2019-11-14T01:32:21.000Z">
<meta property="article:modified_time" content="2020-02-03T07:20:32.000Z">
<meta property="article:tag" content="Hive集群">
<meta property="article:tag" content="HBase开发环境">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2019110915233826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="https://yield-bytes.gitee.io/blog/2019/11/14/%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hive%20HA%E5%92%8C%E6%95%B4%E5%90%88HBase%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%88%E4%B8%80%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>构建高可用Hive HA和整合HBase开发环境（一） | yield-bytes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/blog/atom.xml" title="yield-bytes" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yield-bytes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">分享与沉淀</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blog/categories/" rel="section"><i class="fa fa-th-large fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://gitee.com/yield-bytes" class="github-corner" title="Follow me on Gitee" aria-label="Follow me on Gitee" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yield-bytes.gitee.io/blog/2019/11/14/%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Hive%20HA%E5%92%8C%E6%95%B4%E5%90%88HBase%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%EF%BC%88%E4%B8%80%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个非常专注技术总结与分享的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yield-bytes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          构建高可用Hive HA和整合HBase开发环境（一）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-14 09:32:21" itemprop="dateCreated datePublished" datetime="2019-11-14T09:32:21+08:00">2019-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-03 15:20:32" itemprop="dateModified" datetime="2020-02-03T15:20:32+08:00">2020-02-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Hive/HBase/" itemprop="url" rel="index"><span itemprop="name">HBase</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>19 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&#8195;&#8195;前面的项目中，已经实现了HadoopHA、HBaseHA，本文将加入Hive数据仓库工作，并整合HBase，实现完整的大数据开发项目所具备的开发环境，为后面博客关于数据应用层开发提供支撑。</p>
<h3 id="1、Hive-Requirements"><a href="#1、Hive-Requirements" class="headerlink" title="1、Hive Requirements"></a>1、Hive Requirements</h3><p>​    按官网给出的基本环境</p>
<ul>
<li>Java 1.7：  <a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/HIVE/fixforversion/12329345/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel">Hive versions1.2</a> onward require Java 1.7 or newer. java1.7或更高版本</li>
<li>Hadoop 2.x (preferred)：推荐hadoop2.x版本</li>
</ul>
<a id="more"></a>

<p>hive安装包可在清华镜像源拉取：<code>https://mirrors.tuna.tsinghua.edu.cn/apache/hive/</code></p>
<p>目前stable版本为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apache-hive-2.3.6-bin.tar.gz 2019-08-23 02:53  221M </span><br></pre></td></tr></table></figure>

<p>最新版为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apache-hive-3.1.2-bin.tar.gz 2019-08-27 04:20  266M  </span><br></pre></td></tr></table></figure>

<p>如何定位自身Hadoop版本与hive版本的兼容呢?</p>
<p>例如本blog前面部署hadoop3.1.2，可通过在hive官网查看其对应的版本</p>
<p><code>http://hive.apache.org/downloads.html</code>，官网给出的news：</p>
<blockquote>
<p> 26 August 2019: release 3.1.2 available</p>
<p>This release works with Hadoop 3.x.y.</p>
</blockquote>
<p> hive3.1.2版本支持hadoop3.x.y版本，结合本blog内容，这里使用hive3.1.2：</p>
<p><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/">安装包下载地址</a></p>
<p>从hive官网给出的hadoop版本兼容可以看出hive2.x.y一般是兼容hadoop2.x.y</p>
<h3 id="2、Hive-环境部署"><a href="#2、Hive-环境部署" class="headerlink" title="2、Hive 环境部署"></a>2、Hive 环境部署</h3><h4 id="2-1-配置环境变量"><a href="#2-1-配置环境变量" class="headerlink" title="2.1 配置环境变量"></a>2.1 配置环境变量</h4><p>hive安装包所在路径，个人习惯将所有大数据组件放在/opt目录下，方便管理和配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# pwd</span><br><span class="line">/opt/hive-3.1.2</span><br><span class="line"></span><br><span class="line">[root@nn hive-3.1.2]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 追加到文件后面</span></span><br><span class="line">export HIVE_HOME=/opt/hive-3.1.2</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@nn hive-3.1.2]# source /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看hive版本</span></span><br><span class="line">[root@nn hive-3.1.2] hive --version</span><br><span class="line">Hive 3.1.2</span><br><span class="line">Git git://HW13934/Users/gates/tmp/hive-branch-3.1/hive -r 8190d2be7b7165effa62bd21b7d60ef81fb0e4af</span><br><span class="line">Compiled by gates on ** PDT 2019</span><br><span class="line">From source with checksum 0492c08f784b188c349f6afb1d8d9847</span><br></pre></td></tr></table></figure>



<h4 id="2-2-配置hive-env-sh和hive-site-xml"><a href="#2-2-配置hive-env-sh和hive-site-xml" class="headerlink" title="2.2 配置hive-env.sh和hive-site.xml"></a>2.2 配置hive-env.sh和hive-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# cp hive-default.xml.template  hive-site.xml</span><br><span class="line">[root@nn conf]# cp hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">[root@nn conf]# vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在文件最后修改</span></span><br><span class="line">HADOOP_HOME=/opt/hadoop-3.1.2</span><br><span class="line">export HIVE_CONF_DIR=/opt/hive-3.1.2/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/opt/hive-3.1.2/lib</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Hive-site.xm的配置项比较多，自带模板文件内容长达6900多行，仅给出重要的设置项，其他属性的设置以及描述可参考<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2">官网</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!--元数据库的mysql的配置项--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://nn:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;py_ab2018&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.readOnlyDatastore&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateTables&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;datanucleus.autoCreateColumns&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line">    &lt;!--zookeeper的有关设置--&gt;</span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">      &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hiveserver2_zk&lt;/value&gt;</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.zookeeper.publish.configs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--hiveserver2配置，可使得外部客户端使用thrift RPC协议连接远程hive--&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.client.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.client.password&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;py_ab2018&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--binary对应TCP协议，也可配成http协议--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.transport.mode&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;binary&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--thriftserver对外限制最大最小连接数--&gt;  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.min.worker.threads&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--有关日志文件--&gt;    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/scratchdir&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/resources&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.querylog.location&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/querylog&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/hive-3.1.2/operation-log&lt;/value&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>注意因为hadoop做了HA配置，因此以上的配置需要在主nn节点和backup dn2节点配置上，==在第7.1章节内容将会给出hiveserver2的相关内容==。</p>
<h4 id="2-3-配置Hive的运行日志"><a href="#2-3-配置Hive的运行日志" class="headerlink" title="2.3 配置Hive的运行日志"></a>2.3 配置Hive的运行日志</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">[root@nn hive-3.1.2]# mkdir logs</span><br><span class="line"></span><br><span class="line">[root@nn conf]# cp hive-log4j2.properties.template hive-log4j2.properties</span><br><span class="line">[root@nn conf]# vi hive-log4j2.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">property.hive.log.dir = /root/hive-3.1.2/logs</span><br></pre></td></tr></table></figure>


<h4 id="2-4-加入mysql-connector"><a href="#2-4-加入mysql-connector" class="headerlink" title="2.4  加入mysql connector"></a>2.4  加入mysql connector</h4><p>hive需用通过jdbc连接mysql，该jar需自行下载，并将其拷贝至以下目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# cp mysql-connector-java-5.1.32-bin.jar /opt/hive-3.1.2/lib/</span><br></pre></td></tr></table></figure>


<h4 id="2-5-在mysql建表"><a href="#2-5-在mysql建表" class="headerlink" title="2.5 在mysql建表"></a>2.5 在mysql建表</h4><p>其实这里无需在msyql建表，因为hive-site.xml文件里面已经配置为自动创建元数据库表，hive做初始化时会自动创建。也即本节内容可以忽略。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; create database hive default character set utf8 collate utf8_general_ci</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| hive               |</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> grant all on hive.* to <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;py_ab2018&#x27;</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 容许本地访问,否则hive的schema初始化将无法访问msyql</span></span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;nn&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;localhost&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line">grant all on *.* to &#x27;hive&#x27;@&#x27;127.0.0.1&#x27; identified by &#x27;py_ab2018&#x27;;</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt;  select host,user,authentication_string from mysql.user;  +-----------+--------+-----------------------+</span><br><span class="line">| host      | user   | authentication_string |</span><br><span class="line">+-----------+--------+-----------------------+</span><br><span class="line">| localhost | root   |                       |</span><br><span class="line">| nn        | root   |                       |</span><br><span class="line">| 127.0.0.1 | root   |                       |</span><br><span class="line">| ::1       | root   |                       |</span><br><span class="line">| nn        | hive   |                       |</span><br><span class="line">| %         | hadoop |                       |</span><br><span class="line">| %         | hive   |                       |</span><br><span class="line">| localhost | hive   |                       |</span><br><span class="line">| 127.0.0.1 | hive   |                       |</span><br><span class="line">+-----------+--------+-----------------------+</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> <span class="built_in">exit</span>;(quit;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="2-6-初始化hive-schema"><a href="#2-6-初始化hive-schema" class="headerlink" title="2.6 初始化hive schema"></a>2.6 初始化hive schema</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# schematool  -initSchema -dbType mysql </span><br><span class="line">Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">[root@nn hive-3.1.2]# </span><br></pre></td></tr></table></figure>

<h4 id="2-7-在mysql上查看hive创建的元表"><a href="#2-7-在mysql上查看hive创建的元表" class="headerlink" title="2.7 在mysql上查看hive创建的元表"></a>2.7 在mysql上查看hive创建的元表</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; use hive</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">MariaDB [hive]&gt; show tables;</span><br><span class="line">| AUX_TABLE                     |</span><br><span class="line">| BUCKETING_COLS                |</span><br><span class="line">| CDS                           |</span><br><span class="line">| COLUMNS_V2                    |</span><br><span class="line">| COMPACTION_QUEUE              |</span><br><span class="line">| COMPLETED_COMPACTIONS         |</span><br><span class="line">| COMPLETED_TXN_COMPONENTS      |</span><br><span class="line">| CTLGS                         |</span><br><span class="line">| DATABASE_PARAMS               |</span><br><span class="line">| DBS                           |</span><br><span class="line">| DB_PRIVS                      |</span><br><span class="line">| DELEGATION_TOKENS             |</span><br><span class="line">| FUNCS                         |</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>


<h4 id="2-8-启动hive"><a href="#2-8-启动hive" class="headerlink" title="2.8 启动hive"></a>2.8 启动hive</h4><p>启动hive之前，务必hadoop服务已经启动，若hadoop为HA结构，必须其中一个namenode节点为active节点，例如本项目中，hadoopHA为：nn和dn2都作为namenode节点。</p>
<p>除此之外，还需手动在hdfs上创建hive的工作目录：这里官方的说明如下</p>
<p>In addition, you must use below HDFS commands to create <code>/tmp</code> and <code>/user/hive/warehouse</code> (aka <code>hive.metastore.warehouse.dir</code>) and set them <code>chmod g+w</code> before you can create a table in Hive.</p>
<p>以下就是对/tmp加入group写权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p &#x2F;tmp&#x2F;hive</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse</span><br></pre></td></tr></table></figure>
<p>warehouse目录下放置的就是表对应的数据文件，在后面的章节会提供说明</p>
<p>启动hive，该命令是指启动hive cli，就像mysql shell</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# hive</span><br><span class="line">Hive Session ID = 627577c0-2560-4318-92af-bc2512f91d3b</span><br><span class="line"><span class="meta">hive&gt;</span></span><br></pre></td></tr></table></figure>

<p>以上说明hive部署成功，jps可以看到多了一个RunJar进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn hive-3.1.2]# jps</span><br><span class="line">13042 QuorumPeerMain</span><br><span class="line">20163 JournalNode</span><br><span class="line">19780 NameNode</span><br><span class="line">20709 Jps</span><br><span class="line">19499 DFSZKFailoverController</span><br><span class="line">20299 RunJar</span><br><span class="line">19918 DataNode</span><br></pre></td></tr></table></figure>

<p>==启动过程可能遇到问题==：</p>
<p>1）启动hive会有一个多重绑定的提示</p>
<p>SLF4J: Class path contains multiple SLF4J bindings.<br>SLF4J: Found binding in [jar:file:/opt/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: Found binding in [jar:file:/opt/hadoop-3.1.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</p>
<p>原因：</p>
<p>hadoop/common/lib有个slf4j-log4j的jar包，hive的lib下也有一个slf4j-log4j</p>
<p>那么在环境变量/etc/profile都配置两者的环境，hive启动后，会找到两个slf4j-log4j，因此提示多重绑定</p>
<p>解决办法：</p>
<p>保留hadoop/common/lib有个slf4j-log4j的jar包，将hive lib目录下的slf4j-log4j重命名即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn lib]# mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak</span><br></pre></td></tr></table></figure>
<p>==注意==：<br>当这个hive的日志jar包去掉后，hive日志模式将默认使用hadoop的日志配置，启动hive cli或者在hive cli上执行任何命令时都会不断打印出日志，如果需要进程在hive cli操作数据，那么建议保留hive的log4j包。如果使用外部可视化数据库管理客户端连接hive，那么可删除之。</p>
<p>2） hive在hdfs的/tmp/hive不具有写权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rwxrwxr-x</span><br></pre></td></tr></table></figure>

<p>将用户组以及其他用户加入可读可写可执行权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -chmod -R 777 &#x2F;tmp</span><br></pre></td></tr></table></figure>

<h3 id="3、hive建表测试"><a href="#3、hive建表测试" class="headerlink" title="3、hive建表测试"></a>3、hive建表测试</h3><p>HQL语句跟SQL差别不大，若对sql非常熟悉，HQL拿来即用。相关用法参考官网：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-DDLOperations">DDL语句</a>、<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SQLOperations">HQL查询用法</a></p>
<h4 id="3-1-创建一个员工表"><a href="#3-1-创建一个员工表" class="headerlink" title="3.1 创建一个员工表"></a>3.1 创建一个员工表</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists emp(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">sexual string,</span><br><span class="line">depart_id int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by&#x27;\t&#x27;</span><br><span class="line">stored as textfile;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc emp;</span></span><br><span class="line">OK</span><br><span class="line">id                      int                                         </span><br><span class="line">name                    string                                      </span><br><span class="line">age                     int                                         </span><br><span class="line">sexual                  string                                      </span><br><span class="line">depart_id               int                                         </span><br><span class="line">Time taken: 0.263 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure>

<p>员工表的本地数据emp.txt</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 	Aery	25	Male    1</span><br><span class="line">2 	Bery	23	Female	2</span><br><span class="line">3	Cery	26	Female	3</span><br><span class="line">4	Dery	27	Male		2</span><br></pre></td></tr></table></figure>

<h4 id="3-2-hive-cli导入测试文本数据"><a href="#3-2-hive-cli导入测试文本数据" class="headerlink" title="3.2 hive cli导入测试文本数据"></a>3.2 hive cli导入测试文本数据</h4><p>上面创建一个emp.txt文本数据，若要使用hive将其映射为一张表，需要将数据文件上传到hdfs，hive已经提供相关命令进行此类文件数据的上传操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/opt/hive-3.1.2/test_data/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line"></span><br><span class="line">Loading data to table default.emp</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.768 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from emp;</span></span><br><span class="line">OK</span><br><span class="line">1       Aery    25      Male    1</span><br><span class="line">2       Bery    23      Female  2</span><br><span class="line">3       Cery    26      Femalei 3</span><br><span class="line">4       Dery    27      Male    2</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> select * from emp a <span class="built_in">where</span> a.name=<span class="string">&#x27;Dery&#x27;</span>;</span></span><br><span class="line">OK</span><br><span class="line">4       Dery    27      Male    2</span><br><span class="line">Time taken: 0.327 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>


<p>hive会把本地数据上传到hdfs文件系统上具体路径如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# hdfs dfs -ls /user/hive/warehouse/emp</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root root         73 ** /user/hive/warehouse/emp/emp.txt</span><br></pre></td></tr></table></figure>

<p>从上面可知，hive建的表默认放在hdfs的warehouse目录下，而且上传的用户数据文件放在相应的表名字目录下。</p>
<h4 id="3-3-加载hdfs上的数据"><a href="#3-3-加载hdfs上的数据" class="headerlink" title="3.3  加载hdfs上的数据"></a>3.3  加载hdfs上的数据</h4><p>除了可以直接在hive cli里加载本地数据，也可先把本地数据上传到hdfs上，再通过hive加载</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn test_data]# hdfs dfs -put emp.txt /tmp</span><br><span class="line">[root@nn test_data]# hdfs dfs -ls /tmp</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   3 root supergroup         73 ** /tmp/emp.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先清空之前的数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> truncate table emp;</span></span><br><span class="line">OK</span><br><span class="line">Time taken: 0.957 seconds</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> hive导入hdfs的数据</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">&#x27;/tmp/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data inpath <span class="string">&#x27;/tmp/emp.txt&#x27;</span> into table emp;</span></span><br><span class="line">Loading data to table default.emp</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.593 seconds</span><br><span class="line"></span><br><span class="line">hive导入本地文件所需的实际为：1.768 s，是hdfs导入的3倍。</span><br></pre></td></tr></table></figure>

<p>==todo==<br>hive 按分区上传，上传的数据会指定在相应的分区上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hive按分区删除数据：</span><br><span class="line">alter table table_name drop partition (partition_name=&#x27;分区名&#x27;)</span><br></pre></td></tr></table></figure>


<h3 id="4、为何使用Hive？"><a href="#4、为何使用Hive？" class="headerlink" title="4、为何使用Hive？"></a>4、为何使用Hive？</h3><p>&#8195;&#8195;前面的内容为hive环境构建及其测试，那么在大数据开发项目中，为何要引入Hive组件？</p>
<h4 id="4-1-无Hive组件的大数据处理"><a href="#4-1-无Hive组件的大数据处理" class="headerlink" title="4.1 无Hive组件的大数据处理"></a>4.1 无Hive组件的大数据处理</h4><p>&#8195;&#8195;从本人博客前面几篇关于大数据组件部署和技术架构解析的blog可以了解到，若没有Hive这样的组件，<br>&#8195;&#8195;当需要从hdfs的原始数据做高级数据分析时，首先肯定需要使用java写MapReduce程序，如果再加入Spark分布式内存计算引擎，那么还需使用Scala语言写spark程序（或者使用python写pyspark）。事实上，MapReduce的程序写起来比较繁琐（注意：不是难），占据大量工作和时间。对于大部分数据开发人员（含数据分析），其实更关心的是把这些海量数据“统一处理”后，最终的呈现的数据是否有价值或者提供商业决策。若无Hive这样的组件，整个项目组将耗费大量的人力去开发更低层MapReduce程序，无论业务逻辑简单与否（虽然极其复杂的业务数据需要可能还是得写MP程序才能完成）。</p>
<h4 id="4-2-Hive组件在大数据分析与处理上的优势"><a href="#4-2-Hive组件在大数据分析与处理上的优势" class="headerlink" title="4.2 Hive组件在大数据分析与处理上的优势"></a>4.2 Hive组件在大数据分析与处理上的优势</h4><p>&#8195;&#8195;在大数据处理和分析中，能否有个更高层更抽象的语言层来描述算法和数据处理流程，就像传统数据库的SQL语句。Apache项目大神早已考虑到：传统数据库的数据分析与处理，每个人都在用SQL即可完成各自分析任务，这种方式在大数据hadoop生态必须给予引入。于是就有了Pig和Hive。Pig是接近脚本方式去描述MapReduce，Hive则用的是SQL，它们把脚本和SQL语言翻译成MapReduce程序，然后再丢给底层的MapReduce或者spark计算引擎去计算。也就是说，大数据开发人员只需要用更直观易懂、大家都熟悉的SQL语言去写大数据job即可完成绝大部分MapReduce任务，而且项目组的非计算机背景工作人员也可直接通过写SQL完成相应的大数据分析任务，简直不要太爽！</p>
<p>正因为Hive如此易用和SQL的通用性，Hive逐渐成长成了大数据仓库的核心组件，甚至很多公司的流水线作业集完全是用SQL描述，因为易写易改，一看就懂，容易维护。</p>
<h4 id="4-3-Hive在hadoop项目中的作用"><a href="#4-3-Hive在hadoop项目中的作用" class="headerlink" title="4.3 Hive在hadoop项目中的作用"></a>4.3 Hive在hadoop项目中的作用</h4><ul>
<li><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张关系型数据库的表，并提供类似传统数据库的SQL查询功能</p>
<p>这里如何理解？以本文第3章节内容说明：</p>
<p>这里说的结构化的数据文件，例如emp.txt数据文件，里面的数据是结构化的，每行的字段值用tab键空格隔开，用换行符’\n’进行换行，该数据文件直接存在hdfs上映射为一张关系型数据库的表：因为是结构化数据，一行记录分4列，有即每行都有4个字段，当然可以把该数据文件emp.txt看成是一张数据库表。</p>
</li>
</ul>
<ul>
<li><p>Hive的查询效率取决于使用第一代的MapReduce计算框架还是内存Spark/Tez框架</p>
<p>这句表述如何理解？</p>
</li>
</ul>
<p>==&#8195;&#8195;4.2 章节提到，数据应用开发或者数据分析人员开始用Hive分析数据之后，虽然写SQL即可实现MP任务，但Hive在MapReduce处理任务的速度实在太慢，这是底层默认采用MapReduce计算架构。Spark/Tez作为新一代的内存计算框架既然比MP计算效率更高，当然可以引入到Hive里面，于是就有了Hive on  Spark/Hive on Tez，到此，基本完成一个数据仓库的架构了，有了Hive on  Spark/Hive on Tez，基本解决了中低速数据处理的需求，这里的中低速是指（批数据分析）：例如查询某个栏目截止到昨天的访问量，时效性滞后比较长。==</p>
<p>&#8195;&#8195;而高速数据处理的需求（流数据分析）：例如要查询截止到1小时前，某个栏目的访问量，时效性要求高，近乎实时效果。</p>
<ul>
<li> Hive只适合用来做批量数据统计分析</li>
</ul>
<h3 id="5、Hive与HBase的关系"><a href="#5、Hive与HBase的关系" class="headerlink" title="5、Hive与HBase的关系"></a>5、Hive与HBase的关系</h3><p>&#8195;&#8195;在前面的blog，给出了非常详细的HBase高可用的部署测试的描述，那么在本文中，HBase跟Hive是怎么样结合使用呢？或者他们之间有什么关系吗？</p>
<p>&#8195;&#8195;首先：Hive与HBase是没有联系的，也就是说，在大数据项目中，有Hive+Spark/MapReduce+HDFS+结构化数据，也可以独立完成大数据分析任务，同样，有HBase+HDFS+数据，也可以独立完成大数据分析任务。因为Hbase和Hive在大数据架构中处在不同位置，Hbase主要解决实时高效查询的需求，尤其是Key-Value形式的查询；而Hive主要解决数据处理和计算问题，例如联合查询、统计、汇总等。这两个组件可以独立使用，也可以配合一起使用。</p>
<h4 id="5-1-两者之间的区别"><a href="#5-1-两者之间的区别" class="headerlink" title="5.1 两者之间的区别"></a>5.1 两者之间的区别</h4><ul>
<li><p>Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的随机实时查询，如日志明细、交易清单、轨迹行为等。</p>
</li>
<li><p>Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。</p>
</li>
<li><p>通过元数据来描述Hdfs上的结构化文本数据，通俗点来说，就是定义一张表来描述HDFS上的结构化文本，包括各列数据名称，数据类型是什么等，方便我们处理数据，当前很多SQL ON Hadoop的计算引擎均用的是hive的元数据，如Spark SQL、Impala等；</p>
</li>
<li><p>基于第一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据；<br>也可参考以下两者的各自优点对比图：<br><img src="https://img-blog.csdnimg.cn/2019110915233826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</li>
</ul>
<h4 id="5-2-两者配合使用时的大数据处理流程"><a href="#5-2-两者配合使用时的大数据处理流程" class="headerlink" title="5.2 两者配合使用时的大数据处理流程"></a>5.2 两者配合使用时的大数据处理流程</h4><p>在大数据架构中，Hive和HBase是协作关系，处理流程一般如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191109153101866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>&#8195;&#8195;1）通过ETL工具将数据源抽取到HDFS存储，这里的数据源包括mysql等关系型数据库的数据、ftp、api接口、txt、excel、日志文件等，这里说的抽取有两种意思：一种为脚本式的自动化抽取，例如写个定时任务把ftp的数据定时导入到HDFS中，另外一种抽取则是使用Apache组件Flume，能够实时抽取日志记录到kafka消息组件中，再由消费端（例如存入hbase或者mysql等）消费kafka的日志消息，这部分内容也会在本blog给出。</p>
<p>&#8195;&#8195;2）通过Hive清洗、处理原始数据；</p>
<p>&#8195;&#8195;3）HIve清洗处理后的数据，若面向海量数据随机查询场景，例如key-value，则可存入Hbase；若其他查询场景则可导入到mysql等其他数据库</p>
<p>&#8195;&#8195;4）大数据BI分析、应用的数据接口API开发，可从HBase获得查询数据。</p>
<h4 id="5-3-如果Hbase不需要Hive组件，如何实现易用的查询？"><a href="#5-3-如果Hbase不需要Hive组件，如何实现易用的查询？" class="headerlink" title="5.3 如果Hbase不需要Hive组件，如何实现易用的查询？"></a>5.3 如果Hbase不需要Hive组件，如何实现易用的查询？</h4><p>&#8195;&#8195;在文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/pysense/article/details/102635656">基于HadoopHA服务部署HBaseHA分布式服务（详细版）</a>的第10章节内容，提到操作HBase 表的示例，例如要查询company表的R2行记录，首先启动hbase shell，使用以下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):&gt; get &#39;company&#39;,&#39;R1&#39;,&#39;staff_info:age&#39;</span><br><span class="line"></span><br><span class="line">COLUMN                  CELL                                                          </span><br><span class="line"> staff_info:age         timestamp&#x3D;**, value&#x3D;23  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>&#8195;&#8195;可以看到，这种查询方式适合开发人员或者hbase管理员，而对于已经非常熟悉SQL查询的分析人员来说，无疑非常不友好。Hive正好能提供一种叫“外部表”的机制实现以SQL的形式对HBase的数据进行查询操作，内容在以下章节给出。</p>
<h3 id="6、为HBase引入Hive组件"><a href="#6、为HBase引入Hive组件" class="headerlink" title="6、为HBase引入Hive组件"></a>6、为HBase引入Hive组件</h3><p>&#8195;&#8195;前面提到，引入Hive就是为了能够使用SQL语句轻松完成对于HBase上的数据进行查询任务。<br>==&#8195;&#8195;Hive连接HBase的原理：==<br>&#8195;&#8195;让hive加载到连接hbase的jar包，通过hbase提供的java api即可实现Hive对Hbase的操作，此时可以吧Hive看成是HBase的客户端，类似navicat客户至于mysql，只不过navicat提供UI操作界面，hive是通过cli shell操作，当然我们也可以使用Hive的UI操作工具来实现UI操作（后面会给出基于DBeaver来实现）</p>
<h4 id="6-1-hive-env-sh"><a href="#6-1-hive-env-sh" class="headerlink" title="6.1  hive-env.sh"></a>6.1  hive-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件最后添加</span></span><br><span class="line">export HBASE_HOME=/opt/hbase-2.1.7</span><br></pre></td></tr></table></figure>
<h4 id="6-2-在hive-site-xml添加zookeeper集群"><a href="#6-2-在hive-site-xml添加zookeeper集群" class="headerlink" title="6.2 在hive-site.xml添加zookeeper集群"></a>6.2 在hive-site.xml添加zookeeper集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">&lt;!--zookeeper的有关设置--&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.support.dynamic.service.discovery&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.zookeeper.namespace&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hiveserver2_zk&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;    </span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.zookeeper.publish.configs&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>以上两个配置实现了Hive连接至Hbase</p>
<h4 id="6-3-测试hive操作hbase"><a href="#6-3-测试hive操作hbase" class="headerlink" title="6.3 测试hive操作hbase"></a>6.3 测试hive操作hbase</h4><p>&#8195;&#8195;首先hbase有测试数据，之前创建的company table，里面有两个列簇，这里不再赘述。<br>&#8195;&#8195;在hive创建外部表，用于映射Hbase的列簇，这里以staff_info列簇作为测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> CREATE EXTERNAL TABLE staff_info(</span></span><br><span class="line">rowkey string,</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">sexual string</span><br><span class="line">) </span><br><span class="line">STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; </span><br><span class="line">WITH SERDEPROPERTIES </span><br><span class="line">(&quot;hbase.columns.mapping&quot;=&quot;:key,staff_info:name,staff_info:age,staff_info:sex&quot;) </span><br><span class="line">TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;company&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>外部表创建语法解释：<br>==创建一个外部表，表名为staff_info，字段有4个，（rowkey,name,age,sexual），其中rowkey为对于hbase上的rowkey，该字段不是数据字段，name、age、sexual为数据字段。处理类org.apache.hadoop.hive.hbase.HBaseStorageHandler，hbase到hive的映射关系：:key,列簇:列名1，列簇:列名2…==<br>指定映射HBase的table name</p>
<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">** INFO  [16e10346-1e6d-4bb5-b89b-bd12f3614ec7 main] zookeeper.RecoverableZooKeeper: Process identifier&#x3D;hconnection-0x448892f1 connecting to ZooKeeper ensemble&#x3D;nn:2181,dn1:2181,dn2:2181</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.151 seconds</span><br></pre></td></tr></table></figure>
<p>在hive查询相关hbase的staff_info数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from staff_info;</span><br><span class="line">OK</span><br><span class="line">R1      Bery    23      Female</span><br><span class="line">R2      Dery    27      Male</span><br><span class="line">Time taken: 3.562 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">hive&gt; select * from staff_info a where a.name&#x3D;&#39;Bery&#39;;</span><br><span class="line">OK</span><br><span class="line">R1      Bery    23      Female</span><br><span class="line">Time taken: 1.376 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

<p>以上完成Hive和HBase的开发环境整合配置。</p>
<h3 id="7、使用SQL开发工具连接hive进行高级SQL开发"><a href="#7、使用SQL开发工具连接hive进行高级SQL开发" class="headerlink" title="7、使用SQL开发工具连接hive进行高级SQL开发"></a>7、使用SQL开发工具连接hive进行高级SQL开发</h3><p>&#8195;&#8195;在前面章节内容可以看到，hive的操作直接基于hive服务器上的hive cli上进行，使用hive交互命令式写sql效率会很低，调试也不方便，因此需要外部SQL IDE工具提高开发效率。本文采用DBeaver，也是本人长期使用的数据库管理客户端工具，重点它是开源的，在Mac上用起来流畅、UI有一定设计感！）。</p>
<p>关于DBeaver的介绍（<a target="_blank" rel="noopener" href="https://dbeaver.io/">官网下载</a>）：</p>
<blockquote>
<p>DBeaver 是一个开源、跨平台、基于java语言编写的的通用数据库管理工具和 SQL 客户端，支持 MySQL, PostgreSQL, Oracle, Hive、Spark、elasticsearch等以及其他兼容 JDBC 的数据库(DBeaver可以支持的数据库太多了)</p>
<p>DBeaver 提供一个图形界面用来查看<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BB%93%E6%9E%84/5507713">数据库结构</a>、执行SQL查询和脚本，浏览和导出数据，处理<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/BLOB/543419">BLOB</a>/CLOB 数据，修改数据库结构等。<br><img src="https://img-blog.csdnimg.cn/20191109154205202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到，DBeaver支持各自类型数据库以及hadoop相关的组件，之后会有专门文章用DBeaver开发spark数据分析项目。</p>
</blockquote>
<p>DBeaver连接hive需要做以下几个配置，否则无法成功连接</p>
<h4 id="7-1-配置hive-site-xml和core-site-xml"><a href="#7-1-配置hive-site-xml和core-site-xml" class="headerlink" title="7.1  配置hive-site.xml和core-site.xml"></a>7.1  配置hive-site.xml和core-site.xml</h4><p>hive服务端启用相应的thrift TCP端口，暴露给客户端连接使用。<br>在2.2章节内容，hive-site.xml已经配置了hive server2服务，端口号按默认的10000，监听host为全网地址0.0.0.0，nn和dn2都需要配置hive server2。此外，还需要hadoop的配置文件core-site.xml放通拥有hdfs文件系统的用户，在本blog里，hadoop的用户为root上，需加入以下property<br>==core-site.xml配置如下==</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  &lt;!--放通客户端以root用户访问hdfs--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>如果hadoop文件使用者不是’root‘用户，例如‘foo-bar’用户那么对应的name值为<br><code>&lt;name&gt;hadoop.proxyuser.foo-bar.groups&lt;/name&gt;</code>，<br>以上配置需要在nn和dn2同时配置，因为这两个节点做了hadoop HA。</p>
<p>若不配置“放通客户端以root用户访问hdfs”，使用DBeaver或者jdbc api连接hive server2会提示以下出错信息：</p>
<p>==连接错误提示==<br>Required field ‘serverProtocolVersion’ is unset! Struct:TOpenSessionResp(status:TStatus(statusCode:ERROR_STATUS, infoMessages:[*org.apache.hive.service.cli.HiveSQLException:Failed to open new session: java.lang.RuntimeException: ==org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate root:14:13 ==</p>
<h4 id="7-2-在nn主节点上启动hiveserver2服务"><a href="#7-2-在nn主节点上启动hiveserver2服务" class="headerlink" title="7.2 在nn主节点上启动hiveserver2服务"></a>7.2 在nn主节点上启动hiveserver2服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以前台进程方式打开</span></span><br><span class="line">[root@nn conf]# hiveserver2</span><br><span class="line">Hive Session ID = 1c92d507-7725-4e57-a7fe-03a9ae0cdf13</span><br></pre></td></tr></table></figure>

<p>使用jps -ml查看所有大数据组件服务的情况，RunJar表示hiveserver2服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ~]# jps -ml</span><br><span class="line">16340 org.apache.hadoop.util.RunJar /opt/hive-3.1.2/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.aux.jars ****</span><br><span class="line">14085 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">14710 org.apache.hadoop.hbase.master.HMaster start</span><br><span class="line">5815 org.apache.hadoop.hdfs.tools.DFSZKFailoverController</span><br><span class="line">13273 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">16666 sun.tools.jps.Jps -ml</span><br><span class="line">5451 org.apache.zookeeper.server.quorum.QuorumPeerMain /opt/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">13547 org.apache.hadoop.hdfs.qjournal.server.JournalNode</span><br><span class="line">14876 org.apache.hadoop.hbase.regionserver.HRegionServer start</span><br><span class="line">13135 org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">13951 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br></pre></td></tr></table></figure>
<p>也可查看是否有10000端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ~]# ss -nltp|grep 10000</span><br><span class="line">LISTEN     0      50          :::10000                   :::*                  </span><br><span class="line"> users:((&quot;java&quot;,pid&#x3D;16340,fd&#x3D;506))</span><br></pre></td></tr></table></figure>
<p>至此，hiveserver2已经可以对外提供hive的连接服务。</p>
<h4 id="7-3-配置DBeaver连接hive"><a href="#7-3-配置DBeaver连接hive" class="headerlink" title="7.3 配置DBeaver连接hive"></a>7.3 配置DBeaver连接hive</h4><p>创建新的hive连接<br><img src="https://img-blog.csdnimg.cn/20191109165053150.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在<code>编辑驱动设置</code>里面，选择下载驱动，这里DBeaver会自动去拉取相应的jar驱动包<br><img src="https://img-blog.csdnimg.cn/20191109165332355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">驱动为：<code>hive-jdbc-uber-2.6.5.0-292.jar</code> (Uber开发的驱动？)</p>
<p>测试是否可连，以下提示远程hive服务器的版本为hive3.1.2<br><img src="https://img-blog.csdnimg.cn/20191109165734163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">下图可以看到DBeaver已经可以查看hive之前创建的emp表，以及hive的外部表——hbase的staff_info表<br><img src="https://img-blog.csdnimg.cn/20191109170010278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在DBeaver编辑器上对hive上的emp表进行简单的查询：<br><img src="https://img-blog.csdnimg.cn/20191109171320511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">至此，hive的SQL可视化开发环境已经部署完成，配合DBeaver出色的Tab自动补全，写HQL效率有效提升。</p>
<h4 id="7-4-hiveserver2的webUI"><a href="#7-4-hiveserver2的webUI" class="headerlink" title="7.4 hiveserver2的webUI"></a>7.4 hiveserver2的webUI</h4><p>&#8195;&#8195;在上一节内容，通过命令<code>hiveserver2</code>可启动远程连接服务，其实该命令还启动另外一个进程：hiveserver2自己的webUI服务进程，该web页面可看到每个客户端在hive服务器上执行过的查询语句、会话，包括IP、用户名、当前执行的操作（查询）数量、链接总时长、空闲时长等指标，是管理客户端连接和查询的后台页面。<br>在hiveserver2服务器上也即nn节点上查看10002端号：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ~]# ss -nltp|grep 10002</span><br><span class="line">LISTEN     0      50          :::10002                   :::*                   users:((&quot;java&quot;,pid=16340,fd=511))</span><br></pre></td></tr></table></figure>
<p>web 页面入口：<a target="_blank" rel="noopener" href="http://nn:10002/">http://nn:10002/</a><br>当前连接的客户端会话<br><img src="https://img-blog.csdnimg.cn/20191109173709403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">已经完成的查询语句，这里可以看到HQL使用底层计算框架为MapReduce</p>
<p><img src="https://img-blog.csdnimg.cn/20191109173859568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">至此，已经完成使用外部SQL客户端工具DBeaver连接hive的任务，那么接下来：在Hbase导入大数据文件，部署高可用hiveserver2服务。</p>
<h3 id="8-使用beeline连接hiveserver2"><a href="#8-使用beeline连接hiveserver2" class="headerlink" title="8 使用beeline连接hiveserver2"></a>8 使用beeline连接hiveserver2</h3><p>&#8195;&#8195;在以上章节都提到两种方式连接到hiveserver2，此外，还有hive自带的一个客户端工具beeline，也可以连接到hive，按hive的官方规划，beeline将取代之前版本的hive cli。具体为何取代hive cli，参考官网说明：</p>
<blockquote>
<p>HiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline.<br>HiveCLI is now deprecated in favor of Beeline, as it lacks the<br>multi-user, security, and other capabilities of HiveServer2.  To run<br>HiveServer2 and Beeline from shell:</p>
</blockquote>
<p>连接用法hiveserver2的用法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline -u jdbc:hive2:nn:10000 -n root -p ****</span><br></pre></td></tr></table></figure>
<p>可以看出因为beeline在使用jdbc接口连接时要求带入hive-site.xml配置账户和密码，因此官网说提供了 security功能。<br>具体使用方式这里不再</p>
<h3 id="8、部署高可用的Hive服务"><a href="#8、部署高可用的Hive服务" class="headerlink" title="8、部署高可用的Hive服务"></a>8、部署高可用的Hive服务</h3><p> 以上仅在hdfs、hbase的主节点nn配置hive单集服务，hive可以看做是hdfs对外提供的SQL客户端服务，若nn节点不可用，将导致nn节点hive服务也无法使用，因此实际生产环境，需要将hive部署为HA模式，与hdfs和hbaseHA模式一起构成完整的高可用离线分析大数据开发环境。这部分的内容在下一篇文章给出：构建高可用Hive HA和整合HBase开发环境（二）</p>
<p>​         </p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/Hive%E9%9B%86%E7%BE%A4/" rel="tag"># Hive集群</a>
              <a href="/blog/tags/HBase%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/" rel="tag"># HBase开发环境</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2019/11/06/supervisor%E7%AE%A1%E7%90%86web%E6%9C%8D%E5%8A%A1%E8%BF%9B%E7%A8%8B/" rel="prev" title="supervisor管理web服务进程">
      <i class="fa fa-chevron-left"></i> supervisor管理web服务进程
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2019/11/17/%E6%B7%B1%E5%85%A5functools.wraps%E3%80%81partial/" rel="next" title="深入functools.wraps、partial">
      深入functools.wraps、partial <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Hive-Requirements"><span class="nav-number">1.</span> <span class="nav-text">1、Hive Requirements</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81Hive-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2"><span class="nav-number">2.</span> <span class="nav-text">2、Hive 环境部署</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 配置环境变量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E9%85%8D%E7%BD%AEhive-env-sh%E5%92%8Chive-site-xml"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 配置hive-env.sh和hive-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-%E9%85%8D%E7%BD%AEHive%E7%9A%84%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 配置Hive的运行日志</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-%E5%8A%A0%E5%85%A5mysql-connector"><span class="nav-number">2.4.</span> <span class="nav-text">2.4  加入mysql connector</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-%E5%9C%A8mysql%E5%BB%BA%E8%A1%A8"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 在mysql建表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-%E5%88%9D%E5%A7%8B%E5%8C%96hive-schema"><span class="nav-number">2.6.</span> <span class="nav-text">2.6 初始化hive schema</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-%E5%9C%A8mysql%E4%B8%8A%E6%9F%A5%E7%9C%8Bhive%E5%88%9B%E5%BB%BA%E7%9A%84%E5%85%83%E8%A1%A8"><span class="nav-number">2.7.</span> <span class="nav-text">2.7 在mysql上查看hive创建的元表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-8-%E5%90%AF%E5%8A%A8hive"><span class="nav-number">2.8.</span> <span class="nav-text">2.8 启动hive</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81hive%E5%BB%BA%E8%A1%A8%E6%B5%8B%E8%AF%95"><span class="nav-number">3.</span> <span class="nav-text">3、hive建表测试</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%91%98%E5%B7%A5%E8%A1%A8"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 创建一个员工表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-hive-cli%E5%AF%BC%E5%85%A5%E6%B5%8B%E8%AF%95%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 hive cli导入测试文本数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E5%8A%A0%E8%BD%BDhdfs%E4%B8%8A%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">3.3.</span> <span class="nav-text">3.3  加载hdfs上的数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8Hive%EF%BC%9F"><span class="nav-number">4.</span> <span class="nav-text">4、为何使用Hive？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-%E6%97%A0Hive%E7%BB%84%E4%BB%B6%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 无Hive组件的大数据处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Hive%E7%BB%84%E4%BB%B6%E5%9C%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%A4%84%E7%90%86%E4%B8%8A%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Hive组件在大数据分析与处理上的优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Hive%E5%9C%A8hadoop%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 Hive在hadoop项目中的作用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81Hive%E4%B8%8EHBase%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">5.</span> <span class="nav-text">5、Hive与HBase的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-%E4%B8%A4%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 两者之间的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-%E4%B8%A4%E8%80%85%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8%E6%97%B6%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 两者配合使用时的大数据处理流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-%E5%A6%82%E6%9E%9CHbase%E4%B8%8D%E9%9C%80%E8%A6%81Hive%E7%BB%84%E4%BB%B6%EF%BC%8C%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%98%93%E7%94%A8%E7%9A%84%E6%9F%A5%E8%AF%A2%EF%BC%9F"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 如果Hbase不需要Hive组件，如何实现易用的查询？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81%E4%B8%BAHBase%E5%BC%95%E5%85%A5Hive%E7%BB%84%E4%BB%B6"><span class="nav-number">6.</span> <span class="nav-text">6、为HBase引入Hive组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-hive-env-sh"><span class="nav-number">6.1.</span> <span class="nav-text">6.1  hive-env.sh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-%E5%9C%A8hive-site-xml%E6%B7%BB%E5%8A%A0zookeeper%E9%9B%86%E7%BE%A4"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 在hive-site.xml添加zookeeper集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-%E6%B5%8B%E8%AF%95hive%E6%93%8D%E4%BD%9Chbase"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 测试hive操作hbase</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7%E3%80%81%E4%BD%BF%E7%94%A8SQL%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E8%BF%9E%E6%8E%A5hive%E8%BF%9B%E8%A1%8C%E9%AB%98%E7%BA%A7SQL%E5%BC%80%E5%8F%91"><span class="nav-number">7.</span> <span class="nav-text">7、使用SQL开发工具连接hive进行高级SQL开发</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-%E9%85%8D%E7%BD%AEhive-site-xml%E5%92%8Ccore-site-xml"><span class="nav-number">7.1.</span> <span class="nav-text">7.1  配置hive-site.xml和core-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-%E5%9C%A8nn%E4%B8%BB%E8%8A%82%E7%82%B9%E4%B8%8A%E5%90%AF%E5%8A%A8hiveserver2%E6%9C%8D%E5%8A%A1"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 在nn主节点上启动hiveserver2服务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-%E9%85%8D%E7%BD%AEDBeaver%E8%BF%9E%E6%8E%A5hive"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 配置DBeaver连接hive</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-hiveserver2%E7%9A%84webUI"><span class="nav-number">7.4.</span> <span class="nav-text">7.4 hiveserver2的webUI</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E4%BD%BF%E7%94%A8beeline%E8%BF%9E%E6%8E%A5hiveserver2"><span class="nav-number">8.</span> <span class="nav-text">8 使用beeline连接hiveserver2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8%E3%80%81%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84Hive%E6%9C%8D%E5%8A%A1"><span class="nav-number">9.</span> <span class="nav-text">8、部署高可用的Hive服务</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">一个非常专注技术总结与分享的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yield-bytes</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">577k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:44</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/pjax/pjax.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/pisces.js"></script>


<script src="/blog/js/next-boot.js"></script>

<script src="/blog/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/blog/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
