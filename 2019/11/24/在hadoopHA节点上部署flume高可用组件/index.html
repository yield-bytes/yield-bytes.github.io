<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yield-bytes.gitee.io","root":"/blog/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":240,"width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="&amp;#8195;&amp;#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式">
<meta property="og:type" content="article">
<meta property="og:title" content="在hadoopHA节点上部署flume高可用组件">
<meta property="og:url" content="https://yield-bytes.gitee.io/blog/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/index.html">
<meta property="og:site_name" content="yield-bytes">
<meta property="og:description" content="&amp;#8195;&amp;#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191123154225919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191124103515197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2019-11-24T08:17:22.000Z">
<meta property="article:modified_time" content="2020-11-21T16:54:58.607Z">
<meta property="article:tag" content="flume高可用">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20191123154225919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="https://yield-bytes.gitee.io/blog/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>在hadoopHA节点上部署flume高可用组件 | yield-bytes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/blog/atom.xml" title="yield-bytes" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yield-bytes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">分享与沉淀</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blog/categories/" rel="section"><i class="fa fa-th-large fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://gitee.com/yield-bytes" class="github-corner" title="Follow me on Gitee" aria-label="Follow me on Gitee" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yield-bytes.gitee.io/blog/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个非常专注技术总结与分享的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yield-bytes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          在hadoopHA节点上部署flume高可用组件
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-24 16:17:22" itemprop="dateCreated datePublished" datetime="2019-11-24T16:17:22+08:00">2019-11-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-22 00:54:58" itemprop="dateModified" datetime="2020-11-22T00:54:58+08:00">2020-11-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Flume/" itemprop="url" rel="index"><span itemprop="name">Flume</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&#8195;&#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式</p>
<a id="more"></a>
<div class="table-container">
<table>
<thead>
<tr>
<th>hadoop节点</th>
<th>数据源节点</th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>nn</td>
<td>nn</td>
<td>NameNode,DataNode数据源</td>
</tr>
<tr>
<td>dn2</td>
<td>dn2</td>
<td>NameNode,DateNode数据源</td>
</tr>
<tr>
<td>dn1</td>
<td>dn1</td>
<td>DataNode,数据源</td>
</tr>
</tbody>
</table>
</div>
<p>其他hbase、hive等不再此表给出，因为前面的文件已有相关表格。</p>
<h4 id="1、flume-的基本介绍"><a href="#1、flume-的基本介绍" class="headerlink" title="1、flume 的基本介绍"></a>1、flume 的基本介绍</h4><h5 id="1-1-基本介绍"><a href="#1-1-基本介绍" class="headerlink" title="1.1 基本介绍"></a>1.1 基本介绍</h5><blockquote>
<p>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.  It uses a simple extensible data model that allows for online analytic application.</p>
</blockquote>
<p>简单来说：flume 是一个分布式、高可靠、高可用的用来收集、聚合、转移不同来源的大量日志数据到中央数据仓库的工具</p>
<p>目前flume最新版本为今年1月发布的 Flume 1.9.0，具体优化的内容：</p>
<blockquote>
<ul>
<li>Better SSL/TLS support</li>
<li>Configuration Filters to provide a way to inject sensitive information like passwords into the configuration</li>
<li>Float and Double value support in Context</li>
<li>Kafka client upgraded to 2.0</li>
<li>HBase 2 support</li>
</ul>
</blockquote>
<p>环境要求：</p>
<p>Java Runtime Environment - Java 1.8 or later</p>
<h5 id="1-2-数据流模型"><a href="#1-2-数据流模型" class="headerlink" title="1.2 数据流模型"></a>1.2 数据流模型</h5><p>这里以Flume收取web的日志再将其写入到hdfs作为数据流模型示例说明。<br>（需要注意的flume支持多级配置、扇入、扇出，这里仅介绍单级、单输入、单输出的模式）<br><img src="https://img-blog.csdnimg.cn/20191123154225919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>==Event==<br>Event是Flume定义的一个数据流传输的最小单元。Agent就是一个Flume的实例，本质是一个JVM进程，该JVM进程控制Event数据流从外部日志生产者那里传输到目的地（或者是下一个Agent）。<br>在Flume中，event表示数据传输的一个最小单位，从上图可以看出Agent就是Flume的一个部署实例， 一个完整的Agent中包含了三个组件Source、Channel和Sink，Source是指数据的来源和方式，Channel是一个数据的缓冲池，Sink定义了数据输出的方式和目的地。</p>
<p>==Source组件==<br>Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。（对于实时大数据项目，这个source其实就是不断滚动的log数据文件。）<br>Flume提供了各种source的实现，具体参考官方文档：<br>Flume Sources、Avro Source、Thrift Source、Exec Source、JMS Source、Taildir Source、Kafka Source、NetCat TCP Source、NetCat UDP Source、Syslog Sources、HTTP Source、Custom Source等</p>
<p>==Channel组件==<br>Channel是连接Source和Sink的组件，可以看作一个数据的缓冲区，它可以将事件暂存到内存中也可以持久化到磁盘上， 直到Sink处理完该事件。flume提供多个channel提供对event数据的缓存：<br>Memory Channel、JDBC Channel、Kafka Channel、File Channel、Spillable Memory Channel、Pseudo Transaction Channel、Custom Channel<br>（在本大数据实时项目中，使用Memory Channel即可，生产环境需要根据具体需求而定）</p>
<p>==Sink组件==<br>Sink从Channel取出event数据，并将其写入到文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。<br>Flume也提供了各种sink的实现，具体参考官方说明：<br>HDFS Sink、Hive Sink、Logger Sink、Avro Sink、Thrift Sink、IRC Sink、File Roll Sink、Null Sink、HBaseSinks、AsyncHBaseSink、ElasticSearchSink、Kite Dataset Sink、Kafka Sink、HTTP Sink、Custom Sink<br>（在实时大数据项目中，用sink配置将数据写入kafka sink）</p>
<p>以上的数据模型的详细介绍可以参考中文翻译文档：<a target="_blank" rel="noopener" href="https://flume.liyifeng.org/">地址</a><br>(注意：该翻译文档对应flume1.8版本的内容，若想查阅最新的flume，参考官网1.9原文)</p>
<h4 id="2、flume的配置文件说明"><a href="#2、flume的配置文件说明" class="headerlink" title="2、flume的配置文件说明"></a>2、flume的配置文件说明</h4><p>flume配置遵循Java properties文件格式的文本文件。一个或多个Agent配置可放在同一个配置文件里。配置文件包含Agent的source，sink和channel的各个属性以及他们的数据流连接。</p>
<p>完整的配置流程如下：<br>命名各个组件（定义流）—&gt;连接各个组件—&gt;配置各个组件的属性—&gt;启动Agent</p>
<h5 id="2-1-配置过程"><a href="#2-1-配置过程" class="headerlink" title="2.1 配置过程"></a>2.1 配置过程</h5><p>这里以后面文章——flume整合kafka的配置文件说明。<br>（有较多的blog文章会照搬a1.sources=r1,a1.sinks=k1,a1.channels=c1这样的写法，a1?r1?k1?c1?到底指代什么？所以建议要参考官方文档，否则一头雾水？当然熟悉flume后，可以使用短命名，设置配置属性字符串不会显得太长）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、列出Agent的所有Source、Channel、Sink</span></span><br><span class="line">&lt;Agent&gt;.sources = &lt;Source&gt;</span><br><span class="line">&lt;Agent&gt;.sinks = &lt;Sink&gt;</span><br><span class="line">&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接各个组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Source的关联</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Sink的关联</span></span><br><span class="line">&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、为每个组件配置属性，这些属性就是flume的性能参数，控制flume的各种工作方式，调优配置就在这部分了。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sources</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> channels</span></span><br><span class="line">&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sinks</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br></pre></td></tr></table></figure>
<p>对于本blog的实时大数据项目的配置，Agent名字为：agent_log从本地服务器读取log数据文件，使用内存channel缓存，然后通过kafka Sink从channel读取后发送到kafka集群，它的配置文件应该这样配：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、定义Agent的所有<span class="built_in">source</span>、sink和channel组件</span></span><br><span class="line">agent_log.sources = log-src</span><br><span class="line">agent_log.sinks = kafka-sink</span><br><span class="line">agent_log.channels = log-mem-channel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接三个组件</span></span><br><span class="line">agent_log.sources.log-src.channels =log-mem-channel    # 指定与source:log-src相连接的channel是log-mem-channel</span><br><span class="line">agent_foo.sinks.kafka-sink.channel = log-mem-channel   # 指定与sink:kafka-sink相连接的channel是log-mem-channel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、配置各个组件的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置avro-AppSrv-source的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> log-src 的类型是spooldir，官方建议不要使用tail -F抽取数据文件因会出现丢失</span></span><br><span class="line">agent_log.sources.log-src.type = spooldir         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置<span class="built_in">source</span>数据文件的路径</span></span><br><span class="line">agent_log.sources.log-src.spoolDir = /opt/flume_agent/web_log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置log-mem-channel的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> channel的类型是内存channel</span></span><br><span class="line">agent_log.channels.log-mem-channel.type = memory  </span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> channel的最大容量是1000</span></span><br><span class="line">agent_log.channels.log-mem-channel.capacity = 1000         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>和sink每次从channel写入和读取的Event数量</span></span><br><span class="line">agent_log.channels.log-mem-channel.transactionCapacity = 100    </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置kafka-sink的属性，将数据写入到kafka集群指定topic，实现Flume与Kafka集成</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接channel到kafkasink</span></span><br><span class="line">agent_log.sinks.kafka-sink.channel = log-mem-channel </span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定kafka sink</span></span><br><span class="line">agent_log.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka存放数据的topic</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.topic = webtopic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka sink 使用的 kafka 集群的实例列表</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每批要发送到kafka的消息数量</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.flumeBatchSize = 20</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在成功写入之前，要求有1个副本必须确认消息，保证数据一致性</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.producer.acks = 1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>通过上面的配置，就形成了[log-src]-&gt;[log-mem-channel]-&gt;[log-sink]的数据流，这将使Event通过内存channel（log-mem-channel）从log-src流向Kafka-sink，从而实现数据源-flume-kafka的实时数据流动。</p>
<h4 id="3、单点flume-agent测试"><a href="#3、单点flume-agent测试" class="headerlink" title="3、单点flume agent测试"></a>3、单点flume agent测试</h4><p>本节主要在name节点上部署单个flume agent，用于测试单agent的使用。<br>数据流向：<br>手动写日志内容—-&gt;flume spooldir抽取—-&gt;flume sink到hadoop集群文件系统上</p>
<h5 id="3-1-基本安装"><a href="#3-1-基本安装" class="headerlink" title="3.1 基本安装"></a>3.1 基本安装</h5><p>个人习惯将所有的hadoop组件都放置在同一个dir下，方便管理，如下所示<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hive-3.1.2       xcall.sh       </span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    scala-2.13.1         </span><br><span class="line">hbase-2.1.7   mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br></pre></td></tr></table></figure><br>配置flume-env.sh<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">[root@nn conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line">vi flume-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置java1.8的路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> JAVA_HOME=/opt/jdk1.8.0_16</span></span><br></pre></td></tr></table></figure><br>这里的配置要注意的点：如果已经在系统的环境变量配置JAVA_HOME，那么flume-env.sh可以不用再配置java路径</p>
<p>配置flume-conf.properties<br>这里就是用于配置flume agent的文件。有了第2章节的介绍后，这里有关source、channel、sink配置则相对简单，因此可以使用短字符进行配置<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>数据源为本地某个文件目录，flume监听这个目录下日志文件</span></span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里不需要写成web_log/</span></span><br><span class="line">a1.sources.r1.spoolDir = /opt/flume_log/web_log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录，hdapp为hadoop集群名称，不需要加入端口，否则flume无法写入，</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType=SequenceFile</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat=Writable</span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放在hdfs的文件文件命名方式，其实还有更详细的配置，这里仅给出简单示例，具体可参考官网。</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从flume过来的数据，每128M分割成一个文件</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128000000  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终在hdfs的文件名称为：%Y-%m-%d.TimeStamp.txt</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>这里在配置source.type要注意的是，配成spooldir类型后：允许把要收集的文件放入磁盘上的某个指定目录。它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来。数据解析逻辑是可配置的。<br>与Exec Source不同，Spooling Directory Source是可靠的，即使Flume重新启动或被kill，也不会丢失数据。<br>但这种可靠有一定的代价和限制：指定目录中的文件必须是不可变的、唯一命名的。Flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常：<br>如果文件在写入完成后又被再次写入新内容，Flume将向其日志文件（这是指Flume自己logs目录下的日志文件）打印错误并停止处理。如果在以后重新使用以前的文件名，Flume将向其日志文件打印错误并停止处理。<br>为了避免上述问题，最好在生成新文件的时候文件名加上时间戳，可以通过加入属性项实现：a1.sinks.k1.hdfs.useLocalTimeStamp = true</p>
<h5 id="3-2-启动flume-agent进程"><a href="#3-2-启动flume-agent进程" class="headerlink" title="3.2 启动flume agent进程"></a>3.2 启动flume agent进程</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 命令含义<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">commands:</span><br><span class="line">  help                      display this help text</span><br><span class="line">  agent                     run a Flume agent</span><br><span class="line">  avro-client               run an avro Flume client</span><br><span class="line">  version                   show Flume version info</span><br><span class="line"> global options:</span><br><span class="line">    --conf,-c &lt;conf&gt;          use configs in &lt;conf&gt; directory</span><br><span class="line"> agent options:</span><br><span class="line">    --name,-n &lt;name&gt;          the name of this agent (required)</span><br><span class="line">  --conf-file,-f &lt;file&gt;     specify a config file (required if -z missing)</span><br></pre></td></tr></table></figure><br>创建数据文件，测试flume 能否成功把目录下的文件推到hdfs指定目录上<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn web_log]# pwd</span><br><span class="line">/opt/flume_log/web_log</span><br><span class="line">[root@nn web_log]#  vi log.txt</span><br><span class="line">aaa</span><br><span class="line">bbb</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当文件创建后，发现该log.txt被命名为log.txt.COMPLETED，说明已经被flume 读取过</span></span><br><span class="line">[root@nn web_log]# ls</span><br><span class="line">log.txt.COMPLETED</span><br></pre></td></tr></table></figure><br>hdfs上可看到数据文件已经上传到到/flume/web_log（这里打码了时间）<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn web_log]# hdfs dfs -ls /flume/web_log </span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup        161 **** /flume/web_log/2019-**-**.15**0.txt</span><br></pre></td></tr></table></figure></p>
<h5 id="3-3-将source-type配成tail-F"><a href="#3-3-将source-type配成tail-F" class="headerlink" title="3.3 将source.type配成tail F"></a>3.3 将source.type配成tail F</h5><p>Spooling Directory Source是可靠的，它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来，当此种方式不适合本blog后面开发的实时大数据项目需求。具体说明如下：<br>本blog后面开发的实时大数据项目需求：<br>实时抽取access.log的访问日志，access.log每插入一行，flume 就会把它实时sink到hdfs上（本文用于测试所以先sink到hdfs，若已经到开发阶段，这里会改为sink到kalka集群上）。<br>对于spooldir模式，当log.txt被sink后其文件名变为log.txt.COMM，若继续向log.txt.COMPLETED append数据行，flume不会再抽取该文件，也说明无法把新来的数据sink到hdfs上，显然不符合需求。<br>source需做以下调整，使用exec source：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改<span class="built_in">source</span> <span class="built_in">type</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><br>这里sink的文件滚动策略很重要，若配置不当，flume sink会在hdfs不断滚动生成多个小文件，例如access.log新增一行，触发flume sink在hdfs新增一个对应的文件。<br>以下的配置：access.log在hdfs存放的形式为：<br>/flume/web_log/2019-05-31.1579*.txt<br>每达到128M则开始滚动新建一个文件。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128M</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br></pre></td></tr></table></figure></p>
<p>但exec source方式也有缺点：会丢失数据，例如当flume 挂了重启，之前进来的日志行将不会被重启后flume抽取到，正官方的提示：<br>The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost.</p>
<p>这种数据丢失其实还可以接受，毕竟大部分日志收集应用场景还没到高级事务的严格标准，而且服务器集群运行以及进程运行稳定，即使宕机、断电再重启，也只是一小部分日志行丢失。</p>
<p>==测试结果：==<br>启动flume agent，并将日志实时打印在shell<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]#  bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将日志追加新数据行</span></span><br><span class="line">[root@nn web_log]# echo &#x27;test&#x27;&gt;&gt;access.log </span><br></pre></td></tr></table></figure><br>在hdfs上，存放的文件会以文件名.txt.tmp形式保持打开状态，供flume实时写入，若达到滚动条件，则会生成日期+时间戳.txt的数据文件，再新建另外一个日期+时间戳.txt.tmp文件。</p>
<p>至此，已完成flume的部署，下一步，在三个节点上配置高可用的flume集群。</p>
<h4 id="4、flume高可用配置"><a href="#4、flume高可用配置" class="headerlink" title="4、flume高可用配置"></a>4、flume高可用配置</h4><p>Flume高可用又称Flume NG高可用，NG：Next Generation。<br>flume高可用的实现思路比较清晰：多个节点flume agent  avro sink 到 多个flume collector avro source上，这些flume collector 会有优先级，优先级高的collector负责把数据sink到hdfs或者kafka上。因为agent和collector是多节点运行，在agent端：某个agent挂了，还有其他agent工作；在collecor端，某个collector挂了，还有其他collector继续工作。<br>架构图如下：<br><img src="https://img-blog.csdnimg.cn/20191124103515197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">各个节点规划，考虑到测试虚拟机资源有限，其中两个节点都分布运行agent和collector进程。<br>| 节点 |  flume 角色|<br>|—|—|<br>| nn | agent1，collector 1|<br>| dn1 | agen2 |<br>| dn2 | agent3，collector2 |</p>
<h5 id="4-1-三个agent的flume配置"><a href="#4-1-三个agent的flume配置" class="headerlink" title="4.1  三个agent的flume配置"></a>4.1  三个agent的flume配置</h5><p>三个agent的配置其实都一样，不同的部分：每个agent命名不同。<br>在nn节点的/opt/flume-1.9.0/conf新建一个avro-agent.properties<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出agent1的组件，sinks有两个，分别去到collector1和collector2</span></span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>属性</span></span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel</span></span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector1</span></span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = nn</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector2</span></span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = dn2</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建sink groups，将多个sinks绑定为一个组，agent会向这些组sink 数据，将k1和k2设置负载均衡模式，也可以设置为failover模式，本文使用load_balance</span></span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">agent1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agent1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">agent1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">agent1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> failover模式，只有collector1工作。仅当collector1挂了后，collector2才能启动服务。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> agent1.sinkgroups.g1.processor.type = failover</span></span><br><span class="line"><span class="meta">#</span><span class="bash">值越大，优先级越高，collector1优先级最高</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k1 = 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k2 = 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">发生异常的sink最大故障转移时间（毫秒），这里设为10秒</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.maxpenalty = 10000</span></span><br></pre></td></tr></table></figure><br>将avro-agent.properties拷贝到dn1和dn2节点，agent1这个名字可改，可不改。</p>
<h5 id="4-2-配置-collector"><a href="#4-2-配置-collector" class="headerlink" title="4.2  配置 collector"></a>4.2  配置 collector</h5><p>分别在nn和dn2节点的/opt/flume-1.9.0/conf新建一个avro-collector.properties<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在dn2节点上，则改为collector2，不改也没关系，这里只是为了区分两个collector</span></span><br><span class="line">collector1.sources = r1</span><br><span class="line">collector1.sinks = k1</span><br><span class="line">collector1.channels = c1</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义<span class="built_in">source</span>：这里的<span class="built_in">source</span>配成avro，从而连接agent端sink avro</span></span><br><span class="line">collector1.sources.r1.channels = c1</span><br><span class="line">collector1.sources.r1.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span>的属性：dn2节点需改为dn2</span></span><br><span class="line">collector1.sources.r1.bind = nn</span><br><span class="line">collector1.sources.r1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">定义channel</span></span><br><span class="line">collector1.channels.c1.type = memory</span><br><span class="line">collector1.channels.c1.capacity = 1000</span><br><span class="line">collector1.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">定义sinks：由collector将数据event推到hdfs上</span></span><br><span class="line">collector1.sinks.k1.channel=c1</span><br><span class="line">collector1.sinks.k1.type=hdfs</span><br><span class="line">collector1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">collector1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">collector1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">collector1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">collector1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">collector1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"> </span><br></pre></td></tr></table></figure></p>
<h5 id="4-3-测试flume高可用"><a href="#4-3-测试flume高可用" class="headerlink" title="4.3 测试flume高可用"></a>4.3 测试flume高可用</h5><p>在nn节点和dn2节点启动各自的collector<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">nn节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector1，所以这里启动的--name 为collector1</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">dn2节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector2，所以这里启动的--name 为collector2</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></p>
<p>在nn、dn1和dn2节点启动各自的agent，在shell可以看到以下agent 进程打印的信息，说明三个agent都可以连接到两个collector的source组件k1和k2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k1 started.</span><br><span class="line"> ......</span><br><span class="line">Monitored counter group for type: SINK, name: k2: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k2 started.</span><br></pre></td></tr></table></figure><br>在nn节点上的access.log新增信息 echo ‘foo’ &gt;&gt;access.log后，在hdfs上可以看到<em>*</em>.txt.tmp文件可以相应的文件内容<br>停止collector1经常，此时测试collector2可以正常接替服务。</p>
<p>至此，已完成本文内容。下一篇文章为Hadoop引入Kafka组件，在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是本文测试的hdfs。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/flume%E9%AB%98%E5%8F%AF%E7%94%A8/" rel="tag"># flume高可用</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2019/11/19/hadoop%E9%9B%86%E7%BE%A4%E5%B9%B3%E5%8F%B0%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AEbond%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/" rel="prev" title="hadoop集群平台网络配置bond模式实现高可用">
      <i class="fa fa-chevron-left"></i> hadoop集群平台网络配置bond模式实现高可用
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2019/11/28/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2kafka%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6/" rel="next" title="在hadoopHA节点上部署kafka集群组件">
      在hadoopHA节点上部署kafka集群组件 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81flume-%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">1、flume 的基本介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 基本介绍</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-%E6%95%B0%E6%8D%AE%E6%B5%81%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 数据流模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81flume%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E"><span class="nav-number">2.</span> <span class="nav-text">2、flume的配置文件说明</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 配置过程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E3%80%81%E5%8D%95%E7%82%B9flume-agent%E6%B5%8B%E8%AF%95"><span class="nav-number">3.</span> <span class="nav-text">3、单点flume agent测试</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 基本安装</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-%E5%90%AF%E5%8A%A8flume-agent%E8%BF%9B%E7%A8%8B"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 启动flume agent进程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-%E5%B0%86source-type%E9%85%8D%E6%88%90tail-F"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 将source.type配成tail F</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4%E3%80%81flume%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="nav-number">4.</span> <span class="nav-text">4、flume高可用配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-%E4%B8%89%E4%B8%AAagent%E7%9A%84flume%E9%85%8D%E7%BD%AE"><span class="nav-number">4.1.</span> <span class="nav-text">4.1  三个agent的flume配置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-%E9%85%8D%E7%BD%AE-collector"><span class="nav-number">4.2.</span> <span class="nav-text">4.2  配置 collector</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-3-%E6%B5%8B%E8%AF%95flume%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 测试flume高可用</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">一个非常专注技术总结与分享的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yield-bytes</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">554k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:23</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/pjax/pjax.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/pisces.js"></script>


<script src="/blog/js/next-boot.js"></script>

<script src="/blog/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/blog/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
