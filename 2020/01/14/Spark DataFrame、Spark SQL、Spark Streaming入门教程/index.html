<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yield-bytes.gitee.io","root":"/blog/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":240,"width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="&amp;#8195;&amp;#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):Spark DataFrame、Spark SQL、Spark Streaming 1、RDD、Spark DataFrame、S">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark DataFrame、Spark SQL、Spark Streaming入门教程">
<meta property="og:url" content="https://yield-bytes.gitee.io/blog/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/index.html">
<meta property="og:site_name" content="yield-bytes">
<meta property="og:description" content="&amp;#8195;&amp;#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):Spark DataFrame、Spark SQL、Spark Streaming 1、RDD、Spark DataFrame、S">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-01-14T13:50:59.000Z">
<meta property="article:modified_time" content="2020-02-03T07:05:06.000Z">
<meta property="article:tag" content="Spark DataFrame">
<meta property="article:tag" content="Spark SQL">
<meta property="article:tag" content="Spark Streaming">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yield-bytes.gitee.io/blog/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark DataFrame、Spark SQL、Spark Streaming入门教程 | yield-bytes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/blog/atom.xml" title="yield-bytes" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yield-bytes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">分享与沉淀</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blog/categories/" rel="section"><i class="fa fa-th-large fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://gitee.com/yield-bytes" class="github-corner" title="Follow me on Gitee" aria-label="Follow me on Gitee" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yield-bytes.gitee.io/blog/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="一个非常专注技术总结与分享的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yield-bytes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark DataFrame、Spark SQL、Spark Streaming入门教程
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-01-14 21:50:59" itemprop="dateCreated datePublished" datetime="2020-01-14T21:50:59+08:00">2020-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-03 15:05:06" itemprop="dateModified" datetime="2020-02-03T15:05:06+08:00">2020-02-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>&#8195;&#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):<br><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark DataFrame</a>、<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a>、<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a></p>
<h4 id="1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming"><a href="#1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming" class="headerlink" title="1、RDD、Spark DataFrame、Spark SQL、Spark Streaming"></a>1、RDD、Spark DataFrame、Spark SQL、Spark Streaming</h4><p>&#8195;&#8195;RDD：大家最熟悉的数据结构，主要使用transmissions和actions 相关函数式算子完成数据处理和数理统计，例如map、reduceByKey，rdd没有定义 Schema（一般指未定义字段名及其数据类型）， 所以一般用列表索引号来指定每一个字段。 例如， 在电影数据的推荐例子中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">move_rdd.map(lambda line:line.split(&#39;|&#39;)).map(lambda a_list:(alist[0],a_list[1],a_list[2]))</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<p>每行有15个字段的数据，因此只能通过索引号获取前3个字段的数据，这要求开发者必须掌握 Map/Reduce 编程模式，不过， RDD 功能也最强， 能完成所有 Spark 数据处理与分析需求。</p>
<p>&#8195;&#8195;Spark DataFrame：创建DataFrame时，可以定义 Schema，通过定义每一个字段名与数据类型，以便之后直接用字段名进行数据索引，用法跟Pandas的DataFrame差别不大。Spark DataFrame是一种更高层的API，而且基于PySpark，用起来像Pandas的”手感“，很容易上手。</p>
<p>&#8195;&#8195;Spark SQL 底层是封装了DataFrame（DataFrame封装了底层的RDD） ，让使用者直接用sql的方式操作rdd，进一步降低Spark作为分布式计算框架的使用门槛。<br>&#8195;&#8195;Spark Streaming是本博客重点要解析的数据结构，实际项目将使用它实现流式计算，相关定义参考原文：</p>
<blockquote>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.<br>Spark Streaming具有扩展性、数据吞吐量高、容错的特点，底层基于core Spark API 实现，用于流数据处理。Spark Streaming注入的实时数据源可来自Kafka、Flume、Kinesis或者TCP流等，park Streaming可借助Map、reduce、join和window等高级函数接口搭建复杂的算法用于数据处理。Spark Streaming实时处理后数据可存储到文件系统、数据库或者实时数据展示仪表。</p>
</blockquote>
<h4 id="2、Spark-DataFrame"><a href="#2、Spark-DataFrame" class="headerlink" title="2、Spark DataFrame"></a>2、Spark DataFrame</h4><p>&#8195;&#8195;Spark DataFrame API比较多，既然用于数据处理和计算，当然会有预处理接口以及各统计函数、各种方法，详细参考<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">官网:pyspark.sql.DataFrame</a>以及<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions">pyspark.sql.functions module模块</a></p>
<p>&#8195;&#8195;目前版本中，创建Spark DataFrame的Context接口可以直接用SparkSession接口，无需像RDD创建上下文时用Spark Context。<br>SparkSession：pyspark.sql.SparkSession:Main entry point for DataFrame and SQL functionality.</p>
<h5 id="2-1-创建基本的Spark-DataFrame"><a href="#2-1-创建基本的Spark-DataFrame" class="headerlink" title="2.1 创建基本的Spark DataFrame"></a>2.1 创建基本的Spark DataFrame</h5><p>&#8195;&#8195;创建 Spark DataFrame有多种方式，先回顾Pandas的DataFrame，Pandas可从各类文件、流以及集合中创建df对象，同样 Spark DataFrame也有类似的逻辑<br>首先需加载spark的上下文：SparkSession</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession <span class="comment">#  用于Spark DataFrame的上下文</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,StructType,StructField, LongType,DateType <span class="comment"># 用于定义df字段类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row,Column</span><br><span class="line"></span><br><span class="line"><span class="comment">#本地spark单机模式</span></span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br><span class="line">print(spark)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出spark上下文信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SparkSession - in-memory</span><br><span class="line">SparkContext</span><br><span class="line">Spark UI</span><br><span class="line">Version v2.4.4</span><br><span class="line">Master</span><br><span class="line">    local[*]</span><br><span class="line">AppName</span><br><span class="line">    spark_dataframe</span><br></pre></td></tr></table></figure>


<p>from pyspark.sql.types：df目前支持定义的字段类型（参考源码），用于定义schema，类似关系型数据库建表时，定义表的字段类型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__all__ = [</span><br><span class="line">    <span class="string">&quot;DataType&quot;</span>, <span class="string">&quot;NullType&quot;</span>, <span class="string">&quot;StringType&quot;</span>, <span class="string">&quot;BinaryType&quot;</span>, <span class="string">&quot;BooleanType&quot;</span>, <span class="string">&quot;DateType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;TimestampType&quot;</span>, <span class="string">&quot;DecimalType&quot;</span>, <span class="string">&quot;DoubleType&quot;</span>, <span class="string">&quot;FloatType&quot;</span>, <span class="string">&quot;ByteType&quot;</span>, <span class="string">&quot;IntegerType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;LongType&quot;</span>, <span class="string">&quot;ShortType&quot;</span>, <span class="string">&quot;ArrayType&quot;</span>, <span class="string">&quot;MapType&quot;</span>, <span class="string">&quot;StructField&quot;</span>, <span class="string">&quot;StructType&quot;</span>]</span><br></pre></td></tr></table></figure>


<p>直接用从RDD创建dataframe对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">spark_rdd = spark.sparkContext.parallelize([</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义schema，就像数据库建表的定义：数据模型，定义列名，类型和是否为能为空</span></span><br><span class="line">schema = StructType([StructField(<span class="string">&quot;id&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;item&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;price&quot;</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;pub_date&quot;</span>, DateType(), <span class="literal">True</span>)])</span><br><span class="line"><span class="comment"># 创建Spark DataFrame</span></span><br><span class="line">spark_df= spark.createDataFrame(spark_rdd, schema)</span><br><span class="line"><span class="comment"># 创建一个零时表，用于映射到rdd上</span></span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;iPhone&quot;</span>)</span><br><span class="line"><span class="comment"># 使用Sql语句,语法完全跟sql一致</span></span><br><span class="line">data = spark.sql(<span class="string">&quot;select a.item,a.price from iPhone a&quot;</span>)</span><br><span class="line"><span class="comment"># 查看dataframe的数据</span></span><br><span class="line">print(data.collect())</span><br><span class="line"><span class="comment"># 以表格形式展示数据</span></span><br><span class="line">data.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[Row(item&#x3D;&#39;iPhoneX&#39;, price&#x3D;6000), Row(item&#x3D;&#39;iPhone7&#39;, price&#x3D;4000), Row(item&#x3D;&#39;iPhone4&#39;, price&#x3D;1000)]</span><br><span class="line">+-------+-----+</span><br><span class="line">|   item|price|</span><br><span class="line">+-------+-----+</span><br><span class="line">|iPhoneX| 6000|</span><br><span class="line">|iPhone7| 4000|</span><br><span class="line">|iPhone4| 1000|</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure>

<p>通过该例子，可了解df基本用法，只要从spark上下文加载完数据并转为dataframe类型后，之后调用df的api跟pandas的api大同小异，而且可将dataframe转为Spark SQL，直接使用sql语句操作数据。</p>
<p>上面的例子用了显示定义schema字段类型，pyspark支持自动推理创建df，也即无需原数据定义为rdd，和自动类型，直接传入Python列表的数据，以及定义字段名称即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a_list = [</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line"><span class="comment"># 自动推理创建df,代码内部通过各类if 判断类型实现。</span></span><br><span class="line">spark_df= spark.createDataFrame(a_list, schema=[<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;item&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;pub_date&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h5 id="2-2-从各类数据源创建Spark-DataFrame"><a href="#2-2-从各类数据源创建Spark-DataFrame" class="headerlink" title="2.2  从各类数据源创建Spark DataFrame"></a>2.2  从各类数据源创建Spark DataFrame</h5><p>相关接口方法参考官网<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html">文档</a></p>
<p>==从csv文件创建Spark DataFrame==</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">&#x27;/opt/spark/data/train.csv&#x27;</span></span><br><span class="line">df = spark.read.csv(file,header=<span class="literal">True</span>,inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>==从pandas的DataFrame创建Spark DataFrame==</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pandas_df = pd.DataFrame(np.random.random((<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line">spark_df = spark.createDataFrame(pandas_df, schema=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>==从json创建Spark DataFrame,json文件可以通过远程拉取，或者本地json，并设定json的字段schema==</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">json_df = spark.read.json(<span class="string">&#x27;/opt/data/all-world-cup-players.json&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>==从各类数据库加载数据：==</p>
<p>pg数据库，使用option属性传入参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark_df = spark.read \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>) \</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure>

<p>pg数据库，用关键字参数传入连接参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:postgresql://localhost:5432/&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;db_name.table_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure>

<p>mysql数据库，用关键字参数传入连接参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:mysql://localhost:3306/db_name&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;table_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure>

<p>从hive里面读取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果在SparkSession设置为连接hive，可以直接读取hive数据</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .enableHiveSupport() \      </span><br><span class="line">        .master(<span class="string">&quot;localhost:7077&quot;</span>) \</span><br><span class="line">        .appName(<span class="string">&quot;spark_hive&quot;</span>) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">spark_df=spark.sql(<span class="string">&quot;select * from hive_app_table&quot;</span>)</span><br><span class="line">spark_df.show()</span><br></pre></td></tr></table></figure>
<p>连接数据库需要相关的jar包，例如连接mysql，则需要将mysql-connector放在spark目录的jar目录下。</p>
<h5 id="2-3-Spark-DataFrame持久化数据"><a href="#2-3-Spark-DataFrame持久化数据" class="headerlink" title="2.3 Spark DataFrame持久化数据"></a>2.3 Spark DataFrame持久化数据</h5><p>以csv存储</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.write.csv(path=local_file_path, header=<span class="literal">True</span>, sep=<span class="string">&quot;,&quot;</span>, mode=<span class="string">&#x27;overwrite&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>注意：mode=‘overwrite’ 模式时，表示创建新表，若表名已存在则会被删除，整个表被重写。而 mode=‘append’ 模式就是普通的最加数据。</p>
<p>写入mysql：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;jdbc:mysql://localhost:3306/db_name?characterEncoding=utf-8&amp;autoReconnect=true&amp;useSSL=false&amp;serverTimezone=GMT&#x27;</span></span><br><span class="line">table = <span class="string">&#x27;table_name&#x27;</span></span><br><span class="line">properties = &#123;<span class="string">&quot;user&quot;</span>:<span class="string">&quot;test&quot;</span>,<span class="string">&quot;password&quot;</span>:<span class="string">&quot;test&quot;</span>&#125;</span><br><span class="line">spark_df.write.jdbc(url,table,mode=<span class="string">&#x27;append&#x27;</span>,properties=properties)</span><br></pre></td></tr></table></figure>



<p>写入hive</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开动态分区</span></span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition.mode = nonstrict&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition=true&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定分区写入表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).partitionBy(<span class="string">&quot;name&quot;</span>).insertInto(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用分区，直接将数据保存到Hive新表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from your_db.table_name&quot;</span>).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>默认的方式将会在hive分区表中保存大量的小文件，在保存之前对 DataFrame重新分区，从而控制保存的文件数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.repartition(<span class="number">5</span>).write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>写入redis：<br>这里需要自行实现redis的写入方法，其实也简单，定义入参为dataframe，函数内部连接redis后，从dataframe取出数据再将其插入redis即可。对于写入其他文件或者数据库，需自行实现相应的数据转存逻辑。</p>
<h5 id="2-4-Dataframe常见的API"><a href="#2-4-Dataframe常见的API" class="headerlink" title="2.4 Dataframe常见的API"></a>2.4 Dataframe常见的API</h5><p><a target="_blank" rel="noopener" href="https://github.com/Apress/machine-learning-with-pyspark/blob/master/chapter_2_Data_Processing/sample_data.csv">样例数据参考</a></p>
<p>查看字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark_df.columns </span><br><span class="line"><span class="comment"># [&#x27;ratings&#x27;, &#x27;age&#x27;, &#x27;experience&#x27;, &#x27;family&#x27;, &#x27;mobile&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>spark_df.count() 统计行数<br>查看df的shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print((df.count(),<span class="built_in">len</span>(df.columns))) <span class="comment"># (33, 5)</span></span><br></pre></td></tr></table></figure>

<p>查看dataframe的schema字段定义</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark_df.printSchema()</span><br><span class="line">输出：</span><br><span class="line">root</span><br><span class="line"> |-- ratings: integer (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- experience: double (nullable = true)</span><br><span class="line"> |-- family: integer (nullable = true)</span><br><span class="line"> |-- mobile: string (nullable = true)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>随机抽样探索数据集合：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark_df.sample(False,0.5,0).show(5)</span><br><span class="line">用法：</span><br><span class="line">Signature: spark_df.sample(withReplacement&#x3D;None, fraction&#x3D;None, seed&#x3D;None)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a sampled subset of this :class:&#96;DataFrame&#96;.</span><br><span class="line"></span><br><span class="line">:param withReplacement: Sample with replacement or not (default False).</span><br><span class="line">:param fraction: Fraction of rows to generate, range [0.0, 1.0].</span><br><span class="line">:param seed: Seed for sampling (default a random seed).</span><br></pre></td></tr></table></figure>

<p>查看行记录：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark_df.show(<span class="number">3</span>) </span><br><span class="line">spark_df.head(<span class="number">3</span>) </span><br><span class="line">spark_df.take(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>以python列表返回记录，list中每个元素是Row类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Row(ratings=<span class="number">3</span>, age=<span class="number">32</span>, experience=<span class="number">9.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Vivo&#x27;</span>, age_2=<span class="number">32</span>),</span><br><span class="line"> Row(ratings=<span class="number">3</span>, age=<span class="number">27</span>, experience=<span class="number">13.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Apple&#x27;</span>, age_2=<span class="number">27</span>),</span><br><span class="line"> Row(ratings=<span class="number">4</span>, age=<span class="number">22</span>, experience=<span class="number">2.5</span>, family=<span class="number">0</span>, mobile=<span class="string">&#x27;Samsung&#x27;</span>, age_2=<span class="number">22</span>)]</span><br></pre></td></tr></table></figure>

<p>查看null的行，可以传入isnull函数，也可以自定义lambda函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">spark_df = spark_df.<span class="built_in">filter</span>(isnull(<span class="string">&quot;name&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>选择列数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>).show(<span class="number">2</span>)<span class="comment"># select方法</span></span><br></pre></td></tr></table></figure>

<p>扩展dataframe的列数:withColumn可以在原df上新增列数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure>

<p>注意该方式不会更新到原df，如需替换原df，则更新spark_df即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure>

<p>新增一列数据，并将新增的数据转为double类型，需要用到cast方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,DoubleType,IntegerType</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;age_double&#x27;</span>,spark_df[<span class="string">&#x27;age&#x27;</span>].cast(DoubleType()))</span><br></pre></td></tr></table></figure>

<p>根据条件查询df，使用filter方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Apple&#x27;</span>)</span><br><span class="line"><span class="comment">#筛选记录后，再选出指定的字段记录</span></span><br><span class="line">spark_df.<span class="built_in">filter</span>(df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;ratings&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>选择mobile列值为‘Apple’的记录，多条件查询:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>)</span><br><span class="line">或者：</span><br><span class="line">spark_df.<span class="built_in">filter</span>((spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>)&amp;(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>distinct：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先用select取出要处理的字段，获取不同类型的mobile</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计不同类型mobile的数量</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct().count()</span><br></pre></td></tr></table></figure>

<p>groupBy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().show(<span class="number">5</span>,<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">+-------+-----+</span><br><span class="line">|mobile |count|</span><br><span class="line">+-------+-----+</span><br><span class="line">|MI     |<span class="number">8</span>    |</span><br><span class="line">|Oppo   |<span class="number">7</span>    |</span><br><span class="line">|Samsung|<span class="number">6</span>    |</span><br><span class="line">|Vivo   |<span class="number">5</span>    |</span><br><span class="line">|Apple  |<span class="number">7</span>    |</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure>

<p>groupBy之后，按统计字段进行降序排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().orderBy(<span class="string">&#x27;count&#x27;</span>,ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>groupBy之后，按mobile分组，求出每个字段在该分组的均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).mean().show(<span class="number">2</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|mobile|avg(ratings)     |avg(age)          |avg(experience)   |avg(family)       |avg(age_2)        |</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|MI    |3.5              |30.125            |10.1875           |1.375             |30.125            |</span><br><span class="line">|Oppo  |2.857142857142857|28.428571428571427|10.357142857142858|1.4285714285714286|28.428571428571427|</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">only showing top 2 rows</span><br></pre></td></tr></table></figure>

<p>同理还有<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code>、<code>df.groupBy(&#39;mobile&#39;).max()</code>、<code>df.groupBy(&#39;mobile&#39;).min()</code>等，或者用agg方法，然后传入相应的聚合函数</p>
<p><code>spark_df.groupBy(&#39;mobile&#39;).agg(&#123;&#39;experience&#39;:&#39;sum&#39;&#125;)</code>等同于<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code></p>
<p>用户定义函数UDF：</p>
<p>用户定义函数一般用于对列或者对行的数据进行定制化处理，就sql语句中，价格为数字的字段，根据不同判断条件，给字段加上美元符号或者指定字符等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costom_func</span>(<span class="params">brand</span>):</span></span><br><span class="line">    <span class="keyword">if</span> brand <span class="keyword">in</span> [<span class="string">&#x27;Samsung&#x27;</span>,<span class="string">&#x27;Apple&#x27;</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;High Price&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> brand ==<span class="string">&#x27;MI&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Mid Price&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Low Price&#x27;</span></span><br><span class="line">        </span><br><span class="line">brand_udf=udf(costom_func,StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;price_range&#x27;</span>,brand_udf(spark_df[<span class="string">&#x27;mobile&#x27;</span>])).show(<span class="number">5</span>,<span class="literal">False</span>) <span class="comment"># 使用spark_df[&#x27;mobile&#x27;]或者使用spark_df.mobile都可以</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|price_range|</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |Low Price  |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |High Price |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |High Price |</span><br><span class="line">|4      |37 |16.5      |4     |Apple  |37   |High Price |</span><br><span class="line">|5      |27 |9.0       |1     |MI     |27   |Mid Price  |</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用lambda定义udf</span></span><br><span class="line">age_udf = udf(<span class="keyword">lambda</span> age: <span class="string">&quot;young&quot;</span> <span class="keyword">if</span> age &lt;= <span class="number">30</span> <span class="keyword">else</span> <span class="string">&quot;senior&quot;</span>, StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&quot;age_group&quot;</span>, age_udf(df.age)).show(<span class="number">3</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|age_group|</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |senior   |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |young    |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |young    |</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>

<p>删除重复记录：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重复的行，将被删除</span></span><br><span class="line">spark_df=spark_df.dropDuplicates()</span><br></pre></td></tr></table></figure>

<p>删除一列数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_new=spark_df.drop(<span class="string">&#x27;mobile&#x27;</span>) <span class="comment"># 删除多列 spark_df.drop(&#x27;mobile&#x27;,&#x27;age&#x27;)</span></span><br></pre></td></tr></table></figure>

<h4 id="3、Spark-SQL"><a href="#3、Spark-SQL" class="headerlink" title="3、Spark SQL"></a>3、Spark SQL</h4><p>&#8195;&#8195;在第二部分内容给出了创建spark sql的方法，本章节给出更为详细的内容：这里重点介绍spark sql创建其上下文，完成相应的context设置后，剩下的就是熟悉的写SQL了。<br><strong>第一种方式：将本地文件加载为dataframe</strong><br>之后再使用createOrReplaceTempView方法转为<code>SQL模式</code>，流程如下</p>
<p>用第2节内容的数据做演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.csv(<span class="string">&#x27;sample_data.csv&#x27;</span>,inferSchema=<span class="literal">True</span>,header=<span class="literal">True</span>)</span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;phone_sales&quot;</span>)</span><br><span class="line">df1 = spark.sql(<span class="string">&quot;select age,family,mobile from phone_sales &quot;</span>)</span><br><span class="line">df1.show(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+---+------+-------+</span><br><span class="line">|age|family| mobile|</span><br><span class="line">+---+------+-------+</span><br><span class="line">| 32|     3|   Vivo|</span><br><span class="line">| 27|     3|  Apple|</span><br><span class="line">| 22|     0|Samsung|</span><br><span class="line">+---+------+-------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>

<p>spark.sql用于传入sql语句，返回dataframe对象，故后续的数据处理将变得非常灵活，使用SQL确实能够降低数据处理门槛，再例如：</p>
<p><code>spark_df.groupBy(&#39;mobile&#39;).count().show(5,False)</code> 等同于</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">your_sql=(<span class="string">&quot;select mobile,count(mobile) as count from phone_sales group by mobile &quot;</span></span><br><span class="line">df1 = spark.sql(your_sql)</span><br></pre></td></tr></table></figure>



<p>如果df1集合较大，适合用迭代器方式输出记录（适合逐条处理的逻辑）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> each_record <span class="keyword">in</span> df1.collect(): </span><br><span class="line">	data_process(each_record)</span><br></pre></td></tr></table></figure>


<p><strong>第二种方式：直接连接诸如mysql或者hive的context，基于该context直接运行sql</strong></p>
<p>以mysql为例：<br>（1）配置mysql连接需要相关jar包和路径配置：<br> mysql-connector-java-5.1.48.jar 放入spark目录<code>/opt/spark-2.4.4-bin-hadoop2.7/jars/</code>目录下， mysql-connector包可在mysql自行下载。<br>在spark-env.sh 配置了EXTRA_SPARK_CLASSPATH=/opt/spark-2.4.4-bin-hadoop2.7/jars/（也可不配，spark按默认目录检索）</p>
<p>（2）连接mysql</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession </span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br></pre></td></tr></table></figure>
<p>连接数据库以及读取表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apps_name_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).</span><br><span class="line">options(</span><br><span class="line">	url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/&#x27;</span>,</span><br><span class="line">	dbtable=<span class="string">&#x27;erp_app.apps_name&#x27;</span>,</span><br><span class="line">	user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">	password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure>
<p>read方法详细使用可参考：<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.format">spark.read.format</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pirnt(apps_name_df)</span><br><span class="line"><span class="comment"># DataFrame[id: int, app_log_name: string, log_path: string, log_date: timestamp]</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apps_name_df.show(5)</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">| id| app_log_name|           log_path|           log_date|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">|  1|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  3|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  5|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  7|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  9|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure>

<p>上述连接mysql的erp_app后，直接读取apps_name全部字段的数据，如果想在连接时，指定sql，则需按以下方式进行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/erp_app&#x27;</span>,</span><br><span class="line">                                           dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span>,</span><br><span class="line">                                           user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">                                           password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">                                          ).load()</span><br></pre></td></tr></table></figure>

<p>dbtable这个值可以为一条sql语句，而且格式必须为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span></span><br></pre></td></tr></table></figure>

<p>如果写成以下格式，则提示解析出错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;select id,app_log_name,log_path from apps_name&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="4、Spark-Streaming"><a href="#4、Spark-Streaming" class="headerlink" title="4、Spark Streaming"></a>4、Spark Streaming</h4><p>&#8195;&#8195;以上在介绍dataframe和spark sql的相关用法，都用离线数据进行测试，本章节将给出spark的核心组件之一：spark streaming：实时流式计算（微批处理），该功能将应用于本bg其他项目。有关流式计算的相关概念，可以查看相关参考资料，这里不再累赘。此外，本bg也将给出一篇关于spark streaming的深度解析文章。</p>
<h5 id="4-1-实时计算TCP端口的数据"><a href="#4-1-实时计算TCP端口的数据" class="headerlink" title="4.1 实时计算TCP端口的数据"></a>4.1 实时计算TCP端口的数据</h5><p>&#8195;&#8195;以一个简单demo介绍pyspark实现streaming的流程：<br>在数据源输入端，使用netcat打开一个7070端口，手动持续向netcat shell输入句子；<br>在实时计算端：streaming连接7070端口，并实时计算word count，并将统计结果实时打印。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建本地的streaming context，并指定4个worker线程</span></span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少spark自生成的日志打印</span></span><br><span class="line"><span class="comment"># 每批处理间隔为1秒</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接netcat的tcp端口，用于读取netcat持续输入的行字符串</span></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;192.100.0.10&quot;</span>, <span class="number">7070</span>)</span><br></pre></td></tr></table></figure>
<p>socketTextStream创建的对象称为：Discretized Streams（离散流） ，简称 DStream，是spark的核心概念</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计word的逻辑，这段代码再熟悉不过了。</span></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> value_1, value_2: value_1 + value_2)</span><br><span class="line">wordCounts.pprint() <span class="comment"># 这里wordCounts是&#x27;TransformedDStream&#x27; object，不再是普通的离线rdd</span></span><br></pre></td></tr></table></figure>

<p>启动流计算，并一直等到外部中断程序（相当于线程里面的jion）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)    <span class="comment"># 默认无timeout，程序会一直阻塞在这里</span></span><br></pre></td></tr></table></figure>

<p>启动后，如果你使用jupyter开发，可以看到notebook的cell每隔1秒打印一次</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:50</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:51</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>在netstat shell输入字符串</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# nc -l 7070</span><br><span class="line">this is spark streaming</span><br><span class="line">streaming wordcount is awesome</span><br></pre></td></tr></table></figure>



<p>再查看notebook的cell的的实时打印出了wordcount统计结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;spark&#39;, 1)</span><br><span class="line">(&#39;this&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:58</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;wordcount&#39;, 1)</span><br><span class="line">(&#39;awesome&#39;, 1)</span><br></pre></td></tr></table></figure>

<p>以上实现了一个完整的实时流计算，虽然该streaming的demo较为简单，但却给了大家非常直观的流计算处理设计思路，只需改造相关逻辑，即可满足符合自己业务的需求，在这里给出一个可行的设计：</p>
<p>（1）实时数据源替换为Kafka等组件：启动一个进程，用于运行streaming。streaming的实时数据源来自kafka的topic<br>（2）定制MapReduce的计算逻辑，用于实时预处理流数据<br>（3）将（2）的实时结果保存到redis的list上<br>（4）启动另外一个进程，从结果队列里面取出并存到Hbase集群或者hdfs<br>或者无需使用队列，Spark Streaming实时预处理后直接写入Hbase。</p>
<h5 id="4-2-实时计算本地文件"><a href="#4-2-实时计算本地文件" class="headerlink" title="4.2 实时计算本地文件"></a>4.2 实时计算本地文件</h5><p>&#8195;&#8195;对于python接口，<code>streamingContext.textFileStream(dataDirectory)</code>方法可以实时监听并读取本地目录下的日志文件，但有几点需要指出，参考官方文档指引：</p>
<ul>
<li><p>能实时监听<code>dataDirectory</code>目录下创建的任意类型文件</p>
</li>
<li><p><code>dataDirectory</code>主要分为两种文件系统，第一种为本地文件系统，例如监听<code>/opt/appdata/</code>目录下的所有文件，格式为<code>file:///opt/appdata/</code>，第二种为hdfs文件系统：格式为<code>hdfs://namenode:8040/logs/</code></p>
</li>
<li><p>支持文件正则匹配，例如要监听本地文件目录下，所有以<code>.log</code>作为后缀类型的文件，<code>file:///opt/appdata/*.log</code></p>
</li>
<li><p>要求监听目录下的所有文件，里面的数据格式是一致的，例如1.log和2.log,里面都相同固定格式的日志记录。（每次新增的文件如果数据格式不一样，显然streaming处理逻辑无法完成）</p>
</li>
<li><p>目录下的文件数量越多，streaming扫描耗时将越长</p>
</li>
<li><p>若移动文件到这个监控目录，则无法触发streaming读取该新增文件，必须用流的形式写入到这个目录的文件才能被监听到</p>
</li>
<li><p>最后也是也是最重要的：streaming只处理在时间窗口内创建的新的数据文件，这里如何理解<code>新的数据文件</code>？</p>
<p>例如streaming流计算设为5秒，这个5秒就是时间窗口，若在5秒内目录产生了一个1.log，这个1.log会被读取，当5秒时间窗口已经过了，那么即使1.log有数据更新，streaming也不再reload该文件，为什么会这么设计呢？</p>
<p>流式处理的逻辑：一批一批的实时读取，已经读取过的数据文件，在下一轮时间窗口不再读取。假设在下一轮时间窗口，还读取已处理过的文件（该文件追加了新的数据行），那么该设计逻辑不再是流式处理了。例如<code>/opt/appdata/</code>目录下，有1.log,…100.log，并还会持续新增数据文件，101.log….等，如果streaming在每轮时间窗口还要对已处理过文件：<code>1.log,...100.log</code>再重新读取（读取新增加的数据行），那么spark streaming就无法完成微批的、实时的流式处理逻辑。在下面的实例会加以说明:</p>
</li>
</ul>
<p>spark streaming 监听文件夹实例：</p>
<p>时间窗口为5秒，实时监听<code>/opt/words/</code>目录下的文件，只要有新的文件创建（这里新的文件是指：每次创建的新文件，文件名必须唯一，streaming才会触发读取）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>)<span class="comment"># 时间窗口为5秒</span></span><br><span class="line">lines = ssc.textFileStream(<span class="string">&quot;file:///opt/words/&quot;</span>)</span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">wordCounts.pprint()</span><br><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)  </span><br></pre></td></tr></table></figure>



<p>模拟实时生成数据文件，每5秒生成一份数据文件，并在生成文件前，删除之前的文件（因为这个旧文件已经被spark streaming读取并流式计算过，下一轮时间窗口不再读取，所以可以删除旧文件）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">import</span> random,os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_text</span>(<span class="params">dir_path,file_name</span>):</span></span><br><span class="line">    words=[<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;streaming&#x27;</span>,<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>,<span class="string">&#x27;kafka&#x27;</span>,<span class="string">&#x27;yarn&#x27;</span>,<span class="string">&#x27;zookeeper&#x27;</span>]</span><br><span class="line">    line_num=random.randint(<span class="number">1000</span>,<span class="number">2000</span>)</span><br><span class="line">    text=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(line_num):</span><br><span class="line">        line=<span class="string">&#x27; &#x27;</span>.join([random.choice(words) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">        text=text+line+<span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    data_file_path=os.path.join(dir_path,file_name)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file_path,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(text)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">streaming_gen_data</span>(<span class="params">stream_dir_path</span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        tmp_file=os.listdir(stream_dir_path)</span><br><span class="line">        <span class="keyword">if</span>  tmp_file:<span class="comment"># 如果监听的目录下有旧文件，则直接删除</span></span><br><span class="line">            file_path=os.path.join(stream_dir_path,tmp_file[<span class="number">0</span>])</span><br><span class="line">            os.remove(file_path)</span><br><span class="line">        file_name=<span class="built_in">str</span>(uuid.uuid1())+<span class="string">&#x27;.log&#x27;</span> <span class="comment"># 保证每次新增的文件名唯一</span></span><br><span class="line">        save_text(stream_dir_path,file_name)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    streaming_gen_data(<span class="string">&#x27;/opt/words&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 933)</span><br><span class="line">(&#39;foo&#39;, 970)</span><br><span class="line">(&#39;yarn&#39;, 951)</span><br><span class="line">(&#39;zookeeper&#39;, 938)</span><br><span class="line">(&#39;bar&#39;, 1020)</span><br><span class="line">(&#39;spark&#39;, 990)</span><br><span class="line">(&#39;streaming&#39;, 1021)</span><br><span class="line">(&#39;kafka&#39;, 949)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:35</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 651)</span><br><span class="line">(&#39;zookeeper&#39;, 623)</span><br><span class="line">(&#39;bar&#39;, 593)</span><br><span class="line">(&#39;yarn&#39;, 584)</span><br><span class="line">(&#39;foo&#39;, 659)</span><br><span class="line">(&#39;spark&#39;, 623)</span><br><span class="line">(&#39;streaming&#39;, 592)</span><br><span class="line">(&#39;kafka&#39;, 571)</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>每隔5秒，spark streaming 完成微批计算：实时统计新创建文件的单词数</p>
<p>在这里重点说明 ：<code>file_name=str(uuid.uuid1())+&#39;.log&#39; </code>,这句保证了每次生成的文件使用了唯一文件名称，这样spark streaming才会瞬间监听到变化，及时读取到该文件。</p>
<p>有人会问：在生成文件前，已经删除了旧文件，那么每次创建文件使用固定文件名，对于spark streaming来说应该是唯一的、未加载过的文件才对吧？</p>
<p>解释：当使用os.remove一个文件后，如果等待间隔时长不长（例如几秒钟）又再创建同名文件，linux底层文件系统使用了缓存，用<code>ls -al</code>  查看该文件，会发现新创建文件的创建时间没有及时改变，导致spark streaming认为该文件还是原的旧文件，也就不再读取。</p>
<p>具体说明如下：<br>第一次创建文件的时间为<code>16 16:10</code>，接着下轮生成新文件，删除旧文件，等待5秒后，再创建同名新文件时，会发现创建时间没有改变还是<code>16 16:10</code>，而ssc.textFileStream读取时用的是创建时间去判断是否为新文件，所以才导致<code>明明已经创建新文件，但是</code>ssc.textFileStream却不读取的情况，这是pyspark textFileStream的bug，这个接口不应该只用创建时间判断。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 33847  16 16:10 streaming_data.log # 首次创建文件</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory #目录下的文件被删除</span><br><span class="line">[root@nn words]# ls -al streaming_data.log</span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">-rw-r--r-- 1 root root 31166  16 16:10 streaming_data.log # 5秒后，第二次创建同名的文件，创建时间未改变（如果等待时间去到十来秒，此时创建同名的文件的创建时间会发生变化）</span><br></pre></td></tr></table></figure>

<p>鉴于<code>textFileStream</code>接口使用场景受限，所以spark streaming实时数据源最适合的场景：接收kafka或者flume推来的流数据，这保证spark streaming能够立刻监听流数据的到来时间是已经发生变化，能触发streaming计算。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/Spark-DataFrame/" rel="tag"># Spark DataFrame</a>
              <a href="/blog/tags/Spark-SQL/" rel="tag"># Spark SQL</a>
              <a href="/blog/tags/Spark-Streaming/" rel="tag"># Spark Streaming</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2020/01/11/%E5%9F%BA%E4%BA%8EPySpark%E5%92%8CALS%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B/" rel="prev" title="基于PySpark和ALS算法实现基本的电影推荐流程">
      <i class="fa fa-chevron-left"></i> 基于PySpark和ALS算法实现基本的电影推荐流程
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2020/01/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%BC%82%E6%AD%A5IO%E7%9A%84%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91%E2%80%94%E2%80%94IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%EF%BC%88select%E3%80%81poll%E3%80%81epoll%EF%BC%89/" rel="next" title="深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）">
      深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E3%80%81RDD%E3%80%81Spark-DataFrame%E3%80%81Spark-SQL%E3%80%81Spark-Streaming"><span class="nav-number">1.</span> <span class="nav-text">1、RDD、Spark DataFrame、Spark SQL、Spark Streaming</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E3%80%81Spark-DataFrame"><span class="nav-number">2.</span> <span class="nav-text">2、Spark DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-%E5%88%9B%E5%BB%BA%E5%9F%BA%E6%9C%AC%E7%9A%84Spark-DataFrame"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 创建基本的Spark DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-%E4%BB%8E%E5%90%84%E7%B1%BB%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%9B%E5%BB%BASpark-DataFrame"><span class="nav-number">2.2.</span> <span class="nav-text">2.2  从各类数据源创建Spark DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-Spark-DataFrame%E6%8C%81%E4%B9%85%E5%8C%96%E6%95%B0%E6%8D%AE"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Spark DataFrame持久化数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-4-Dataframe%E5%B8%B8%E8%A7%81%E7%9A%84API"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 Dataframe常见的API</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%E3%80%81Spark-SQL"><span class="nav-number">3.</span> <span class="nav-text">3、Spark SQL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4%E3%80%81Spark-Streaming"><span class="nav-number">4.</span> <span class="nav-text">4、Spark Streaming</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97TCP%E7%AB%AF%E5%8F%A3%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 实时计算TCP端口的数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 实时计算本地文件</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="https://www.linuxprobe.com/wp-content/uploads/2018/06/QQ%E5%9B%BE%E7%89%8720180625205006.png">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">一个非常专注技术总结与分享的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yield-bytes</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">575k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:43</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/pjax/pjax.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/pisces.js"></script>


<script src="/blog/js/next-boot.js"></script>

<script src="/blog/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/blog/js/local-search.js"></script>













    <div id="pjax">
  

  

  

    </div>
</body>
</html>
