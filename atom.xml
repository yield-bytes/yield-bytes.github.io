<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yield-bytes</title>
  
  <subtitle>分享与沉淀</subtitle>
  <link href="https://yield-bytes.gitee.io/blog/atom.xml" rel="self"/>
  
  <link href="https://yield-bytes.gitee.io/blog/"/>
  <updated>2020-11-22T15:42:14.271Z</updated>
  <id>https://yield-bytes.gitee.io/blog/</id>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>利用pandas将分组后的组内多行归并为一行</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/11/22/%E5%88%A9%E7%94%A8pandas%E5%B0%86groupby%E5%88%86%E7%BB%84%E5%90%8E%E5%B0%86%E7%BB%84%E5%86%85%E5%A4%9A%E8%A1%8C%E4%B8%BA%E4%B8%80%E8%A1%8C/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/11/22/%E5%88%A9%E7%94%A8pandas%E5%B0%86groupby%E5%88%86%E7%BB%84%E5%90%8E%E5%B0%86%E7%BB%84%E5%86%85%E5%A4%9A%E8%A1%8C%E4%B8%BA%E4%B8%80%E8%A1%8C/</id>
    <published>2020-11-21T16:00:00.000Z</published>
    <updated>2020-11-22T15:42:14.271Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;本文的使用场景相对常见，尤其来了一份含几千上万行Excel表（或者长报告里需要插入表格数据），需要将某列的单元格长字符串展开展开为多行，或者将groupby后将同组的多行记录拼接为单个字符串，使用pandas可以快速完成。（文章由jupyter notebook导出的Markdown文件生成）</p><h3 id="groupby后将同组的多行值连接为一个长字符串"><a href="#groupby后将同组的多行值连接为一个长字符串" class="headerlink" title="groupby后将同组的多行值连接为一个长字符串"></a>groupby后将同组的多行值连接为一个长字符串</h3><h4 id="使用df-groupby、apply、pd-merge"><a href="#使用df-groupby、apply、pd-merge" class="headerlink" title="使用df.groupby、apply、pd.merge"></a>使用df.groupby、apply、pd.merge</h4><p>（在mysql中，可以直接使用GROUP_CONCAT(字段 SEPARATOR ‘、’)方法实现）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;省份&#x27;</span>:[<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;上海&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;大学名称&#x27;</span>:[<span class="string">&#x27;复旦大学&#x27;</span>,<span class="string">&#x27;中山大学&#x27;</span>,<span class="string">&#x27;同济大学&#x27;</span>,<span class="string">&#x27;华南理工大学&#x27;</span>,<span class="string">&#x27;上海交通大学&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df=pd.DataFrame(data)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><a id="more"></a><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学名称</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学</td>    </tr>    <tr>      <th>2</th>      <td>上海</td>      <td>同济大学</td>    </tr>    <tr>      <th>3</th>      <td>广东</td>      <td>华南理工大学</td>    </tr>    <tr>      <th>4</th>      <td>上海</td>      <td>上海交通大学</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flatten_rows=<span class="keyword">lambda</span> s:<span class="string">&#x27;、&#x27;</span>.join(s) <span class="comment"># 这就是分组后，将组内多行合并为一行的处理逻辑，这里用顿号连接行字符串</span></span><br><span class="line"><span class="comment"># 使用apply方法,as_index=False,表示不将省份作为groupby的行索引</span></span><br><span class="line">df1=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>)[<span class="string">&#x27;大学名称&#x27;</span>].apply(flatten_rows)</span><br><span class="line">df1</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学名称</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>)[<span class="string">&#x27;大学名称&#x27;</span>].agg(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">df2</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学名称</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>2</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数量统计一列合并到df1，用省份关联即可,指定合并后，非key列的后缀，以便区分列来源哪个dataframe</span></span><br><span class="line">df1.merge(df2,on=<span class="string">&#x27;省份&#x27;</span>,suffixes=[<span class="string">&#x27;_df1&#x27;</span>,<span class="string">&#x27;_df2&#x27;</span>]) </span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学名称_df1</th>      <th>大学名称_df2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>      <td>2</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">merged_df=pd.merge(df1,df2,on=<span class="string">&#x27;省份&#x27;</span>,suffixes=[<span class="string">&#x27;_df1&#x27;</span>,<span class="string">&#x27;_df2&#x27;</span>]) <span class="comment"># 也可直接用pd.merge方法来做</span></span><br><span class="line">merged_df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学名称_df1</th>      <th>大学名称_df2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>      <td>2</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">merged_df.columns=[<span class="string">&#x27;省份&#x27;</span>,<span class="string">&#x27;大学列表&#x27;</span>,<span class="string">&#x27;数量&#x27;</span>]</span><br><span class="line">merged_df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学列表</th>      <th>数量</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>      <td>2</td>    </tr>  </tbody></table></div><h4 id="df-groupby和agg方法"><a href="#df-groupby和agg方法" class="headerlink" title="df.groupby和agg方法"></a>df.groupby和agg方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">groupby_df=df.groupby(<span class="string">&#x27;省份&#x27;</span>,as_index=<span class="literal">False</span>).agg(&#123;<span class="string">&#x27;大学名称&#x27;</span>: [flatten_rows,<span class="string">&#x27;count&#x27;</span>]&#125;)</span><br><span class="line">groupby_df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead tr th &#123;    text-align: left;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr>      <th></th>      <th>省份</th>      <th colspan="2" halign="left">大学名称</th>    </tr>    <tr>      <th></th>      <th></th>      <th>&lt;lambda_0&gt;</th>      <th>count</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>      <td>2</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">groupby_df.columns=[<span class="string">&#x27;省份&#x27;</span>,<span class="string">&#x27;大学列表&#x27;</span>,<span class="string">&#x27;数量&#x27;</span>]</span><br><span class="line">groupby_df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省份</th>      <th>大学列表</th>      <th>数量</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>      <td>3</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>      <td>2</td>    </tr>  </tbody></table></div><h3 id="同组一行长字符串展开为多行"><a href="#同组一行长字符串展开为多行" class="headerlink" title="同组一行长字符串展开为多行"></a>同组一行长字符串展开为多行</h3><p>此内容为前面的逆向处理（若使用mysql，需较为复杂的sql语句实现，但不也难）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data2=&#123;</span><br><span class="line">    <span class="string">&#x27;省&#x27;</span>:pd.Series([<span class="string">&#x27;上海&#x27;</span>,<span class="string">&#x27;广东&#x27;</span>,<span class="string">&#x27;广西&#x27;</span>]),</span><br><span class="line">    <span class="string">&#x27;大学列表&#x27;</span>:pd.Series([<span class="string">&#x27;复旦大学、同济大学、上海交通大学&#x27;</span>,<span class="string">&#x27;中山大学、华南理工大学&#x27;</span>,<span class="literal">None</span>])</span><br><span class="line">&#125;</span><br><span class="line">df=pd.DataFrame(data2)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>大学列表</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>    </tr>    <tr>      <th>2</th>      <td>广西</td>      <td>None</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df21=df.copy()</span><br><span class="line"><span class="comment"># 创建数据集要小心，用pd.Series创建，则每行数据为字符串类型</span></span><br><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="使用pd-explode方法快速将一行展开为多行"><a href="#使用pd-explode方法快速将一行展开为多行" class="headerlink" title="使用pd.explode方法快速将一行展开为多行"></a>使用pd.explode方法快速将一行展开为多行</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pd.explode要求单元格的值为一个列表，如[&#x27;复旦大学&#x27;, &#x27;同济大学&#x27;, &#x27;上海交通大学&#x27;]，因此需原单元格长字符串做处理</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_func</span>(<span class="params">cell</span>):</span> <span class="comment"># 每个cell的长字符串分割，注意对于空值的处理</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> cell:</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    <span class="keyword">return</span> cell.split(<span class="string">&#x27;、&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>]=df21[<span class="string">&#x27;大学列表&#x27;</span>].apply(split_func) <span class="comment"># 大学列表这一列的每行值都是字符串类型</span></span><br><span class="line">df21[<span class="string">&#x27;大学列表&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>[&#39;复旦大学&#39;, &#39;同济大学&#39;, &#39;上海交通大学&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df21.explode(<span class="string">&#x27;大学列表&#x27;</span>,ignore_index=<span class="literal">False</span>) <span class="comment">#ignore_index=False使得DataFrame对象保持原一行对应的索引号</span></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>大学列表</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>同济大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>华南理工大学</td>    </tr>    <tr>      <th>2</th>      <td>广西</td>      <td>None</td>    </tr>  </tbody></table></div><p>以上重点解析：对cell单元格的长字符串进行分割为一个列表，需自行设计一个分割函数<br>例如字符串：’复旦大学、同济大学、上海交通大学’<br>同理其他更为复杂的字符串:”黄小明201、李小明202|黄小明203%…”,采用正则分割即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pandas.DataFrame.explode()内部调用pandas.Series.explode()：</span></span><br><span class="line"><span class="string">        result = df[column].explode()</span></span><br><span class="line"><span class="string">        # 删除原df指定展开列后，再与该列单独使用pandas.Series.explode得到的结果进行索引关联合并</span></span><br><span class="line"><span class="string">        result = df.drop([column], axis=1).join(result) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pandas.Series.explode()内部使用reshape.explode</span></span><br><span class="line"><span class="string">values, counts = reshape.explode(np.asarray(self.array))</span></span><br><span class="line"><span class="string"># 该方法来pandas._libs基础库目录的reshape.cpython-37m-darwin.so*，这里已经被编译为动态链接库</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># pandas.Series.explode()方法表示将一列中单元格为列表值展开为多行</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series([[1, 2, 3], &#x27;foo&#x27;, [], [3, 4]])</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s</span></span><br><span class="line"><span class="string">0    [1, 2, 3]</span></span><br><span class="line"><span class="string">1          foo</span></span><br><span class="line"><span class="string">2           []</span></span><br><span class="line"><span class="string">3       [3, 4]</span></span><br><span class="line"><span class="string">dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>s.explode()</span></span><br><span class="line"><span class="string">0      1</span></span><br><span class="line"><span class="string">0      2</span></span><br><span class="line"><span class="string">0      3</span></span><br><span class="line"><span class="string">1    foo</span></span><br><span class="line"><span class="string">2    NaN</span></span><br><span class="line"><span class="string">3      3</span></span><br><span class="line"><span class="string">3      4</span></span><br><span class="line"><span class="string">dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 可以同一单元格展开多行后，索引号跟原单元格索引号相同，因此可用原df通过join展开多行的Series，就能快速完成全表展开。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="使用stack-join方法快速将一行展开为多行"><a href="#使用stack-join方法快速将一行展开为多行" class="headerlink" title="使用stack+join方法快速将一行展开为多行"></a>使用stack+join方法快速将一行展开为多行</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df22=df.copy()</span><br><span class="line">df22</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>大学列表</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>    </tr>    <tr>      <th>2</th>      <td>广西</td>      <td>None</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df22_1=df22[<span class="string">&#x27;大学列表&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;、&#x27;</span>,expand=<span class="literal">True</span>) <span class="comment"># 将一个cell值扩展为多列的cell值，该方法能自动处理cell空值</span></span><br><span class="line">df22_1</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>复旦大学</td>      <td>同济大学</td>      <td>上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>中山大学</td>      <td>华南理工大学</td>      <td>None</td>    </tr>    <tr>      <th>2</th>      <td>None</td>      <td>None</td>      <td>None</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#三、将扩展的列转成多行，有些列的单元格值为None，转为行记录需要去掉</span></span><br><span class="line">df22_2=df22_1.stack(dropna=<span class="literal">True</span>)</span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure><pre><code>0  0      复旦大学   1      同济大学   2    上海交通大学1  0      中山大学   1    华南理工大学dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df22_2.index</span><br></pre></td></tr></table></figure><pre><code>MultiIndex([(0, 0),            (0, 1),            (0, 2),            (1, 0),            (1, 1)],           )</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 观察以上索引，若能把MultiIndex的第1列索引留下，则可关联到原表的0，1索引号</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df22_2.reset_index(level=<span class="number">1</span>,drop=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure><pre><code>0      复旦大学0      同济大学0    上海交通大学1      中山大学1    华南理工大学dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df22_2.name=<span class="string">&#x27;new_col&#x27;</span></span><br><span class="line">df22_2</span><br></pre></td></tr></table></figure><pre><code>0      复旦大学0      同济大学0    上海交通大学1      中山大学1    华南理工大学Name: new_col, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span>(df22[<span class="string">&#x27;大学列表&#x27;</span>]) <span class="comment"># 删除原数据集大学列表这一列，用于后面合并</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df22_3=df22.join(df22_2) <span class="comment"># 默认使用两个df的索引号进行关联合并。最终完成将一行.注意df22是dataframe才有join方法，df22_2是Series，没有join方法</span></span><br><span class="line">df22_3</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>new_col</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>同济大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>华南理工大学</td>    </tr>    <tr>      <th>2</th>      <td>广西</td>      <td>NaN</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主要思路：将需要展开为多行的单元格值展开为多列，通过旋转操作将其变为一列Series，重置索引号，最终使用两个df索引号进行join关联合并,除了用pd.join，还可使用pd.concat两个省列与大学名称列关联合并为一张dataframe</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># df22[&#x27;省&#x27;]属于Series类型，df22_2也是Series类型，两者通过索引号进行关联后，按列方向合并</span></span><br><span class="line">df22_3=pd.concat([df22[<span class="string">&#x27;省&#x27;</span>],df22_2],join=<span class="string">&#x27;inner&#x27;</span>,axis=<span class="number">1</span>)</span><br><span class="line">df22_3</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>new_col</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>同济大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>华南理工大学</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df22_3.columns=[<span class="string">&#x27;省&#x27;</span>,<span class="string">&#x27;大学名称&#x27;</span>]</span><br><span class="line">df22_3 <span class="comment"># 注意到广西所在行已被删除，最终可将None行垂直合并到df22_3即可</span></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>大学名称</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>同济大学</td>    </tr>    <tr>      <th>0</th>      <td>上海</td>      <td>上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>华南理工大学</td>    </tr>  </tbody></table></div><h4 id="自行设计合并逻辑"><a href="#自行设计合并逻辑" class="headerlink" title="自行设计合并逻辑"></a>自行设计合并逻辑</h4><p>上海    复旦大学、同济大学、上海交通大学<br>广东    中山大学、华南理工大学<br>生成两个Series列数据<br>[‘上海’,’上海’,’上海’,’广东’,’广东’]<br>[‘复旦大学’,’同济大学’,’上海交通大学’,’中山大学’,’华南理工大学’]<br>再将这两个Series生成Dataframe即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df23=df.copy()</span><br><span class="line">df23</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>大学列表</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学、同济大学、上海交通大学</td>    </tr>    <tr>      <th>1</th>      <td>广东</td>      <td>中山大学、华南理工大学</td>    </tr>    <tr>      <th>2</th>      <td>广西</td>      <td>None</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">province_list=[] <span class="comment"># 存放省的Series的值</span></span><br><span class="line">college_list=[] <span class="comment"># 存放大学名称Series的值</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df23.itertuples():</span><br><span class="line">    prov_cell=row[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> row[<span class="number">2</span>]:</span><br><span class="line">        mult_cell=row[<span class="number">2</span>].split(<span class="string">&#x27;、&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> mult_cell:</span><br><span class="line">            province_list.append(prov_cell)</span><br><span class="line">            college_list.append(item)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 注意对cell为空值得处理</span></span><br><span class="line">        province_list.append(prov_cell)</span><br><span class="line">        college_list.append(row[<span class="number">2</span>])</span><br><span class="line">    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">province_list</span><br></pre></td></tr></table></figure><pre><code>[&#39;上海&#39;, &#39;上海&#39;, &#39;上海&#39;, &#39;广东&#39;, &#39;广东&#39;, &#39;广西&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">college_list</span><br></pre></td></tr></table></figure><pre><code>[&#39;复旦大学&#39;, &#39;同济大学&#39;, &#39;上海交通大学&#39;, &#39;中山大学&#39;, &#39;华南理工大学&#39;, None]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data23=&#123;</span><br><span class="line">    <span class="string">&#x27;省&#x27;</span>:province_list,</span><br><span class="line">    <span class="string">&#x27;大学名称&#x27;</span>:college_list</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(data23)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>省</th>      <th>大学名称</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>上海</td>      <td>复旦大学</td>    </tr>    <tr>      <th>1</th>      <td>上海</td>      <td>同济大学</td>    </tr>    <tr>      <th>2</th>      <td>上海</td>      <td>上海交通大学</td>    </tr>    <tr>      <th>3</th>      <td>广东</td>      <td>中山大学</td>    </tr>    <tr>      <th>4</th>      <td>广东</td>      <td>华南理工大学</td>    </tr>    <tr>      <th>5</th>      <td>广西</td>      <td>None</td>    </tr>  </tbody></table></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;本文的使用场景相对常见，尤其来了一份含几千上万行Excel表（或者长报告里需要插入表格数据），需要将某列的单元格长字符串展开展开为多行，或者将groupby后将同组的多行记录拼接为单个字符串，使用pandas可以快速完成。（文章由jupyter notebook导出的Markdown文件生成）&lt;/p&gt;
&lt;h3 id=&quot;groupby后将同组的多行值连接为一个长字符串&quot;&gt;&lt;a href=&quot;#groupby后将同组的多行值连接为一个长字符串&quot; class=&quot;headerlink&quot; title=&quot;groupby后将同组的多行值连接为一个长字符串&quot;&gt;&lt;/a&gt;groupby后将同组的多行值连接为一个长字符串&lt;/h3&gt;&lt;h4 id=&quot;使用df-groupby、apply、pd-merge&quot;&gt;&lt;a href=&quot;#使用df-groupby、apply、pd-merge&quot; class=&quot;headerlink&quot; title=&quot;使用df.groupby、apply、pd.merge&quot;&gt;&lt;/a&gt;使用df.groupby、apply、pd.merge&lt;/h4&gt;&lt;p&gt;（在mysql中，可以直接使用GROUP_CONCAT(字段 SEPARATOR ‘、’)方法实现）&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;data=&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;省份&amp;#x27;&lt;/span&gt;:[&lt;span class=&quot;string&quot;&gt;&amp;#x27;上海&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;广东&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;上海&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;广东&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;上海&amp;#x27;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&amp;#x27;大学名称&amp;#x27;&lt;/span&gt;:[&lt;span class=&quot;string&quot;&gt;&amp;#x27;复旦大学&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;中山大学&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;同济大学&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;华南理工大学&amp;#x27;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&amp;#x27;上海交通大学&amp;#x27;&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df=pd.DataFrame(data)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;df&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="数据分析与挖掘" scheme="https://yield-bytes.gitee.io/blog/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98/"/>
    
    
  </entry>
  
  <entry>
    <title>基于Gitee Pages和Hexo搭建个人开源博客</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/11/22/%E5%9F%BA%E4%BA%8EGitee%20Pages%E5%92%8CHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%BC%80%E6%BA%90%E5%8D%9A%E5%AE%A2/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/11/22/%E5%9F%BA%E4%BA%8EGitee%20Pages%E5%92%8CHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%BC%80%E6%BA%90%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-11-21T16:00:00.000Z</published>
    <updated>2020-11-22T11:12:22.282Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;本blog用于归档如何在GitHub搭建个人博客的过程，内容参考来自本篇文章<a href="https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog">《如何用 GitHub 从零开始搭建一个博客》</a> 以及hexo中文官网的<a href="https://hexo.io/zh-cn/docs/">文档</a>。</p><p>更新：这篇文章写于今年年初，个人博客在年初已使用GitHub Pages搭建开源博客主页，今年下半年GitHub Pages所有以github.io为尾的博客地址在国内都无法正常访问，因此将其切换到国内Gitee才是正确选择，确实也需支持国内开源平台。</p><p><img src="https://img-blog.csdnimg.cn/2020112219074494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><h4 id="选用GitHub-Gitee发布开源博客的理由"><a href="#选用GitHub-Gitee发布开源博客的理由" class="headerlink" title="选用GitHub/Gitee发布开源博客的理由"></a>选用GitHub/Gitee发布开源博客的理由</h4><p>个人就使用CSDN相关看法：</p><ul><li>优势：<br>首先CSDN流量不愁，不同博主之间可以进行留言、私信等方式进行技术交流等，其次CSDN提供不错Markdown编辑器工具，例如在线草稿、离线草稿、图片拖曳与上传、定时保存等功能相对完善。</li><li>不足：<br>发布文章后，博客页面两栏窗口过多广告，对于推崇简洁主义的开发者来说（尤其已经习惯MacOS暗黑模式高度focus的UI）较为烦人，毕竟CSDN平台运营存在大量的商业行为。<br>而GitHub/Gitee搭建的博客为开源博客，以静态文件方式发布，通过整合一些博客框架提供的简洁模板，可以为技术文章撰写者和阅读者提供“无打扰”的简约而清爽的阅读环境。</li></ul><h4 id="GitHub-Gitee可构建个人开源博客主页"><a href="#GitHub-Gitee可构建个人开源博客主页" class="headerlink" title="GitHub/Gitee可构建个人开源博客主页"></a>GitHub/Gitee可构建个人开源博客主页</h4><p>GitHub/Gitee上除了最重要的代码仓库功能，还有GitHub Pages （国内为Gitee Pages）功能，可用于部署静态网页文件，只要发布者整合基本的工具，通过博客框架将本地Markdown文件编译为静态网页文件，再部署到GitHub/Gitee仓库，访问URL即可看到该静态网页。因此技术撰写者需要做的事情为：</p><ul><li>配置GitHub /GitHub Pages 本地博客开发环境</li><li>在本地完成Markdown文章</li><li>push 到GitHub/Gitee完成公网的博客发布</li></ul><p>目前GitHub Pages在国内已无法正常访问（VPN除外），因此开源博客主页在个人的Gitee仓库部署。</p><h3 id="Gitee-pages相关环境配置"><a href="#Gitee-pages相关环境配置" class="headerlink" title="Gitee pages相关环境配置"></a>Gitee pages相关环境配置</h3><h4 id="新建一个Repository用于存博客文件"><a href="#新建一个Repository用于存博客文件" class="headerlink" title="新建一个Repository用于存博客文件"></a>新建一个Repository用于存博客文件</h4><p>首先在个人Gitee 新建一个仓库，名称为blog，当然gitee开启pages功能后，主页url会被gitee自动设为</p><p><code>https://yield-bytes.gitee.io/blog</code></p><p>需注意：若基于GitHub Pages 部署博客，需要按以下规则建立仓库名称：</p><p>名称为 {yourblogname}.github.io，仓库名称必须以github.io 为后缀结尾，国内的gitee不需要此方式。</p><h4 id="本地git的配置"><a href="#本地git的配置" class="headerlink" title="本地git的配置"></a>本地git的配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name  &quot;用户名&quot; </span><br><span class="line"></span><br><span class="line">注意这里gitee的邮箱配置：如果在gitee设置了不公开个人邮箱，那么gitee会帮你设置一个隐私邮箱***@user.noreply.gitee.com，后续git命令提交都需要设置这个隐私邮箱，而不是设置注册邮箱</span><br><span class="line"></span><br><span class="line">git config --global user.email &quot;***@user.noreply.gitee.com&quot; </span><br><span class="line"></span><br><span class="line"># 避免git gui中的中文乱码</span><br><span class="line">git config --global gui.encoding utf-8</span><br><span class="line"></span><br><span class="line"># 避免git status显示的中文名乱码</span><br><span class="line">git config --global core.quotepath off</span><br></pre></td></tr></table></figure><h4 id="配置码云-ssh-key"><a href="#配置码云-ssh-key" class="headerlink" title="配置码云 ssh key"></a>配置码云 ssh key</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &#39;注册码云的邮箱&#39;</span><br></pre></td></tr></table></figure><p>其中-t指定密钥类型，这里设置rsa即可，-C是密钥的注释，这里设置成邮箱方便分辨</p><p>将公钥打印出来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~&#x2F;.ssh&#x2F;id_rsa.pub</span><br></pre></td></tr></table></figure><p> 在码云上添加ssh 公钥：在个人设置里面找到相应的ssh key添加界面处理即可</p><p>测试连接是否成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">**** % ssh gitee@gitee.com</span><br><span class="line">Hi ***! You&#39;ve successfully authenticated, but GITEE.COM does not provide shell access.</span><br><span class="line">Connection to gitee.com closed.</span><br></pre></td></tr></table></figure><p>测试往仓库推一个新建文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir MyTecBlog</span><br><span class="line">cd MyTecBlog</span><br><span class="line">git init</span><br><span class="line">touch README.md</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m &quot;首次提交&quot;</span><br><span class="line">git remote add origin git@gitee.com:yield-bytes&#x2F;blog.git # 指定push的线上仓库，这里就是前面创建的blog仓库</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><p>在web端的gitee仓库可看到README.md文件以及提交记录</p><h3 id="Hexo环境配置"><a href="#Hexo环境配置" class="headerlink" title="Hexo环境配置"></a>Hexo环境配置</h3><p>Hexo依赖node.js，借用相关npm包，使得搭建出来的GitHub博客不会过于简单。<br>MacOS直接用brew安装即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install node </span><br><span class="line">node -v # node.js的版本</span><br><span class="line">npm -v # 包管理版本</span><br></pre></td></tr></table></figure><p>安装 Hexo博客框架<br>借助Hexo博客框架，可以快速部署和设计博客，Hexo自带命令行命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#拉取源设置为淘宝镜像</span><br><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>在macOS安装 hexo-cli可能会提示未安装xcode CommandLineTools</p><ul><li><p><code>xcode-select --print-path</code>查看 command-line tools 的安装路径，不出意外显示的结果应该是<code>/Library/Developer/CommandLineTools</code></p></li><li><p><code>sudo rm -r -f /Library/Developer/CommandLineTools，</code>卸载 command-line tools </p></li><li><p><code>xcode-select --install，重新安装</code>command-line tools </p></li></ul><h3 id="创建hexo博客项目"><a href="#创建hexo博客项目" class="headerlink" title="创建hexo博客项目"></a>创建hexo博客项目</h3><p>创建一个名为yield-bytes的博客项目（名字可以任意取）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder hexo init yield-bytes</span><br><span class="line">yymac@wonder yield-bytes % ls</span><br><span class="line">_config.ymlpackage.jsonthemes</span><br><span class="line">node_modulesscaffolds</span><br><span class="line">package-lock.jsonsource</span><br></pre></td></tr></table></figure><p>可以看到yield-bytes为一个目录，该目录下有相应的node模块目录、配置文件、主题目录等。<br>将项目编译成静态文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo generate</span><br></pre></td></tr></table></figure><p>项目目录下多了一个public目录，可以看到这些就是静态网页内容：css、html、js等</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder public % ls</span><br><span class="line">2020cssindex.html</span><br><span class="line">archivesfancyboxjs</span><br></pre></td></tr></table></figure><p>其中index.html就是hexo的demo博客展示页面，通过在public目录下运行<code>hexo server</code>将页面作为web服务跑起来</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder public % hexo server</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure><p>在浏览器可以看到hexo的demo博客主页</p><h4 id="将本地demo博客部署到gitee"><a href="#将本地demo博客部署到gitee" class="headerlink" title="将本地demo博客部署到gitee"></a>将本地demo博客部署到gitee</h4><p>只需要在hexo创建的项目目录下的<code>_config.yml</code>加入gitee.io仓库的相关配置，hexo可一键部署到gitee上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>修改<code>_config.yml</code>，需要修改两处地方</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># URL</span><br><span class="line"># If your site is put in a subdirectory, set url as &#x27;http://example.com/child&#x27; and root as &#x27;/child/&#x27;</span><br><span class="line">url: https://yield-bytes.gitee.io/blog</span><br><span class="line">root: /blog/</span><br><span class="line">  </span><br><span class="line"># Deployment</span><br><span class="line"># Docs: https://hexo.io/docs/one-command-deployment</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@gitee.com:yield-bytes/blog.git</span><br><span class="line">  branch: master</span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>上述提示本地静态网页文件已经push到gitee的blog仓库<br>打开网址<code>https://yield-bytes.gitee.io/blog</code>，即可看到demo主页</p><h3 id="配置和完善个人博客"><a href="#配置和完善个人博客" class="headerlink" title="配置和完善个人博客"></a>配置和完善个人博客</h3><p>前面章节仅完成基本的搭建，从本章开始，将完善个人博客的配置以及UI。在<code>_config.yml </code>文件里面，分了很多部分，都可用于配置博客不同功能</p><h4 id="修改博客简介等基本内容："><a href="#修改博客简介等基本内容：" class="headerlink" title="修改博客简介等基本内容："></a>修改博客简介等基本内容：</h4> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  4 # Site</span><br><span class="line"> 5 title: Yield-Bytes</span><br><span class="line"> 6 subtitle: &#39;分享与沉淀&#39;</span><br><span class="line"> 7 description: &#39;这是一个非常专注于技术总结与分享的博客&#39;</span><br><span class="line"> 8 keywords: &quot;Python,BigData,Web开发,数据分析,深度学习&quot;</span><br><span class="line"> 9 author: yield-bytes</span><br><span class="line">10 language: zh-CN</span><br><span class="line">11 timezone: &#39;Asia&#x2F;Shanghai&#39;</span><br></pre></td></tr></table></figure><p> 本地运行刷新即可看到主页修改后的效果。</p><h4 id="为博客设置主题"><a href="#为博客设置主题" class="headerlink" title="为博客设置主题"></a>为博客设置主题</h4><p>博客当然可以设置为不同风格的UI，称为主题，在前十年那会，网易163博客还很流行，博客主可根据平台提供不同模板将自己的博客打造更加高级，有些模板还需要VIP或者付费。在当今开源时代，Hexo提供很多不错的博客模板，网址:<code>https://hexo.io/themes/</code>，可直接clone使用，以next主题为例子的配置过程：<br>在<code>yield-bytes/themes</code>目录下仅有一个默认的landscape主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder themes % ls</span><br><span class="line">landscape</span><br></pre></td></tr></table></figure><p>将next主题拉到该目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder themes % ls</span><br><span class="line">yymac@wonder themes % git clone https://github.com/theme-next/hexo-theme-next</span><br><span class="line">hexo-theme-nextlandscape</span><br></pre></td></tr></table></figure><p>在yield-bytes根目录下修改_config.yml 文件，在 theme 配置部分，修改为 hexo-theme-next:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4 # Extensions</span><br><span class="line">3 ## Plugins: https://hexo.io/plugins/</span><br><span class="line">2 ## Themes: https://hexo.io/themes/ # 官方提供更多开源的主题以及插件</span><br><span class="line">1 theme:  hexo-theme-next  # 更换主题</span><br></pre></td></tr></table></figure><p>重启hexo server即可看到主页已经换成next主题</p><h4 id="对主题进行深度定制"><a href="#对主题进行深度定制" class="headerlink" title="对主题进行深度定制"></a>对主题进行深度定制</h4><ul><li>更换样式</li></ul><p>每个主题的目录下也有一个<code>_config.yml</code>博客样式配置文件，这里可进行深度定制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder hexo-theme-next % ls</span><br><span class="line">LICENSE.mdcrowdin.ymllanguagesscripts</span><br><span class="line">README.mddocslayoutsource</span><br><span class="line">_config.ymlgulpfile.jspackage.json</span><br></pre></td></tr></table></figure><p>next 默认有四个样式，这里设为Pisces样式，也可打开暗黑模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br><span class="line"></span><br><span class="line"># Dark Mode</span><br><span class="line">darkmode: true</span><br></pre></td></tr></table></figure><ul><li>更换title的logo</li></ul><p>由于logo需要出裁剪为一定尺寸的png图片，需自行设计。在next主题下的<code>_config.yml</code>的favicon部分可进行更换logo的配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">favicon:</span><br><span class="line">   small: &#x2F;images&#x2F;favicon-16x16-next.png</span><br><span class="line">   medium: &#x2F;images&#x2F;favicon-32x32-next.png</span><br><span class="line">   apple_touch_icon: &#x2F;images&#x2F;apple-touch-icon-next.png</span><br><span class="line">   safari_pinned_tab: &#x2F;images&#x2F;logo.svg</span><br></pre></td></tr></table></figure><ul><li>avatar 设置头像后者主页的标识图像</li></ul><p>只需将图片放在该路径：themes/hexo-theme-next/source/images 路径，在next主题下的<code>_config.yml </code>文件下配置为该图片路径，也可设为网络图片路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">7 avatar:</span><br><span class="line">6   # Replace the default image and set the url here.</span><br><span class="line">5   url: #&#x2F;images&#x2F;avatar.gif</span><br><span class="line"># url: #&#x2F;images&#x2F;avatar.gif</span><br><span class="line">4   # If true, the avatar will be dispalyed in circle.</span><br><span class="line">3   rounded: true</span><br><span class="line">2   # If true, the avatar will be rotated with the cursor.</span><br><span class="line">1   rotated: false</span><br></pre></td></tr></table></figure><ul><li><p>为个人博客开启RSS(用处不大)<br>在hexo创建的博客项目目录下安装feed插件,博客项目安装的所有插件都放置在node_modules目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder  yield-bytes % npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure><p>安装完成之后不需要其他的配置，以后每次编译生成站点的时候就会自动生成 RSS Feed 文件</p></li><li><p>修改code代码的显示样式</p></li></ul><p>在文章中，经常需要贴上代码，为保证阅读效果，可将默认浅灰色样式设为Mac样式，看起还不错。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">12 codeblock:</span><br><span class="line">11   # Code Highlight theme</span><br><span class="line">10   # Available values: normal | night | night eighties | night blue | night  bright | solarized | solarized dark | galactic</span><br><span class="line"> 9   # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line"> 8   highlight_theme: solarized dark</span><br><span class="line"> 7   # Add copy button on codeblock</span><br><span class="line"> 6   copy_button:</span><br><span class="line"> 5     enable: true</span><br><span class="line"> 4     # Show text copy result.</span><br><span class="line"> 3     show_result: true</span><br><span class="line"> 2     # Available values: default | flat | mac</span><br><span class="line"> 1     style: mac</span><br></pre></td></tr></table></figure><p>default样式：<br><img src="https://img-blog.csdnimg.cn/20200201140722675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>mac样式：<br><img src="https://img-blog.csdnimg.cn/2020020114043519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ul><li>back2top设置页面滚动逻辑、阅读进度</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">14 back2top:</span><br><span class="line">13   enable: true</span><br><span class="line">12   # Back to top in sidebar.</span><br><span class="line">11   sidebar: true</span><br><span class="line">10   # Scroll percent label in b2t button.</span><br><span class="line"> 9   scrollpercent: true</span><br><span class="line"> 8</span><br><span class="line"> 7 # Reading progress bar</span><br><span class="line"> 6 reading_progress:</span><br><span class="line"> 5   enable: true</span><br><span class="line"> 4   # Available values: top | bottom</span><br><span class="line"> 3   position: top</span><br><span class="line"> 2   color: &quot;#37c6c0&quot;</span><br><span class="line"> 1   height: 3px</span><br><span class="line"></span><br><span class="line">10 # Bookmark Support</span><br><span class="line"> 9 bookmark:</span><br><span class="line"> 8   enable: true</span><br><span class="line"> 7   # Customize the color of the bookmark.</span><br><span class="line"> 6   color: &quot;#222&quot;</span><br><span class="line"> 5   # If auto, save the reading progress when closing the page or clicking th     e bookmark-icon.</span><br><span class="line"> 4   # If manual, only save it by clicking the bookmark-icon.</span><br><span class="line"> 3   save: auto</span><br><span class="line"> 2</span><br></pre></td></tr></table></figure><p>打开Bookmark Support功能后，可以提示阅读体验，例如关闭该文章后，再打开浏览，可以恢复到上次阅读位置，类似微信阅读文章的体验。</p><ul><li>为每篇文章关联GitHub Banner</li></ul><p>大家阅读一些技术文章应该经常看到文章右上角有GitHub的图标，该链接就是去GitHub Repository:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   4 # `Follow me on GitHub` banner in the top-right corner.</span><br><span class="line">   3 github_banner:</span><br><span class="line">   2   enable: true</span><br><span class="line">   1   permalink: https://github.com/yield-bytes/yield-bytes.github.io</span><br><span class="line">403    title: Follow me on GitHub</span><br></pre></td></tr></table></figure><ul><li>为博客正确显示math 的Markdown语法</li></ul><p>为了能让文章中能正常显示数学公式（常见数据挖掘、深度学习等文章），可通过hexo为next主题引入相关插件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % npm install  hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>注意：这里不要安装hexo-renderer-pandoc，该库的js有bug，无法正确解析Markdown文章，会导致hexo运行报错</p><p>在主题配置打开math配置即可：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   1 math:</span><br><span class="line">512    enable: true</span><br><span class="line">   1   # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">   2   # That is it only render those page which has `mathjax: true` in Front-ma     tter.</span><br><span class="line">   3   # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">   4   per_page: true</span><br><span class="line">   5</span><br><span class="line">   6   # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJa     x support.</span><br><span class="line">   7   mathjax:</span><br><span class="line">   8     enable: true</span><br><span class="line">   9     # See: https://mhchem.github.io/MathJax-mhchem/</span><br><span class="line">  10     mhchem: true</span><br></pre></td></tr></table></figure><ul><li>开启无刷新加载页面</li></ul><p>为进一步提升博客体验，hexo支持页面实现无刷新加载，借助pjax插件即可。<br>首先在配置文件里开启pjax</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1   # pjax: //cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js</span><br><span class="line">938    pjax: true</span><br></pre></td></tr></table></figure><p>在hexo-theme-next目录下，安装该插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder hexo-theme-next % pwd</span><br><span class="line">***/yield-bytes/themes/hexo-theme-next</span><br><span class="line">yymac@wonder hexo-theme-next % git clone https://github.com/theme-next/theme-next-pjax source/lib/pjax</span><br></pre></td></tr></table></figure><p>到处，已经完成对搭建博客的较为深度的定制，其他定制可参考官网：<a href="https://theme-next.org/docs/getting-started/%EF%BC%8C%E8%BF%99%E9%87%8C%E4%B8%8D%E5%86%8D%E7%B4%AF%E8%B5%98%E3%80%82">https://theme-next.org/docs/getting-started/，这里不再累赘。</a></p><h3 id="文章设置"><a href="#文章设置" class="headerlink" title="文章设置"></a>文章设置</h3><p>前置章节主要对博客主题及其UI做定制配置，本章节介绍如何在搭建的Gitee Pages上发布新文章等内容。</p><h4 id="增加文章"><a href="#增加文章" class="headerlink" title="增加文章"></a>增加文章</h4><p>新增一篇名为《深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）》的文章<br>在本地yield-bytes根目录创建文章，文章类型为Markdown格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo new 深入理解异步IO的底层逻辑——IO多路复 用（select、poll、epoll）</span><br><span class="line">INFO  Created:***yield-bytes/source/_posts/深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）.md</span><br></pre></td></tr></table></figure><p>所有新建的文章的都拷贝到该目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % cd source/_posts/</span><br><span class="line">yymac@wonder _posts % ls</span><br><span class="line">hello-world.md</span><br><span class="line">深入理解异步IO的底层逻辑——IO多路复用(select、poll、epoll).md</span><br></pre></td></tr></table></figure><p>每篇文章的头部为该文章的元数据，例如标题，创建时间，标签，文章分类，有关文章属性更为详细的配置，可以参考：<a href="https://hexo.io/zh-cn/docs/writing.html">https://hexo.io/zh-cn/docs/writing.html</a></p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  ---</span><br><span class="line">  title: 深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）</span><br><span class="line">  date: 2020-02-01 21:50:46</span><br><span class="line">tags: </span><br><span class="line"><span class="bullet">-</span> IO多路复用</span><br><span class="line"><span class="bullet">-</span> epoll</span><br><span class="line">categories:</span><br><span class="line"><span class="bullet">-</span> Python进阶</span><br><span class="line">  ---</span><br></pre></td></tr></table></figure><p>头部元数据之后就是Markdown的正文</p><h4 id="修改文章字体"><a href="#修改文章字体" class="headerlink" title="修改文章字体"></a>修改文章字体</h4><p>在next主题的配置文件<code>_config.yml</code>里面，font 部分可以设置文章显示字体，默认博客站点的字体大小为1，个人在global设为0.8后，文字看起相对舒服</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">16 font:</span><br><span class="line">15   enable: true</span><br><span class="line">14</span><br><span class="line">13   # Uri of fonts host, e.g. //fonts.googleapis.com (Default).</span><br><span class="line">12   host:</span><br><span class="line">11</span><br><span class="line">10   # Font options:</span><br><span class="line"> 9   # <span class="code">`external: true`</span> will load this font family from <span class="code">`host`</span> above.</span><br><span class="line"> 8   # <span class="code">`family: Times New Roman`</span>. Without any quotes.</span><br><span class="line"> 7   # <span class="code">`size: x.x`</span>. Use <span class="code">`em`</span> as unit. Default: 1 (16px)</span><br><span class="line"> 6</span><br><span class="line"> 5   # Global font settings used for all elements inside <span class="xml"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span>.</span><br><span class="line"> 4   global:</span><br><span class="line"> 3     external: true</span><br><span class="line"> 2     family: Lato</span><br><span class="line"> 1     size: 0.8</span><br></pre></td></tr></table></figure><h4 id="为博客左侧增加标签页、分类页等链接"><a href="#为博客左侧增加标签页、分类页等链接" class="headerlink" title="为博客左侧增加标签页、分类页等链接"></a>为博客左侧增加标签页、分类页等链接</h4><p>上面搭建的博客仅有少量栏目例如文章栏目、主页栏目，每次发布文章前，我们需要为其打个标签，以便当文章数量较多时，读者可在标签页查看相关的标签，从而快速找到相关文章，例如一篇streaming和kafka整合的文章，标签可以打为：streaming、kafka、实时流计算等。此外还需多文章进行分类，例如Python进阶分类、spark相关的分类、数据分析与挖掘的分类、数据结构与算法的分类、Linux相关的分类等<br>创建标签页和分类页：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % hexo new page tags</span><br><span class="line">INFO  Created: ***/yield-bytes/source/tags/index.md</span><br><span class="line"></span><br><span class="line">yymac@wonder yield-bytes % hexo new page categories</span><br><span class="line">INFO  Created: ***/yield-bytes/source/categories/index.md</span><br></pre></td></tr></table></figure><p>创建的标签页以及分类页都是Markdown文件<br>标签页的md：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: tags</span><br><span class="line">  2 date: <span class="strong">****</span> 10:58:03</span><br><span class="line">  3 ---</span><br><span class="line">~</span><br></pre></td></tr></table></figure><p>分类页的md：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: categories</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 11:32:51</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 ---</span></span></span><br></pre></td></tr></table></figure><p>将tags的md指定为标签页：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: tags</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 10:58:03</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 type: tags</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  4 comments: false  # 这里表示关闭当前文章的评论</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  5 ---</span></span></span><br></pre></td></tr></table></figure><p>将categories制定为分类页：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1   ---</span><br><span class="line">  1 title: categories</span><br><span class="line">  2 date: <span class="strong">**<span class="emphasis">* 11:32:51</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  3 type: categories</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  4 comments: false</span></span></span><br><span class="line"><span class="strong"><span class="emphasis">  5 ---</span></span></span><br></pre></td></tr></table></figure><p>在next主题的配置文章的menu部分新增tags访问路径和categories访问路径，这里就是配置页面路由的地方，可自行新增路径。</p><p>配置说明：<code> tags: /tags/ || fa fa-tags</code>表示tags的路径为/tags/，对应的icon图标为fa fa-tags</p><p>这里有一个图标与名称对应的地址：<code>https://v3.bootcss.com/components/#glyphicons</code>  ，将选中的icon图标名称如glyphicons glyphicons-th-large改为fa fa-th-large即可</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  #about: /about/ || fa fa-user</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th-large</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br><span class="line">  #schedule: /schedule/ || fa fa-calendar</span><br><span class="line">  #sitemap: /sitemap.xml || fa fa-sitemap</span><br><span class="line">  #commonweal: /404/ || heartbeat</span><br></pre></td></tr></table></figure><p>刷新后访问<code>http://localhost:4000/tags/</code>即可<br><img src="https://img-blog.csdnimg.cn/20200202132540606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200202132613168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">若需要对文章添加多个分类，用以下格式书写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">categories:</span><br><span class="line">- [Spark,kafka]</span><br></pre></td></tr></table></figure><p>再增加一个404页面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new 404</span><br></pre></td></tr></table></figure><p>在yield-bytes/source/404/路径下，更改index.md内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">   title: 404 Not Found</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">   &lt;center&gt;</span><br><span class="line">   对不起，您所访问的页面不存在或者已删除。</span><br><span class="line">   您可以&lt;a href&#x3D;&quot;https:&#x2F;&#x2F;yield-bytes.gitee.io&#x2F;blog&quot;&gt;点击此处&lt;&#x2F;a&gt;返回首页。</span><br><span class="line">   &lt;&#x2F;center&gt;</span><br><span class="line"></span><br><span class="line">  &lt;blockquote class&#x3D;&quot;blockquote-center&quot;&gt;</span><br><span class="line">      yield-bytes</span><br><span class="line">  &lt;&#x2F;blockquote&gt;</span><br></pre></td></tr></table></figure><h4 id="博客左侧menu菜单显示图片"><a href="#博客左侧menu菜单显示图片" class="headerlink" title="博客左侧menu菜单显示图片"></a>博客左侧menu菜单显示图片</h4><p>在next主题的配置文件_config.yml下加入Avatar地址即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar:</span><br><span class="line">  # Replace the default image and set the url here.</span><br><span class="line">  url: your avatar image url</span><br><span class="line">  # If true, the avatar will be dispalyed in circle.</span><br><span class="line">  rounded: true</span><br><span class="line">  # If true, the avatar will be rotated with the cursor.</span><br><span class="line">  rotated: false</span><br></pre></td></tr></table></figure><h4 id="多篇文章在首页的展示逻辑"><a href="#多篇文章在首页的展示逻辑" class="headerlink" title="多篇文章在首页的展示逻辑"></a>多篇文章在首页的展示逻辑</h4><p>若博客已发布多篇文章，next主题默认在首页里将在可视化区域窗口内展示所有文章的所有完整内容，而不是只显示每篇文章的摘要部分，这将导致无法预览多篇文章内容。处理方式很简单，hexo支持对每篇文章提供只显示摘要部分，文章的更多内容则用more提示来指引读者。<br>处理方式：<br>在每篇Markdown文章比较靠前的位置，加入<code>&lt;!--more--&gt;</code>标识即可，例如下面的两篇文章：<br>第一篇文章，在首页中，只给它显示前言的内容即可<br><img src="https://img-blog.csdnimg.cn/2020020213171413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>第二篇spark streaming的文章，在首页，将该文的第1章节前面几句作为文章摘要显示<br><img src="https://img-blog.csdnimg.cn/2020020213193456.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最后在查看首尔显示效果：<br><img src="https://img-blog.csdnimg.cn/20200202132150362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到首页的多篇文章只展示<code>&lt;!--more--&gt;</code> 前面的内容。</p><h4 id="为博客增加全站搜索功能"><a href="#为博客增加全站搜索功能" class="headerlink" title="为博客增加全站搜索功能"></a>为博客增加全站搜索功能</h4><p>开启博客全文搜索功能需要加入相关插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure><p>首先在yield-bytes项目的<code>_config.yml</code>新增搜索配置：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2 search:</span><br><span class="line">3   path: search.xml</span><br><span class="line">4   field: post</span><br><span class="line">5   format: html</span><br><span class="line">6   limit: 1000</span><br></pre></td></tr></table></figure><p>再到next主题的<code>_config.yml</code>开启相关检索配置，这里启动 Local Search功能，这里的配置也提示了需要安装依赖hexo-generator-searchdb插件：</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">   8 # Local Search</span><br><span class="line">   7 # Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span><br><span class="line">   6 local<span class="emphasis">_search:</span></span><br><span class="line"><span class="emphasis">   5   enable: true</span></span><br><span class="line"><span class="emphasis">   4   # If auto, trigger search by changing input.</span></span><br><span class="line"><span class="emphasis">   3   # If manual, trigger search by pressing enter key or search button.</span></span><br><span class="line"><span class="emphasis">   2   trigger: auto</span></span><br><span class="line"><span class="emphasis">   1   # Show top n results per article, show all results by setting to -1</span></span><br><span class="line"><span class="emphasis">761    top_</span>n<span class="emphasis">_per_</span>article: 5</span><br><span class="line">   1   # Unescape html strings to the readable one.</span><br><span class="line">   2   unescape: false</span><br><span class="line">   3   # Preload the search data when the page loads.</span><br><span class="line">   4   preload: false</span><br><span class="line">   5</span><br></pre></td></tr></table></figure><p>上述表示输入关键字后自动触发搜索，并只显示5条命中记录。从效果来看，不得不佩服hexo博客框架（结合各类第三方插件）确实强大。（在独立开发的博客项目中，若要启用全文检索，则需引入elasticsearch技术栈）<br><img src="https://img-blog.csdnimg.cn/20200202151834914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>至此，博客的文章板块已经定制完毕，重新编译部署到gitee即可看到效果</p><h4 id="为GitHub-Pages配置自定义域名-注意以下不是Gitee-Pages的配置"><a href="#为GitHub-Pages配置自定义域名-注意以下不是Gitee-Pages的配置" class="headerlink" title="为GitHub Pages配置自定义域名(注意以下不是Gitee Pages的配置)"></a>为GitHub Pages配置自定义域名(注意以下不是Gitee Pages的配置)</h4><p>GitHub pages为git用户提供免费的自定义域名服务，所搭建的博客站点可无需使用类似<code>https://yourrepo.github.io</code>作为访问地址</p><p>在github博客项目仓库的Settings里面的GitHub Pages 可进行自定义域名设置：</p><p><a href="https://io.yield-bytes.cn/">https://io.yield-bytes.cn</a></p><h4 id="为文章添加字数统计和阅读时长"><a href="#为文章添加字数统计和阅读时长" class="headerlink" title="为文章添加字数统计和阅读时长"></a>为文章添加字数统计和阅读时长</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">post_wordcount:</span><br><span class="line">  item_text: true</span><br><span class="line">  wordcount: true         # 单篇 字数统计</span><br><span class="line">  min2read: true          # 单篇 阅读时长</span><br><span class="line">  totalcount: false       # 网站 字数统计</span><br><span class="line">  separated_meta: true</span><br><span class="line">  </span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>安装相关插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yymac@wonder yield-bytes % $ npm install eslint --save</span><br><span class="line">yymac@wonder yield-bytes % $ npm install hexo-wordcount --save</span><br><span class="line">yymac@wonder yield-bytes % $ npm install hexo-symbols-count-time --save</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在博客yield-bytes项目根目录的_config.yml文件的最后加入以下配置</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true                # 文章字数统计</span><br><span class="line">  time: true                   # 文章阅读时长</span><br><span class="line">  total_symbols: true          # 站点总字数统计</span><br><span class="line">  total_time: true             # 站点总阅读时长</span><br><span class="line">  exclude_codeblock: false     # 排除代码字数统计</span><br></pre></td></tr></table></figure><p>在next主题的配置文件已有相关文章计数配置</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true     # 是否另起一行（true的话不和发表时间等同一行）</span><br><span class="line">  item_text_post: true     # 首页文章统计数量前是否显示文字描述（本文字数、阅读时长）</span><br><span class="line">  item_text_total: false   # 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长）</span><br></pre></td></tr></table></figure><p>注意字体统计效果需重新hexo clean 和hexo generate才有效果，直接重启本地hexo server将无效</p><p><img src="https://img-blog.csdnimg.cn/20200204182425284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>最后重新部署一遍博客即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><h3 id="自动更新Gitee-Pages"><a href="#自动更新Gitee-Pages" class="headerlink" title="自动更新Gitee Pages"></a>自动更新Gitee Pages</h3><p>GitHub Pages只要 push上去，主页文章就会自动更新，但Gitee Pages的个人版无法实现自动更新，需要手动在仓库设置中点击更新按钮，若想自动化该过程，可以用selenium爬虫工具实现，npm有一个插件可以实现该过程——官网<a href="https://developer.aliyun.com/mirror/npm/package/gitee-publish/v/1.0.18">地址</a></p><p>当然使用Python也可以快速开发一个自动更新脚本</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;本blog用于归档如何在GitHub搭建个人博客的过程，内容参考来自本篇文章&lt;a href=&quot;https://mp.weixin.qq.com/s/sXH031TVK8-ZVG4KLVYyog&quot;&gt;《如何用 GitHub 从零开始搭建一个博客》&lt;/a&gt; 以及hexo中文官网的&lt;a href=&quot;https://hexo.io/zh-cn/docs/&quot;&gt;文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;更新：这篇文章写于今年年初，个人博客在年初已使用GitHub Pages搭建开源博客主页，今年下半年GitHub Pages所有以github.io为尾的博客地址在国内都无法正常访问，因此将其切换到国内Gitee才是正确选择，确实也需支持国内开源平台。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2020112219074494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="其他" scheme="https://yield-bytes.gitee.io/blog/categories/%E5%85%B6%E4%BB%96/"/>
    
    
  </entry>
  
  <entry>
    <title>基于PySpark整合Spark Streaming与Kafka</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/03/06/%E5%9F%BA%E4%BA%8EPySpark%E6%95%B4%E5%90%88Spark%20Streaming%E4%B8%8EKafka/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/03/06/%E5%9F%BA%E4%BA%8EPySpark%E6%95%B4%E5%90%88Spark%20Streaming%E4%B8%8EKafka/</id>
    <published>2020-03-05T16:00:00.000Z</published>
    <updated>2020-11-21T16:40:30.686Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;本文内容主要给出基于PySpark程序，整合Spark Streaming和Kafka，实现实时消费和处理topic消息，为PySpark开发大数据实时计算项目提供基本参考。</p><h4 id="1-程序环境准备："><a href="#1-程序环境准备：" class="headerlink" title="1 程序环境准备："></a>1 程序环境准备：</h4><p>&#8195;&#8195;这里不再使用Spark的集群环境，因涉及的计算资源测试环境受限，目前两台虚拟机：1个vcore+2G内存，其中一台虚拟机启动Spark Streaming服务进程，另外一台虚拟机启动kafka进程。</p><ul><li>虚拟机A：启动单实例kafka服务</li><li>虚拟机B：运行PySpark程序</li></ul><p>&#8195;&#8195;在VM A，程序环境要求安装jdk1.8以上以及与kafka匹配版本的scala版本<br>版本兼容说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka：kafka_2.11-2.4.0</span><br><span class="line">java：java version &quot;1.8.0_11&quot;</span><br><span class="line">scala： Scala 2.12.0</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;这里需要注意：如果使用kafka_2.12版本以上，需要使用jdk1.8.0_212以上；kafka_2.12与jdk1.8.0_11有不兼容地方，kafka启动报错提示<code>java.lang.VerifyError: Uninitialized object exists on backward branch 209</code>。</p><a id="more"></a><h5 id="1-1-基本配置"><a href="#1-1-基本配置" class="headerlink" title="1.1 基本配置"></a>1.1 基本配置</h5><p>（1）配置单机zk这里无需依赖ZooKeeper集群，只需使用kafka自带的zk服务即可<br>vim /opt/kafka_2.11-2.4.0/config/zookeeper.properties </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;zookeeper # zk的snapshot数据存储路径</span><br><span class="line">clientPort&#x3D;2181 # 按默认端口</span><br></pre></td></tr></table></figure><p>（2）配置kafka的，路径<code>/opt/kafka_2.11-2.4.0/config/ server.properties</code></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log.dirs=/opt/kafka-logs # 存放kafka数据目录</span><br><span class="line">zookeeper.connect=127.0.0.1:2181 # 按默认连接本机zk即可</span><br></pre></td></tr></table></figure><h5 id="1-2-启动zk和kafka"><a href="#1-2-启动zk和kafka" class="headerlink" title="1.2 启动zk和kafka"></a>1.2 启动zk和kafka</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# pwd</span><br><span class="line">/opt/kafka_2.12-2.4.0</span><br><span class="line"></span><br><span class="line">[root@nn kafka_2.11-2.4.0]#  nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>kafka server后台启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# nohup bin/kafka-server-start.sh config/server.properties 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h5 id="1-3-测试单实例Kafka"><a href="#1-3-测试单实例Kafka" class="headerlink" title="1.3 测试单实例Kafka"></a>1.3 测试单实例Kafka</h5><p>&#8195;&#8195;对于kafka单节点而言，这里只能使用1个分区且1个replication-factor，topic名称为sparkapp</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sparkapp</span><br><span class="line">Created topic sparkapp.</span><br></pre></td></tr></table></figure><p>打开一个新的shell,用于启动producer</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sparkapp</span><br></pre></td></tr></table></figure><p>再打开一个新的shell,用于启动consumer</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092  --topic sparkapp</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;在producer shell输入字符串，consumer端可以看到相应输出，说明单机的kafka可以正常运行，下面将使用Spark Streaming实时读取kafka的输入流</p><h4 id="2-整合streaming和kafka"><a href="#2-整合streaming和kafka" class="headerlink" title="2  整合streaming和kafka"></a>2  整合streaming和kafka</h4><h5 id="2-1-配置依赖包"><a href="#2-1-配置依赖包" class="headerlink" title="2.1 配置依赖包"></a>2.1 配置依赖包</h5><p>&#8195;&#8195;具体说明<a href="http://spark.apache.org/docs/2.4.4/streaming-kafka-integration.html">参考官方文档</a>spark streaming连接kafka需要依赖两个jar包（注意版本号）：<br>spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar： <a href="http://archiva-maven-storage-prod.oss-cn-beijing.aliyuncs.com/repository/central/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.3/spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar?Expires=1579170671&OSSAccessKeyId=LTAIfU51SusnnfCC&Signature=Yh6l7ZwWEfW0QPkwKlrDAdXrGxs=">下载链接</a><br>spark-streaming-kafka-0-8_2.11-2.4.4.jar：  <a href="https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.4.4/spark-streaming-kafka-0-8_2.11-2.4.4.jar">下载链接</a><br>&#8195;&#8195;将这两个jar包放在spark 的jars目录下，需要注意的是：这两个jar包缺一不可，如果是在Spark集群上做测试，那么每个Spark节点都需要放置这两个jars包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@nn jars]# pwd</span><br><span class="line">/opt/spark-2.4.4-bin-hadoop2.7/jars</span><br><span class="line"></span><br><span class="line">[root@nn jars]# ls spark-streaming-kafka-0-8</span><br><span class="line">spark-streaming-kafka-0-8_2.11-2.4.4.jar</span><br><span class="line">spark-streaming-kafka-0-8-assembly_2.11-2.4.3.jar</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;(关于spark-streaming-kafka的jar包依赖说明：就像python连接kafka，需要使用pip 安装kafka这个库）</p><h5 id="2-2-Spark-Streaming实时消费Kafka消息"><a href="#2-2-Spark-Streaming实时消费Kafka消息" class="headerlink" title="2.2 Spark Streaming实时消费Kafka消息"></a>2.2 Spark Streaming实时消费Kafka消息</h5><p>&#8195;&#8195;使用spark自带的直连kafka，实现实时计算wordcount，可以看到写普通的PySpark逻辑相对简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.kafka <span class="keyword">import</span> KafkaUtils</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(appName=<span class="string">&quot;streamingkafka&quot;</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少shell打印日志</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>) <span class="comment"># 5秒的计算窗口</span></span><br><span class="line">    brokers=<span class="string">&#x27;127.0.0.1:9092&#x27;</span></span><br><span class="line">    topic = <span class="string">&#x27;sparkapp&#x27;</span></span><br><span class="line">    <span class="comment"># 使用streaming使用直连模式消费kafka </span></span><br><span class="line">    kafka_streaming_rdd = KafkaUtils.createDirectStream(ssc, [topic], &#123;<span class="string">&quot;metadata.broker.list&quot;</span>: brokers&#125;)</span><br><span class="line">    lines_rdd = kafka_streaming_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    counts = lines_rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">        .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    <span class="comment"># 将workcount结果打印到当前shell    </span></span><br><span class="line">    counts.pprint()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure><p>spark streaming流默认接收的是utf-8编码的字符串</p><p>KafkaUtils接口<code>createDirectStream</code>说明：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Parameters:</span><br><span class="line">    ssc – StreamingContext object.</span><br><span class="line">    topics – list of topic_name to consume.</span><br><span class="line">    kafkaParams – Additional params for Kafka.</span><br><span class="line">    fromOffsets – Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.</span><br><span class="line">    keyDecoder – A function used to decode key (default is utf8_decoder).</span><br><span class="line">    valueDecoder – A function used to decode value (default is utf8_decoder).</span><br><span class="line">    messageHandler – A function used to convert KafkaMessageAndMetadata. You can assess meta using messageHandler (default is None).</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">A DStream object</span><br></pre></td></tr></table></figure><p>spark streaming 从 kafka 接收数据，有两种方式<br>（1）使用Direct API，这是更底层的kafka API<br>（2）使用receivers方式，这是更为高层次的API</p><p>&#8195;&#8195;在本博客后面讨论streaming的原理同时也给出Direct模式的相关详细的解析。当前测试使用为Direct模式，在虚拟机B的Spark目录下，启动application，启动命令需要带上指定的jars包。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --jars spark-streaming-kafka-0-8_2.11-2.4.4.jar direct_stream.py </span><br></pre></td></tr></table></figure><p>&#8195;&#8195;在虚拟机A的producer shell端，输入字符串句子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.11-2.4.0]# bin/kafka-console-producer.sh --broker-list localhost:9 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">welcome to pyspark kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">从这里开始  将开发一个 由sparkstreaming 完成的 实时计算的 大数据项目</span></span><br></pre></td></tr></table></figure><p>&#8195;&#8195;在spark-submit窗口，可以看到spark streaming消费并处理kafka生成的实时流字符串结果：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:28</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;welcome&#x27;, 1)</span><br><span class="line">(&#x27;to&#x27;, 1)</span><br><span class="line">(&#x27;pyspark&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:34</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;从这里开始&#x27;, 1)</span><br><span class="line">(&#x27;&#x27;, 1)</span><br><span class="line">(&#x27;将开发一个&#x27;, 1)</span><br><span class="line">(&#x27;由sparkstreaming&#x27;, 1)</span><br><span class="line">(&#x27;完成的&#x27;, 1)</span><br><span class="line">(&#x27;实时计算的&#x27;, 1)</span><br><span class="line">(&#x27;大数据项目&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 09:34:36</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;以上完成基于PySpark整合Spark Streaming与Kafka的测试。</p><h5 id="2-3-关于以上测试过程有关offset简单说明"><a href="#2-3-关于以上测试过程有关offset简单说明" class="headerlink" title="2.3 关于以上测试过程有关offset简单说明"></a>2.3 关于以上测试过程有关offset简单说明</h5><p>&#8195;&#8195;该测试并没有给出consumer自己管理消息的offset，在上面测试中，例如，producer连续生产5条消息，那么消息体可以看出以下简单构成：<br>| offset| msg |<br>|–|–|<br>|  0|123@qq.com|<br>|  1|124@qq.com  |<br>|  2|125@qq.com |<br>|  3|126@qq.com  |<br>|  5|127@qq.com  |<br>&#8195;&#8195;上面的测试中，streaming 以Direct模式连接kafka，每消费一条消息，streaming默认自动commit offset到kafka，以期实现当下一批streaming去kafka取消息时，是按顺延下一条来取，保证没有重复处理消息，也不会漏了消息，这是什么意思呢？<br>&#8195;&#8195;例如当前streaming 消费offset=1的消息后，自动将消费位置offset=1告诉kafka：你记住我已经把第1个位置消息处理了，如果我下次找你kafka消费，请你找出offset=2的消息给我，但如果你将offset=0的消息给我，说明你让我重复消费消息，如果将offset=4消息给我，说明你让我漏了处理offset=3的消息。<br>&#8195;&#8195;根据以上说明，例如producer已经生产了offset=9共10消息，即使将当前spark streaming进程再消费offset=1的消息后，被退出，之后重启，spark streaming从kafka消费的消息将是offset=2的消息，而不是offset=10的消息。虽然默认配置有一定合理性，但也有这种情况，导致无法实现“仅消费一次而且保证业务正常”，参考以下场景：<br>&#8195;&#8195;spark streaming当前进程消费了offset=1的消息后，在业务处理过程中程序出错导致没有将办理业务详情发送到用户<code>124@qq.com</code>，因为spark streaming默认自动提交offset的位置给到kafka，因此spark streaming在一批处理中将消费offset=2的消息。若你想倒回去重新处理offset=1的消息，以保证邮件正确送到给用户，那么只能自己用外部数据库存放成功完成业务的offset，也即是自行管理offset，而不是被动的自动提交到kafka保存消费的offset。<br>&#8195;&#8195;kafka的offset消费位置的管理详解将在之后的文章给出，只有将offset的消费位置交由客户端自行管理，才能灵活实现各种需求：重新消费、只消费一次等</p><h4 id="3-Spark-Streaming与Kafka整合的两种方式"><a href="#3-Spark-Streaming与Kafka整合的两种方式" class="headerlink" title="3 Spark Streaming与Kafka整合的两种方式"></a>3 Spark Streaming与Kafka整合的两种方式</h4><p>&#8195;&#8195;在上面的整合测试里，用的streaming直连kafka进行消费消息。目前Spark Streaming 与 Kafka 的结合主要有两种方式：Receiver Dstream和Direct Dstream，目前企业实际项目主要采用 Direct Dstream 的模式，为何我这边可以断言企业主要使用Direct Dstream模式呢？因为在企业中，他们主力用Java和Scala，考虑企业需求方面，肯定使用spark-streaming-kafka-0-10版本的整合包，而这一版本不再支持Receiver模式。除非某些企业用了Pyspark作为spark应用开发，否则基本没人用Receiver模式。Spark官网也给出整合Kafka的指引<a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html">链接</a><br><img src="https://img-blog.csdnimg.cn/20200205144614446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;因为基于PySpark开发实时流计算程序，这里只能选择spark-streaming-kafka-0-8开发包，从官方提示可知，spark-streaming-kafka-0-10是stable版本而且支持ssl安全传输，支持offset commit（支持手动提交，这个非常重要，自行控制消息位置从哪条开始处理，保证准确消费）和dynamic topic subscription，这就是为何要用Scala语言开发面向高级需求的Spark程序或者streaming程序，亲儿子！<br>&#8195;&#8195;对于两种连接连接方式，有必要给出讨论和对比，以便加深streaming消费kafka topic更深理论知识。</p><h5 id="3-1-基于Receiver消费消息方式"><a href="#3-1-基于Receiver消费消息方式" class="headerlink" title="3.1 基于Receiver消费消息方式"></a>3.1 基于Receiver消费消息方式</h5><p><strong>原理图（已启用WAL机制）</strong>：<br><img src="https://img-blog.csdnimg.cn/20200303193251128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   （原理图需要注意的地方：如果Receiver模式下，未开启WAL用于备份接收的消息，那么图中Save data to WAL是不存在的。）<br>&#8195;&#8195;早期版本的Spark Streaming与Kafka的整合方式为Receiver从Kafka消费消息，在提交Spark Streaming任务后，Spark会划出指定的Receiver来持续不断、异步读取kafka数据，这个Receiver其实是Executor（jvm进程）的一个常驻线程，跟task类似，为何它是常驻的？因为它需要不断监听Kafka的Producer生产的消息，从这点也可以看出，Receiver收到的消息是存放在Executor的内存中，换句话说，占用了Executor的内存。至于Receiver线程内部使用哪种数据结构存放接收的消息？对于先进先消费，后进后消费场景，显然使用queue最适合（通过队列实现多线程的生产-消费编程逻辑）。当Driver这边提交job后，Executors从Receiver拿到消息去交给task处理。在执行完之后，Receiver向Kafka的Zookeeper提交offset，告诉Kafka记主它当前已消费的位置。<br>&#8195;&#8195;早期的设计中，Spark Streaming为了零丢失地消费kafka消息，增加对接收到的消息进行预写日志处理（Write Ahead Log， WAL）这个WAL是放在hdfs的checkpoint 目录下，开启该功能后，Receiver除了将接收到消息存放到Executor内存中，还将其同步写入到hdfs上的WAL日志文件。因此，当一直运行的Spark Streaming任务突然挂了，后期启动时，Streaming也可以自动从hfds的checkpoint目录下的WAL日志找回丢失的消息。</p><h6 id="Receiver连接方式的缺点"><a href="#Receiver连接方式的缺点" class="headerlink" title="Receiver连接方式的缺点"></a>Receiver连接方式的缺点</h6><p>&#8195;&#8195;从上面receiver工作原理可以总结其缺点出将出现在内存方面、wal日志影响吞吐量等方面存在设计上的缺点：<br><strong>（1）占用cpu+内存</strong>：每个receiver需要单独占用一个vcore以及相应内存，如果Receiver并发数量多，占用Executor更多cpu和内存资源，这些资源本应用来跑tasks做计算用的，这就出现浪费资源的情况。</p><p><strong>（2）WAL拖累整体处理效率</strong>：为了不丢数据需要开启WAL，也即Receiver将接收到的数据写一份备份到文件系统上（hdfs的checkpoint目录），既然有落到磁盘自然会有IO，这降低了<code>kafka+streaming</code>这个组合实时处理消息的效率，换句话说：增加job的执行时间。此外，开启WAL，还有造成重复消费的可能。</p><p><strong>（3）接收数量大于处理速率</strong>： 若Receiver并发数量设置不合理，接受消息速率大于streaming处理消息的速率，就会出现数据积压在队列中，最终可能会导致程序异常退出。这里也是面试常见的问题：例如提高Receiver的并发数量，就可以提高streaming处理能力吗？首先，Receiver异步接收kafka消息，不参与计算，真正执行计算的是streaming，如果streaming并发性没有调高，整个计算能力也没有提高。一定要记着：kafka跟streaming是需要两边同时调优，才能打得计算能力的整体提升，不能只调优一边，这一个组合！！</p><p>（补充知识点：Receiver的并发数据量是怎么确定？<br>&#8195;&#8195;在KafkaUtils.createStream()中，可以指定topic的partition数量，该数量就是Receiver消费此topic的并发数（其实就是Executor 启动消费此topic的线程数量）但需要指出的是：Kafka中topic的partition与Spark中RDD的partition是两个不同的概念，两者没有关联关系。）</p><h5 id="3-2-基于Direct消费消息方式"><a href="#3-2-基于Direct消费消息方式" class="headerlink" title="3.2 基于Direct消费消息方式"></a>3.2 基于Direct消费消息方式</h5><p>原理图：<br><img src="https://img-blog.csdnimg.cn/20200305173531689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;当Receiver的工作原理及其缺点理解后，Direct模式将更容易理解。Driect模式下，Streaming定时主动查询Kafka，以获得指定topic的所有partition的最新offset，结合上一批次已保存的offset位置，Streaming就可以确定出每个批次拉取消息offset的范围，例如第1批次的消息（offset范围0-100）正在处理过程中，streaming指定特定的线程定时去Kafka查询第2批次最新的offset，发现最新值为300，那么如果streaming没有限制每批次的最大消费速率，在第2批次取消息时，会一次性取回offset=101到300的消息记录，这个就是所谓的offset ranges。当让如果streaming没有限制每批次的最大消费速率就是每批次100，那么即使最新的offset位置为300，第2批次消费的offset 访问只能是101~200共计100条消费记录。<br>&#8195;&#8195;当处理数据的job启动时，就会使用kafka的简单Consumer API来获取kafka中指定offset范围的数据。此外，Streaming已消费的offset不再交由Zookeeper来管理，而是手动采用外部存储数据库如mysql、redis等存放和管理已消费的offset。<br>以下为Scala代码演示从rdd拿到offset ranges属性的逻辑（rdd当然本身包含消息数据）<br>​```java<br>directKafkaStream.map {<br>           …<br> }.foreachRDD { batchRdd =&gt;<br>    // 获取当前rdd数据对应的offset<br>    val offsetRanges = batchRdd.asInstanceOf[HasOffsetRanges].offsetRanges<br>    // 运行计算任务<br>    doCompute(batchRdd)<br>    // 使用外部数据库自行保存和管理offsetRanges<br>    saveToRedis(offsetRanges)<br> }</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&amp;#8195;&amp;#8195;而Receiver方式下没有关于offset的处理逻辑，这是因为streaming在该模式下内部通过kafka consumer high level API 提交到zk保存。</span><br><span class="line">​&#96;&#96;&#96;java</span><br><span class="line">receiverkafkaStream.map &#123;</span><br><span class="line">           ...</span><br><span class="line"> &#125;.foreachRDD &#123; streamRdd &#x3D;&gt;</span><br><span class="line">    &#x2F;&#x2F; 运行计算任务</span><br><span class="line">    doCompute(rdd)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h6 id="Direct连接方式的优点"><a href="#Direct连接方式的优点" class="headerlink" title="Direct连接方式的优点"></a>Direct连接方式的优点</h6><p><strong>（1）提高计算资源利率</strong>：不像Receiver那样还占用Executor的一部分内存和计算资源，Direct方式下的Executor的代码实现踢掉Receiver这块设计，因此可以实现计算和内存资源全部用在计算任务，因为streaming定时主动去kafka拉取batch 消息，拉过来直接计算，而不是像Receiver不断接收消息不断地存放在内存中。</p><p><strong>（2）无需开启WAL</strong>：Receiver方式需要开启WAL机制以保证不丢失消息，这种方式加大了集群的计算延迟和效率，而Direct的方式，无需开启WAL机制，因为Kafka集群有partition做了高可用，只要streaming消费方自己存放和管理好已经消费过的offset，那么即使程序异常退出等，也可利用已存储的offset去Kafka消费丢失的消息。</p><p><strong>（3）可保证exactly once的消费语义</strong>：基于Receiver的方式，使用kafka的高阶API来在Zookeeper中保存消费过的offset。这是消费kafka数据的传统方式。这种方式配合WAL机制，可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和Zookeeper之间可能是不同步的。基于Direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据时消费一次且仅消费一次。</p><p><strong>（4）计算程序更稳定</strong>：Receiver模式是通过异步持续不断的读取数据，当集群出现网络、计算负载跟不上等因素，导致streaming计算任务侧出现延迟和堆积，而Receiver却还在持续接收kafka消息，此种情况容易导致Executor内存溢出或者其他异常抛出，从而引起计算程序退出，换句话说，Receiver模式的streaming实时计算可靠性和稳定性欠缺。对于Direct模式，Driver在触发batch计算任务时，才会去kafka拉消息回来并计算，而且给streaming加入最大消费速率控制后，整个实时计算集群鲁棒性更强。</p><p><strong>（5）Dstream 的rdd分区数与kafka分区一致</strong>：<br>&#8195;&#8195;Direct模式下，Spark Streaming创建的rdd分区数跟Kafka的partition数量一致，也就是说Kafka partitions和streaming rdd partitions之间有一对一的映射关系，这样的好处是明显和直观的：只要增加kafka topic partition数量，就可以直接增大spark streaming的计算的并发数。<br>&#8195;&#8195;当然，Direct模式不足的地方就是需要自行实现可靠的offset管理逻辑，但对于开发方向来说，这点很容易实现，我个人若对offset管理，将优先选用redis，而且是集群！<br>&#8195;&#8195;以上有关Spark Streaming 整合Kafka的方式和原理分析必须要理解，否则在后面的实时计算平台的代码开发上，有些逻辑你不一定能处理好。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;本文内容主要给出基于PySpark程序，整合Spark Streaming和Kafka，实现实时消费和处理topic消息，为PySpark开发大数据实时计算项目提供基本参考。&lt;/p&gt;
&lt;h4 id=&quot;1-程序环境准备：&quot;&gt;&lt;a href=&quot;#1-程序环境准备：&quot; class=&quot;headerlink&quot; title=&quot;1 程序环境准备：&quot;&gt;&lt;/a&gt;1 程序环境准备：&lt;/h4&gt;&lt;p&gt;&amp;#8195;&amp;#8195;这里不再使用Spark的集群环境，因涉及的计算资源测试环境受限，目前两台虚拟机：1个vcore+2G内存，其中一台虚拟机启动Spark Streaming服务进程，另外一台虚拟机启动kafka进程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;虚拟机A：启动单实例kafka服务&lt;/li&gt;
&lt;li&gt;虚拟机B：运行PySpark程序&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;#8195;&amp;#8195;在VM A，程序环境要求安装jdk1.8以上以及与kafka匹配版本的scala版本&lt;br&gt;版本兼容说明：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;kafka：kafka_2.11-2.4.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;java：java version &amp;quot;1.8.0_11&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scala： Scala 2.12.0&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&amp;#8195;&amp;#8195;这里需要注意：如果使用kafka_2.12版本以上，需要使用jdk1.8.0_212以上；kafka_2.12与jdk1.8.0_11有不兼容地方，kafka启动报错提示&lt;code&gt;java.lang.VerifyError: Uninitialized object exists on backward branch 209&lt;/code&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/categories/Spark/"/>
    
    
    <category term="Spark Streaming" scheme="https://yield-bytes.gitee.io/blog/tags/Spark-Streaming/"/>
    
    <category term="Kafka" scheme="https://yield-bytes.gitee.io/blog/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>深入理解异步IO的底层逻辑——IO多路复用（select、poll、epoll）</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/01/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%BC%82%E6%AD%A5IO%E7%9A%84%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91%E2%80%94%E2%80%94IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%EF%BC%88select%E3%80%81poll%E3%80%81epoll%EF%BC%89/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/01/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%BC%82%E6%AD%A5IO%E7%9A%84%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91%E2%80%94%E2%80%94IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%EF%BC%88select%E3%80%81poll%E3%80%81epoll%EF%BC%89/</id>
    <published>2020-01-21T12:16:10.000Z</published>
    <updated>2020-02-03T07:05:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面两篇文章<a href="https://blog.csdn.net/pysense/article/details/103721630">《gevent与协程》</a>和<a href="https://blog.csdn.net/pysense/article/details/103745410">《asyncio与协程》</a>，讨论了有关协程异步编程方面的内容，从代码层面和基本的demo可以大致理解协程的工作方式。如果要深入理解为何单线程基于事件的驱动可以在“低能耗”的条件下达到高性能的IO服务，则要研究Linux底层实现原理——IO多路复用，而理解IO多路复用的前提是对文件描述符有较为深入的理解，因此本文把文件描述符和IO多路复用放在同一篇文章里，形成全局的体系化认知，这就是本文讨论的内容。</p><a id="more"></a><h4 id="1、理解文件描述符"><a href="#1、理解文件描述符" class="headerlink" title="1、理解文件描述符"></a>1、理解文件描述符</h4><h5 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h5><p>&#8195;&#8195;在Linux中，一切皆文件，而理解文件描述符才能理解“一切皆文件”的真实含义，IO多路复用的select、poll和epoll机制正是通过操作文件描述符集合来处理IO事件。<br>含义，这里引用百度的介绍：</p><blockquote><p>&#8195;&#8195;文件描述符是一个索引号，是一个非负整数，它指向普通的文件或者I/O设备，它是连接用户空间和内核空间纽带。在linux系统上<a href="https://baike.baidu.com/item/%E5%86%85%E6%A0%B8/108410">内核</a>（kernel）利用文件描述符（file descriptor）来访问文件。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要使用文件描述符来指定待读写的文件。（在Windows系统上，文件描述符被称作文件句柄）</p></blockquote><p>当你看完本篇内容后，再回它这段解释，总结得真到位！在后面会给出为何文件描述符是一个非负整数，而不是其他更为复杂数据结构呢（例如hash map、list、链表等）？</p><h5 id="1-2-打开一个文件"><a href="#1-2-打开一个文件" class="headerlink" title="1.2 打开一个文件"></a>1.2 打开一个文件</h5><p>&#8195;&#8195;当某个进程打开一个已有文件或创建一个新文件时，内核向该进程返回一个文件描述符（一个非负整数）。<br>(在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于<a href="https://baike.baidu.com/item/UNIX">UNIX</a>、<a href="https://baike.baidu.com/item/Linux">Linux</a>这样的操作系统。)<br>这里以打开的iPython shell进程调用os.open为例，OS是Centos7.5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> os</span><br><span class="line">In [<span class="number">6</span>]: fd = os.<span class="built_in">open</span>( <span class="string">&quot;/opt/test.txt&quot;</span>, os.O_RDWR|os.O_CREAT) <span class="comment"># os.O_RDWR读写模式打开，os.O_CREAT若文件不存在则创建               </span></span><br><span class="line">In [<span class="number">7</span>]: fd                                                                             </span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">17</span> <span class="comment"># 这个17就是file descriptor</span></span><br></pre></td></tr></table></figure><p>在Python里面，os.open方法返回文件描述符是更为底层API，而open方法是返回python文件对象，是更贴近用户的API。</p><p>在linux系统上查看以上iPython进程打开的所有文件描述符示例：（这里就是一个文件描述表的大致形式，每一个文件描述符指向一个文件或者设备）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# ll /proc/11622/fd #11622为ipython的shell进程</span><br><span class="line">total 0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 0 -&gt; /dev/pts/0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 1 -&gt; /dev/pts/0</span><br><span class="line">lr-x------ 1 root root 64 **** 16:43 10 -&gt; pipe:[41268]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 11 -&gt; pipe:[41268]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 12 -&gt; anon_inode:[eventpoll]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 13 -&gt; socket:[41269]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 14 -&gt; socket:[41270]</span><br><span class="line">lr-x------ 1 root root 64 **** 16:43 15 -&gt; pipe:[41271]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 16 -&gt; pipe:[41271]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 17 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 18 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 19 -&gt; /opt/test.txt</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 2 -&gt; /dev/pts/0</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 20 -&gt; anon_inode:[eventpoll]</span><br><span class="line">l-wx------ 1 root root 64 **** 16:43 3 -&gt; /dev/null</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 4 -&gt; /root/.ipython/profile_default/history.sqlite</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 5 -&gt; /root/.ipython/profile_default/history.sqlite</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 6 -&gt; anon_inode:[eventpoll]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 7 -&gt; socket:[41266]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 8 -&gt; socket:[41267]</span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 9 -&gt; anon_inode:[eventpoll]</span><br></pre></td></tr></table></figure><p>因为在ipython里面，<code>fd = os.open( &quot;/opt/test.txt&quot;, os.O_RDWR)</code> 运行3次，也就文件<code>/opt/test.txt</code>打开3次，所以返回个文件描述符:17、18、19（从这里说明，同一进程可以同一时刻打开同一文件多次）</p><p>11622进程号指向当前iPython shell，查看它打开的文件描述符18，指向被打开文件：<code>/opt/test.txt</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# ll /proc/11622/fd/18 </span><br><span class="line">lrwx------ 1 root root 64 **** 16:43 /proc/11622/fd/18 -&gt; /opt/test.txt</span><br></pre></td></tr></table></figure><p>关闭文件描述符就关闭了所打开的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [14]: os.close(19)                                                                  </span><br><span class="line">In [15]: os.close(18)                                                                  </span><br><span class="line">In [16]: os.close(17)</span><br></pre></td></tr></table></figure><h5 id="1-3-对文件描述符进行读写"><a href="#1-3-对文件描述符进行读写" class="headerlink" title="1.3 对文件描述符进行读写"></a>1.3 对文件描述符进行读写</h5><p>读：通过给定文件描述符读文件内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># os.read()方法的docstring</span></span><br><span class="line"><span class="string">os.read()</span></span><br><span class="line"><span class="string">Signature: os.read(fd, length, /)</span></span><br><span class="line"><span class="string">Docstring: Read from a file descriptor.  Returns a bytes object.</span></span><br><span class="line"><span class="string">Type:      builtin_function_or_method</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd=os.<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,os.O_RDWR|os.O_CREAT)</span><br><span class="line">data=os.read(fd,<span class="number">64</span>) <span class="comment">#指定读文件前64byte内容 </span></span><br><span class="line">print(data) <span class="comment"># b&#x27;foo\nbar\n\n&#x27;</span></span><br></pre></td></tr></table></figure><p>写：通过给定文件描述符将数据写入到文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># os.read()方法的docstring</span></span><br><span class="line"><span class="string">Signature: os.write(fd, data, /)</span></span><br><span class="line"><span class="string">Docstring: Write a bytes object to a file descriptor.</span></span><br><span class="line"><span class="string">Type:      builtin_function_or_method</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd=os.<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,os.O_RDWR|os.O_CREAT)</span><br><span class="line">byte_nums=os.write(fd,<span class="string">b&#x27;save data by file descriptor directly \n&#x27;</span>) <span class="comment"># 注意要写入byte类型的数据</span></span><br><span class="line">print(byte_nums) <span class="comment"># 返回写入byte字符串长度（字符个数）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>了解基本调用底层的os读写文件描述符的方法，也可以封装出一个类似内建open方法的定制myopen类。</p><h5 id="1-4-通过管道打开文件描述符"><a href="#1-4-通过管道打开文件描述符" class="headerlink" title="1.4 通过管道打开文件描述符"></a>1.4 通过管道打开文件描述符</h5><p>也可以通过管道pipe方法（创建一个无名管道）同时打开一个读文件描述符以及一个写文件描述符。（有关管道的定义和理解本文不再累赘，可参考其他博文。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">fd_read,fd_write=os.pipe()</span><br><span class="line">print(<span class="string">&#x27;fd_read:&#x27;</span>,fd_read,<span class="string">&#x27;fd_write:&#x27;</span>,fd_write) <span class="comment">#系统返回两个整数3、4， fd_read: 3 fd_write: 4</span></span><br><span class="line">os.write(fd_write,<span class="string">b&#x27;foo&#x27;</span>) <span class="comment"># 向管道的写端写入数据</span></span><br><span class="line">os.read(fd_read,<span class="number">64</span>) <span class="comment"># 从管道的读端读取数据</span></span><br></pre></td></tr></table></figure><p>创建管道时总是返回相邻的两个整数，因为stderr为2，故之后创建的文件描述符只能从3开始，示意图如下：<br><img src="https://img-blog.csdnimg.cn/20200105162510770.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如果尝试向管道另外一端的fd_write描述符读取数据，就会报错，所以对于管道，读数据只能在读文件描述符上读操作，写入数据只能在写文件描述符操作。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.read(fd_write,64)</span><br><span class="line">OSError: [Errno 9] Bad file descriptor</span><br></pre></td></tr></table></figure><p>如果已经把fd_read读取完好后，此时管道为空，若再读取该管道，进程会被阻塞，因为写管道端没有数据写入，这是管道的性质之一——数据一旦被读走，便不在管道中存在，若此时还继续向读端反复读取，则进程会被阻塞。</p><p>注意写入管道的字符个数是有限制的，当超过管道容量时，写入操作被阻塞，可以通过以下方法精确策略出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pipe_capacity</span>(<span class="params">size</span>):</span></span><br><span class="line">    fd_read,fd_write=os.pipe()</span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    print(<span class="string">&quot;start to count&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,size+<span class="number">1</span>):</span><br><span class="line">        os.write(fd_write,<span class="string">b&#x27;a&#x27;</span>)</span><br><span class="line">        total=i</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">&quot;end to count,total bytes:&quot;</span>,total)</span><br><span class="line"></span><br><span class="line">get_pipe_capacity(<span class="number">64</span>*<span class="number">1024</span>)</span><br><span class="line">输出：</span><br><span class="line">start to count</span><br><span class="line">end to count,total <span class="built_in">bytes</span>: <span class="number">65536</span></span><br></pre></td></tr></table></figure><p>往管道写入<code>64*1024</code> 大小的byte时，管道写端未发生阻塞，当把写入的byte数改为：写入<code>64*1024+1 </code>时，写入操作被阻塞了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">get_pipe_capacity(<span class="number">64</span>*<span class="number">1024</span>+<span class="number">1</span>)</span><br><span class="line">输出:</span><br><span class="line">start to count <span class="comment"># 执行流被阻塞，无后续输出。</span></span><br></pre></td></tr></table></figure><p>通过该方法可以精确测量出pipe默认容量为64KB。<br>看到这部内容，是否有人联想到在使用subprocess执行某些cmd命令后，一直卡在读取输出上？<br>常见用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">p= subprocess.Popen(your_cmd, shell= <span class="literal">True</span>, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) <span class="comment"># 问题出现在：标准输出使用了管道，而管道有容量限制，当命令返回的数据大小超过管道64KB时，执行流卡在这里</span></span><br><span class="line">bytes_result, err = p.communicate(timeout=<span class="number">1</span>)</span><br><span class="line">str_result = <span class="built_in">str</span>(bytes_result, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> p.returncode != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> str_result:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>问题出现在：subprocess.Popen标准输出使用了管道，而管道有容量限制，当你的your_cmd返回的数据大小超过管道64KB时（stdout获取返回数据，用了管道存放），执行流卡在subprocess.Popen这里，其实进程阻塞了。<br>既然知道管道有容量限制，那么可以将stdout定向到本地文件系统，那么输出的数据就存放到容量更大的文件，建议使用临时文件作为重定向输出，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stdout_by_tempfile</span>():</span></span><br><span class="line">    <span class="comment"># SpooledTemporaryFile也是一个普通文件对象，当然支持with协议（它的源码实现了__enter__和__exit__方法）</span></span><br><span class="line">    <span class="keyword">with</span> tempfile.SpooledTemporaryFile(buffering=<span class="number">1</span>*<span class="number">1024</span>) <span class="keyword">as</span> tf:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建一个临时文件对象，注意这个bufferfing不是限制只能存储1024字节的数据，而是输出内容超过1024字节后，自动将输出的数据缓存到临时文件里&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            fd = tf.fileno() <span class="comment"># 返回文件描述符，这个文件描述符不再指向管道，而是指向某个临时文件</span></span><br><span class="line">            p = subprocess.Popen(your_cmd,stdin=fd,stdout=fd,stderr=fd,shell=<span class="literal">True</span>) <span class="comment"># 将stdout输出的内容定向到文件描述符指向的文件</span></span><br><span class="line">            p.communicate(timeout=<span class="number">5</span>) <span class="comment"># 指定输出超时时间</span></span><br><span class="line">            tf.seek(<span class="number">0</span>) <span class="comment"># 将文件对象指针放置起始位置，以便读取从头到尾的完整的已存数据</span></span><br><span class="line">            output_data = tf.readlines() <span class="comment"># 一次读取临时文件的所有数据（这是读取的是byte类型），也可用迭代器（如果数据上几百M）这里tf就是普通文件对象，因为也有readlines、readline、write、tell等常见文件操作方法</span></span><br><span class="line">            save_to_db(output_data) <span class="comment"># 将your_cmd输出的数据存到db或者其他地方</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>经过对文件描述符的讨论，现在你可以轻松“操作进程的stdin或者stdout”</p><h5 id="1-5-常见的文件描述符0、1、2"><a href="#1-5-常见的文件描述符0、1、2" class="headerlink" title="1.5 常见的文件描述符0、1、2"></a>1.5 常见的文件描述符0、1、2</h5><p>&#8195;&#8195;在Linux系统上，每个进程都有属于自己的stdin、stdout、stderr。标准输入（standard input）的文件描述符是 0，标准输出（standard output）是 1，标准错误（standard error）是 2。尽管这种习惯并非<a href="https://baike.baidu.com/item/Unix">Unix</a>内核的特性，但是因为一些 shell 和很多应用程序都使用这种习惯，因此，如果内核不遵循这种习惯的话，很多应用程序将不能使用。</p><p>&#8195;&#8195;在上面的iPython例子中，ll /proc/11622/fd 其实是列出属于11622进程的文件描述符表，可以看到，每个进程拥有的fd数值从0到linux限制的最大值，其中每个进程自己的0、1、2就是用于当前进程的标准输入、标准输出和标准错误。</p><blockquote><p>关于文件描述符表简单介绍：操作系统内核为每个进程在u_block结构中维护文件描述符表，所有属于该进程的文件描述符都在该表中建立索引:数值–&gt;某个文件。你可以把整个文件描述表看成是一个C语言的数组，数组的元素指向文件引用，数组的下表就是它的文件描述符</p></blockquote><p>下面，已iPython的一个shell进程为例，如何将stdin、stdout、stderr的0、1、2替换为其他文件描述符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以用sys模块，例如：sys.stdout.fileno() </span></span><br><span class="line">In [<span class="number">4</span>]: stdin_fd=os.sys.stdin.fileno() <span class="comment"># 当前iPython shell进程的标准输入</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: stdin_fd</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: stdout_fd=os.sys.stdout.fileno() <span class="comment">#当前iPython shell进程的标准输出</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: stdout_fd</span><br><span class="line">Out[<span class="number">7</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: stderr_fd=os.sys.stderr.fileno() <span class="comment">#当前iPython shell进程的标准错误</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: stderr_fd</span><br><span class="line">Out[<span class="number">9</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure><p>os.sys.stdin等是什么呢？其实这些对象跟open(file_name,mode) 打开文件返回的文件对象是一样的，例如下面：os.sys.stdin是以utf-8编码的只读模式的文件对象，os.sys.stdout以及os.sys.stderr是以utf-8编码的写模式的文件对象，既然是文件对象，那么读对象就支持read、readline等方法，写对象则支持write等方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: os.sys.stdin</span><br><span class="line">Out[<span class="number">21</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stdin&gt;&#x27;</span> mode=<span class="string">&#x27;r&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: os.sys.stdout</span><br><span class="line">Out[<span class="number">22</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stdout&gt;&#x27;</span> mode=<span class="string">&#x27;w&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: os.sys.stderr</span><br><span class="line">Out[<span class="number">23</span>]: &lt;_io.TextIOWrapper name=<span class="string">&#x27;&lt;stderr&gt;&#x27;</span> mode=<span class="string">&#x27;w&#x27;</span> encoding=<span class="string">&#x27;UTF-8&#x27;</span>&gt;</span><br></pre></td></tr></table></figure><p>将文件描述符2：stdout替换为其他打开某个文件的文件描述符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: f=<span class="built_in">open</span>(<span class="string">&#x27;/opt/test.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">In [<span class="number">3</span>]: f.fileno()</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">11</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: os.sys.stdout=f</span><br><span class="line">In [<span class="number">5</span>]: os.sys.stdout.fileno() <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br><span class="line">In [<span class="number">7</span>]: print(<span class="string">&#x27;stdout redirect to file&#x27;</span>) <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br><span class="line">In [<span class="number">8</span>]: os.sys.stdout <span class="comment"># 输出不再打印到当前shell，而且写入文件：/opt/test.txt</span></span><br></pre></td></tr></table></figure><p>查看/opt/test.txt文件内容</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> % cat test.txt</span><br><span class="line"></span><br><span class="line">11</span><br><span class="line"></span><br><span class="line">stdout redirect to file</span><br><span class="line"></span><br><span class="line">&lt;_io.TextIOWrapper name=&#x27;/opt/test.txt&#x27; mode=&#x27;w&#x27; encoding=&#x27;UTF-8&#x27;&gt;</span><br></pre></td></tr></table></figure><p>可以看到当前进程os.sys.stdout标准输出不再是2，而是11，指向某个已打开的文件。<br>当拿到一个已知的文件描述符后（一个非负整数），那么可以调用os.write(fd,bstr)方法向fd指向的文件写入数据，例如向文件描述符为11写入b’foo’字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: os.write(<span class="number">11</span>,<span class="string">b&#x27;foo\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>查看文件/opt/test.txt内容：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% cat test.txt</span><br><span class="line"></span><br><span class="line">11</span><br><span class="line"></span><br><span class="line">stdout redirect to file</span><br><span class="line"></span><br><span class="line">&lt;_io.TextIOWrapper name=&#x27;/opt/test.txt&#x27; mode=&#x27;w&#x27; encoding=&#x27;UTF-8&#x27;&gt;</span><br><span class="line">foo</span><br><span class="line">3</span><br></pre></td></tr></table></figure><p>==这一小节内容是想表述这么一个逻辑：如果要进程要对文件写入数据、或者读取数据（这不就是IO吗），底层必须通过文件描述符来实现，这就为讨论IO多路复用提供很好的知识背景，因为IO多路复用就是涉及到client向server写入数据，或者从server读取数据的需求。==</p><h5 id="1-6-进程打开文件描述符的个数"><a href="#1-6-进程打开文件描述符的个数" class="headerlink" title="1.6 进程打开文件描述符的个数"></a>1.6 进程打开文件描述符的个数</h5><p>&#8195;&#8195;文件描述符的有效范围是 0 到 OPEN_MAX。centos7.5默认每个进程最多可以打开 1024个文件（0 -1023）。对于 FreeBSD 、Mac OS X  和 Solaris 来说，每个进程最多可以打开文件的多少取决于<a href="https://baike.baidu.com/item/%E7%B3%BB%E7%BB%9F%E5%86%85%E5%AD%98">系统内存</a>的大小，int 的大小，以及系统管理员设定的限制。Linux 2.4.22 强制规定最多不能超过 1,048,576 。</p><p>调整文件描述符打开数量的限制：</p><p>管理用户可以在etc/security/limits.conf配置文件中设置他们的文件描述符极限，如下例所示。<br><code>softnofile 10240</code><br><code>hardnofile 20480</code><br>系统级文件描述符极限还可以通过将以下三行添加到/etc/rc.d/rc.local启动脚本中来设置：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\#Increasesystem-widefiledescriptorlimit.</span><br><span class="line">echo4096&gt;/proc/sys/fs/file-max</span><br><span class="line">echo16384&gt;/proc/sys/fs/inode-max</span><br></pre></td></tr></table></figure><p>在一些基于IO事件实现的高性能中间件例如redis、nginx、gevent等，在其官方的调优教程，一般会建议将系统打开文件描述符的数量设为大值，以便发挥并发性能。</p><h5 id="1-7-文件描述符底层原理"><a href="#1-7-文件描述符底层原理" class="headerlink" title="1.7 文件描述符底层原理"></a>1.7 文件描述符底层原理</h5><p>&#8195;&#8195;之所以将文件描述符的底层原理放在本节最后讨论，是考虑到，当前面的内容你已经理解后，那么再讨论背后原理，将更容易理解。<br>&#8195;&#8195;总结1.2~1.6的内容：</p><ul><li>进程只有拿到文件描述符才能向它指向的物理文件写入数据或者读取数据，然后再把这些数据用socket方式（通过网卡）远程传输给client。</li><li>文件描述符就是操作系统为了高效管理已打开文件所创建的一个索引。给os.wirte传入fd，进程非常迅速通过fd找到已打开的文件，进程高效率了，作为操作系统当然也更高效管理这些进程。</li></ul><p>&#8195;&#8195;那么不禁会提问：为什么进程只有拿到文件描述符才能向它指向的物理文件写入数据或者读取数据？本节内容回答此问题，相关图或者表述参考这些文章：<a href="https://blog.csdn.net/qq_28114615/article/details/94590598">《Linux中文件描述符的理解(文件描述符、文件表项、i-node)》</a>（推荐这篇文章，作者从源码的角度解析fd的理解）、<a href="https://blog.csdn.net/wan13141/article/details/89433379">《Linux文件描述符到底是什么？》</a></p><p>&#8195;&#8195;基本知识背景：理解数组、指针、结构体以及内存，c语言的结构体像Python的类，都是为了封装属性和方法，形成一个“具备多个功能”的object。<br><strong>原理</strong><br>&#8195;&#8195;一个 Linux 进程启动后，它在内核中每一个打开的文件都需要由3种数据结构支撑运行：</p><ul><li><p>每个进程对应一张打开文件描述符表，属于进程级的数据结构，进程通过调用系统IO方法（传入文件描述符）访问文件数据（用户态切到内核态）；</p></li><li><p>内核维持一张打开文件表，文件表由多个文件表项组成，属于系统级数据结构，该文件表创建者和管理由内核负责，每个进程可共享；</p></li><li><p>每个打开的文件对应一个i-node数据结构，系统通过i-node可以取到位于磁盘的数据（用于返回给用户态，内核态切回用户态），存在于内核中。<br>（机智的小伙伴应该联想到这个技术点：为何用户程序读取文件数据，会出现用户态到内核态切换，然后再由内核态转到用户态？上面3个表可以回答这个问题）</p></li></ul><p>三者的关系图如下：</p><p><img src="https://img-blog.csdnimg.cn/20200105184452579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><strong>文件描述符表</strong><br>&#8195;&#8195;在Linux中，对于每一个进程，都会分配一个PCB（进程控制块——Processing Control Block），在C代码实现上，这个数据结构名为<code>task_struct</code>，它里面有一个成员变量<code>*files</code>(属于files_struct类型)，<code>files_struct</code>的指针又指向一个<code>指针数组fd_array</code>，数组每一个元素都是一个指向<code>file类型的指针</code>，该进程打开的每个文件都属于file类型。从这里得出：<br>所谓文件描述符，就是fd_array[NR_OPEN_DEFAULT]这个指针数组的索引号，这也回答了为何文件描述符为非负整数。</p><p><code>task_struct类型--&gt;*files指针(files_struct类型)--&gt;fd_array(文件描述符表)</code><br>==task_struct类型的定义(省略部分代码)==：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> &#123;</span></span><br><span class="line">......</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">files_struct</span> *<span class="title">files</span>;</span> </span><br><span class="line">......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>==files_struct类型的定义(省略部分代码)==：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">files_struct</span> &#123;</span></span><br><span class="line">......</span><br><span class="line"><span class="keyword">int</span> next_fd; #进程新打开一个文件对应的文件描述符</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> __<span class="title">rcu</span> * <span class="title">fd_array</span>[<span class="title">NR_OPEN_DEFAULT</span>];</span> <span class="comment">//进程级打开文件描述符表</span></span><br><span class="line">......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>系统级文件表</strong><br>&#8195;&#8195;每一个打开的文件都对应于一个file结构体（c语言上用结构体，而其他高级语言例如python或者java则称为<code>file类型</code>或者<code>file对象</code>），在该结构体中，f_flags描述了文件标志，f_pos描述了文件的偏移位置，而在<code>f_path中有含有一个指向一个inode结点的指针</code>，因此f_path非常关键，它直接指向物理文件存储的inode节点。<br>文件表指向逻辑大致如下：<br><code>file类型 --&gt; f_path变量（path类型 --&gt; *dentry指针（dentry类型）--&gt; d_inode指针（inode类型）</code></p><p>==file类型的定义(省略部分代码)==：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> &#123;</span></span><br><span class="line">......</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">path</span><span class="title">f_path</span>;</span>     <span class="comment">//属于path类型，包括目录项以及i-node</span></span><br><span class="line"><span class="keyword">atomic_long_t</span>f_count;  <span class="comment">//文件打开次数</span></span><br><span class="line"><span class="keyword">fmode_t</span>f_mode;   <span class="comment">//文件打开时的mode，对应于open函数的mode参数</span></span><br><span class="line">......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>==path类型的定义(省略部分代码)==：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">path</span> &#123;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vfsmount</span> *<span class="title">mnt</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">dentry</span> *<span class="title">dentry</span>;</span><span class="comment">//目录项</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>==dentry类型的定义(省略部分代码)==：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">dentry</span> &#123;</span></span><br><span class="line">......</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">inode</span> *<span class="title">d_inode</span>;</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">super_block</span> *<span class="title">d_sb</span>;</span><span class="comment">/* The root of the dentry tree *</span></span><br><span class="line"><span class="comment"> ......</span></span><br><span class="line"><span class="comment">&#125;;</span></span><br></pre></td></tr></table></figure><p>从以上“file类型嵌套链”可知：进程打开一个文件后，系统给它返回一个文件描述符fd，进程通过fd调用系统io方法，系统（内核）通过f_path再到dentry指针找到物理文件的inode，从而找到相应的数据块。</p><p><strong>系统级的文件i-node表</strong><br>&#8195;&#8195;继续上面内容，内核找到i-node节点后，就能获取文件数据块在磁盘上的位置以及文件大小等文件的元数据信息，使得进程能够根据已打开文件对应的文件描述符一路定位到磁盘上相应文件的位置，从而进行文件读写。<br>==inode类型的定义(省略部分代码)==：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">inode</span> &#123;</span></span><br><span class="line">    .......</span><br><span class="line"><span class="keyword">umode_t</span>i_mode;     <span class="comment">//权限</span></span><br><span class="line"><span class="keyword">uid_t</span>i_uid;      <span class="comment">//用户id</span></span><br><span class="line"><span class="keyword">gid_t</span>i_gid;      <span class="comment">//组id</span></span><br><span class="line">    .......</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span>i_ino;   <span class="comment">//inode节点号</span></span><br><span class="line"><span class="keyword">loff_t</span>i_size;   <span class="comment">//文件大小</span></span><br><span class="line">.......</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">timespec</span><span class="title">i_atime</span>;</span>  <span class="comment">//最后一次访问(access)的时间</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">timespec</span><span class="title">i_mtime</span>;</span>  <span class="comment">//最后一次修改(modify)的时间</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">timespec</span><span class="title">i_ctime</span>;</span>  <span class="comment">//最后一次改变(change)的时间</span></span><br><span class="line">    .......</span><br><span class="line"><span class="keyword">blkcnt_t</span>i_blocks;    <span class="comment">//块数</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">address_space</span>*<span class="title">i_mapping</span>;</span>   <span class="comment">//块地址映射</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>上面有Linux文件常见的基本属性，例如访问时间、修改时间、权限、属主等，系统级的文件表里每一个文件表项都会指向i-node，这个i-node对应磁盘中的一个物理文件。</p><h5 id="本节小结"><a href="#本节小结" class="headerlink" title="本节小结"></a>本节小结</h5><p>&#8195;&#8195;到处，有关文件描述符的底层原理介绍完毕，本节内容也只是抛砖引玉，读者可自行去检索Linux文件系统原理或者VFS虚拟文件系统原理等底层文件系统知识(从这里联想到不得不佩服Apache开发hdfs文件系统的大咖团队，他们对Linux文件系统的底层实现应该且必须是绝对掌握的)。如果你能理解以上全部内容，那么在第2部分的IO多路复用中提到的大部分概念，将不再晦涩难懂。</p><h4 id="2、-IO多路复用原理"><a href="#2、-IO多路复用原理" class="headerlink" title="2、 IO多路复用原理"></a>2、 IO多路复用原理</h4><p>&#8195;&#8195;IO：input和output，一般指数据的写入、数据的读取。IO主要分为两类：硬盘 IO和网络IO，本内容主要针对网络 IO。复用的含义？复用当然理解为重复使用某个<code>事物</code>，而在本文，这个<code>事物</code>是一个线程，因此，IO多路复用，是指并发的socket连接复用一个IO线程(换句话说：只需要一个线程，即可为多个client同时提供socket连接请求)。在第1章节中，如果用户程序要将数据写入或者读取数据，那么它在底层必须通过文件描述符才能达到相应操作结果，因此IO多路复用与文件描述符密切相连，这就是为何在第一章节里给出了大量有关文件描述符知识的原因。</p><h5 id="2-1-IO触发用户空间与内核空间之间的切换"><a href="#2-1-IO触发用户空间与内核空间之间的切换" class="headerlink" title="2.1 IO触发用户空间与内核空间之间的切换"></a>2.1 IO触发用户空间与内核空间之间的切换</h5><p>在本博客前面有关大数据项目的文章里，其中<a href="https://blog.csdn.net/pysense/article/details/103301847">《深入理解kafka》</a>提到kafka通过通过sendfile（零拷贝机制）提高消费者端的吞吐量，其中就提到用户空间与内核空间之间的切换，结合第1章节内容简要介绍：<br><img src="https://img-blog.csdnimg.cn/2020010622220268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p><ul><li>用户程序通过系统调用获得网络和文件的数据</li><li>内核负责网络和文件数据的读写 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">file_path=<span class="string">&#x27;/opt/test.txt&#x27;</span> <span class="comment"># 上下文为用户空间</span></span><br><span class="line">fd=os.<span class="built_in">open</span>(file_path,os.O_RDWR|os.O_CREAT) <span class="comment"># 用户空间切换到内核空间</span></span><br><span class="line">data=os.read(fd,<span class="number">64</span>) <span class="comment">#指定读文件前64byte内容  # 上下文从用户空间切换到内核空间，数据准备好后，上下文再从内核空间再切换到用户空间</span></span><br><span class="line">print(data) <span class="comment"># 上下文为用户空间</span></span><br></pre></td></tr></table></figure>对于os.read的过程，用文件描述符的背景也可以理解：read底层用fd读取文件数据的流程：进程级文件描述符，到系统级文件表，再到系统级i-node表。从进程级到系统级，这里从代码层面展示用户空间到内核的空间的上下文切换<br>所以只要有网络IO或者磁盘IO，必然会发生用户空间到内核空间的上下文切换。</li></ul><p>图的原理参考<br><a href="https://www.cnblogs.com/yanguhung/p/10145755.html">https://www.cnblogs.com/yanguhung/p/10145755.html</a></p><h5 id="2-2-IO模型的介绍"><a href="#2-2-IO模型的介绍" class="headerlink" title="2.2 IO模型的介绍"></a>2.2 IO模型的介绍</h5><p><strong>IO模型基本分类</strong>：<br>（1）Blocking I/O（同步阻塞IO）：最常见也最传统IO模型，即代码语句按顺序执行若某一条语句执行需等待那么后面的代码会被阻塞，例如常见顺序步骤：读取文件、等待内核返回数据、拿到数据、处理输出<br>（2）同步非阻塞IO（Non-blocking IO）：默认创建的socket为阻塞型，将socket设置为NONBLOCK，业务流程则变为同步非阻塞IO<br>（3）IO多路复用（IO Multiplexing ）：即经典的Reactor设计模式，有时也称为异步阻塞IO，Java中的Selector和Linux中的epoll都是这种模型。<br>（4）异步IO（Asynchronous IO）：即经典的Proactor设计模式，也称为异步非阻塞IO<br>==这里也给出个人在知乎看到一篇关于IO模型更为形象的回答：<a href="https://www.zhihu.com/question/32163005">链接</a>==，通过购买火车票的场景来介绍5种IO模型（本章节未提到的信号驱动的IO模型）</p><p><a href="https://mp.weixin.qq.com/s/E3PYOSCuO4O6JB2FpHyZCg">https://mp.weixin.qq.com/s/E3PYOSCuO4O6JB2FpHyZCg</a></p><p><strong>同步和异步</strong><br>&#8195;&#8195;同步是指用户线程发起IO请求后需要等待或者轮询内核IO操作完成后才能继续执行；例如内核读文件需要耗时10秒，那么用户线程发起读取文件IO后，等待内核从磁盘拷贝到内存10秒，接着用户线程才能进行下一步对文件内容进行其他操作，按顺序执行。<br>&#8195;&#8195;而异步是指用户线程发起IO请求后仍继续执行，当内核IO操作完成后会通知用户线程，或者调用用户线程注册的回调函数。</p><p><strong>阻塞和非阻塞</strong><br>&#8195;&#8195;阻塞是指内核空间IO操作需要为把数据返回到用户空间；而非阻塞是指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成。</p><p>以下为四个模型的内容，图和部分文参考此篇<a href="https://www.cse.huji.ac.il/course/2004/com1/Exercises/Ex4/I.O.models.pdf">《英文原文文章》</a></p><h6 id="同步阻塞IO"><a href="#同步阻塞IO" class="headerlink" title="同步阻塞IO"></a>同步阻塞IO</h6><p>&#8195;&#8195;同步阻塞IO模型是最简单的IO模型，如图1所示：<img src="https://img-blog.csdnimg.cn/20200111121353731.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;用户线程通过系统调用recvfrom方法向内核发起IO读文件操作（application switch to kernel），后面的代码被阻塞，用于线程处于等待当中，当内核已经从磁盘拿到数据并加载到内核空间，然后将数据拷贝到用户空间（kernel switch to application），用户线程再进行最后的data process数据处理。</p><p>同步阻塞IO模型的伪代码描述为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read(socket, buffer) # 执行流被阻塞，直到buffer有数据可以读或者内核抛给用户程序一个error信号，程序才会往下执行。</span><br><span class="line">process(buffer) </span><br></pre></td></tr></table></figure><p>缺点分析：<br>&#8195;&#8195;用在多线程高并发场景（例如10万并发），服务端与客户端一对一连接，对于server端来说，将大量消耗内存和CPU资源（用户态到内核态的上下文切换），并发能力受限。</p><h6 id="同步非阻塞IO"><a href="#同步非阻塞IO" class="headerlink" title="同步非阻塞IO"></a>同步非阻塞IO</h6><p>&#8195;&#8195;同步非阻塞IO是在同步阻塞IO的基础上，将socket设置为NONBLOCK。这样做用户线程可以在发起IO请求后可以立即返回，原理图如下：<br><img src="https://img-blog.csdnimg.cn/20200111123715373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;在该图中，用户线程前面3次不断发起调用recvfrom，内核还未准备好数据，因此只能返回error of EWOULDBLOCK，直到最后一次调用recvfrom时，内核已经将数据拷贝到用户buffer端，此次可读取到数据，接下来就是process the data。</p><p>同步非阻塞IO模型的伪代码描述为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">while true:</span><br><span class="line">        try:</span><br><span class="line">        streaming_data&#x3D;read(buffer)</span><br><span class="line">     do_someting(streaming_data)</span><br><span class="line">    do_foo(streaming_data)    </span><br><span class="line">    do_bar(streaming_data)     </span><br><span class="line">    except error of EWOULDBLOCK:</span><br><span class="line">         print(&#39;kernel not ready for data yet,going to next loop&#39;)</span><br><span class="line">         pass</span><br><span class="line">    sleep(0.1)</span><br></pre></td></tr></table></figure><p>该模式有两个明显的缺点：</p><p>&#8195;&#8195;第一点：即client需要循环system call，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请求的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求。如果有10万个客户端连接，那么将消耗大量的serverCPU资源和占用带宽。</p><p>&#8195;&#8195;第二点：虽然设定了一个间隔时间去轮询，但也会发生一定响应延迟，因为每间隔一小段时间去轮询一次read操作，而任务可能在两次轮询之间的任意时间就已经完成，这会导致整体数据吞吐量的降低。</p><p>&#8195;&#8195;（以上的流程就像你在Starbucks店点了一杯cappuccino ，付款后，咖啡师正在制作中，而你却每隔0.1秒从座位走到点餐台问咖啡师OK了没，以至于你根本无法腾出时间享受<code>用一台MacBook Pro优雅的coding的下午茶美好时光</code>。当然如果仅有你1个人以这种方式去询问，咖啡师应该还可以接受（假设“客户是上帝这个真理“在Starbucks能够严格实施）。假设有10万个客户，都以这方式去轮询咖啡师，想象下画面…）</p><h6 id="IO多路复用模式"><a href="#IO多路复用模式" class="headerlink" title="IO多路复用模式"></a>IO多路复用模式</h6><p>&#8195;&#8195;前面两种模式缺点明显，那么 IO多路复用模式就是为了解决以上两种情况，IO多路复用是指内核一旦发现进程指定的一个或者多个IO事件准备读取，它就通知该进程，原理图如下：<br><img src="https://img-blog.csdnimg.cn/20200111144223563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;前面两种IO模型用户线程直接调用recvfrom来等待内核返回数据，而IO复用则通过调用select（还有poll或者epoll）系统方法，此时用户线程会阻塞在select语句处，等待内核copy数据到用户态，用户再收到内核返回可读的socket文件描述符，伪代码如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">while true:</span><br><span class="line">all_fds=select()# 执行流在此处阻塞，当之前注册的socket文件描述符集合有其中的fd发生IO事件，内核会放回所有fds（注意：select不会返回具体发生IO事件的fd，需要用户线程自行查找）</span><br><span class="line">    for each_fd in all_fds:</span><br><span class="line">        if can_read(fd):  # 遍历内核返回每个socket文件描述符对象来判断到底是哪个流产生的IO事件。</span><br><span class="line">        process_data(fd) # 找到了发生IO事件的文件描述符fd</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此IO模型优点：<br>&#8195;&#8195;用户线程终于可以实现一个线程内同时发起和处理多个socket的IO请求，用户线程注册多个socket，（对于内核就是文件描述符集合），然后不断地调用select读取被激活的socket 文件描述符。（在这里，select看起就像是用户态和内核态之间的一个代理）<br>缺点在下文会谈到。</p><h6 id="IO多路复用适用场景："><a href="#IO多路复用适用场景：" class="headerlink" title="IO多路复用适用场景："></a>IO多路复用适用场景：</h6><p>&#8195;&#8195;从Redis、Nginx等这些强大的用于高并发网络访问的中间件可知，IO多路复用目前使用最突出的场景就是：socket连接，也即web服务，一般指高性能网络服务。<br>&#8195;&#8195;与多进程和多线程技术的简单粗暴的业务实现不同，I/O多路复用技术的最大优势是系统开销小，系统不必创建多进程或者多线程，也不必维护这些进程/线程的复杂上下文以及内存管理，从而大大减小了系统的开销，极大提升响应时间。</p><h4 id="3、深入理解select、poll"><a href="#3、深入理解select、poll" class="headerlink" title="3、深入理解select、poll"></a>3、深入理解select、poll</h4><p>&#8195;&#8195;上面第2节内容提到了IO多路复用的基本工作原理，目前linux支持I/O多路复用的系统调用常见有 select，poll，epoll（linux2.4内核前主要是select和poll，epoll方法则是从Linux 2.6内核引入），它们都是实现这么一个逻辑：一个进程可以监听多个文件描述符（10k-100k不等，看服务器性能），一旦某个文件描述符就绪（一般是读就绪或者写就绪），内核返回这些可读写的文件描述符给到用户线程，从而让用户线程进行相应的读写操作，这一过程支持并发请求。<br>下面就linux实现IO多路复用三种方式进行详细讨论：</p><h5 id="理解select函数"><a href="#理解select函数" class="headerlink" title="理解select函数"></a>理解select函数</h5><p>&#8195;&#8195;select：在一段时间内，监听用户线程感兴趣的文件描述符上面的可读、可写和异常等事件，在这里通过简单介绍其C接口的用法即可理解select功能，API：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/select.h&gt;</span></span></span><br><span class="line"><span class="keyword">int</span> select（<span class="keyword">int</span> nfds, fd_set * readfds, fd_set * writefds, fd_set * exceptfds, <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> * <span class="title">timeout</span>);</span></span><br></pre></td></tr></table></figure><p>函数参数解释，<a href="https://my.oschina.net/ijaychen/blog/184647">参考文章</a><br><code>nfds</code>：<br>&#8195;&#8195;非负整数的变量，表示当前线程打开的所有件文件描述符集的总数，nfds=maxfdp+1，计算方法就是当前线程打开的最大文件描述符+1</p><p><code>*readfds</code>:<br>&#8195;&#8195;fd_set集合类型的指针变量，表示当前线程接收到内核返回的可读事件文件描述符集合（有数据到了这个状态称之为读事件），如果这个集合中有一个文件可读，内核给select返回一个大于0的值，表示有文件可读，如果没有可读的文件，则根据timeout参数再判断是否超时，若内核阻塞当前线程的时长超出timeout，select返回0，若发生错误返回负值。传入NULL值，表示不关心任何文件的读变化</p><p><code>*writefd</code>:<br>&#8195;&#8195;当前有多少个写事件（关心输出缓冲区是否已满）<br>最后一个结构体表示每个几秒钟醒来做其他事件，用来设置select等待时间</p><p><code>*exceptfds</code>：<br>&#8195;&#8195;监视文件描述符集合中的有抛出异常的fd</p><p><code>timeout</code>：<br>&#8195;&#8195;select()的超时结束时间，它可以使select处于三种状态：<br>（1）若将NULL以形参传入，select置于阻塞状态，当前线程一直等到内核监视文件描述符集合中某个文件描述符发生变化为止；<br>（2）若将时间值设为0秒0毫秒，表示非阻塞，不管文件描述符是否有变化，都立刻返回继续执行，文件无变化返回0，有变化返回一个正值；<br>（3）timeout的值大于0，等待时长，即select在timeout时间内阻塞，超时后返回-1，否则在超时后不管怎样一定返回。</p><p><code>select函数返回值</code>：<br>&#8195;&#8195;执行成功则返回绪的文件描述符的总数。如果在超过时间内没有任何文件描述符准备就绪，将返回0；失败则返回-1并设置errno；若在select等待事件内程序接收到信号，则select立即返回-1，并设置errno为EINTER。<br>（从这里可以得出：写C的同学尤其是Unix 网络开发方向，对什么select、poll、epoll早已轻车熟路）</p><p><strong>select的优点</strong><br>&#8195;&#8195;select目前几乎在所有的平台上支持，其良好跨平台支持。</p><p> <strong>select的缺点</strong></p><p>（1）打开的文件描述符有最大值限制<br>&#8195;&#8195;默认1024，当然可自行设为较大值，例如10万，取决于服务器性内存和cpu配置。 </p><p>（2）对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。<br>&#8195;&#8195;当一个线程发起socket请求数较大时例如100，用户线程每次select()都会触发server端的内核遍历所有文件描述符，如果有1万个client发起这种IO请求，server的内核要遍历1万*100=100万的文件描述符。可想而知这种时间复杂度为o(n)是非常低效率的。<br>（3）第2点说了，当并发量大时，服务端提供server socket连接的进程需要维护一个用来存放大量fd的数据结构（参考1.7章节的内容：<code>task_struct类型--&gt;*files指针(files_struct类型)--&gt;fd_array(文件描述符表)</code>），会导致用户态和内核态之间在传递该数据结构时复制占用内存开销大。<br><a href="https://www.itnotebooks.com/?p=1106">https://www.itnotebooks.com/?p=1106</a></p><h5 id="理解poll函数"><a href="#理解poll函数" class="headerlink" title="理解poll函数"></a>理解poll函数</h5><p>本节内容部分参考<a href="https://blog.csdn.net/skypeng57/article/details/82743681">《poll函数解析》</a>，oll函数的定义：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">poll</span><span class="params">(struct pollfd *fds, <span class="keyword">nfds_t</span> nfds, <span class="keyword">int</span> timeout)</span></span>;</span><br></pre></td></tr></table></figure><p>pollfd类型定义：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span>&#123;</span></span><br><span class="line">　　<span class="keyword">int</span> fd;              <span class="comment">//文件描述符：socket或者其他输入设备的对应fd</span></span><br><span class="line">　　<span class="keyword">short</span> events;    <span class="comment">//用户向内核注册感兴趣的事件（读事件、写事件、异常事件）</span></span><br><span class="line">　　<span class="keyword">short</span> revents;   <span class="comment">//内核返回给用户注册的就绪事件</span></span><br><span class="line">　　&#125;;</span><br></pre></td></tr></table></figure><p>events有以下三大类：<br><img src="https://img-blog.csdnimg.cn/2020011122281639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">例如fd=10，events=POLLRDNORM<br>revents：返回用户在调用poll注册的感兴趣且已就绪的事件</p><p>参数说明：<br>pollfd类型的<code>*fds</code>变量：传入socket的文件描述符，用户线程通过fds[i].events注册感兴趣事件(可读、可写、异常)，<br><code>nfds</code>:<br>跟select的nfds参数相同<br><code>timeout</code>:<br>INFTIM:永远等待<br>0:立即返回，不阻塞<br>大于0:等待给定时长</p><p><code>函数返回值</code>：<br>成功时，poll() 返回结构体中 revents事件不为 0 的文件描述符个数；<br>如果在超时前没有任何事件发生，poll()返回 0；</p><p><code>工作流程</code><br>（1）pollfd初始化，传入socket的文件描述符，设置感兴趣事件event，以及内核revent。设置时间限制（用户线程通过fds[i].events传入感兴趣事件，内核通过修改fds[i].revents向用户线程返回已经就绪的事件）<br>（2）用户线程调用poll，并阻塞于此处<br>（3）内核返回就绪事件，并处理该事件</p><p>==select与poll本质差别不大，只是poll没有最大文件描述符的限制，因为它是基于链表来存储的==</p><p><code>poll缺点</code>：<br>（1）大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是否有意义。<br>（2）poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd</p><p>这里引用了这篇文章<a href="https://www.cnblogs.com/zhuwbox/p/4222382.html">《linux 下 poll 编程》</a>代码来说明poll流程，程序逻辑并不难理解，能够让poll返回就绪的事件，是内核驱动通过中断信号来判断事件是否发生：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line">......</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OPEN_MAX 1024</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> listenfd, connfd, sockfd, i, maxi;</span><br><span class="line">    <span class="keyword">char</span> buf[MAXLINE];</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> <span class="title">client</span>[<span class="title">OPEN_MAX</span>];</span><span class="comment">//存放客户端发来的所有socket对应的文件描述符，限定最大可用文件描述符1024</span></span><br><span class="line">    ......</span><br><span class="line">    client[<span class="number">0</span>].fd = listenfd;<span class="comment">// 传入socket对应的文件描述符</span></span><br><span class="line">    client[<span class="number">0</span>].events = POLLRDNORM;<span class="comment">//关心监听套机字的读事件</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(;;)</span><br><span class="line">    &#123;</span><br><span class="line">        nready = poll(client, maxi + <span class="number">1</span>, <span class="number">-1</span>); <span class="meta">#server 进程调用poll，用户线程在此处阻塞，直到内核返回就绪的POLLRDNORM事件的文件描述符集合</span></span><br><span class="line">        <span class="keyword">if</span>(client[<span class="number">0</span>].revents &amp; POLLRDNORM) # 如果收到内核返回client注册的事件</span><br><span class="line">        &#123;</span><br><span class="line">            connfd = accept(listenfd, (SA *) &amp;cliaddr, &amp;clilen); # 获取每个client socket连接对应的文件描述符</span><br><span class="line">            <span class="keyword">if</span>(connfd &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; OPEN_MAX; ++i)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(client[i].fd &lt; <span class="number">0</span>)</span><br><span class="line">                    client[i].fd = connfd; # 将客户端的请求的fd加到polled这个列表</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(i == OPEN_MAX)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;too many clients&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            client[i].events = POLLRDNORM;# 为客户端的fd注册可读事件</span><br><span class="line">            <span class="keyword">if</span>(i &gt; maxi)</span><br><span class="line">            &#123;</span><br><span class="line">                maxi = i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(--nready &lt;=<span class="number">0</span> )</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; OPEN_MAX; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>((sockfd = client[i].fd) &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(client[i].revents &amp; POLLRDNORM | POLLERR) <span class="meta"># server通过轮询所有的文件描述符，如果revents有读事件或者异常事件</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>((n = read(sockfd, buf, MAXLINE)) &lt; <span class="number">0</span>)# 读取数据</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span>(errno == ECONNRESET)</span><br><span class="line">                    &#123;</span><br><span class="line">                        close(sockfd); # 若该就绪的文件描述符返回的异常事件，则重置</span><br><span class="line">                        client[i].fd = <span class="number">-1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;read error!\n&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(n == <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    close(sockfd);</span><br><span class="line">                    client[i].fd = <span class="number">-1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    write(sockfd, buf,  n);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(--nready &lt;= <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;从上面看，不管select和poll都需要在返回后，都需要通过遍历文件描述符来获取已经就绪的socket（<code>for(i = 1; i &lt; OPEN_MAX; ++i)</code>==&gt;<code>if(client[i].revents &amp; POLLRDNORM | POLLERR)</code>）。事实上，高并发连接中例如10k个连接，在同一时刻可能只有小部分的socket fd处于就绪状态，但server端进程却为此不断的遍历，当注册的描述符数量的增长，其效率也会线性下降。<br>该图为select、poll和Epoll性能对比（还有一个更高性能的Kqueue）<br><img src="https://img-blog.csdnimg.cn/20200111174200198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到，随着socket连接数量增大（对应文件描述符数量也增加），select、poll处理响应更慢，epoll响应速度几乎不受文件描述符数量的影响。</p><h4 id="4、深入理解epoll"><a href="#4、深入理解epoll" class="headerlink" title="4、深入理解epoll"></a>4、深入理解epoll</h4><p>&#8195;&#8195;考虑到epoll为本文核心内容，而且知识相对更有深度，因此将其单独作为1个章节讨论。部分内容参考的以下文章（吸收多篇文章内容后，你会赞叹epoll设计的精巧）：<br><a href="https://blog.csdn.net/daaikuaichuan/article/details/83862311">《epoll原理详解及epoll反应堆模型》</a><br><a href="https://www.itnotebooks.com/?p=1106">《Linux下的I/O复用与epoll详解》</a><br><a href="https://blog.csdn.net/mango_song/article/details/42643971">《我读过最好的Epoll模型讲解》</a><br><a href="https://mp.weixin.qq.com/s/FRg_lSHDiZofzTZApU6z9Q">《 彻底搞懂epoll高效运行的原理 》</a></p><h5 id="4-1-epoll的c语言接口详解："><a href="#4-1-epoll的c语言接口详解：" class="headerlink" title="4.1 epoll的c语言接口详解："></a>4.1 epoll的c语言接口详解：</h5><p>这里介绍epoll的IO模型完成创建过程：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_create</span><span class="params">(<span class="keyword">int</span> size)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><p>功能说明：创建一个epoll实例，参数<code>size</code>用来指定该epoll对象可以管理的socket文件描述符的个数。在Linux 2.6.8以后的版本中，参数 size 已被忽略，但是必须大于0。</p></li><li><p>函数返回值：一个代表新创建的epoll实例的文件描述符epoll_fd，这个描述符由server端持有，用于<code>统管所有client请求的socket连接对应的文件描述符</code></p></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_ctl</span><span class="params">(<span class="keyword">int</span> epfd, <span class="keyword">int</span> op, <span class="keyword">int</span> fd, struct epoll_event *event)</span></span>;  </span><br></pre></td></tr></table></figure><ul><li>epfd：epoll_create创建的epoll对象，以文件描述符形式epoll_fd返回，对于一个server进程，它对于只有一个epoll_fd。</li><li>op:监听socket时告诉内核要监听针对这个socket（其实是socket对应的文件描述符fd）什么类型的事件，主要有以下三种要监听的事件类型需要注册到epoll_fd上：</li><li><ul><li>EPOLL_CTL_ADD：注册新的fd到epfd中；</li></ul></li><li><ul><li>EPOLL_CTL_MOD：修改已经注册的fd的监听事件；</li></ul></li><li><ul><li>EPOLL_CTL_DEL：从epfd中删除一个fd；</li></ul></li></ul><ul><li><p>fd：epfd需要监听的fd，主要指server为client的socket请求创建对应文件描述符，来一个socket连接就对应新建一个文件描述符，</p></li><li><p>epoll_event：告诉内核需要对所注册文件描述符的什么类型事件进行监听，和poll函数支持的事件（读、写、异常）类型基本相同，不同是epoll还可以监听两个额外的事件：EPOLLET和EPOLLONESHOT（水平触发和边缘触发），这是epoll高性能的关键，文章后面会深入讨论。epoll_event结构如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> &#123;</span></span><br><span class="line">  <span class="keyword">__uint32_t</span> events;  <span class="comment">/* Epoll的事件类型 */</span></span><br><span class="line">  <span class="keyword">epoll_data_t</span> data;  <span class="comment">/* User data variable */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其中events就包括以下事件：</p></li><li><p>EPOLLIN ： 监听的文件描述符可读（包括client主动向server断开socket连接的事件）</p></li><li><p>EPOLLOUT：  监听的文件描述符可写；</p></li><li><p>EPOLLPRI： 监听的文件描述符有紧急的数据可读；</p></li><li><p>EPOLLERR： 监听的文件描述符有异常事件；</p></li><li><p>EPOLLRDHUP： 监听的文件描述对应的socket连接被挂断；这里挂断像不像打电话给对方，对方挂断你的电话的意思？没错，这里是指socket连接的client断开了TCP连接（TCP半开），此时epoll监听对应的socket文件描述符会触发一个EPOLLRDHUP事件。==（这里也需要给出一个知识：使用 2.6.17 之后版本内核，对端连接断开触发的 epoll 事件会包含 EPOLLIN | EPOLLRDHUP，即 0x2001。有了这个事件，对端断开连接的异常就可以在TCP层进行处理，无需到应用层处理，提高断开响应速度。）==<br>EPOLLET：将EPOLL设为边缘触发模式Edge Triggered，一般用法如下：<br><code>ev.events = EPOLLIN|EPOLLET; //监听读状态同时设置ET模式</code><br>如果要设为水平触发Level Triggered，只需：<br><code>ev.events = EPOLLIN //默认就是水平触发模式</code></p></li><li><p>EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket文件描述符，需要再次把这个socket加入到EPOLL队列里。</p></li><li><p>关于epoll_event类型的data用法如下：<br>定义一个变量ev，类型为struct epoll_event<br>ev.data.fd = 10;//将要监听的文件描述符绑定到ev.data.fd<br>ev.events = EPOLLIN|EPOLLET; //监听读状态同时设置ET模式</p></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">epoll_wait</span> <span class="params">( <span class="keyword">int</span> epfd, struct epoll_event* events, <span class="keyword">int</span> maxevents, <span class="keyword">int</span> timeout )</span></span>;</span><br></pre></td></tr></table></figure><ul><li><p>函数返回值：成功时返回有IO（或者异常）事件就绪的文件描述符的数量，如果在timeout时间内没有描述符准备好则返回0。出错时，epoll_wait()返回-1并且把errno设置为对应的值</p></li><li><p>events：内核检测监听描述符发生了事件，内核将这些描述符的所有就绪事件以events数组返回给用户，。</p></li><li><p>maxevents：指定最多监听多少个事件类型</p></li><li><p>timeout：指定epoll的超时时间，单位是毫秒。当timeout为-1是，epoll_wait调用将永远阻塞，直到某个时间发生。当timeout为0时，epoll_wait调用将立即返回。</p></li></ul><p>==为更好理解该epoll_wait，这里给个简单epoll工作流程说明：==</p><ul><li><p>1、假设socket server进程持有的epoll_fd为3，即<code>epoll_fd=epoll_create(size=1024);</code></p></li><li><p>2、假设现有2个client向server发起socket连接，server给它分配的文件描述符是4和5，并且注册的事件为：EPOLLIN可读事件，注册过程如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ev.data.fd = 4</span><br><span class="line">ev.events = EPOLLIN</span><br><span class="line">epoll_ctl(epfd=4, op=EPOLL_CTL_ADD, fd=4, &amp;ev);  # 这里不是C语言的写法，只是为了方便说明原理，将关键字参数也列出来，用类似python的参数语法</span><br><span class="line"></span><br><span class="line">ev.data.fd = 5</span><br><span class="line">ev.events = EPOLLIN</span><br><span class="line">epoll_ctl(epfd=4, op=EPOLL_CTL_ADD, fd=5, &amp;ev);  </span><br></pre></td></tr></table></figure></li><li><p>3、假设现在文件描述符5可读事件触发（例如内核已经完成将data.log拷贝到用户空间）</p></li><li><p>4、调用epoll_wait(epfd=3, events,maxevents=10, timeout=-1)，返回1，表示当前内核告诉server进程有1个文件描述符发生了事件，这里events存放的是就绪文件描述符及其事件：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ready_fds=epoll_wait(epfd=3, events,maxevents=10, timeout=-1)</span><br><span class="line">for i in range(ready_fds）：</span><br><span class="line">ev=events[i] //从内核返回的事件数组中取出epoll_event类型</span><br><span class="line">print(ev.data.fd) //这里返回的就绪文件描述符是5，对应client的第二个socket连接</span><br><span class="line">print(ev.evevts）//这里返回EPOLLIN可读事件</span><br><span class="line">os.read(ev.data.fd)//读取文件描述符5指向的数据</span><br></pre></td></tr></table></figure><p>==请重点关注第4点，这里可以初步回答epoll适合的场景以及为何epoll比select和poll更高效的原因：（这里说了是<code>初步</code>，第4.2和4.3节将给出更有深度的内容）<br>&#8195;&#8195;epoll适合的场景：适用于连接数量大且长连接，但活动连接较少的情况。如何解释？在上面的例子中，我们假设了2个client socket连接，现在，我们假设10万个socket连接，而当中”活跃“（就绪）的文件描述符只有100个，调用epoll_wait返回100，接着在for循环里面将非常快速处理完可读事件。==<br>&#8195;&#8195;也就是说，使用epoll的IO效率，不会随着socket连接数（文件描述符connect_fd数量）的增加而降低。因为内核只返回少量活跃就绪的fd才会被回调处理；<br>&#8195;&#8195;换句话说：epoll几乎跟外部连接总数无关，它只管“就绪”的连接，这就是Epoll的效率就会远远高于select和poll的原因。<br>&#8195;&#8195;现在大家应该可以理解第3节最后给出的Libevet Benchmark图：100个active条件下，连接的文件描述符从2500到15000，可以看到epoll高性能几乎保持稳定，因为它只需处理这固定的100个就绪的fds，而select和poll，要处理的是从2500个到15000个，因此处理时长也是线性增长，效率越来越低。</p></li></ul><p>==简单总结epoll3个方法即可完成高效且并发数高的文件描述符监听的基本流程==</p><ul><li>A、server端持有可统管所有socket文件描述符的唯一epoll_fd=epoll_create()</li><li>B 、epoll_ctl(epoll_fd，添加或者删除所有待监听socket文件描述符以及它们的事件类型)</li><li>C、返回有事件发生的就绪文件描述符 =epoll_wait(epoll_fd)</li></ul><h5 id="4-2-epoll用红黑树管理所有监听文件描述符"><a href="#4-2-epoll用红黑树管理所有监听文件描述符" class="headerlink" title="4.2 epoll用红黑树管理所有监听文件描述符"></a>4.2 epoll用红黑树管理所有监听文件描述符</h5><h6 id="用python设计一个伪epoll模型？"><a href="#用python设计一个伪epoll模型？" class="headerlink" title="用python设计一个伪epoll模型？"></a>用python设计一个伪epoll模型？</h6><p>&#8195;&#8195;在4.1介绍epoll的相关函数中，大家应该有这么一个<code>开发者直觉</code>：<br>每次有新的文件描述符，则将其注册到一个”由内核管理的数据结构“，而且还是向该数据结构添加、删除文件描述符感兴趣的事件。ok，既然提到这个数据结构可以<code>添加、删除</code>，这个直觉告诉我们，内核是否用一个类似python列表（或者链表）的方式来管理所有监听文件描述符呢（其中列表中每项用）？不妨假设epoll就是用python列表管理fds，来讨论”原版epoll“的设计：</p><ol><li><p>epoll_create(size)是内核创建一个python列表<br>epoll_list= epoll_create(size=100) //全局list变量，用户进程持有，可监听100个外部文件描述符。</p></li><li><p>假设现在有2个新的外部socket连接请求server，3个tcp的socket连接对应文件描述符为4、5、6，注册事件为EPOLLIN可读<br>注册过程如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoll_list= epoll_create(size=<span class="number">100</span>) </span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">connect_fd=socket_obj.accept().fd</span><br><span class="line">epoll_ctl(epoll_list, op=<span class="string">&#x27;EPOLL_CTL_ADD&#x27;</span>,&#123;<span class="string">&#x27;fd&#x27;</span>:connect_fd,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>op都是添加，说明是向epoll_list进行append操作</p></li><li><p>由于用户空间持有epoll_list，若内核要处理epoll，需将epoll_list拷贝到内核空间，对于内核，它看到的epoll_list如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoll_list=[ </span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">6</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li><li><p>假设现在文件描述符4、5可读，最终调用回调函数处理业务：<br><code>ready_fds_list=epoll_wait(epoll_list,events,maxevents=10, timeout=-1)</code># epoll_list又从内核空间拷贝到用户空间，当前仅有2个文件描述符，性能还ok，而且假设返回的是就绪文件描述符列表:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ready_fds_list=[ </span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;fd&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;listen_event&#x27;</span>:<span class="string">&#x27;EPOLLIN&#x27;</span>&#125;,</span><br><span class="line">]</span><br><span class="line"><span class="keyword">for</span>  each_fd <span class="keyword">in</span> ready_fds_list:</span><br><span class="line">callback(each_fd)</span><br></pre></td></tr></table></figure><p>经过上面4个步骤，我们貌似造出了一个可易于理解的、python版本的<code>epoll</code>。下面介绍<code>原版epoll设计</code></p><h6 id="真epoll设计"><a href="#真epoll设计" class="headerlink" title="真epoll设计"></a>真epoll设计</h6></li></ol><p><strong>第一点</strong>：上面简陋python版本epoll，用列表来管理所有监听的文件描述符<br><code>epoll_list= epoll_create(size=100) </code><br>==真实设计==：<code>epoll_fd= epoll_create(size=100) </code>，Linux用一个特殊文件描述符，并创建了eventpoll实例，eventpoll里面指向了一个<code>红黑树</code>，这棵树就是用于存放所有监听的文件描述符及其事件。</p><p><strong>第二点</strong>：上面简陋python版本epoll，用列表来管理所有监听的文件描述符，每次有新的socket连接，注册fd以及获取活动fd都会发生用户态到内核态、内核态到用户态切换：拷贝的对象——epoll_list。假设当前服务器有10万个socket连接请求，那么将发生10万次用户态到内核态切换，以及10万次内核态到用户态的切换，显然效率极低。<br>==真实设计==：第一点说了，用epoll_fd指向一颗存放在内核空间的红黑树，如何避免用户态和内核态频繁切换？Linux用mmap()函数解决。mmap将用户空间的一块地址和内核空间的一块地址同时映射到同一块物理内存地址，使得这块物理内存对用户进程和内核均可访问，砍掉用户态和内核态切换的环节（注意区别zero copy）。也就是说内核拿到epoll_fd后，可直接操作epoll_fd指向的红黑树上存放的所有被监听的文件描述符，妙了！<br>==这里说的epoll_fd是如何实现在底层指向一个红黑树呢?==<br>用户进程调用：epoll_fd=epoll_create(size)时，用户进程mmap映射的一块物理地址上就创建一个eventpoll结构体对象，该对象包含一个红黑树的根节点，从而实现epoll_fd由此至终都指向这颗红黑树（内核也可直接访问）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventpoll</span>&#123;</span>  </span><br><span class="line">    ....  </span><br><span class="line">    <span class="comment">/*红黑树的根节点，这颗树的节点存储着epoll_fd所有要监听的文件描述符及该文件描述符关联的其他属性*/</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rb_root</span>  <span class="title">rbr</span>;</span>  </span><br><span class="line">    <span class="comment">/*双链表中则存放着将要通过epoll_wait返回给用户的满足条件文件描述符、事件类型*/</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">rdlist</span>;</span>  </span><br><span class="line">    <span class="comment">/*还有其他成员变量，这里省略了，因为我们更关注存放就绪事件链表和存放被监听的所有文件描述符的红黑树 */</span></span><br><span class="line">    ....  </span><br><span class="line">&#125;;  </span><br></pre></td></tr></table></figure><p>&#8195;&#8195;上面的rdllist是一个双向链表，用于存储就绪事件的文描述符。用户进程代码执行epoll_wait时，执行流阻塞（在底层，是内核让进程休眠），直到监听的文件描述符有中断事件发生，内核将这些就绪文件描述符添加到rdlist里面，最后返回用户的是events数组，数组包含就绪的fd及其事件类型。</p><p><strong>第三点</strong>：用列表存放大量项，但需要进行增或者删除操作时，列表时间复杂度为<br>&#8195;&#8195;时间复杂度O(N)，想象下10万个连接的时间复杂度<br>==真实设计==：Linux为epoll设计了一个红黑树的数据结构，当调用epoll_ctl函数用于添加或者删除一个文件描述符时，对应内核而已，都是在红黑树的节点去处理，而红黑树本身插入和删除性能比列表高，时间复杂度O(logN)，N为树的高度，太巧妙了。<br>&#8195;&#8195;这个红黑树上的节点存放什么数据呢？每个节点称为epoll item结构，里面的组成如下：<br>fd:被epoll对象监听的文件描述符（client发起的socket连接对应的文件描述符）<br>event：要监听该fd的就绪事件，例如前面说的EPOLLIN可读事件<br>file:在1.6章节提到的系统文件表中，一个文件描述符对应系统文件表中一个文件项，这个文件项就是file类型，用于指向inode）<br>ready_node:双向链表中一个就绪事件的节点，可指回双向链表。</p><p><img src="https://img-blog.csdnimg.cn/20200118225411426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;用户进程调用epoll_create函数，对应在内核就已经创建了一个全局唯一eventpoll实例（object），对应背后完整的数据结构如下，示意图来源于<a href="https://zhuanlan.zhihu.com/p/50984245">该文章</a>：<br>&#8195;&#8195;图中的list就是双向链表，可以清楚看出epoll主要用了两个struct类型（当然不止这2个成员变量，还有锁、等待队列）和两种数据结构，完成高并发IO模型的构建，设计巧妙。因此，如果大家提高个人开发能力以及设计能力，数据结构必须要精通！（这里涉及到内核对红黑树、链表的操作是线程安全的，源码用了锁保操作原子性）<br><img src="https://img-blog.csdnimg.cn/20200119210515593.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#8195;&#8195;关于更深入的epoll红黑树以及事件就绪链表等底层的代码实现和图解析，这里有两篇文章，写得很好，作者根据英文原版内容自行理解后的整理：<br><a href="https://www.cnblogs.com/sduzh/p/6714281.html">《Linux内核笔记：epoll实现原理（一）》</a><br><a href="https://www.cnblogs.com/sduzh/p/6793879.html">《Linux内核笔记：epoll实现原理（二）》</a><br>（英文原版的链接现无法访问，地址<a href="https://idndx.com/2014/09/01/the-implementation-of-epoll-1/">《the-implementation-of-epoll》</a> )</p><h5 id="4-3-level-trigger和edge-trigger"><a href="#4-3-level-trigger和edge-trigger" class="headerlink" title="4.3 level trigger和edge trigger"></a>4.3 level trigger和edge trigger</h5><p>&#8195;&#8195;epoll工作模式支持水平触发(level trigger，简称LT，又称普通模式)和边缘触发(edge trigger，简称ET，又称“高速模式”)，而select和poll只支持LT模式。这里说的触发需要通过以下详细的说明来体会其内涵：<br>==level trigger模式的触发条件：==</p><ul><li>对于读就绪事件，只要用户程序没有读完fd的数据，也即缓冲内容不为空，epoll_wait还会继续返回该fd，让用户程序继续读该fd</li><li>对于写就绪事件，只要用户程序未向fd写满数据，也即缓冲区还不满，epoll_wait还会继续返回该fd，让用户程序继续对该fd写操作</li></ul><p>原理解释：<br>&#8195;&#8195;假设当前用户进程添加监听的文件描述符为4，以下简称为4fd，当该4fd有可读可写就绪事件时，epoll_wait()有返回，于是用户程序去进行读写操作，如果当前这一轮用户程序没有把4fd数据一次性全部读写完，那么下一轮调用 epoll_wait()时，它还会返回4fd这个事件对象，让你继续把4fd的缓存区上读或者写。如果用户程序一直不去读写，它会一直通知返回4fd。<br>参考代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">　　<span class="keyword">int</span> epfd,nfds;</span><br><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>,<span class="title">events</span>[10];</span> <span class="comment">//ev用于注册事件，数组用于返回要处理的事件</span></span><br><span class="line">　　epfd = epoll_create(<span class="number">1</span>); <span class="comment">//监听标准输入描述符，用于做测试</span></span><br><span class="line">　　ev.data.fd = STDIN_FILENO; <span class="comment">//标准输入描述符绑定到用户data的fd变量</span></span><br><span class="line">　　ev.events = EPOLLIN; <span class="comment">//监听读事件，且默认为LT水平触发事件</span></span><br><span class="line">　　epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;ev); <span class="comment">//注册epoll事件</span></span><br><span class="line">　　<span class="keyword">for</span>(;;)</span><br><span class="line">　　&#123;</span><br><span class="line">　　　　nfds = epoll_wait(epfd, events, <span class="number">5</span>, <span class="number">-1</span>); <span class="comment">//内核返回就绪事件</span></span><br><span class="line">　　　　<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++)</span><br><span class="line">　　　　&#123;</span><br><span class="line">　　　　　　<span class="keyword">if</span>(events[i].data.fd==STDIN_FILENO) <span class="comment">//如果返回的事件对象的fd为标准输入fd，则打印字符串，注意到，用户程序没有在缓存区读取数据</span></span><br><span class="line">　　　　　　　　<span class="built_in">printf</span>(<span class="string">&quot;epoll LT mode&quot;</span>);</span><br><span class="line"></span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;上面代码最关键的地方：在<code>if(events[i].data.fd==STDIN_FILENO) </code>之后，用户程序没有在标准输入的缓存区读取数据，根据水平触发原理，epoll_wait一直返回STDIN_FILENO这个就绪读事件，该代码最终效果：屏幕标准输出一直打印”epoll LT mode”字符串。<br>&#8195;&#8195;上面的过程可以解释为何LT模式是epoll工作效率较低的模式，具体说明如下：<br>&#8195;&#8195;假设除了感兴趣监听的文件描述符4fd，还有另外100个我不需要读写文件描述符（监听它们不代表一定要处理他们的就绪读写事件），最终会出现这样场景：epoll_wait每次都把这100个fd返回，而我只想对4fd进行读写，因此导致程序必须从101个fd中检索出4fd，若这些100fd以更高的优先级返回，那么用户则更晚才能拿到4fd，最终降低业务处理效率。</p><p>==edge trigger模式的触发条件：==</p><ul><li><p>对于读就绪事件，常见触发条件：<br> 缓冲区由空变为不空的时候（有数据可读时）<br> 当有新增数据到达时，即缓冲区中的待读数据变多时</p></li><li><p>对于写就绪事件，常见触发条件<br>  缓冲区由满变为空的时候（可写）<br> 当有旧数据被发送走，即缓冲区中的内容变少的时</p></li></ul><p>原理解释：<br>&#8195;&#8195;假设当前用户进程添加监听的文件描述符为4，以下简称为4fd，当该4fd有可读可写就绪事件时，epoll_wait()有返回，于是用户程序去读写操作，如果当前这一轮用户程序没有把4fd数据一次性全部读写完，那么下次调用epoll_wait()时，它不会再返回这个4fd就绪事件，直到在4fd上出现新的可读写事件才会通知你。这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。<br>参考代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/epoll.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">　　<span class="keyword">int</span> epfd,nfds;</span><br><span class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">ev</span>,<span class="title">events</span>[10];</span> <span class="comment">//ev用于注册事件，数组用于返回要处理的事件</span></span><br><span class="line">　　epfd = epoll_create(<span class="number">1</span>); <span class="comment">//监听标准输入描述符，用于做测试</span></span><br><span class="line">　　ev.data.fd = STDIN_FILENO; <span class="comment">//标准输入描述符绑定到用户data的fd变量</span></span><br><span class="line">　　ev.events = EPOLLIN|EPOLLET; <span class="comment">//监听读事件，而且开启ET触发事件</span></span><br><span class="line">　　epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &amp;ev); <span class="comment">//注册epoll事件</span></span><br><span class="line">　　<span class="keyword">for</span>(;;)</span><br><span class="line">　　&#123;</span><br><span class="line">　　　　nfds = epoll_wait(epfd, events, <span class="number">5</span>, <span class="number">-1</span>); <span class="comment">//内核返回就绪事件</span></span><br><span class="line">　　　　<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nfds; i++)</span><br><span class="line">　　　　&#123;</span><br><span class="line">　　　　　　<span class="keyword">if</span>(events[i].data.fd==STDIN_FILENO) <span class="comment">//如果返回的事件对象的fd为标准输入fd，则打印字符串，注意到，用户程序没有在缓存区读取数据</span></span><br><span class="line">　　　　　　　　<span class="built_in">printf</span>(<span class="string">&quot;epoll ET mode&quot;</span>);</span><br><span class="line"></span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;程序运行效果：<code>ev.events = EPOLLIN|EPOLLET</code>，将epoll监听的标准输入文件描述符设为ET模式，当向stdin敲入字符串abc时，缓存区由空转为不空，触发epoll_wait返回就绪事件，而之后用户程序并没有把缓冲区读取数据，根据ET原理，程序只打印一次”epoll ET mode”后就被阻塞。因为epoll_wait只通知一次，下次不再通知用户该4fd事件。除非外界再向stdin敲入字符串以至缓存区新增了数据，epoll_wait就会通知用户这个4fd有就绪事件。</p><h5 id="4-4-水平触发和边缘触发的小结"><a href="#4-4-水平触发和边缘触发的小结" class="headerlink" title="4.4 水平触发和边缘触发的小结"></a>4.4 水平触发和边缘触发的小结</h5><p>&#8195;&#8195;以某个被监听的文件描述符发生读事件作为示例：</p><ul><li>a.对于某个监听的文件描述符fd，假设其指向的读缓冲区初始时刻为空</li><li>b. 假设内核拷贝了4KB数据到用户进程的读缓冲区</li><li>c.不管水平触发还是边缘触发模式，epoll_wait此时都会返回可读就绪事件</li><li>d. 若采用水平触发方式，用户读取了2KB的数据，读缓冲区还剩余2KB数据，epoll_wait还会继续返回（通知）用户fd有可读就绪事件，直到读缓冲变为空为止。</li><li>f.若采用边缘触发方式，用户读取了2KB的数据，读缓冲区还剩余2KB数据，epoll_wait不再返回（通知）用户fd有可读就绪事件，除非读缓存区被用户进程或者内核写入新增数据例如1KB（此时读取缓冲变为3KB数据)那么epoll_wait才会通知用户有可读就绪事件。</li></ul><p>&#8195;&#8195;到此，已经完成对epoll深入解析的内容，当你掌握这些底层原理后，再回看当前出色中间件或框架如redis、nginx、node.js、tornado等，真香！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面两篇文章&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103721630&quot;&gt;《gevent与协程》&lt;/a&gt;和&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103745410&quot;&gt;《asyncio与协程》&lt;/a&gt;，讨论了有关协程异步编程方面的内容，从代码层面和基本的demo可以大致理解协程的工作方式。如果要深入理解为何单线程基于事件的驱动可以在“低能耗”的条件下达到高性能的IO服务，则要研究Linux底层实现原理——IO多路复用，而理解IO多路复用的前提是对文件描述符有较为深入的理解，因此本文把文件描述符和IO多路复用放在同一篇文章里，形成全局的体系化认知，这就是本文讨论的内容。&lt;/p&gt;</summary>
    
    
    
    <category term="Python进阶" scheme="https://yield-bytes.gitee.io/blog/categories/Python%E8%BF%9B%E9%98%B6/"/>
    
    
    <category term="IO多路复用" scheme="https://yield-bytes.gitee.io/blog/tags/IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"/>
    
    <category term="epoll" scheme="https://yield-bytes.gitee.io/blog/tags/epoll/"/>
    
  </entry>
  
  <entry>
    <title>Spark DataFrame、Spark SQL、Spark Streaming入门教程</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/01/14/Spark%20DataFrame%E3%80%81Spark%20SQL%E3%80%81Spark%20Streaming%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</id>
    <published>2020-01-14T13:50:59.000Z</published>
    <updated>2020-02-03T07:05:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):<br><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark DataFrame</a>、<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a>、<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a></p><h4 id="1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming"><a href="#1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming" class="headerlink" title="1、RDD、Spark DataFrame、Spark SQL、Spark Streaming"></a>1、RDD、Spark DataFrame、Spark SQL、Spark Streaming</h4><p>&#8195;&#8195;RDD：大家最熟悉的数据结构，主要使用transmissions和actions 相关函数式算子完成数据处理和数理统计，例如map、reduceByKey，rdd没有定义 Schema（一般指未定义字段名及其数据类型）， 所以一般用列表索引号来指定每一个字段。 例如， 在电影数据的推荐例子中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">move_rdd.map(lambda line:line.split(&#39;|&#39;)).map(lambda a_list:(alist[0],a_list[1],a_list[2]))</span><br></pre></td></tr></table></figure><a id="more"></a><p>每行有15个字段的数据，因此只能通过索引号获取前3个字段的数据，这要求开发者必须掌握 Map/Reduce 编程模式，不过， RDD 功能也最强， 能完成所有 Spark 数据处理与分析需求。</p><p>&#8195;&#8195;Spark DataFrame：创建DataFrame时，可以定义 Schema，通过定义每一个字段名与数据类型，以便之后直接用字段名进行数据索引，用法跟Pandas的DataFrame差别不大。Spark DataFrame是一种更高层的API，而且基于PySpark，用起来像Pandas的”手感“，很容易上手。</p><p>&#8195;&#8195;Spark SQL 底层是封装了DataFrame（DataFrame封装了底层的RDD） ，让使用者直接用sql的方式操作rdd，进一步降低Spark作为分布式计算框架的使用门槛。<br>&#8195;&#8195;Spark Streaming是本博客重点要解析的数据结构，实际项目将使用它实现流式计算，相关定义参考原文：</p><blockquote><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.<br>Spark Streaming具有扩展性、数据吞吐量高、容错的特点，底层基于core Spark API 实现，用于流数据处理。Spark Streaming注入的实时数据源可来自Kafka、Flume、Kinesis或者TCP流等，park Streaming可借助Map、reduce、join和window等高级函数接口搭建复杂的算法用于数据处理。Spark Streaming实时处理后数据可存储到文件系统、数据库或者实时数据展示仪表。</p></blockquote><h4 id="2、Spark-DataFrame"><a href="#2、Spark-DataFrame" class="headerlink" title="2、Spark DataFrame"></a>2、Spark DataFrame</h4><p>&#8195;&#8195;Spark DataFrame API比较多，既然用于数据处理和计算，当然会有预处理接口以及各统计函数、各种方法，详细参考<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame">官网:pyspark.sql.DataFrame</a>以及<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions">pyspark.sql.functions module模块</a></p><p>&#8195;&#8195;目前版本中，创建Spark DataFrame的Context接口可以直接用SparkSession接口，无需像RDD创建上下文时用Spark Context。<br>SparkSession：pyspark.sql.SparkSession:Main entry point for DataFrame and SQL functionality.</p><h5 id="2-1-创建基本的Spark-DataFrame"><a href="#2-1-创建基本的Spark-DataFrame" class="headerlink" title="2.1 创建基本的Spark DataFrame"></a>2.1 创建基本的Spark DataFrame</h5><p>&#8195;&#8195;创建 Spark DataFrame有多种方式，先回顾Pandas的DataFrame，Pandas可从各类文件、流以及集合中创建df对象，同样 Spark DataFrame也有类似的逻辑<br>首先需加载spark的上下文：SparkSession</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession <span class="comment">#  用于Spark DataFrame的上下文</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,StructType,StructField, LongType,DateType <span class="comment"># 用于定义df字段类型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row,Column</span><br><span class="line"></span><br><span class="line"><span class="comment">#本地spark单机模式</span></span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br><span class="line">print(spark)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出spark上下文信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SparkSession - in-memory</span><br><span class="line">SparkContext</span><br><span class="line">Spark UI</span><br><span class="line">Version v2.4.4</span><br><span class="line">Master</span><br><span class="line">    local[*]</span><br><span class="line">AppName</span><br><span class="line">    spark_dataframe</span><br></pre></td></tr></table></figure><p>from pyspark.sql.types：df目前支持定义的字段类型（参考源码），用于定义schema，类似关系型数据库建表时，定义表的字段类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__all__ = [</span><br><span class="line">    <span class="string">&quot;DataType&quot;</span>, <span class="string">&quot;NullType&quot;</span>, <span class="string">&quot;StringType&quot;</span>, <span class="string">&quot;BinaryType&quot;</span>, <span class="string">&quot;BooleanType&quot;</span>, <span class="string">&quot;DateType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;TimestampType&quot;</span>, <span class="string">&quot;DecimalType&quot;</span>, <span class="string">&quot;DoubleType&quot;</span>, <span class="string">&quot;FloatType&quot;</span>, <span class="string">&quot;ByteType&quot;</span>, <span class="string">&quot;IntegerType&quot;</span>,</span><br><span class="line">    <span class="string">&quot;LongType&quot;</span>, <span class="string">&quot;ShortType&quot;</span>, <span class="string">&quot;ArrayType&quot;</span>, <span class="string">&quot;MapType&quot;</span>, <span class="string">&quot;StructField&quot;</span>, <span class="string">&quot;StructType&quot;</span>]</span><br></pre></td></tr></table></figure><p>直接用从RDD创建dataframe对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">spark_rdd = spark.sparkContext.parallelize([</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义schema，就像数据库建表的定义：数据模型，定义列名，类型和是否为能为空</span></span><br><span class="line">schema = StructType([StructField(<span class="string">&quot;id&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;item&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;price&quot;</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">                     StructField(<span class="string">&quot;pub_date&quot;</span>, DateType(), <span class="literal">True</span>)])</span><br><span class="line"><span class="comment"># 创建Spark DataFrame</span></span><br><span class="line">spark_df= spark.createDataFrame(spark_rdd, schema)</span><br><span class="line"><span class="comment"># 创建一个零时表，用于映射到rdd上</span></span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;iPhone&quot;</span>)</span><br><span class="line"><span class="comment"># 使用Sql语句,语法完全跟sql一致</span></span><br><span class="line">data = spark.sql(<span class="string">&quot;select a.item,a.price from iPhone a&quot;</span>)</span><br><span class="line"><span class="comment"># 查看dataframe的数据</span></span><br><span class="line">print(data.collect())</span><br><span class="line"><span class="comment"># 以表格形式展示数据</span></span><br><span class="line">data.show()</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[Row(item&#x3D;&#39;iPhoneX&#39;, price&#x3D;6000), Row(item&#x3D;&#39;iPhone7&#39;, price&#x3D;4000), Row(item&#x3D;&#39;iPhone4&#39;, price&#x3D;1000)]</span><br><span class="line">+-------+-----+</span><br><span class="line">|   item|price|</span><br><span class="line">+-------+-----+</span><br><span class="line">|iPhoneX| 6000|</span><br><span class="line">|iPhone7| 4000|</span><br><span class="line">|iPhone4| 1000|</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure><p>通过该例子，可了解df基本用法，只要从spark上下文加载完数据并转为dataframe类型后，之后调用df的api跟pandas的api大同小异，而且可将dataframe转为Spark SQL，直接使用sql语句操作数据。</p><p>上面的例子用了显示定义schema字段类型，pyspark支持自动推理创建df，也即无需原数据定义为rdd，和自动类型，直接传入Python列表的数据，以及定义字段名称即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a_list = [</span><br><span class="line">    (<span class="number">11</span>, <span class="string">&quot;iPhoneX&quot;</span>,<span class="number">6000</span>, datetime.date(<span class="number">2017</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">12</span>, <span class="string">&quot;iPhone7&quot;</span>,<span class="number">4000</span>, datetime.date(<span class="number">2016</span>,<span class="number">9</span>,<span class="number">10</span>)),</span><br><span class="line">    (<span class="number">13</span>, <span class="string">&quot;iPhone4&quot;</span>,<span class="number">1000</span>, datetime.date(<span class="number">2006</span>,<span class="number">6</span>,<span class="number">8</span>))]</span><br><span class="line"><span class="comment"># 自动推理创建df,代码内部通过各类if 判断类型实现。</span></span><br><span class="line">spark_df= spark.createDataFrame(a_list, schema=[<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;item&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;pub_date&#x27;</span>])</span><br></pre></td></tr></table></figure><h5 id="2-2-从各类数据源创建Spark-DataFrame"><a href="#2-2-从各类数据源创建Spark-DataFrame" class="headerlink" title="2.2  从各类数据源创建Spark DataFrame"></a>2.2  从各类数据源创建Spark DataFrame</h5><p>相关接口方法参考官网<a href="http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html">文档</a></p><p>==从csv文件创建Spark DataFrame==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">&#x27;/opt/spark/data/train.csv&#x27;</span></span><br><span class="line">df = spark.read.csv(file,header=<span class="literal">True</span>,inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>==从pandas的DataFrame创建Spark DataFrame==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pandas_df = pd.DataFrame(np.random.random((<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line">spark_df = spark.createDataFrame(pandas_df, schema=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br></pre></td></tr></table></figure><p>==从json创建Spark DataFrame,json文件可以通过远程拉取，或者本地json，并设定json的字段schema==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">json_df = spark.read.json(<span class="string">&#x27;/opt/data/all-world-cup-players.json&#x27;</span>)</span><br></pre></td></tr></table></figure><p>==从各类数据库加载数据：==</p><p>pg数据库，使用option属性传入参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark_df = spark.read \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>) \</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure><p>pg数据库，用关键字参数传入连接参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">url=<span class="string">&#x27;jdbc:postgresql://localhost:5432/&#x27;</span>,</span><br><span class="line">dbtable=<span class="string">&#x27;db_name.table_name&#x27;</span>,</span><br><span class="line">user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure><p>mysql数据库，用关键字参数传入连接参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(</span><br><span class="line">url=<span class="string">&#x27;jdbc:mysql://localhost:3306/db_name&#x27;</span>,</span><br><span class="line">dbtable=<span class="string">&#x27;table_name&#x27;</span>,</span><br><span class="line">user=<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">password=<span class="string">&#x27;test&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure><p>从hive里面读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果在SparkSession设置为连接hive，可以直接读取hive数据</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .enableHiveSupport() \      </span><br><span class="line">        .master(<span class="string">&quot;localhost:7077&quot;</span>) \</span><br><span class="line">        .appName(<span class="string">&quot;spark_hive&quot;</span>) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">spark_df=spark.sql(<span class="string">&quot;select * from hive_app_table&quot;</span>)</span><br><span class="line">spark_df.show()</span><br></pre></td></tr></table></figure><p>连接数据库需要相关的jar包，例如连接mysql，则需要将mysql-connector放在spark目录的jar目录下。</p><h5 id="2-3-Spark-DataFrame持久化数据"><a href="#2-3-Spark-DataFrame持久化数据" class="headerlink" title="2.3 Spark DataFrame持久化数据"></a>2.3 Spark DataFrame持久化数据</h5><p>以csv存储</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.write.csv(path=local_file_path, header=<span class="literal">True</span>, sep=<span class="string">&quot;,&quot;</span>, mode=<span class="string">&#x27;overwrite&#x27;</span>)</span><br></pre></td></tr></table></figure><p>注意：mode=‘overwrite’ 模式时，表示创建新表，若表名已存在则会被删除，整个表被重写。而 mode=‘append’ 模式就是普通的最加数据。</p><p>写入mysql：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;jdbc:mysql://localhost:3306/db_name?characterEncoding=utf-8&amp;autoReconnect=true&amp;useSSL=false&amp;serverTimezone=GMT&#x27;</span></span><br><span class="line">table = <span class="string">&#x27;table_name&#x27;</span></span><br><span class="line">properties = &#123;<span class="string">&quot;user&quot;</span>:<span class="string">&quot;test&quot;</span>,<span class="string">&quot;password&quot;</span>:<span class="string">&quot;test&quot;</span>&#125;</span><br><span class="line">spark_df.write.jdbc(url,table,mode=<span class="string">&#x27;append&#x27;</span>,properties=properties)</span><br></pre></td></tr></table></figure><p>写入hive</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开动态分区</span></span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition.mode = nonstrict&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;set hive.exec.dynamic.partition=true&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定分区写入表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).partitionBy(<span class="string">&quot;name&quot;</span>).insertInto(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用分区，直接将数据保存到Hive新表</span></span><br><span class="line">spark_df.write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from your_db.table_name&quot;</span>).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>默认的方式将会在hive分区表中保存大量的小文件，在保存之前对 DataFrame重新分区，从而控制保存的文件数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.repartition(<span class="number">5</span>).write.mode(<span class="string">&quot;append&quot;</span>).saveAsTable(<span class="string">&quot;your_db.table_name&quot;</span>)</span><br></pre></td></tr></table></figure><p>写入redis：<br>这里需要自行实现redis的写入方法，其实也简单，定义入参为dataframe，函数内部连接redis后，从dataframe取出数据再将其插入redis即可。对于写入其他文件或者数据库，需自行实现相应的数据转存逻辑。</p><h5 id="2-4-Dataframe常见的API"><a href="#2-4-Dataframe常见的API" class="headerlink" title="2.4 Dataframe常见的API"></a>2.4 Dataframe常见的API</h5><p><a href="https://github.com/Apress/machine-learning-with-pyspark/blob/master/chapter_2_Data_Processing/sample_data.csv">样例数据参考</a></p><p>查看字段</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark_df.columns </span><br><span class="line"><span class="comment"># [&#x27;ratings&#x27;, &#x27;age&#x27;, &#x27;experience&#x27;, &#x27;family&#x27;, &#x27;mobile&#x27;]</span></span><br></pre></td></tr></table></figure><p>spark_df.count() 统计行数<br>查看df的shape</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print((df.count(),<span class="built_in">len</span>(df.columns))) <span class="comment"># (33, 5)</span></span><br></pre></td></tr></table></figure><p>查看dataframe的schema字段定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark_df.printSchema()</span><br><span class="line">输出：</span><br><span class="line">root</span><br><span class="line"> |-- ratings: integer (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- experience: double (nullable = true)</span><br><span class="line"> |-- family: integer (nullable = true)</span><br><span class="line"> |-- mobile: string (nullable = true)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>随机抽样探索数据集合：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark_df.sample(False,0.5,0).show(5)</span><br><span class="line">用法：</span><br><span class="line">Signature: spark_df.sample(withReplacement&#x3D;None, fraction&#x3D;None, seed&#x3D;None)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a sampled subset of this :class:&#96;DataFrame&#96;.</span><br><span class="line"></span><br><span class="line">:param withReplacement: Sample with replacement or not (default False).</span><br><span class="line">:param fraction: Fraction of rows to generate, range [0.0, 1.0].</span><br><span class="line">:param seed: Seed for sampling (default a random seed).</span><br></pre></td></tr></table></figure><p>查看行记录：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark_df.show(<span class="number">3</span>) </span><br><span class="line">spark_df.head(<span class="number">3</span>) </span><br><span class="line">spark_df.take(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>以python列表返回记录，list中每个元素是Row类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Row(ratings=<span class="number">3</span>, age=<span class="number">32</span>, experience=<span class="number">9.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Vivo&#x27;</span>, age_2=<span class="number">32</span>),</span><br><span class="line"> Row(ratings=<span class="number">3</span>, age=<span class="number">27</span>, experience=<span class="number">13.0</span>, family=<span class="number">3</span>, mobile=<span class="string">&#x27;Apple&#x27;</span>, age_2=<span class="number">27</span>),</span><br><span class="line"> Row(ratings=<span class="number">4</span>, age=<span class="number">22</span>, experience=<span class="number">2.5</span>, family=<span class="number">0</span>, mobile=<span class="string">&#x27;Samsung&#x27;</span>, age_2=<span class="number">22</span>)]</span><br></pre></td></tr></table></figure><p>查看null的行，可以传入isnull函数，也可以自定义lambda函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">spark_df = spark_df.<span class="built_in">filter</span>(isnull(<span class="string">&quot;name&quot;</span>))</span><br></pre></td></tr></table></figure><p>选择列数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>).show(<span class="number">2</span>)<span class="comment"># select方法</span></span><br></pre></td></tr></table></figure><p>扩展dataframe的列数:withColumn可以在原df上新增列数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure><p>注意该方式不会更新到原df，如需替换原df，则更新spark_df即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark_df.withColumn(<span class="string">&quot;age_2&quot;</span>,(spark_df[<span class="string">&quot;age&quot;</span>]))</span><br></pre></td></tr></table></figure><p>新增一列数据，并将新增的数据转为double类型，需要用到cast方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType,DoubleType,IntegerType</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;age_double&#x27;</span>,spark_df[<span class="string">&#x27;age&#x27;</span>].cast(DoubleType()))</span><br></pre></td></tr></table></figure><p>根据条件查询df，使用filter方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Apple&#x27;</span>)</span><br><span class="line"><span class="comment">#筛选记录后，再选出指定的字段记录</span></span><br><span class="line">spark_df.<span class="built_in">filter</span>(df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).select(<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;ratings&#x27;</span>,<span class="string">&#x27;mobile&#x27;</span>)</span><br></pre></td></tr></table></figure><p>选择mobile列值为‘Apple’的记录，多条件查询:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark_df.<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>).<span class="built_in">filter</span>(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>)</span><br><span class="line">或者：</span><br><span class="line">spark_df.<span class="built_in">filter</span>((spark_df[<span class="string">&#x27;mobile&#x27;</span>]==<span class="string">&#x27;Vivo&#x27;</span>)&amp;(spark_df[<span class="string">&#x27;experience&#x27;</span>] &gt;<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>distinct：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先用select取出要处理的字段，获取不同类型的mobile</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计不同类型mobile的数量</span></span><br><span class="line">spark_df.select(<span class="string">&#x27;mobile&#x27;</span>).distinct().count()</span><br></pre></td></tr></table></figure><p>groupBy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().show(<span class="number">5</span>,<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">+-------+-----+</span><br><span class="line">|mobile |count|</span><br><span class="line">+-------+-----+</span><br><span class="line">|MI     |<span class="number">8</span>    |</span><br><span class="line">|Oppo   |<span class="number">7</span>    |</span><br><span class="line">|Samsung|<span class="number">6</span>    |</span><br><span class="line">|Vivo   |<span class="number">5</span>    |</span><br><span class="line">|Apple  |<span class="number">7</span>    |</span><br><span class="line">+-------+-----+</span><br></pre></td></tr></table></figure><p>groupBy之后，按统计字段进行降序排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).count().orderBy(<span class="string">&#x27;count&#x27;</span>,ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>groupBy之后，按mobile分组，求出每个字段在该分组的均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.groupBy(<span class="string">&#x27;mobile&#x27;</span>).mean().show(<span class="number">2</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|mobile|avg(ratings)     |avg(age)          |avg(experience)   |avg(family)       |avg(age_2)        |</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">|MI    |3.5              |30.125            |10.1875           |1.375             |30.125            |</span><br><span class="line">|Oppo  |2.857142857142857|28.428571428571427|10.357142857142858|1.4285714285714286|28.428571428571427|</span><br><span class="line">+------+-----------------+------------------+------------------+------------------+------------------+</span><br><span class="line">only showing top 2 rows</span><br></pre></td></tr></table></figure><p>同理还有<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code>、<code>df.groupBy(&#39;mobile&#39;).max()</code>、<code>df.groupBy(&#39;mobile&#39;).min()</code>等，或者用agg方法，然后传入相应的聚合函数</p><p><code>spark_df.groupBy(&#39;mobile&#39;).agg(&#123;&#39;experience&#39;:&#39;sum&#39;&#125;)</code>等同于<code>spark_df.groupBy(&#39;mobile&#39;).sum()</code></p><p>用户定义函数UDF：</p><p>用户定义函数一般用于对列或者对行的数据进行定制化处理，就sql语句中，价格为数字的字段，根据不同判断条件，给字段加上美元符号或者指定字符等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costom_func</span>(<span class="params">brand</span>):</span></span><br><span class="line">    <span class="keyword">if</span> brand <span class="keyword">in</span> [<span class="string">&#x27;Samsung&#x27;</span>,<span class="string">&#x27;Apple&#x27;</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;High Price&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> brand ==<span class="string">&#x27;MI&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Mid Price&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Low Price&#x27;</span></span><br><span class="line">        </span><br><span class="line">brand_udf=udf(costom_func,StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&#x27;price_range&#x27;</span>,brand_udf(spark_df[<span class="string">&#x27;mobile&#x27;</span>])).show(<span class="number">5</span>,<span class="literal">False</span>) <span class="comment"># 使用spark_df[&#x27;mobile&#x27;]或者使用spark_df.mobile都可以</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|price_range|</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |Low Price  |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |High Price |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |High Price |</span><br><span class="line">|4      |37 |16.5      |4     |Apple  |37   |High Price |</span><br><span class="line">|5      |27 |9.0       |1     |MI     |27   |Mid Price  |</span><br><span class="line">+-------+---+----------+------+-------+-----+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用lambda定义udf</span></span><br><span class="line">age_udf = udf(<span class="keyword">lambda</span> age: <span class="string">&quot;young&quot;</span> <span class="keyword">if</span> age &lt;= <span class="number">30</span> <span class="keyword">else</span> <span class="string">&quot;senior&quot;</span>, StringType())</span><br><span class="line">spark_df.withColumn(<span class="string">&quot;age_group&quot;</span>, age_udf(df.age)).show(<span class="number">3</span>,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|ratings|age|experience|family|mobile |age_2|age_group|</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">|3      |32 |9.0       |3     |Vivo   |32   |senior   |</span><br><span class="line">|3      |27 |13.0      |3     |Apple  |27   |young    |</span><br><span class="line">|4      |22 |2.5       |0     |Samsung|22   |young    |</span><br><span class="line">+-------+---+----------+------+-------+-----+---------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure><p>删除重复记录：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重复的行，将被删除</span></span><br><span class="line">spark_df=spark_df.dropDuplicates()</span><br></pre></td></tr></table></figure><p>删除一列数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_new=spark_df.drop(<span class="string">&#x27;mobile&#x27;</span>) <span class="comment"># 删除多列 spark_df.drop(&#x27;mobile&#x27;,&#x27;age&#x27;)</span></span><br></pre></td></tr></table></figure><h4 id="3、Spark-SQL"><a href="#3、Spark-SQL" class="headerlink" title="3、Spark SQL"></a>3、Spark SQL</h4><p>&#8195;&#8195;在第二部分内容给出了创建spark sql的方法，本章节给出更为详细的内容：这里重点介绍spark sql创建其上下文，完成相应的context设置后，剩下的就是熟悉的写SQL了。<br><strong>第一种方式：将本地文件加载为dataframe</strong><br>之后再使用createOrReplaceTempView方法转为<code>SQL模式</code>，流程如下</p><p>用第2节内容的数据做演示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.csv(<span class="string">&#x27;sample_data.csv&#x27;</span>,inferSchema=<span class="literal">True</span>,header=<span class="literal">True</span>)</span><br><span class="line">spark_df.registerTempTable(<span class="string">&quot;phone_sales&quot;</span>)</span><br><span class="line">df1 = spark.sql(<span class="string">&quot;select age,family,mobile from phone_sales &quot;</span>)</span><br><span class="line">df1.show(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+---+------+-------+</span><br><span class="line">|age|family| mobile|</span><br><span class="line">+---+------+-------+</span><br><span class="line">| 32|     3|   Vivo|</span><br><span class="line">| 27|     3|  Apple|</span><br><span class="line">| 22|     0|Samsung|</span><br><span class="line">+---+------+-------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure><p>spark.sql用于传入sql语句，返回dataframe对象，故后续的数据处理将变得非常灵活，使用SQL确实能够降低数据处理门槛，再例如：</p><p><code>spark_df.groupBy(&#39;mobile&#39;).count().show(5,False)</code> 等同于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">your_sql=(<span class="string">&quot;select mobile,count(mobile) as count from phone_sales group by mobile &quot;</span></span><br><span class="line">df1 = spark.sql(your_sql)</span><br></pre></td></tr></table></figure><p>如果df1集合较大，适合用迭代器方式输出记录（适合逐条处理的逻辑）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> each_record <span class="keyword">in</span> df1.collect(): </span><br><span class="line">data_process(each_record)</span><br></pre></td></tr></table></figure><p><strong>第二种方式：直接连接诸如mysql或者hive的context，基于该context直接运行sql</strong></p><p>以mysql为例：<br>（1）配置mysql连接需要相关jar包和路径配置：<br> mysql-connector-java-5.1.48.jar 放入spark目录<code>/opt/spark-2.4.4-bin-hadoop2.7/jars/</code>目录下， mysql-connector包可在mysql自行下载。<br>在spark-env.sh 配置了EXTRA_SPARK_CLASSPATH=/opt/spark-2.4.4-bin-hadoop2.7/jars/（也可不配，spark按默认目录检索）</p><p>（2）连接mysql</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession </span><br><span class="line">spark=SparkSession.builder.master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&#x27;spark_dataframe&#x27;</span>).getOrCreate()</span><br></pre></td></tr></table></figure><p>连接数据库以及读取表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apps_name_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).</span><br><span class="line">options(</span><br><span class="line">url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/&#x27;</span>,</span><br><span class="line">dbtable=<span class="string">&#x27;erp_app.apps_name&#x27;</span>,</span><br><span class="line">user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">).load()</span><br></pre></td></tr></table></figure><p>read方法详细使用可参考：<a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.format">spark.read.format</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pirnt(apps_name_df)</span><br><span class="line"><span class="comment"># DataFrame[id: int, app_log_name: string, log_path: string, log_date: timestamp]</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apps_name_df.show(5)</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">| id| app_log_name|           log_path|           log_date|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">|  1|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  3|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  5|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  7|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">|  9|BI-Access-Log|&#x2F;opt&#x2F;data&#x2F;apps_log&#x2F;|*******************|</span><br><span class="line">+---+-------------+-------------------+-------------------+</span><br><span class="line">only showing top 5 rows</span><br></pre></td></tr></table></figure><p>上述连接mysql的erp_app后，直接读取apps_name全部字段的数据，如果想在连接时，指定sql，则需按以下方式进行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark_df=spark.read.<span class="built_in">format</span>(<span class="string">&#x27;jdbc&#x27;</span>).options(url=<span class="string">&#x27;jdbc:mysql://192.168.142.5:3306/erp_app&#x27;</span>,</span><br><span class="line">                                           dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span>,</span><br><span class="line">                                           user=<span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">                                           password=<span class="string">&#x27;bar_bar&#x27;</span></span><br><span class="line">                                          ).load()</span><br></pre></td></tr></table></figure><p>dbtable这个值可以为一条sql语句，而且格式必须为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;(select id,app_log_name,log_path from apps_name) as temp&#x27;</span></span><br></pre></td></tr></table></figure><p>如果写成以下格式，则提示解析出错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbtable=<span class="string">&#x27;select id,app_log_name,log_path from apps_name&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="4、Spark-Streaming"><a href="#4、Spark-Streaming" class="headerlink" title="4、Spark Streaming"></a>4、Spark Streaming</h4><p>&#8195;&#8195;以上在介绍dataframe和spark sql的相关用法，都用离线数据进行测试，本章节将给出spark的核心组件之一：spark streaming：实时流式计算（微批处理），该功能将应用于本bg其他项目。有关流式计算的相关概念，可以查看相关参考资料，这里不再累赘。此外，本bg也将给出一篇关于spark streaming的深度解析文章。</p><h5 id="4-1-实时计算TCP端口的数据"><a href="#4-1-实时计算TCP端口的数据" class="headerlink" title="4.1 实时计算TCP端口的数据"></a>4.1 实时计算TCP端口的数据</h5><p>&#8195;&#8195;以一个简单demo介绍pyspark实现streaming的流程：<br>在数据源输入端，使用netcat打开一个7070端口，手动持续向netcat shell输入句子；<br>在实时计算端：streaming连接7070端口，并实时计算word count，并将统计结果实时打印。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建本地的streaming context，并指定4个worker线程</span></span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>) <span class="comment"># 减少spark自生成的日志打印</span></span><br><span class="line"><span class="comment"># 每批处理间隔为1秒</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接netcat的tcp端口，用于读取netcat持续输入的行字符串</span></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;192.100.0.10&quot;</span>, <span class="number">7070</span>)</span><br></pre></td></tr></table></figure><p>socketTextStream创建的对象称为：Discretized Streams（离散流） ，简称 DStream，是spark的核心概念</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计word的逻辑，这段代码再熟悉不过了。</span></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> value_1, value_2: value_1 + value_2)</span><br><span class="line">wordCounts.pprint() <span class="comment"># 这里wordCounts是&#x27;TransformedDStream&#x27; object，不再是普通的离线rdd</span></span><br></pre></td></tr></table></figure><p>启动流计算，并一直等到外部中断程序（相当于线程里面的jion）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)    <span class="comment"># 默认无timeout，程序会一直阻塞在这里</span></span><br></pre></td></tr></table></figure><p>启动后，如果你使用jupyter开发，可以看到notebook的cell每隔1秒打印一次</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:50</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:51</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>在netstat shell输入字符串</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# nc -l 7070</span><br><span class="line">this is spark streaming</span><br><span class="line">streaming wordcount is awesome</span><br></pre></td></tr></table></figure><p>再查看notebook的cell的的实时打印出了wordcount统计结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;spark&#39;, 1)</span><br><span class="line">(&#39;this&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:54</span><br><span class="line">-------------------------------------------</span><br><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 16:14:58</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;streaming&#39;, 1)</span><br><span class="line">(&#39;is&#39;, 1)</span><br><span class="line">(&#39;wordcount&#39;, 1)</span><br><span class="line">(&#39;awesome&#39;, 1)</span><br></pre></td></tr></table></figure><p>以上实现了一个完整的实时流计算，虽然该streaming的demo较为简单，但却给了大家非常直观的流计算处理设计思路，只需改造相关逻辑，即可满足符合自己业务的需求，在这里给出一个可行的设计：</p><p>（1）实时数据源替换为Kafka等组件：启动一个进程，用于运行streaming。streaming的实时数据源来自kafka的topic<br>（2）定制MapReduce的计算逻辑，用于实时预处理流数据<br>（3）将（2）的实时结果保存到redis的list上<br>（4）启动另外一个进程，从结果队列里面取出并存到Hbase集群或者hdfs<br>或者无需使用队列，Spark Streaming实时预处理后直接写入Hbase。</p><h5 id="4-2-实时计算本地文件"><a href="#4-2-实时计算本地文件" class="headerlink" title="4.2 实时计算本地文件"></a>4.2 实时计算本地文件</h5><p>&#8195;&#8195;对于python接口，<code>streamingContext.textFileStream(dataDirectory)</code>方法可以实时监听并读取本地目录下的日志文件，但有几点需要指出，参考官方文档指引：</p><ul><li><p>能实时监听<code>dataDirectory</code>目录下创建的任意类型文件</p></li><li><p><code>dataDirectory</code>主要分为两种文件系统，第一种为本地文件系统，例如监听<code>/opt/appdata/</code>目录下的所有文件，格式为<code>file:///opt/appdata/</code>，第二种为hdfs文件系统：格式为<code>hdfs://namenode:8040/logs/</code></p></li><li><p>支持文件正则匹配，例如要监听本地文件目录下，所有以<code>.log</code>作为后缀类型的文件，<code>file:///opt/appdata/*.log</code></p></li><li><p>要求监听目录下的所有文件，里面的数据格式是一致的，例如1.log和2.log,里面都相同固定格式的日志记录。（每次新增的文件如果数据格式不一样，显然streaming处理逻辑无法完成）</p></li><li><p>目录下的文件数量越多，streaming扫描耗时将越长</p></li><li><p>若移动文件到这个监控目录，则无法触发streaming读取该新增文件，必须用流的形式写入到这个目录的文件才能被监听到</p></li><li><p>最后也是也是最重要的：streaming只处理在时间窗口内创建的新的数据文件，这里如何理解<code>新的数据文件</code>？</p><p>例如streaming流计算设为5秒，这个5秒就是时间窗口，若在5秒内目录产生了一个1.log，这个1.log会被读取，当5秒时间窗口已经过了，那么即使1.log有数据更新，streaming也不再reload该文件，为什么会这么设计呢？</p><p>流式处理的逻辑：一批一批的实时读取，已经读取过的数据文件，在下一轮时间窗口不再读取。假设在下一轮时间窗口，还读取已处理过的文件（该文件追加了新的数据行），那么该设计逻辑不再是流式处理了。例如<code>/opt/appdata/</code>目录下，有1.log,…100.log，并还会持续新增数据文件，101.log….等，如果streaming在每轮时间窗口还要对已处理过文件：<code>1.log,...100.log</code>再重新读取（读取新增加的数据行），那么spark streaming就无法完成微批的、实时的流式处理逻辑。在下面的实例会加以说明:</p></li></ul><p>spark streaming 监听文件夹实例：</p><p>时间窗口为5秒，实时监听<code>/opt/words/</code>目录下的文件，只要有新的文件创建（这里新的文件是指：每次创建的新文件，文件名必须唯一，streaming才会触发读取）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext </span><br><span class="line">sc = SparkContext(<span class="string">&quot;local[4]&quot;</span>, <span class="string">&quot;streaming wordcount test&quot;</span>)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>)<span class="comment"># 时间窗口为5秒</span></span><br><span class="line">lines = ssc.textFileStream(<span class="string">&quot;file:///opt/words/&quot;</span>)</span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">wordCounts.pprint()</span><br><span class="line">ssc.start()            </span><br><span class="line">ssc.awaitTermination(timeout=<span class="literal">None</span>)  </span><br></pre></td></tr></table></figure><p>模拟实时生成数据文件，每5秒生成一份数据文件，并在生成文件前，删除之前的文件（因为这个旧文件已经被spark streaming读取并流式计算过，下一轮时间窗口不再读取，所以可以删除旧文件）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">import</span> random,os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_text</span>(<span class="params">dir_path,file_name</span>):</span></span><br><span class="line">    words=[<span class="string">&#x27;spark&#x27;</span>,<span class="string">&#x27;streaming&#x27;</span>,<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;hadoop&#x27;</span>,<span class="string">&#x27;kafka&#x27;</span>,<span class="string">&#x27;yarn&#x27;</span>,<span class="string">&#x27;zookeeper&#x27;</span>]</span><br><span class="line">    line_num=random.randint(<span class="number">1000</span>,<span class="number">2000</span>)</span><br><span class="line">    text=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(line_num):</span><br><span class="line">        line=<span class="string">&#x27; &#x27;</span>.join([random.choice(words) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">        text=text+line+<span class="string">&#x27;\n&#x27;</span></span><br><span class="line">    data_file_path=os.path.join(dir_path,file_name)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(data_file_path,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(text)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">streaming_gen_data</span>(<span class="params">stream_dir_path</span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        tmp_file=os.listdir(stream_dir_path)</span><br><span class="line">        <span class="keyword">if</span>  tmp_file:<span class="comment"># 如果监听的目录下有旧文件，则直接删除</span></span><br><span class="line">            file_path=os.path.join(stream_dir_path,tmp_file[<span class="number">0</span>])</span><br><span class="line">            os.remove(file_path)</span><br><span class="line">        file_name=<span class="built_in">str</span>(uuid.uuid1())+<span class="string">&#x27;.log&#x27;</span> <span class="comment"># 保证每次新增的文件名唯一</span></span><br><span class="line">        save_text(stream_dir_path,file_name)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    streaming_gen_data(<span class="string">&#x27;/opt/words&#x27;</span>)</span><br></pre></td></tr></table></figure><p>测试结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:30</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 933)</span><br><span class="line">(&#39;foo&#39;, 970)</span><br><span class="line">(&#39;yarn&#39;, 951)</span><br><span class="line">(&#39;zookeeper&#39;, 938)</span><br><span class="line">(&#39;bar&#39;, 1020)</span><br><span class="line">(&#39;spark&#39;, 990)</span><br><span class="line">(&#39;streaming&#39;, 1021)</span><br><span class="line">(&#39;kafka&#39;, 949)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: *** 15:00:35</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#39;hadoop&#39;, 651)</span><br><span class="line">(&#39;zookeeper&#39;, 623)</span><br><span class="line">(&#39;bar&#39;, 593)</span><br><span class="line">(&#39;yarn&#39;, 584)</span><br><span class="line">(&#39;foo&#39;, 659)</span><br><span class="line">(&#39;spark&#39;, 623)</span><br><span class="line">(&#39;streaming&#39;, 592)</span><br><span class="line">(&#39;kafka&#39;, 571)</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>每隔5秒，spark streaming 完成微批计算：实时统计新创建文件的单词数</p><p>在这里重点说明 ：<code>file_name=str(uuid.uuid1())+&#39;.log&#39; </code>,这句保证了每次生成的文件使用了唯一文件名称，这样spark streaming才会瞬间监听到变化，及时读取到该文件。</p><p>有人会问：在生成文件前，已经删除了旧文件，那么每次创建文件使用固定文件名，对于spark streaming来说应该是唯一的、未加载过的文件才对吧？</p><p>解释：当使用os.remove一个文件后，如果等待间隔时长不长（例如几秒钟）又再创建同名文件，linux底层文件系统使用了缓存，用<code>ls -al</code>  查看该文件，会发现新创建文件的创建时间没有及时改变，导致spark streaming认为该文件还是原的旧文件，也就不再读取。</p><p>具体说明如下：<br>第一次创建文件的时间为<code>16 16:10</code>，接着下轮生成新文件，删除旧文件，等待5秒后，再创建同名新文件时，会发现创建时间没有改变还是<code>16 16:10</code>，而ssc.textFileStream读取时用的是创建时间去判断是否为新文件，所以才导致<code>明明已经创建新文件，但是</code>ssc.textFileStream却不读取的情况，这是pyspark textFileStream的bug，这个接口不应该只用创建时间判断。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 33847  16 16:10 streaming_data.log # 首次创建文件</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory #目录下的文件被删除</span><br><span class="line">[root@nn words]# ls -al streaming_data.log</span><br><span class="line">ls: cannot access streaming_data.log: No such file or directory</span><br><span class="line">[root@nn words]# ls -al streaming_data.log </span><br><span class="line">-rw-r--r-- 1 root root 31166  16 16:10 streaming_data.log # 5秒后，第二次创建同名的文件，创建时间未改变（如果等待时间去到十来秒，此时创建同名的文件的创建时间会发生变化）</span><br></pre></td></tr></table></figure><p>鉴于<code>textFileStream</code>接口使用场景受限，所以spark streaming实时数据源最适合的场景：接收kafka或者flume推来的流数据，这保证spark streaming能够立刻监听流数据的到来时间是已经发生变化，能触发streaming计算。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;本文介绍Spark DataFrame、Spark SQL、Spark Streaming入门使用教程，这些内容将为后面几篇进阶的streaming实时计算的项目提供基本计算指引，本文绝大部分内容来自Spark官网文档(基于PySpark):&lt;br&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;Spark DataFrame&lt;/a&gt;、&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html&quot;&gt;Spark SQL&lt;/a&gt;、&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html&quot;&gt;Spark Streaming&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming&quot;&gt;&lt;a href=&quot;#1、RDD、Spark-DataFrame、Spark-SQL、Spark-Streaming&quot; class=&quot;headerlink&quot; title=&quot;1、RDD、Spark DataFrame、Spark SQL、Spark Streaming&quot;&gt;&lt;/a&gt;1、RDD、Spark DataFrame、Spark SQL、Spark Streaming&lt;/h4&gt;&lt;p&gt;&amp;#8195;&amp;#8195;RDD：大家最熟悉的数据结构，主要使用transmissions和actions 相关函数式算子完成数据处理和数理统计，例如map、reduceByKey，rdd没有定义 Schema（一般指未定义字段名及其数据类型）， 所以一般用列表索引号来指定每一个字段。 例如， 在电影数据的推荐例子中:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;move_rdd.map(lambda line:line.split(&amp;#39;|&amp;#39;)).map(lambda a_list:(alist[0],a_list[1],a_list[2]))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/categories/Spark/"/>
    
    
    <category term="Spark DataFrame" scheme="https://yield-bytes.gitee.io/blog/tags/Spark-DataFrame/"/>
    
    <category term="Spark SQL" scheme="https://yield-bytes.gitee.io/blog/tags/Spark-SQL/"/>
    
    <category term="Spark Streaming" scheme="https://yield-bytes.gitee.io/blog/tags/Spark-Streaming/"/>
    
  </entry>
  
  <entry>
    <title>基于PySpark和ALS算法实现基本的电影推荐流程</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/01/11/%E5%9F%BA%E4%BA%8EPySpark%E5%92%8CALS%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/01/11/%E5%9F%BA%E4%BA%8EPySpark%E5%92%8CALS%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B/</id>
    <published>2020-01-11T02:23:19.000Z</published>
    <updated>2020-02-03T03:55:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;本文内容第一部分给出Pyspark常见算子的用法，第二部分则参考书籍《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。本文内容为大数据实时分析项目提供基本的入门知识。</p><h4 id="1、PySpark简介"><a href="#1、PySpark简介" class="headerlink" title="1、PySpark简介"></a>1、PySpark简介</h4><p>&#8195;&#8195;本节内容的图文一部分参考了这篇文章<a href="http://sharkdtu.com/posts/pyspark-internal.html">《PySpark 的背后原理 》</a>，个人欣赏此博客作者，博文质量高，看完受益匪浅！Spark的内容不再累赘，可参考本博客<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》</a>。PySpark的工作原理图示如下：<br><img src="https://img-blog.csdnimg.cn/20200108220627968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><a id="more"></a><p>&#8195;&#8195;在这里，Py4J 是一个用 Python 和 Java 编写的库，它可以让Python代码实现动态访问JVM的Java对象，同时JVM也能够回调 Python对象。因此PySpark就是在Spark外围包装一层Python API，借助Py4j实现Python和Java的交互（这里的交互就是通过socket实现，传字节码），进而实现通过Python编写Spark应用程序。<br><img src="https://img-blog.csdnimg.cn/20200108215814813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;在Driver端，PySparkContext通过Py4J启动一个JVM并产生一个JavaSparkContext；在Executor端，则不需要借助Py4j，因为Executor端运行的是由Driver传过来的Task业务逻辑（其实就是java的字节码）。</p><h4 id="2、Pyspark接口用法"><a href="#2、Pyspark接口用法" class="headerlink" title="2、Pyspark接口用法"></a>2、Pyspark接口用法</h4><h5 id="读取数据源"><a href="#读取数据源" class="headerlink" title="读取数据源"></a>读取数据源</h5><p>PySpark支持多种数据源读取，常见接口如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.pickleFile() <span class="comment"># &lt;class &#x27;pyspark.rdd.RDD&#x27;&gt;</span></span><br><span class="line">sc.textFile() <span class="comment"># &lt;class &#x27;pyspark.rdd.RDD&#x27;&gt;</span></span><br><span class="line">spark.read.json() <span class="comment"># &lt;class &#x27;pyspark.sql.dataframe.DataFrame&#x27;&gt;</span></span><br><span class="line">spark.read.text() <span class="comment"># &lt;class &#x27;pyspark.sql.dataframe.DataFrame&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>例如读取本地要注意，格式为<code>file://+文件绝对路径</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;file:///home/mparsian/dna_seq.txt&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取hdfs上文件数据</span></span><br><span class="line">sc.textFile(<span class="string">&quot;your_hadoop/data/moves.txt&quot;</span>)</span><br></pre></td></tr></table></figure><h5 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h5><p>Spark的算子分为两类：Transformation和Action。<br>Transformation仅仅是定义逻辑，并不会立即执行，有lazy特性，目的是将一个RDD转为新的RDD，可以基于RDDs形成lineage（DAG图）；<br>Action：触发Job运行，真正触发driver运行job；</p><p><strong>第一类算子：Transformation</strong></p><ul><li>map(func): 返回一个新的RDD，func会作用于每个map的key，例如在wordcount例子要<code>rdd.map(lambda a, (a, 1))</code>将数据转换成(a, 1)的形式以便之后做reduce<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">   )</span><br><span class="line">word_map_rdd = word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w, <span class="number">1</span>))</span><br><span class="line">mapping = word_map_rdd.collect()</span><br><span class="line">print(mapping)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">[(<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;pyspark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure></li></ul><ul><li> mappartitions(func, partition):  Return a new RDD by applying a function to each partition of this RDD.和map不同的地方在于map的func应用于每个元素，而这里的func会应用于每个分区，能够有效减少调用开销，减少func初始化次数。减少了初始化的内存开销。<br>例如将一个数据集合分成2个区，再对每个区进行累加，该方法适合对超大数据集合的分区累加处理，例如有1亿个item，分成100个分区，有10台服务器，那么每台服务器就可以负责自己10个分区的数据累加处理。<br>官方也提到mappartitions中如果一个分区太大，一次计算的话可能直接导致内存溢出。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">10</span>, <span class="number">22</span>, <span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">each_partition</span>):</span> </span><br><span class="line"><span class="keyword">yield</span> <span class="built_in">sum</span>(each_partition)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">[[<span class="number">10</span>, <span class="number">22</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">rdd.mapPartitions(f).glom().collect()</span><br><span class="line">[[<span class="number">32</span>], [<span class="number">7</span>]]</span><br></pre></td></tr></table></figure><ul><li><p>filter(func): 返回一个新的RDD，func会作用于每个map的key，用于筛选数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize ([<span class="string">&quot;fooo&quot;</span>, <span class="string">&quot;bbbar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;Aoo&quot;</span>])</span><br><span class="line">rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="string">&#x27;foo&#x27;</span> <span class="keyword">in</span> x).collect()</span><br><span class="line"><span class="comment"># [&#x27;fooo&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>flatMap(func): 返回一个新的RDD，func用在每个item，并把item切分为多个元素返回，例如wordcount例子的分类<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize ([<span class="string">&quot;this is pyspark&quot;</span>, <span class="string">&quot;this is spark&quot;</span>])</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27; &#x27;</span>)).collect()</span><br><span class="line"><span class="comment">#可以看到每个item为一句话，经过func后，分解为多个单词（多个元素）</span></span><br><span class="line"><span class="comment"># [&#x27;this&#x27;, &#x27;is&#x27;, &#x27;pyspark&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;spark&#x27;]</span></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize ((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> x:(<span class="number">2</span>*x,<span class="number">3</span>*x)).collect()</span><br><span class="line"><span class="comment"># 对原来每个item分别乘2乘3，func返回两个item</span></span><br><span class="line"><span class="comment"># [2, 3, 4, 6, 6, 9]</span></span><br></pre></td></tr></table></figure><ul><li>flatMapValues(func)：flatMapValues类似于mapValues，不同的在于flatMapValues应用于元素为key-value对的RDD中Value。每个一kv对的Value被输入函数映射为一系列的值，然后这些值再与原RDD中的Key组成一系列新的KV对。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;aoo&quot;</span>]), (<span class="string">&quot;age&quot;</span>, [<span class="string">&quot;12&quot;</span>, <span class="string">&quot;20&quot;</span>])])</span><br><span class="line">rdd.flatMapValues(<span class="keyword">lambda</span> x:x).collect()</span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">[(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;aoo&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;12&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;20&#x27;</span>)]</span><br></pre></td></tr></table></figure><ul><li><p>mapValues(func): 返回一个新的RDD，对RDD中的每一个value应用函数func。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;aoo&quot;</span>]), (<span class="string">&quot;age&quot;</span>, [<span class="string">&quot;12&quot;</span>, <span class="string">&quot;20&quot;</span>])])</span><br><span class="line">rdd.mapValues(<span class="keyword">lambda</span> value:<span class="built_in">len</span>(value)).collect()</span><br><span class="line"><span class="comment"># [(&#x27;name&#x27;, 3), (&#x27;age&#x27;, 2)]</span></span><br></pre></td></tr></table></figure></li><li><p>distinct(): 去除重复的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.distinct().collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 10), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure></li><li><p>subtractByKey(other): 删除在RDD1与RDD2的key相同的项</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.subtractByKey(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure><ul><li>subtract(other): 取差集<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.subtract(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>intersection(other): 交集运算，保留在两个RDD中都有的元素</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd1.intersection(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 10)]</span></span><br></pre></td></tr></table></figure><p>有关key-value类型的处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="comment"># 取出所有item的key</span></span><br><span class="line">rdd.keys().collect() <span class="comment"># [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;]</span></span><br><span class="line"><span class="comment"># 取出所有的values</span></span><br><span class="line">rdd.values().collect() <span class="comment"># [1, 10, 1, 1]</span></span><br></pre></td></tr></table></figure><ul><li><p><code>foldByKey</code>(<em>zeroValue</em>, <em>func</em>, <em>numPartitions=None</em>)</p><p>Merge the values for each  key using an associative function “func” and a neutral “zeroValue” which  may be added to the result an arbitrary number of times, and must not  change the result (e.g., 0 for addition, or  1 for multiplication.).<br>其实foldByKey也像reduceBykey，对同一key中的value进行合并，例如对相同key进行value累加，zeroValue=0表示累加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.foldByKey(<span class="number">0</span>, <span class="keyword">lambda</span> x,y:x+y).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对相同key进行value累乘，注意zeroValue=1代表累乘：</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line">rdd.foldByKey(<span class="number">1</span>, <span class="keyword">lambda</span> x,y:x*y).collect()</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 4), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure><ul><li>groupByKey(numPartitions=None): 将(K, V)数据集上所有Key相同的数据聚合到一起，得到的结果是(K, (V1, V2…))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">len</span>).collect())</span><br><span class="line"><span class="comment"># 统计数据集每个key的个数总和</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 3), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">list</span>).collect())</span><br><span class="line"><span class="comment"># 将每个key的v聚合到一个list里面</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, [1, 10, 1]), (&#x27;b&#x27;, [1])]</span></span><br></pre></td></tr></table></figure><ul><li>reduceByKey(func, numPartitions=None):此算子最常用， 将(K,  V)数据集上所有Key相同的数据聚合到一起，func的参数即是每两个K-V中的V。可以使用这个函数来进行计数，例如reduceByKey(lambda  a,b:a+b)就是将key相同数据的Value进行相加。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;foo&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;bar&quot;</span>, <span class="number">3</span>)])</span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> x, y : x + y).collect() <span class="comment"># [(&#x27;foo&#x27;, 3), (&#x27;bar&#x27;, 3)]  </span></span><br><span class="line">x.reduceByKey(<span class="built_in">max</span>).collect() <span class="comment">#  [(&#x27;foo&#x27;, 2), (&#x27;bar&#x27;, 3)]</span></span><br></pre></td></tr></table></figure><ul><li>join(other, numPartitions=None): 将(K, V)和(K, W)类型的数据进行JOIN操作，得到的结果是这样(K, (V, W))</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">&quot;bar&quot;</span>, <span class="number">10</span>) , (<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&quot;bar&quot;</span>, <span class="number">12</span>) , (<span class="string">&quot;foo&quot;</span>, <span class="number">12</span>)])</span><br><span class="line">rdd1.join(rdd2).collect()</span><br><span class="line"><span class="comment"># [(&#x27;bar&#x27;, (10, 12)), (&#x27;foo&#x27;, (1, 12))]</span></span><br></pre></td></tr></table></figure><ul><li>union(other): 并集运算，合并两个RDD</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd1 &#x3D; sc.parallelize([(&quot;a&quot;, 10) ,(&quot;b&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">rdd2 &#x3D; sc.parallelize([(&quot;a&quot;, 10) ,(&quot;c&quot;, 1), (&quot;a&quot;, 1)])</span><br><span class="line">rdd1.union(rdd2).collect()</span><br><span class="line"># [(&#39;a&#39;, 10), (&#39;b&#39;, 1), (&#39;a&#39;, 1), (&#39;a&#39;, 10), (&#39;c&#39;, 1), (&#39;a&#39;, 1)]</span><br></pre></td></tr></table></figure><p>还有更多的transmission算子这里不再一一列举，可以参考官网PySpark API文档。</p><p>第二类算子：Action</p><ul><li><p>collect(): 以数组的形式，返回数据集中所有的元素。在数据探索阶段常用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line">word_map_rdd = word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> w: (w, <span class="number">1</span>))</span><br><span class="line">word_map_rdd.collect()</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">[(<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;pyspark&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;kafka&#x27;</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>), (<span class="number">10</span>, <span class="number">1</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>collectAsMap将k-v数据rdd集合转为python字典类型，同一key的项，只取第一项，其他的项被忽略</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),(<span class="string">&quot;a&quot;</span>, <span class="number">10</span>) ,(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.collectAsMap() <span class="comment"># &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 1&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>count(): 返回数据集中元素的个数</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line">word_rdd.count() <span class="comment"># 8</span></span><br></pre></td></tr></table></figure><ul><li>take(n): 返回数据集的前N个元素</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;pyspark&quot;</span>, <span class="string">&quot;kafka&quot;</span>,<span class="string">&quot;kafka&quot;</span>, <span class="number">10</span>,<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">word_rdd.take(<span class="number">3</span>) <span class="comment"># [&#x27;foo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure><ul><li>takeOrdered(n): 升序排列，取出前N个元素<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;foo&quot;</span>, <span class="string">&quot;bar&quot;</span>, <span class="string">&quot;foo&quot;</span>, <span class="string">&quot;zoo&quot;</span>, <span class="string">&quot;aoo&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>) <span class="comment"># [&#x27;aoo&#x27;, &#x27;bar&#x27;, &#x27;foo&#x27;]</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>takeOrdered(n, key=lambda num: -num): 降序排列，取出前N个元素<br>key=lambda num: -num只适用数值型的rdd，其实就将每项数值变为负数再排列</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>).takeOrdered(<span class="number">3</span>,key=<span class="keyword">lambda</span> num:-num)</span><br><span class="line">print(rdd)</span><br></pre></td></tr></table></figure><p>字符串的rdd排序，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">word_rdd = sc.parallelize (</span><br><span class="line">   [<span class="string">&quot;fooo&quot;</span>, <span class="string">&quot;bbbar&quot;</span>, <span class="string">&quot;ffoo&quot;</span>, <span class="string">&quot;zoo&quot;</span>, <span class="string">&quot;aoo&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按字符长度降序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>,key=<span class="keyword">lambda</span> item:-<span class="built_in">len</span>(item))</span><br><span class="line"><span class="comment"># 按字符长度升序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>,key=<span class="built_in">len</span>)</span><br><span class="line"><span class="comment">#按字母升序排序再取前3项</span></span><br><span class="line">word_rdd.takeOrdered(<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>countByKey(): 对同一key值累计其计数，例如wordcount</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;bar&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;foo&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.countByKey().items()</span><br><span class="line"><span class="comment"># dict_items([(&#x27;foo&#x27;, 2), (&#x27;bar&#x27;, 1)])以元组的方式返回</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>countByValue():对值分组统计</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">9</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">rdd.countByValue().items()</span><br><span class="line"><span class="comment"># dict_items([(9, 2), (10, 3)])</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Persistence(持久化)<br>persist(): 将数据按默认的方式进行持久化<br> unpersist(): 取消持久化<br>saveAsTextFile(path): 将数据集保存至文件</p></li><li><p>创建rdd对象时指定分区，<br><code>parallelize(c, numSlices=None)</code><br>对每个元素都分区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>], <span class="number">5</span>).glom().collect()</span><br><span class="line"><span class="comment"># [[0], [2], [3], [4], [6]]</span></span><br></pre></td></tr></table></figure></li></ul><p>glom方法：Return an RDD created by coalescing all elements within each partition into a list<br>指定两个分区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd=sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line">[[<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]]</span><br></pre></td></tr></table></figure><ul><li>广播rdd<br>给定一个key为id的字段数据集合，给定其id，求字段对应的value</li></ul><p>非广播方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apples = sc.parallelize([(<span class="number">1</span>, <span class="string">&#x27;iPhone X&#x27;</span>),(<span class="number">2</span>, <span class="string">&#x27;iPhone 8&#x27;</span>),(<span class="number">5</span>, <span class="string">&#x27;iPhone 11&#x27;</span>)])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将该数据集合转为字典</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apples_dict=apples.collectAsMap()</span><br><span class="line"><span class="comment"># &#123;1: &#x27;iPhone X&#x27;, 2: &#x27;iPhone 8&#x27;, 5: &#x27;iPhone 11&#x27;&#125;</span></span><br></pre></td></tr></table></figure><p>给定id集合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ids = sc.parallelize([<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>通过map方法取出ids对应的value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:apples_dict[x]).collect()</span><br><span class="line"><span class="comment"># [&#x27;iPhone 8&#x27;, &#x27;iPhone X&#x27;, &#x27;iPhone 11&#x27;]</span></span><br></pre></td></tr></table></figure><p>这种方式，在ids与apples_dict之间的映射转换，每一个id查找映射，都需要将ids和apples_dict传到worker节点上计算，如果有100万个id，而且apples_dict是个超大字典，那么就需要进行100万次上传worker再计算结果，显然效率极低，也不合理。</p><p>使用广播方式可避免这种情况<br>将apples_dict转为广播变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apples_dict_bc=sc.broadcast(apples_dict)</span><br><span class="line">print(<span class="built_in">type</span>(apples_dict_bc))</span><br><span class="line"><span class="comment"># &lt;class &#x27;pyspark.broadcast.Broadcast&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>给定id集合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ids = sc.parallelize([<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>id对应的value，使用apples_dict_bc.value[x]这个广播变量，获取id对应的value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:apples_dict_bc.value[x]).collect()</span><br><span class="line"><span class="comment"># [&#x27;iPhone 8&#x27;, &#x27;iPhone X&#x27;, &#x27;iPhone 11&#x27;]</span></span><br></pre></td></tr></table></figure><p>在开始计算时，apples_dict_bc会传到worker node的内存上（如果数据集合太大，有部分数据则存在磁盘）。之后worker 可以一直使用这个“常驻内存广播变量”处理映射任务，即使有100万个id，客户端只需要把id传到worker即可，这个大apples_dict_bc数据集合则无需再传送到worker，大大减少时间。</p><ul><li>累加器accumulator：</li></ul><p>创建测试数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>创建accumulator累加器total，用于累加数集合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total=sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>创建accumulator累加器counter，用于计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">counter=sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>使用foreach，对每一项都使用total累计该元素的值，counter累加已处理的元素个数，注意：counter这个accumulator变量是自增1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach(<span class="keyword">lambda</span> item:[total.add(item),counter.add(<span class="number">1</span>)])</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">total.value # 15.0</span><br><span class="line">counter.value 5</span><br></pre></td></tr></table></figure><h5 id="完整的wordcount示例"><a href="#完整的wordcount示例" class="headerlink" title="完整的wordcount示例"></a>完整的wordcount示例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="keyword">as</span> sc</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_spark_context</span>()</span></span><br><span class="line">    conf=SparkConf().setAppName(&quot;word_count&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">    spark_context=sc.getOrCreate(conf)    </span><br><span class="line">    <span class="keyword">return</span> spark_context</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_count</span>(<span class="params">spark_sc,input_file,output_dir,delimiter=<span class="string">&#x27; &#x27;</span></span>):</span></span><br><span class="line">    data_rdd=spark_sc.textFile(input_file) <span class="comment"># </span></span><br><span class="line">    word_rdd=text_rdd.flatMap(<span class="keyword">lambda</span> line:line.split(delimiter))</span><br><span class="line">    count_rdd=word_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> v1,v2:v1+v2)</span><br><span class="line">    count_rdd.saveAsTextFile(output_dir) <span class="comment">#注意这里参数为文件夹 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    sc_obj=create_spark_context()</span><br><span class="line">    word_count(sc_obj,<span class="string">&quot;file:///opt/data.txt&quot;</span>,<span class="string">&quot;file:///opt/word_count_output&quot;</span>)</span><br></pre></td></tr></table></figure><p>查看存放的输出结果，计算结果的输出文件放在part-00000这个文件，而_SUCCESS文件是无内容的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]<span class="comment"># ls word_count_output/</span></span><br><span class="line">part-<span class="number">00000</span>  _SUCCESS</span><br><span class="line"></span><br><span class="line">[root@nn word_count_output]<span class="comment"># cat part-00000 </span></span><br><span class="line">(<span class="string">&#x27;linux&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;is&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;the&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;best&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="string">&#x27;centos&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;macos&#x27;</span>, <span class="number">2</span>)</span><br><span class="line">(<span class="string">&#x27;redhat&#x27;</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="3、基于PySpark和ALS的电影推荐流程"><a href="#3、基于PySpark和ALS的电影推荐流程" class="headerlink" title="3、基于PySpark和ALS的电影推荐流程"></a>3、基于PySpark和ALS的电影推荐流程</h4><p>&#8195;&#8195;本节内容参考书籍pdf版本《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。<br>&#8195;&#8195;(有一点需要指出的是：该书的作者似乎为出书而出书，在前面十来章内容，冗长且基础，大量截图以及table，其实大部分内容可言简意赅。但他们似乎为了出书为了销量，需把这本书打造“很厚，页数多，专业技术书籍”的印象，但其精华只有后面关于pyspark.mllib机器学习示例的内容。)</p><h5 id="数据集背景"><a href="#数据集背景" class="headerlink" title="数据集背景"></a>数据集背景</h5><p>数据源：<code>https://grouplens.org/datasets/movielens/</code><br>这里有非常详细的电影训练数据，适合项目练手<br>数据信息：<br>MovieLens 100K<br>movie ratings.<br>Stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies</p><p>数据样例结构：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ml-100k]# ls</span><br><span class="line">allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base  u.genre  u.occupation</span><br><span class="line">mku.sh     u1.test  u3.base  u4.test  ua.base  ub.test  u.info   u.user</span><br><span class="line">README     u2.base  u3.test  u5.base  ua.test  u.data   u.item</span><br></pre></td></tr></table></figure><p>有关数据结构的说明，可以查看README文件，例如u.data:4个字段，user id | item id | rating | timestamp.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">196     242     3       881250949</span><br><span class="line">186     302     3       891717742</span><br></pre></td></tr></table></figure><h5 id="读取用户数据"><a href="#读取用户数据" class="headerlink" title="读取用户数据"></a>读取用户数据</h5><p>探索基本数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user_rdd=sc.textFile(<span class="string">&quot;file:///opt/ml-100k/u.data&quot;</span>)</span><br><span class="line">user_rdd.count()<span class="comment"># 100000</span></span><br><span class="line">user_rdd.first() <span class="comment"># &#x27;196\t242\t3\t881250949&#x27;</span></span><br></pre></td></tr></table></figure><p>因ALS入参为3个字段，故只需取出user_rdd前3个字段的:用户id，产品id以及评分:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">raw_rating_rdd=user_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27;\t&#x27;</span>)[:<span class="number">3</span>]) <span class="comment"># 每行分割后为一个包含4个元素的列表，取前3项即可</span></span><br><span class="line">raw_rating_rdd.take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[[<span class="string">&#x27;196&#x27;</span>, <span class="string">&#x27;242&#x27;</span>, <span class="string">&#x27;3&#x27;</span>],[<span class="string">&#x27;186&#x27;</span>, <span class="string">&#x27;302&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]] <span class="comment"># 注意，每个item是列表</span></span><br></pre></td></tr></table></figure><p>ALS训练数据格式的入参为一组元组类型的数据：Rating(user,product,rating)，过还需做以下转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rating_rdd=raw_rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]))<span class="comment"># x[0],x[1],x[2]对应用户id，电影id，评分</span></span><br><span class="line">rating_rdd.take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="string">&#x27;196&#x27;</span>, <span class="string">&#x27;242&#x27;</span>, <span class="string">&#x27;3&#x27;</span>), (<span class="string">&#x27;186&#x27;</span>, <span class="string">&#x27;302&#x27;</span>, <span class="string">&#x27;3&#x27;</span>)]<span class="comment"># rdd的每个item为元组类型</span></span><br></pre></td></tr></table></figure><p>查看不重复的用户总量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">total_users=rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>]).distinct().count()</span><br><span class="line">total_users <span class="comment"># 943</span></span><br></pre></td></tr></table></figure><p>查看不重复的电影总量（同上）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">total_moves&#x3D;rating_rdd.map(lambda x:x[1]).distinct().count()</span><br><span class="line">total_moves # 1682</span><br></pre></td></tr></table></figure><h5 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h5><p>大致处理流程：读取文件=&gt;user_rdd=&gt;raw_rating_rdd=&gt;rating_rdd，这里rating_rdd的格式就是ALS训练数据的格式Rating(user,product,rating)，然后再用ALS.train，训练结束后，就会创建模型对象MatrixFactorizationModel</p><p><strong>这里简单介绍ALS算法</strong>：Alternating Least Squares matrix factorization，其实就是（交替）最小二乘法，这里为何使用ALS？因为它同时考虑了User和Item两个方面，即即可基于用户进行推荐又可基于物品，所以适合推荐型的场景，模型一般如下：<br><img src="https://img-blog.csdnimg.cn/20200109205253156.png" alt="Am×n=Um×k×Vk×n"><br>原始协同矩阵是一个<code>m*n</code>的矩阵，是由m<em>k和k</em>n两个矩阵相乘得到的，其中k&lt;&lt;m,n，U表示用户矩阵，V表示商品矩阵，k为U、V矩阵的的秩。学过线性代数应该知道<code>A*B=C</code>，两个矩阵相乘的结果，这就是所谓协同矩阵。<br><img src="https://img-blog.csdnimg.cn/20200109204624664.png" alt="在这里插入图片描述"><br>协同推荐就等同于<code>C=A*B</code>矩阵分解，矩阵分解（协同推荐矩阵是一个稀疏矩阵，因为不是所有的用户都对产品评分）最终又可以转换成了一个优化问题。将用户u对商品V的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。在这个矩阵分解的训练过程中，评分缺失项得到了填充，那么这个填充的项就可以根据用户ID进行推荐。<br>更详细内容可以参考这两篇文章：<a href="https://blog.csdn.net/YMPzUELX3AIAp7Q/article/details/85241209">文章1</a>、<a href="https://www.cnblogs.com/xiguage119/p/10813393.html">文章2</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"><span class="comment"># 注意ALS算法是基于矩阵运算，因此需要环境安装numpy库</span></span><br></pre></td></tr></table></figure><p><code>ALS.train(ratings,rank,iterations=5,lambda_=0.01)</code><br>ratings:训练数据集合，就是上面提到的Rating(user,product,rating)，也即是rating_rdd这个经过预处理的数据集</p><p>一句完成训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model=ALS.train(rating_rdd,<span class="number">10</span>,<span class="number">10</span>,<span class="number">0.01</span>)</span><br><span class="line">model<span class="comment"># &lt;pyspark.mllib.recommendation.MatrixFactorizationModel at 0x7f3159bc8048&gt;</span></span><br></pre></td></tr></table></figure><p>该模型对象有几个属性：<br>model.rank # 10 分解为稀疏矩阵的秩<br>userFeatures 为分解后的用户矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.userFeatures().take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="number">1</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.7229161262512207</span>, <span class="number">0.036963045597076416</span>, <span class="number">0.23517486453056335</span>, -<span class="number">0.18118669092655182</span>, -<span class="number">1.4776617288589478</span>, -<span class="number">1.0425325632095337</span>, <span class="number">0.3823653757572174</span>, -<span class="number">0.3569445312023163</span>, -<span class="number">0.2874303162097931</span>, <span class="number">0.0020452593453228474</span>])),</span><br><span class="line"> (<span class="number">2</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.3199065327644348</span>, <span class="number">0.41293472051620483</span>, <span class="number">0.12430011481046677</span>, -<span class="number">0.42582616209983826</span>, -<span class="number">0.4546814560890198</span>, -<span class="number">1.496929407119751</span>, <span class="number">0.6246935725212097</span>, <span class="number">0.49794384837150574</span>, -<span class="number">0.3813674747943878</span>, <span class="number">0.7599969506263733</span>]))]</span><br></pre></td></tr></table></figure><p>productFeatures为分解后的电影（产品）矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.productFeatures().take(<span class="number">2</span>)</span><br><span class="line">输出：</span><br><span class="line">[(<span class="number">1</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.9663546681404114</span>, <span class="number">0.0724567249417305</span>, <span class="number">0.22562265396118164</span>, -<span class="number">0.14772379398345947</span>, -<span class="number">1.3601692914962769</span>, -<span class="number">1.1434344053268433</span>, <span class="number">1.0299423933029175</span>, -<span class="number">0.17817920446395874</span>, -<span class="number">1.0483288764953613</span>, <span class="number">0.4326847195625305</span>])),</span><br><span class="line"> (<span class="number">2</span>,</span><br><span class="line">  array(<span class="string">&#x27;d&#x27;</span>, [-<span class="number">0.701686441898346</span>, -<span class="number">0.44971194863319397</span>, <span class="number">0.36079081892967224</span>, -<span class="number">0.1727607101202011</span>, -<span class="number">0.4821830689907074</span>, -<span class="number">1.1037342548370361</span>, <span class="number">0.8413264155387878</span>, -<span class="number">0.08249323815107346</span>, -<span class="number">1.0539320707321167</span>, <span class="number">0.6040329337120056</span>]))]</span><br></pre></td></tr></table></figure><h5 id="调用已训练的模型"><a href="#调用已训练的模型" class="headerlink" title="调用已训练的模型"></a>调用已训练的模型</h5><p>model已经封装好几个常用的方法，api使用简便</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Signature: model.recommendProducts(user, num)</span><br><span class="line">Docstring:</span><br><span class="line">Recommends the top &quot;num&quot; number of products for a given user and</span><br><span class="line">returns a list of Rating objects sorted by the predicted rating in</span><br><span class="line">descending order.</span><br></pre></td></tr></table></figure><p>例如给用户199推荐前5部电影</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.recommendProducts(<span class="number">199</span>,<span class="number">5</span>)</span><br><span class="line">[Rating(user=<span class="number">199</span>, product=<span class="number">854</span>, rating=<span class="number">10.774026140227157</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">962</span>, rating=<span class="number">9.30074590770409</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">1176</span>, rating=<span class="number">8.813180359193545</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">1280</span>, rating=<span class="number">8.11317788460314</span>),</span><br><span class="line"> Rating(user=<span class="number">199</span>, product=<span class="number">718</span>, rating=<span class="number">7.8722593701756995</span>)]</span><br></pre></td></tr></table></figure><p>这个结果表示，rating值越大，越排在越前面，代表更为优先推荐，首先推荐给用户199的为854这部电影<br>根据用户ID:199和电影ID:854，查询预测评分:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.predict(<span class="number">199</span>,<span class="number">854</span>) <span class="comment"># 10.774026140227157</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>使用用得更多的场合是：将某部电影推荐给感兴趣的用户，可通过model.recommendUsers得出这些用户，例如，将电影ID为154，推荐给前10个用户</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model.recommendUsers(<span class="number">154</span>,<span class="number">10</span>)</span><br><span class="line">输出：</span><br><span class="line">[Rating(user=<span class="number">133</span>, product=<span class="number">154</span>, rating=<span class="number">6.346890714591231</span>),</span><br><span class="line"> Rating(user=<span class="number">866</span>, product=<span class="number">154</span>, rating=<span class="number">6.10978058348641</span>),</span><br><span class="line"> Rating(user=<span class="number">50</span>, product=<span class="number">154</span>, rating=<span class="number">6.018355541192427</span>),</span><br><span class="line"> Rating(user=<span class="number">783</span>, product=<span class="number">154</span>, rating=<span class="number">5.991043569104054</span>),</span><br><span class="line"> Rating(user=<span class="number">310</span>, product=<span class="number">154</span>, rating=<span class="number">5.658875199814674</span>),</span><br><span class="line"> Rating(user=<span class="number">809</span>, product=<span class="number">154</span>, rating=<span class="number">5.636975519395109</span>),</span><br><span class="line"> Rating(user=<span class="number">78</span>, product=<span class="number">154</span>, rating=<span class="number">5.4898250475467725</span>),</span><br><span class="line"> Rating(user=<span class="number">762</span>, product=<span class="number">154</span>, rating=<span class="number">5.47223950904501</span>),</span><br><span class="line"> Rating(user=<span class="number">273</span>, product=<span class="number">154</span>, rating=<span class="number">5.318862413529849</span>),</span><br><span class="line"> Rating(user=<span class="number">264</span>, product=<span class="number">154</span>, rating=<span class="number">5.295430734770273</span>)]</span><br></pre></td></tr></table></figure><p>可以快速得出对电影ID为154最感兴趣的前10个用户，不过在推荐的信息里面，看不到电影名称，还需关联电影名的数据，从而形成完整的推荐信息。</p><p>加载电影详情数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">move_info_rdd=sc.textFile(<span class="string">&quot;file:///opt/ml-100k/u.item&quot;</span>)</span><br><span class="line">move_info_rdd.take(<span class="number">3</span>)</span><br><span class="line">输出：</span><br><span class="line">[<span class="string">&#x27;1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0&#x27;</span>]</span><br></pre></td></tr></table></figure><p>查看u.item电影详情表的字段说明，总共有19个字段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">u.item     -- Information about the items (movies); this <span class="keyword">is</span> a tab separated</span><br><span class="line">              <span class="built_in">list</span> of</span><br><span class="line">              movie <span class="built_in">id</span> | movie title | release date | video release date |</span><br><span class="line">              IMDb URL | unknown | Action | Adventure | Animation |</span><br><span class="line">              Children<span class="string">&#x27;s | Comedy | Crime | Documentary | Drama | Fantasy |</span></span><br><span class="line"><span class="string">              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |</span></span><br><span class="line"><span class="string">              Thriller | War | Western |</span></span><br></pre></td></tr></table></figure><p>作为测试，无需使用全部字段，只需挑出感兴趣的字段即可：电影id，电影名，url</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">move_splited_rdd=move_info_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取3个字段，将转为map类型，name:电影名，url：电影ur </span></span><br><span class="line">func=<span class="keyword">lambda</span> a_list:(<span class="built_in">int</span>(a_list[<span class="number">0</span>]),<span class="string">&#x27;name:%s,url:%s&#x27;</span>%(a_list[<span class="number">1</span>],a_list[<span class="number">4</span>]))</span><br><span class="line">move_map_info_rdd=move_splited_rdd.<span class="built_in">map</span>(func).collectAsMap() <span class="comment">#move_map_info_rdd 已经是字典类</span></span><br><span class="line">print(move_map_info_rdd)</span><br><span class="line"><span class="comment"># python字典类型的电影信息</span></span><br><span class="line">&#123;<span class="number">1</span>: <span class="string">&#x27;name:Toy Story (1995) url:http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)&#x27;</span>,</span><br><span class="line"> <span class="number">2</span>: <span class="string">&#x27;name:GoldenEye (1995) url:http://us.imdb.com/M/title-exact?GoldenEye%20(1995)&#x27;</span>,</span><br><span class="line"> ......</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>move_map_info_rdd的key就是电影ID，因此只需要关联model.recommendUsers(154,10)输出的<code>Rating(user=133, product=154, rating=6.346890714591231),</code> product id，即可输出完整的推荐信息如下：<br>给用户id为199的用户推荐3部电影</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result=model.recommendProducts(<span class="number">199</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> result:</span><br><span class="line">    print(<span class="string">f&#x27;user:<span class="subst">&#123;r.user&#125;</span>,moveid:<span class="subst">&#123;r.product&#125;</span>,move_info:<span class="subst">&#123;move_map_info_rdd[r.product]&#125;</span>,rating:<span class="subst">&#123;r.rating&#125;</span>&#x27;</span>)</span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">user:199,moveid:854,move_info:name:Bad Taste (1987) url:http://us.imdb.com/M/title-exact?Bad%20Taste%20(1987),rating:10.774026140227157</span><br><span class="line"></span><br><span class="line">user:199,moveid:962,move_info:name:Ruby in Paradise (1993) url:http://us.imdb.com/M/title-exact?Ruby%20in%20Paradise%20(1993),rating:9.30074590770409</span><br><span class="line"></span><br><span class="line">user:199,moveid:1176,move_info:name:Welcome To Sarajevo (1997) url:http://us.imdb.com/M/title-exact?Welcome+To+Sarajevo+(1997),rating:8.813180359193545</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将model持久化到本地后，再封装为完整的逻辑，方便重新使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.save(sc,<span class="string">&#x27;/opt/ml-100k/asl_model&#x27;</span>) <span class="comment"># sc为spark程序开头的spark context</span></span><br><span class="line"><span class="comment"># 若再次存储再会提示出错，所以一般是这么用：</span></span><br><span class="line"><span class="keyword">try</span>：</span><br><span class="line">    model.save(sc,path)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>model以一个目录的形式保存，而且还保存了user和product的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ml-100k]# tree asl_model/</span><br><span class="line">asl_model/</span><br><span class="line">├── data</span><br><span class="line">│   ├── product</span><br><span class="line">│   │   ├── part-00000-bf34d65a-81e8-4124-a254-6e6044b8da2d-c000.snappy.parquet</span><br><span class="line">│   │   └── _SUCCESS</span><br><span class="line">│   └── user</span><br><span class="line">│       ├── part-00000-3953175d-e560-42a5-8de3-fcc86a4b625c-c000.snappy.parquet</span><br><span class="line">│       └── _SUCCESS</span><br><span class="line">└── metadata</span><br><span class="line">    ├── part-00000</span><br><span class="line">    └── _SUCCESS</span><br></pre></td></tr></table></figure><p>如何加载已训练好的本地模型？使用load方法即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load(sc,<span class="string">&#x27;/opt/ml-100k/asl_model&#x27;</span>) <span class="comment"># path为</span></span><br></pre></td></tr></table></figure><h5 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h5><p>将以上的处理流程封装类，便于调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="keyword">as</span> sc</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</span><br><span class="line"><span class="keyword">import</span> os,datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MoveRecommend</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,model_path,user_path,move_path,app_name=<span class="string">&quot;move_recommend&quot;</span>,master=<span class="string">&quot;local[*]&quot;</span></span>):</span></span><br><span class="line">        self.app_name=app_name</span><br><span class="line">        self.master=master</span><br><span class="line">        self.sc=self.create_spark_context()</span><br><span class="line">        self.train_rank=<span class="number">10</span> <span class="comment"># 稀疏矩阵分解的秩</span></span><br><span class="line">        self.train_iter=<span class="number">10</span> <span class="comment"># 迭代次数</span></span><br><span class="line">        self.train_lambda=<span class="number">0.01</span> <span class="comment"># 正则化参数(惩罚因子)        </span></span><br><span class="line">        self.user_path=user_path </span><br><span class="line">        self.move_path=move_path</span><br><span class="line">        self.model_path=model_path</span><br><span class="line">        self.model=self.get_model()</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_spark_context</span>(<span class="params">self</span>):</span></span><br><span class="line">        conf=SparkConf().setAppName(self.app_name).setMaster(self.master)</span><br><span class="line">        spark_context=sc.getOrCreate(conf)    </span><br><span class="line">        <span class="keyword">return</span> spark_context</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;如果给定的目录没有model，则重新训练model，如果已有model，则直接加载使用&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(self.model_path):</span><br><span class="line">            print(<span class="string">f&#x27;model not found,start traing at <span class="subst">&#123;self.get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> self.train_and_save()</span><br><span class="line">        <span class="keyword">return</span> model.load(self.sc,self.model_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_and_save</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;只用训练集，训练model并持久化到本地目录&quot;&quot;&quot;</span></span><br><span class="line">        user_rdd=self.sc.textFile(<span class="string">&quot;file://&quot;</span>+self.user_path)</span><br><span class="line">        raw_rating_rdd=user_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&#x27;\t&#x27;</span>)[:<span class="number">3</span>]) <span class="comment"># 每行分割后为一个包含4个元素的列表，取前3项即可</span></span><br><span class="line">        rating_rdd=raw_rating_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]))<span class="comment"># x[0],x[1],x[2]对应用户id，电影id，评分</span></span><br><span class="line">        model=ALS.train(rating_rdd,self.train_rank,self.train_iter,self.train_lambda)</span><br><span class="line">        model.save(self.sc,self.model_path)</span><br><span class="line">        print(<span class="string">f&#x27;model training done at <span class="subst">&#123;self.get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> model </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_move_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个字典列表，每个字典存放3个电影详情字段&quot;&quot;&quot;</span>        </span><br><span class="line">        move_info_rdd=self.sc.textFile(<span class="string">&quot;file://&quot;</span>+self.move_path)</span><br><span class="line">        move_splited_rdd=move_info_rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line">        <span class="comment"># 提取3个字段，将转为map类型，name:电影名，url：电影ur </span></span><br><span class="line">        func=<span class="keyword">lambda</span> a_list:(<span class="built_in">int</span>(a_list[<span class="number">0</span>]),<span class="string">&#x27;name:%s,url:%s&#x27;</span>%(a_list[<span class="number">1</span>],a_list[<span class="number">4</span>]))</span><br><span class="line">        move_map_info_rdd=move_splited_rdd.<span class="built_in">map</span>(func).collectAsMap() <span class="comment">#move_map_info_rdd 已经是字典类 </span></span><br><span class="line">        <span class="keyword">return</span> move_map_info_rdd</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend_product_by_userid</span>(<span class="params">self,user_id,num=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据给定用户id，向其推荐top N部电影&quot;&quot;&quot;</span>                </span><br><span class="line">        result= self.model.recommendProducts(user_id,num)</span><br><span class="line">        move_dict=self.get_move_dict()</span><br><span class="line">        <span class="keyword">return</span> [(r.user,r.product,move_dict[r.product],r.rating) <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend_user_by_moveid</span>(<span class="params">self,move_id,num=<span class="number">5</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据给定电影ID，推荐对该电影感兴趣的top N 个用户&quot;&quot;&quot;</span>     </span><br><span class="line">        result=self.model.recommendUsers(move_id,num)</span><br><span class="line">        move_dict=self.get_move_dict()</span><br><span class="line">        <span class="keyword">return</span> [(r.user,r.product,move_dict[r.product],r.rating) <span class="keyword">for</span> r <span class="keyword">in</span> result]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m=MoveRecom(model_path=<span class="string">&#x27;/opt/ml-100k/costom_model&#x27;</span>,user_path=<span class="string">&#x27;/opt/ml-100k/u.data&#x27;</span>,move_path=<span class="string">&#x27;/opt/ml-100k/u.item&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出训练时间：<br>model not found,start traing at 26:45<br>model training done at 27:06</p><h5 id="项目难点说明"><a href="#项目难点说明" class="headerlink" title="项目难点说明"></a>项目难点说明</h5><p>&#8195;&#8195;上面的例子只是给出demo流程，而且数据已准备，但如果针对实际项目，则需要你处理以下两个主要难点：<br>（1） 训练数据的获取、整理和加工，并将这一流程自动化。<br>（2）模型的训练，以及根据新数据重新训练模型，以保证模型推荐效果最优，并将这一流程自动化。<br>&#8195;&#8195;至于其他工作，例如web 层面的开发，以及Apps或者说底层数据的存储，对于全栈开发者来说，并无大碍，只是需要耗费更多精力而已。</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本文给出了较为入门的基于PySpark实现的推荐类的业务流程，该逻辑其实是离线的模式：训练数据已经加工好，模型训练也没有进行深度调优。事实上，如果将其作为一个生产可用项目来实施，需将大数据生态圈相关技术栈以及web 开发进行整合，此类项目的架构设计一般有下面三部分：</p><ul><li>需推荐的业务数据（包括训练集和测试集）收集、计算、存储：大数据生态圈相关技术栈实现</li><li>模型训练方面：离线存储PySpark计算后生成的训练模型，而且需要定时训练和更新该模型文件，以便保持最优模型。</li><li>以web api的方式提供推荐数据：为BI或者其他应用以get、post的方式提供推荐数据，例如post一个用户ID，返回相应的推荐条目</li></ul><p>以下简要说明两种基本架构图：<br><strong>第一种：适合数据量不大，几个节点组成的小型“大数据”服务</strong><br><img src="https://img-blog.csdnimg.cn/20200111095444386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;这种架构较为简单，数据源本身已经存储在各个业务的原有数据库中或者日志文件，开发者无需借助hadoop存储组件，自行实现数据源抽取模块，接着只需PySpark读取这些数据并训练成模型文件即可，模型文件管理可以通过定时训练更新，最后通过web API的形式为上层应用提供推荐或者匹配记录。<br>需要注意的是：构建web API方式这里用了Python栈，当然可用Java栈或者Go栈</p><p><strong>第二种：适合数据量大的中大型大数据服务</strong><br><img src="https://img-blog.csdnimg.cn/20200111100116354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;此类架构适合那些几十GB到几百GB级别甚至是TB级别的分布式大数据节点集群，此类场景需引入hadoop相关生态圈的技术栈，用于处理大量属鸡的存储和计算：Flume、Kafka、HBase、Hive，在计算层提供分布式的Spark组件支撑离线模型计算。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;本文内容第一部分给出Pyspark常见算子的用法，第二部分则参考书籍《Python spark2.0 Hadoop机器学习与大数据实战》的电影推荐章节。本文内容为大数据实时分析项目提供基本的入门知识。&lt;/p&gt;
&lt;h4 id=&quot;1、PySpark简介&quot;&gt;&lt;a href=&quot;#1、PySpark简介&quot; class=&quot;headerlink&quot; title=&quot;1、PySpark简介&quot;&gt;&lt;/a&gt;1、PySpark简介&lt;/h4&gt;&lt;p&gt;&amp;#8195;&amp;#8195;本节内容的图文一部分参考了这篇文章&lt;a href=&quot;http://sharkdtu.com/posts/pyspark-internal.html&quot;&gt;《PySpark 的背后原理 》&lt;/a&gt;，个人欣赏此博客作者，博文质量高，看完受益匪浅！Spark的内容不再累赘，可参考本博客&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103641824&quot;&gt;《深入理解Spark》&lt;/a&gt;。PySpark的工作原理图示如下：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200108220627968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/tags/Spark/"/>
    
    <category term="PySpark推荐" scheme="https://yield-bytes.gitee.io/blog/tags/PySpark%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>深入解析asyncio与协程</title>
    <link href="https://yield-bytes.gitee.io/blog/2020/01/04/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90asyncio%E4%B8%8E%E5%8D%8F%E7%A8%8B/"/>
    <id>https://yield-bytes.gitee.io/blog/2020/01/04/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90asyncio%E4%B8%8E%E5%8D%8F%E7%A8%8B/</id>
    <published>2020-01-04T13:10:48.000Z</published>
    <updated>2020-02-03T07:04:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面的文章，已经通过gevent实现高并发的协程，本文将详细讨论Python标准库异步IO——asyncio。在Python3.4中引入了协程的概念以及asyncio。asyncio底层调用yield from语法，将任务变成生成器后挂起，这种方式无法实现协程之间的自动切换，在Python3.5中正式确立引入了async和await 的语法，所有的这些工作都使得Python实现异步编程变得更容易上手。</p><a id="more"></a><h4 id="1、asyncio的基本概念"><a href="#1、asyncio的基本概念" class="headerlink" title="1、asyncio的基本概念"></a>1、asyncio的基本概念</h4><ul><li><p>event_loop 事件循环：每一个需要异步执行的任务都要注册到事件循环中，事件循环负责管理和调度这些任务之间的执行流程（遇到IO则自动切换协程等）。</p></li><li><p>coroutine 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。</p></li><li><p>task 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含任务的各种状态。</p></li><li><p>future： 代表将来执行或没有执行的任务的结果。它和task上没有本质的区别</p></li><li><p>async/await 关键字：在python3.5及以上，用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。</p></li></ul><p>&#8195;&#8195;在异步的模式里，所有代码逻辑都会运行在一个forever事件循环中（你可以把整个事件循环看成一个总控中心，它监听着当前线程创建的多个协程发发生的事件），它可以同时执行多个协程，这些协程异步地执行，直到遇到 await 关键字，事件循环将会挂起该协程，事件循环这个总控再把当前线程控制权分配给其他协程，直到其他的协程也挂起或者执行完毕，再进行下一个协程的执行。</p><h4 id="2、使用asyncio"><a href="#2、使用asyncio" class="headerlink" title="2、使用asyncio"></a>2、使用asyncio</h4><h5 id="2-1-使用async关键字和await定义协程"><a href="#2-1-使用async关键字和await定义协程" class="headerlink" title="2.1 使用async关键字和await定义协程"></a>2.1 使用async关键字和await定义协程</h5><p>在Python3.5之前，要实现协程方式的写法一般如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="keyword">from</span> asyncio.sleep(<span class="number">1</span>) </span><br></pre></td></tr></table></figure><p>在Python3.5以后，全面使用async关键字和await定义协程，代码显更直观。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># async 定义了mytask为协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="comment"># 这里就像gevent的sleep方法模拟IO，而且该协程会被asyncio自动切换</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>) <span class="comment"># await 要求该行语句的IO是有返回值的例如response=request.get(url)，如果直接使用await time.sleep(2),则无法创建协程对象</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建事件循环对象，该事件循环由当前主线程拥有 </span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">tasks=[mytask(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)] <span class="comment"># 这里mytask()是协程对象，不会离开运行。</span></span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks)) <span class="comment"># 这里实行的逻辑就像gevent.joinall(tasks)一样，表示loop一直运行直到所有的协程tasks都完成</span></span><br></pre></td></tr></table></figure><p>&#8195;&#8195;代码中通过async关键字定义一个协程（coroutine），不过该协程不能直接运行，需将它注册到事件循环loop里面，由后者在协程内部发生IO时（asyncio.sleep(2)）时候调用协程。asyncio.get_event_loop方法可以创建一个事件循环，然后使用run_until_complete将协程注册到事件循环，并启动事件循环。<br>输出，从结果可以看出，5个协程同一时刻并发运行。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">task-1 started at:17:12:37</span><br><span class="line">task-0 started at:17:12:37</span><br><span class="line">task-3 started at:17:12:37</span><br><span class="line">task-2 started at:17:12:37</span><br><span class="line">task-4 started at:17:12:37</span><br><span class="line">task-1 done at:17:12:38</span><br><span class="line">task-0 done at:17:12:38</span><br><span class="line">task-3 done at:17:12:38</span><br><span class="line">task-2 done at:17:12:38</span><br><span class="line">task-4 done at:17:12:38</span><br></pre></td></tr></table></figure><p>关于await要求该句为返回值：<br>await asyncio.sleep(2)，这里可以看看sleep返回什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sleep</span>(<span class="params">delay, result=<span class="literal">None</span>, *, loop=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Coroutine that completes after a given time (in seconds).&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> delay == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">yield</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> loop <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        loop = events.get_event_loop()</span><br><span class="line">    future = loop.create_future()</span><br><span class="line">    h = future._loop.call_later(delay,</span><br><span class="line">                                futures._set_result_unless_cancelled,</span><br><span class="line">                                future, result)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">yield</span> <span class="keyword">from</span> future)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        h.cancel()</span><br></pre></td></tr></table></figure><p>如果设为delay值，且loop事件循环已创建（即使代码未创建它也会自动创建），返回的是future对象(yield from future)，而这里可以挂起当前协程，直到future完成</p><h5 id="2-2-task对象"><a href="#2-2-task对象" class="headerlink" title="2.2 task对象"></a>2.2 task对象</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...同上</span><br><span class="line"><span class="comment"># async 定义了mytask为协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mytask</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    <span class="comment"># 这里就像gevent的sleep方法模拟IO，而且该协程会被asyncio自动切换</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;ok&#x27;</span></span><br><span class="line"></span><br><span class="line">coro=mytask(<span class="number">1</span>)</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task=loop.create_task(coro) <span class="comment"># 将协程对象封装为task对象</span></span><br><span class="line">print(<span class="string">&#x27;before register to loop:&#x27;</span>,task)</span><br><span class="line">loop.run_until_complete(future=task)</span><br><span class="line">print(<span class="string">&#x27;after loop completed,task return the result:&#x27;</span>,task.result())</span><br></pre></td></tr></table></figure><p>查看打印结果：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">before register to loop: &lt;Task pending coro=&lt;mytask() running at /opt/asyn.py:9&gt;&gt;</span><br><span class="line">task-1 started at:17:39:06</span><br><span class="line">task-1 done at:17:39:07</span><br><span class="line">after loop completed,task return the result: ok</span><br></pre></td></tr></table></figure><p>将协程封装为task对象后，task在注册到事件循环之前为pending状态，1秒后，task 结束，并且通过task.result()可以获取协程结果值。<br>task对象也可用asyncio.ensure_future(coro)创建（接收coro协程或者future对象），它内部封装了loop.create_task</p><h5 id="2-2-future对象"><a href="#2-2-future对象" class="headerlink" title="2.2 future对象"></a>2.2 future对象</h5><p>前面定义说了future表示将来执行或没有执行的任务的结果，task是future的子类。<br>基本的方法有：<br>• cancel(): 取消future的执行，调度回调函数<br>• result(): 返回future代表的结果<br>• exception(): 返回future中的Exception<br>• add_done_callback(fn): 添加一个回调函数，当future执行的时候会调用这个回调函数。<br>• set_result(result): 将future标为运行完成，并且设置return值，该方法常用<br>使用future，可以在协程结束后自行回调函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line">...同上</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coru_1</span>(<span class="params">future_obj,N</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coru_1 started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    total=<span class="built_in">sum</span>(<span class="built_in">range</span>(N))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future_obj.set_result(<span class="string">&#x27;coru_1 returns:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total))</span><br><span class="line">    print(<span class="string">&#x27;coru_1 done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coru_2</span>(<span class="params">future_obj,N</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coru_2 started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    total=<span class="built_in">sum</span>(<span class="built_in">range</span>(N))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future_obj.set_result(<span class="string">&#x27;coru_2 returns:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total))</span><br><span class="line">    print(<span class="string">&#x27;coru_2 done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call_back</span>(<span class="params">future_obj</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;saved to redis at :&#x27;</span>,get_time(),future_obj,future_obj.result())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    f1=asyncio.Future()</span><br><span class="line">    f2=asyncio.Future()</span><br><span class="line">    tasks=[coru_1(f1,<span class="number">10</span>),coru_2(f2,<span class="number">20</span>)]</span><br><span class="line">    f1.add_done_callback(call_back)</span><br><span class="line">    f2.add_done_callback(call_back)</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">coru_1 started at:16:52:07</span><br><span class="line">coru_2 started at:16:52:07</span><br><span class="line">coru_1 done at:16:52:09</span><br><span class="line">coru_2 done at:16:52:09</span><br><span class="line">saved to redis at : 16:52:10 &lt;Future finished result=&#x27;coru_1 returns:45&#x27;&gt; coru_1 returns:45</span><br><span class="line">saved to redis at : 16:52:11 &lt;Future finished result=&#x27;coru_2 returns:190&#x27;&gt; coru_2 returns:190</span><br></pre></td></tr></table></figure><p>两个协程同时启动且在同一时间结束运行。之后开始回调，可以看到协程1先回调，1秒完成后，再切换到协程2回调。</p><h5 id="2-3-获取协程并发执行后的所有返回值"><a href="#2-3-获取协程并发执行后的所有返回值" class="headerlink" title="2.3 获取协程并发执行后的所有返回值"></a>2.3 获取协程并发执行后的所有返回值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;task-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line">    </span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">coros=[read_file(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)] <span class="comment"># 创建多个协程</span></span><br><span class="line">tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros]<span class="comment"># 将协程封装为task对象 </span></span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line"><span class="comment"># 或者loop.run_until_complete(asyncio.gether(*tasks))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重点在这里，当所有的协程结束后，可批量获取所有协程的返回结果</span></span><br><span class="line">get_all_result=[ t.result() <span class="keyword">for</span> t <span class="keyword">in</span> tasks]</span><br><span class="line">print(get_all_result)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">task-0 started at:15:53:08</span><br><span class="line">task-1 started at:15:53:08</span><br><span class="line">task-2 started at:15:53:08</span><br><span class="line">task-3 started at:15:53:08</span><br><span class="line">[&#x27;task-0 done at:15:53:09&#x27;, &#x27;task-1 done at:15:53:09&#x27;, &#x27;task-2 done at:15:53:09&#x27;, &#x27;task-3 done at:15:53:09&#x27;]</span><br></pre></td></tr></table></figure><p>以上也无需使用future的回调机制获取协程返回值，直接在loop结束后，从task对象的result方法即可获得协程返回值。<br>需要注意的是：<br>用于等待所有协程完成的方法asyncio.wait和asyncio.gather，都是接受多个future或coro组成的列表，区别：asyncio.gather内边调用ensure_future方法将列表中不是task的coro封装为future对象，而wait则没有。</p><h5 id="2-4-asyncio-gather-vs-asyncio-wait"><a href="#2-4-asyncio-gather-vs-asyncio-wait" class="headerlink" title="2.4 asyncio.gather vs asyncio.wait"></a>2.4 asyncio.gather vs asyncio.wait</h5><p>这里再给两个例子说明这两者的区别以及应用场合：<br>asyncio.gather</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">group_id,coro_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;group&#123;&#125;-task&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(group_id,coro_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(coro_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;group&#123;&#125;-task&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(group_id,coro_id,get_time())</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建三组tasks</span></span><br><span class="line">tasks1=[asyncio.ensure_future(coro(<span class="number">1</span>,i))<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">5</span>)]</span><br><span class="line">tasks2=[asyncio.ensure_future(coro(<span class="number">2</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>,<span class="number">8</span>)]</span><br><span class="line">tasks3=[asyncio.ensure_future(coro(<span class="number">3</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">group1=asyncio.gather(*tasks1) <span class="comment"># 对第1组的协程进行分组，group1</span></span><br><span class="line">group2=asyncio.gather(*tasks2) <span class="comment"># 对第2组的协程进行分组，group2</span></span><br><span class="line">group3=asyncio.gather(*tasks3) <span class="comment"># 对第3组的协程进行分组，group3</span></span><br><span class="line"></span><br><span class="line">all_groups=asyncio.gather(group1,group2,group3) <span class="comment"># 把3个group再聚合成一个大组，也是就所有协程对象的被聚合到一个大组</span></span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">all_group_result=loop.run_until_complete(all_groups) </span><br><span class="line"><span class="keyword">for</span> index,group <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_group_result): <span class="comment"># 获取每组协程的输出</span></span><br><span class="line">    print(<span class="string">&#x27;group &#123;&#125; result:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index+<span class="number">1</span>,group))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">group1-task1 started at:35:19</span><br><span class="line">group1-task2 started at:35:19</span><br><span class="line">group1-task3 started at:35:19</span><br><span class="line">group1-task4 started at:35:19</span><br><span class="line">group2-task6 started at:35:19</span><br><span class="line">group2-task7 started at:35:19</span><br><span class="line">group3-task8 started at:35:19</span><br><span class="line">group3-task9 started at:35:19</span><br><span class="line">group 1 result:[&#x27;group1-task1 done at:35:21&#x27;, &#x27;group1-task2 done at:35:21&#x27;, &#x27;group1-task3 done at:35:21&#x27;, &#x27;group1-task4 done at:35:21&#x27;]</span><br><span class="line">group 2 result:[&#x27;group2-task6 done at:35:21&#x27;, &#x27;group2-task7 done at:35:21&#x27;]</span><br><span class="line">group 3 result:[&#x27;group3-task8 done at:35:21&#x27;, &#x27;group3-task9 done at:35:21&#x27;]</span><br></pre></td></tr></table></figure><p>从打印结果可知，每组协程都在同一时刻开始以及同一时刻结束，asyncio.gather就是用于在更高层面对task进行分组，以不同的组管理不同的协程，你可以看出gather是一个粗粒度组织协程，自动收集所有协程结束后的返回值。</p><p>asyncio.wait</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line">tasks=[coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">first_complete,unfinished1=loop.run_until_complete(asyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED))</span><br><span class="line"><span class="comment"># 获取首个已结束的协程返回值,注意这里firt_complete是一个set()</span></span><br><span class="line"></span><br><span class="line">first_done_task=first_complete.pop()</span><br><span class="line">print(<span class="string">&#x27;首个完成的协程返回值：&#x27;</span>,first_done_task.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished1))</span><br><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第二阶段已完成的协程返回值</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished2:</span><br><span class="line">    print(t.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第二阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished3,unfinished3=loop.run_until_complete(asyncio.wait(unfinished2))</span><br><span class="line"><span class="comment"># 获取第三阶段已完成的协程返回值</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished3:</span><br><span class="line">    print(t.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished3))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">coro-5 started at:23:40</span><br><span class="line">coro-1 started at:23:40</span><br><span class="line">coro-6 started at:23:40</span><br><span class="line">coro-7 started at:23:40</span><br><span class="line">coro-2 started at:23:40</span><br><span class="line">coro-8 started at:23:40</span><br><span class="line">coro-3 started at:23:40</span><br><span class="line">coro-9 started at:23:40</span><br><span class="line">coro-4 started at:23:40</span><br><span class="line">首个完成的协程返回值： coro-1 done at:23:41</span><br><span class="line">还未结束的协程数量： 8</span><br><span class="line">coro-2 done at:23:42</span><br><span class="line">coro-3 done at:23:43</span><br><span class="line">coro-4 done at:23:44</span><br><span class="line">还未结束的协程数量： 5</span><br><span class="line">coro-6 done at:23:46</span><br><span class="line">coro-7 done at:23:47</span><br><span class="line">coro-9 done at:23:49</span><br><span class="line">coro-5 done at:23:45</span><br><span class="line">coro-8 done at:23:48</span><br><span class="line">还未结束的协程数量： 0</span><br></pre></td></tr></table></figure><p>从输出结果可以很清看出<code>asyncio.wait</code>很精确的控制协程运行过程，通过<code>wait(return_when=asyncio.FIRST_COMPLETED)</code>可拿到运行完成的协程，通过<code>wait(timeout)</code>控制指定时间后放回已完成的协程。</p><h5 id="2-5-嵌套协程的实现（协程内调用协程）"><a href="#2-5-嵌套协程的实现（协程内调用协程）" class="headerlink" title="2.5 嵌套协程的实现（协程内调用协程）"></a>2.5 嵌套协程的实现（协程内调用协程）</h5><h6 id="一种易于理解的调用异步函数的方式"><a href="#一种易于理解的调用异步函数的方式" class="headerlink" title="一种易于理解的调用异步函数的方式"></a>一种易于理解的调用异步函数的方式</h6><p>&#8195;&#8195;在介绍嵌套协程或者闭包协程、协程内调用协程的概念前，先看看普通函数内部调用普通函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func1</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func2</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func3</span>(<span class="params">data</span>):</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gel_all_data</span>():</span></span><br><span class="line">    result1=func1(<span class="string">&#x27;foo&#x27;</span>)</span><br><span class="line">    result2=func2(result1)</span><br><span class="line">    result3=func3(result2)</span><br><span class="line">    <span class="keyword">return</span> (result1,result2,result3)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>&#8195;&#8195;在同步的编程思维下，大家很容易理解get_all_data函数内部调用func1等三个外部函数来获取相应返回值，其实将同步改为异步的过程很简单：<br>==在每个函数前面使用关键字async向python解释器声明这是异步函数，如果需要调用外部异步函数，需使用await关键字==，将上面的同步编程改成异步编程的模式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func1</span>(<span class="params">start_data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>) <span class="comment"># 要使用asyncio的异步sleep方法，它会让出线程控制权给其他协程，而内建的sleep为同步性质</span></span><br><span class="line">    <span class="keyword">return</span> start_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func2</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">func3</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data*<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_all_data</span>():</span></span><br><span class="line">    result1=<span class="keyword">await</span> func1(<span class="string">&#x27;foo&#x27;</span>) <span class="comment"># 在异步函数内部，使用await关键字调用其他异步函数，并获取该异步函数的返回值。执行流会在此将当前线程控权让出</span></span><br><span class="line">    result2=<span class="keyword">await</span> func2(result1)<span class="comment"># 同上</span></span><br><span class="line">    result3=<span class="keyword">await</span> func3(result2) <span class="comment"># 同上</span></span><br><span class="line">    <span class="keyword">return</span>(result1,result2,result3)</span><br></pre></td></tr></table></figure><p>该异步的get_all_data其实要实现的需求为：一个协程内部调用其他协程，而且可以将返回值放置在不同的协程上，可以实现链式的协程调度，这看起来就是一个协程任务流 。<br>当熟悉了这种异步的编程模式后，可以玩一些更为进阶的例子：</p><h6 id="协程嵌套示例"><a href="#协程嵌套示例" class="headerlink" title="协程嵌套示例"></a>协程嵌套示例</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">inner_coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">5</span>) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    coros=[inner_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros] </span><br><span class="line">    inner_tasks,pendings=<span class="keyword">await</span> asyncio.wait(tasks) <span class="comment"># 这句实现了协程中再调用协程</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    <span class="comment"># 使用asyncio.wait(tasks)可以在外层协程里面获取嵌套协程的运行返回值</span></span><br><span class="line">    <span class="keyword">for</span> task <span class="keyword">in</span> inner_tasks:</span><br><span class="line">        print(task.result())</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">outter_coro started at:14:52:59</span><br><span class="line">coro-0 started at:14:52:59</span><br><span class="line">coro-1 started at:14:52:59</span><br><span class="line">coro-2 started at:14:52:59</span><br><span class="line">coro-3 started at:14:52:59</span><br><span class="line"></span><br><span class="line">outter_coro done at:14:53:04</span><br><span class="line">coro-1 done at:14:53:04</span><br><span class="line">coro-3 done at:14:53:04</span><br><span class="line">coro-2 done at:14:53:04</span><br><span class="line">coro-0 done at:14:53:04</span><br></pre></td></tr></table></figure><p>可以看到外层协程和内层协程同时启动（当然外层协程函数最先执行），而且都在同一个时刻结束。<br>内层协程返回值只能在外程协程内部获取，能否在<br>loop.run_until_complete(outter_coro()) 之后，一次性获取协程返回值？ 需要改用asyncio.gather方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;outter_coro started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    coros=[inner_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">    tasks=[asyncio.ensure_future(coro) <span class="keyword">for</span> coro <span class="keyword">in</span> coros] </span><br><span class="line">    print(<span class="string">&#x27;outter_coro done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(get_time()))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">all_coro_result=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> all_coro_result:</span><br><span class="line">    print(t)</span><br><span class="line">    </span><br><span class="line">loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">outter_coro started at:58:44</span><br><span class="line">outter_coro done at:58:44</span><br><span class="line">coro-0 started at:58:44</span><br><span class="line">coro-1 started at:58:44</span><br><span class="line">coro-2 started at:58:44</span><br><span class="line">coro-3 started at:58:44</span><br><span class="line">coro-0 done at:58:46</span><br><span class="line">coro-1 done at:58:46</span><br><span class="line">coro-2 done at:58:46</span><br><span class="line">coro-3 done at:58:46</span><br></pre></td></tr></table></figure><p>从外层协程outter_coro的启动时刻和结束时刻都一样可以看出，outter_coro和return await asyncio.gather(*tasks)是异步执行的，且在outter_coro结束后，loop事件循环只需管理coro-0到coro-3这4个协程。</p><p>从以上两种嵌套协程返回值的写法，可以看到这样逻辑：</p><ul><li>外层协程直接返回 awaitable对象给loop，loop就可以在最后获取所有协程的返回值；<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>()</span></span><br><span class="line"><span class="function"><span class="title">return</span> <span class="title">await</span> <span class="title">asyncio</span>.<span class="title">wait</span>(<span class="params">tasks</span>) # 返回<span class="title">awaitable</span>对象给下文<span class="title">loop</span>，这里用<span class="title">asyncio</span>.<span class="title">wait</span>挂起所有协程</span></span><br><span class="line"><span class="function"></span></span><br><span class="line">done,pending=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">print(task.result())</span><br></pre></td></tr></table></figure></li><li>外层协程没有返回 awaitable对象给loop，loop无法获取所有协程的返回值，只能在外程协程里面获取所有协程返回值<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outter_coro</span>()</span></span><br><span class="line">done,pending=await asyncio.wait(tasks) # 没有返回awaitable对象给下文loop</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">print(task.result())</span><br><span class="line"></span><br><span class="line">loop.run_until_complete(outter_coro())</span><br></pre></td></tr></table></figure><h5 id="2-6-如何取消运行中协程"><a href="#2-6-如何取消运行中协程" class="headerlink" title="2.6 如何取消运行中协程"></a>2.6 如何取消运行中协程</h5>future（task）对象主要有以下几个状态：<br>pending、running、done、cancelled<br>创建future（task）的时候，task为pending状态:<br><code>tasks=[asyncio.ensure_future(coro) for coro in coros]</code><br>事件循环调用执行的时候且协程未结束时对应tsak为running状态，<br><code>loop.run_until_complete(outter_coro())</code><br>事件循环运行结束后，所有的task为done状态，<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">done,pending=loop.run_until_complete(outter_coro())</span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> done:</span><br><span class="line">print(task.result())</span><br></pre></td></tr></table></figure>那么最后一个cancelled状态如何实现呢？例如你想在某些协程未done之前将其cancel掉，如何处理？引用2.4章节的asyncio.wait例子说明：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time())</span><br><span class="line"></span><br><span class="line">tasks=[coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"></span><br><span class="line">first_complete,unfinished1=loop.run_until_complete(asyncio.wait(tasks,return_when=asyncio.FIRST_COMPLETED))</span><br><span class="line"><span class="comment"># 获取首个已结束的协程返回值,注意这里firt_complete是一个set()</span></span><br><span class="line"></span><br><span class="line">first_done_task=first_complete.pop()</span><br><span class="line">print(<span class="string">&#x27;首个完成的协程返回值：&#x27;</span>,first_done_task.result())</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished1))</span><br><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> finished2:</span><br><span class="line">    print(t.result())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> task <span class="keyword">in</span> unfinished2: <span class="comment"># 取消剩余未运行的task</span></span><br><span class="line">    print(<span class="string">&#x27;cancell unfinished task:&#x27;</span>,task,<span class="string">&#x27;==&gt;is canceled:&#x27;</span>,task.cancel())</span><br><span class="line"></span><br></pre></td></tr></table></figure>输出结果：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">coro-4 started at:17:47</span><br><span class="line">coro-5 started at:17:47</span><br><span class="line">coro-1 started at:17:47</span><br><span class="line">coro-6 started at:17:47</span><br><span class="line">coro-7 started at:17:47</span><br><span class="line">coro-2 started at:17:47</span><br><span class="line">coro-8 started at:17:47</span><br><span class="line">coro-3 started at:17:47</span><br><span class="line">coro-9 started at:17:47</span><br><span class="line">首个完成的协程返回值： coro-1 done at:17:48</span><br><span class="line">还未结束的协程数量： 8</span><br><span class="line">coro-2 done at:17:49</span><br><span class="line">coro-3 done at:17:50</span><br><span class="line">coro-4 done at:17:51</span><br><span class="line">还未结束的协程数量： 5</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br><span class="line">cancell unfinished task: &lt;Task pending coro&#x3D;&lt;coro() running at &#x2F;opt&#x2F;cancel_task.py:10&gt; wait_for&#x3D;&lt;Future cancelled&gt;&gt; &#x3D;&#x3D;&gt;is canceled: True</span><br></pre></td></tr></table></figure>从输出结果可看到，8个协程task并发运行，最早结束的是coro-1，接着是coro-2、coro-3、coro-4，因为设定asyncio.wait(unfinished1,timeout=3) 3秒超时，只要超过3秒后，loop返回这些未运行的task，接着再逐个取消，可以看到5个协程被取消，True表示当前协程取消成功。</li></ul><h5 id="2-7-理解loop的相关方法"><a href="#2-7-理解loop的相关方法" class="headerlink" title="2.7  理解loop的相关方法"></a>2.7  理解loop的相关方法</h5><h6 id="loop-run-until-complate-vs-loop-run-forever"><a href="#loop-run-until-complate-vs-loop-run-forever" class="headerlink" title="loop.run_until_complate vs loop.run_forever"></a>loop.run_until_complate vs loop.run_forever</h6><p>&#8195;&#8195;<code>loop.run_until_complate</code>可以在程序的不同位置多次调用，例如在2.4 <code>asyncio.gather vs asyncio.wait</code> 提到的<code>asyncio.wait </code>用法，同一程序中能出现多个<code>loop.run_until_complate</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将第一阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished2,unfinished2=loop.run_until_complete(asyncio.wait(unfinished1,timeout=<span class="number">3</span>))</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将第二阶段未完成的协程注册到loop里面</span></span><br><span class="line">finished3,unfinished3=loop.run_until_complete(asyncio.wait(unfinished2))</span><br><span class="line">print(<span class="string">&#x27;还未结束的协程数量：&#x27;</span>,<span class="built_in">len</span>(unfinished3))</span><br></pre></td></tr></table></figure><p>而对于 loop.run_forever，在同一程序中，只能有一个，因为该事件是在当前线程后台永久运行:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;start a coro&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;coro done&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro())</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.run_forever() <span class="comment"># 程序不会退出，loop一直挂在这里，等待其他future对象</span></span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro()) <span class="comment">#程序没有报错，但执行流永远不会到达这里，该句永远不会运行</span></span><br><span class="line">loop.run_forever() <span class="comment"># 程序没有报错，但执行流永远不会到达这里,该语句永远不会运行 </span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start a coro</span><br><span class="line">coro done</span><br><span class="line">,,,,</span><br><span class="line"># 等待程序退出</span><br></pre></td></tr></table></figure><h6 id="loop-stop"><a href="#loop-stop" class="headerlink" title="loop.stop()"></a>loop.stop()</h6><p>上面提到如果程序仅有loop.run_forever()，那么当future完成后，程序一直没有退出，若要求实现当future完成后，程序也需要正常退出，可以这样处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span>  datetime</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">n</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;start a coro at &#x27;</span>,get_time())</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(n)</span><br><span class="line">    print(<span class="string">&#x27;coro done at &#x27;</span>,get_time())</span><br><span class="line"></span><br><span class="line">future_obj=asyncio.ensure_future(coro(<span class="number">2</span>))</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.stop() <span class="comment"># 在run_forever()前，先stop</span></span><br><span class="line">print(<span class="string">&#x27;stop后，loop事件还在运行？ at &#x27;</span>,get_time())</span><br><span class="line">loop.run_forever() </span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop后，loop事件还在运行？ at  43:11</span><br><span class="line">start a coro at  43:11</span><br></pre></td></tr></table></figure><p>从输出可以看到，上面的示例代码都是异步并发运行。<br><code>print(&#39;stop后，loop事件还在运行？ at &#39;,get_time())</code>语句跟future任务同时运行<br>==注意：如果把loop.stop()方法放在run_forever后面，可预见，程序不会退出==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">loop.run_forever() </span><br><span class="line">loop.stop() <span class="comment"># 执行流永远不会到达这一句</span></span><br><span class="line">print(<span class="string">&#x27;stop后，loop事件还在运行？ at &#x27;</span>,get_time()) <span class="comment"># 执行流永远不会到达这一句</span></span><br></pre></td></tr></table></figure><h6 id="loop-call-soon-loop-call-later"><a href="#loop-call-soon-loop-call-later" class="headerlink" title="loop.call_soon/loop.call_later"></a>loop.call_soon/loop.call_later</h6><p>这两个方法用于在异步函数里面调用同步函数(普通函数)，且可以实现立刻调用或者稍后调用：<br><code>loop.call_soon(callback, *args, context=None）</code>: 立刻调用，并返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">name,stat=<span class="number">1</span></span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>,name,<span class="string">&#x27;keyword args:&#x27;</span>,stat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">loop</span>):</span></span><br><span class="line">    loop.call_soon(callback,<span class="string">&#x27;get first callback&#x27;</span>)</span><br><span class="line">    wrapper_func=functools.partial(callback,stat=<span class="number">2</span>)</span><br><span class="line">    loop.call_soon(wrapper_func,<span class="string">&#x27;get second call back&#x27;</span>)</span><br><span class="line"></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    loop.run_until_complete(run(loop))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><p>打印：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">args: get first callback keyword args: 1</span><br><span class="line">args: get second call back keyword args: 2</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;异步调用同步函数其实已经破坏了异步的并发机制，因此很少使用这些非异步的方法。<br>&#8195;&#8195;此外loop.call_soon不支持协程函数传入关键字，因此可以通过偏函数先把关键字参数”传入“callback的kwargs里面，之后在call_soon里面，就可以利用这个被”包装过的callback“再传入位置参数即可(loop.run_until_complete传入关键字参数也一样这么处理)</p><p><code>loop.call_later(delay, callback, *args, context=None) </code>： 再给定一个时间之后，再调用callback。context默认当前线程的上下文</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">name,stat=<span class="number">1</span></span>):</span></span><br><span class="line">    print(<span class="string">&#x27;args:&#x27;</span>,name,<span class="string">&#x27;keyword args:&#x27;</span>,stat)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">loop</span>):</span></span><br><span class="line">    loop.call_later(<span class="number">2</span>,callback,<span class="string">&#x27;get first callback&#x27;</span>)</span><br><span class="line">    loop.call_soon(callback,<span class="string">&#x27;callback soon&#x27;</span>)    </span><br><span class="line">    wrapper_func=functools.partial(callback,stat=<span class="number">0</span>)</span><br><span class="line">    loop.call_later(<span class="number">1</span>,wrapper_func,<span class="string">&#x27;get second callback&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>) <span class="comment"># 这里如果不设sleep，那么call_soon执行后loop马上退出，导致2个有延时运行的callback也退出了。这里要大于等于delay时间最长的call_later</span></span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    loop.run_until_complete(run(loop))</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><p>打印：可以看到call_soon最先完成回调，接着才是设为1秒后运行的回调，2秒的回调</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">args: callback soon keyword args: 1</span><br><span class="line">args: get second callback keyword args: 0</span><br><span class="line">args: get first callback keyword args: 1</span><br></pre></td></tr></table></figure><p>同样，该loop.call_later不常用。</p><h4 id="3、asyncio进阶用法"><a href="#3、asyncio进阶用法" class="headerlink" title="3、asyncio进阶用法"></a>3、asyncio进阶用法</h4><p>&#8195;&#8195;在上面的例子中，一个主线程创建一个永久事件循环（该永久事件不会自动退出，而是run forever，除非主线程运行后没有阻塞或者手动中断程序运行），再把所有的协程注册到该永久事件循环，该方式较为基础用法的异步模式。在这一节，🔚asyncio高级用法，用线程1创建一个forever事件循环，线程2可以向事件循环中动态添加协程，而且不受主线程阻塞。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_another_loop</span>(<span class="params">loop</span>):</span></span><br><span class="line">    asyncio.set_event_loop(loop)</span><br><span class="line">    loop.run_forever()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">task_id</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">    print(<span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,get_time()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">new_loop = asyncio.new_event_loop()</span><br><span class="line"><span class="comment"># start一个新的线程1，用于启动一个永久事件循环</span></span><br><span class="line">t = threading.Thread(target=start_another_loop,args=(new_loop,))</span><br><span class="line">t.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前线程向线程1的loop注册tasks</span></span><br><span class="line">asyncio.run_coroutine_threadsafe(read_file(<span class="number">5</span>),new_loop)</span><br><span class="line">asyncio.run_coroutine_threadsafe(read_file(<span class="number">5</span>),new_loop)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">coro-5 started at:09:30</span><br><span class="line">coro-2 started at:09:30</span><br><span class="line">coro-2 done at:09:35</span><br><span class="line">coro-5 done at:09:35</span><br></pre></td></tr></table></figure><p>这个动态添加协程task对象有何用？如果task的参数是从redis队列实时取得，然后交由run_coroutine_threadsafe向loop注册协程，那么不就实现基于协程producer-consumer模式。</p><h5 id="利用redis-队列实现loop循环事件动态添加协程"><a href="#利用redis-队列实现loop循环事件动态添加协程" class="headerlink" title="利用redis 队列实现loop循环事件动态添加协程"></a>利用redis 队列实现loop循环事件动态添加协程</h5><p>&#8195;&#8195;这种方式，可以实现并发模式，producer：向redis 队列push 数据(这个数据是指协程task需要的参数，例如sleep的)，consumer：使用asyncio.run_coroutine_threadsafe(read_file(msg),new_loop)不断消费producer的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> datetime,time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCoro</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host=<span class="string">&#x27;127.0.0.1&#x27;</span>,port=<span class="number">6379</span>,key=<span class="string">&#x27;coro_queue&#x27;</span>,max_redis_conns=<span class="number">1000</span>,semaphore=<span class="number">2</span></span>):</span></span><br><span class="line">        self.r_pool=redis.ConnectionPool(host=host,port=port,max_connections=max_redis_conns)</span><br><span class="line">        self.r_conn=redis.Redis(connection_pool=self.r_pool)</span><br><span class="line">        self.r_queue_key=key</span><br><span class="line">        self.semaphore=semaphore</span><br><span class="line">        self.new_loop=asyncio.new_event_loop()</span><br><span class="line">        self.start()      </span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">self,task_id</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; 协程的worker，这里模拟IO耗时操作 &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&#x27;coro-&#123;&#125; started at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,self.get_time()))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(task_id) <span class="comment"># 模拟读取文件的耗时IO</span></span><br><span class="line">        print(<span class="string">&#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(task_id,self.get_time()))</span><br><span class="line">        <span class="comment">#return &#x27;coro-&#123;&#125; done at:&#123;&#125;&#x27;.format(task_id,get_time()</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forever_loop</span>(<span class="params">self,loop_obj</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于主线程启动一个永久事件循环，接收来自另外一个线程注册的协程对象&quot;&quot;&quot;</span></span><br><span class="line">        asyncio.set_event_loop(loop_obj)</span><br><span class="line">        loop_obj.run_forever()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_forever_loop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用一个主线程去启动一个永久事件循环&quot;&quot;&quot;</span></span><br><span class="line">        t=threading.Thread(target=self.forever_loop,args=(self.new_loop,))</span><br><span class="line">        t.start()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forever_consumer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;由另外一个子线层启动，该线程不断从redis队列获取数据，并用run_coroutine_threadsafe不断向new_loop注册task对象&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            task_id=self.r_conn.rpop(self.r_queue_key)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> task_id:</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            task_id=task_id.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            asyncio.run_coroutine_threadsafe(self.coro(<span class="built_in">int</span>(task_id)),self.new_loop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_forever_consumer</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;用一个子线程用于向事件循环注册协程对象 &quot;&quot;&quot;</span></span><br><span class="line">        t=threading.Thread(target=self.forever_consumer)</span><br><span class="line">        t.start()</span><br><span class="line">        t.join()<span class="comment"># 这里要阻塞当前线程，否则就无法实现不但从redis队列获取任务了。若不阻塞，主线程start()后，子线程start（）后，程序立即结束</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;在一个方法里面，同时启动两个线程，简化api&quot;&quot;&quot;</span></span><br><span class="line">        self.start_forever_loop()</span><br><span class="line">        self.forever_consumer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    coro=MyCoro() </span><br><span class="line">    coro.start()</span><br></pre></td></tr></table></figure><p>以上代码的逻辑：<br>创建两个线程，</p><ul><li>线程1负责启动一个forever事件循环，用于接收另外一个线程2注册的协程对象（task对象）</li><li>线程2负责不断从redis队列获取任务数据后，再把协程注册到线程1启动的事件循环，从而实现loop循环事件动态添加协程。<br>该例子只需要2个线程，即可实现高并发模式。</li></ul><p>测试：在redis-cli里面，先lpush一个10，再lpush1个2秒，1个4秒，最终线程2按顺序创建3个线程，都会注册到loop里面，注意对比3个协程的完成时间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># redis-cli 在coro_queue队列添加数据</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 10</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 2</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH coro_queue 4</span><br><span class="line">(integer) 1</span><br><span class="line"></span><br><span class="line"># 程序输出</span><br><span class="line">coro-10 started at:56:48</span><br><span class="line">coro-2 started at:56:50</span><br><span class="line">coro-4 started at:56:51</span><br><span class="line">coro-2 done at:56:52</span><br><span class="line">coro-4 done at:56:55</span><br><span class="line">coro-10 done at:56:58</span><br></pre></td></tr></table></figure><p>从打印的时刻可以很清楚看到，3个协程并发执行，总共10秒完成。若同步模式，则需要2+4+10=16秒才能完成。</p><h4 id="4、asyncio最适合的使用场景"><a href="#4、asyncio最适合的使用场景" class="headerlink" title="4、asyncio最适合的使用场景"></a>4、asyncio最适合的使用场景</h4><p>&#8195;&#8195;从redis、Nginx、node.js、Tornado、Twisted这些使用IO多路复用技术的中间件或者框架，可以很明确的推出结论：异步逻辑非常适合处理有Network IO且高并发的socket连接场景，因为这些场景往往需要等待IO，例如：访问web接口数据、访问网页、访问数据库，都是client向server发起网络IO。因此本节给出asyncio的3个场景：异步爬虫，高并发的socket服务、数据库连接。<br>&#8195;&#8195;但是：asyncio的周边库似乎有不少坑，而且距离稳定生产环境有一定距离，参考<a href="https://www.zhihu.com/question/266094857/answer/304655007">知乎文章吐槽的asyncio</a>，所有很多文章介绍asyncio基本使用场合，或者自行开发的小工具，很少文章能给出一个使用asyncio实现的复杂项目。当然，协程肯定不适合CPU计算场景。<br>&#8195;&#8195;目前star较高的几个协程异步库有：<a href="https://github.com/aio-libs/aiohttp">aiohttp</a>、<a href="https://github.com/aio-libs/aioredis">aioredis</a>、<a href="https://github.com/aio-libs/aiomysql">aiomysql</a>、<a href="https://github.com/aio-libs/aiopg">aiopg</a>。aiopg:is a library for accessing a PostgreSQL database from the asyncio 。以上四个协程异步库底层通过封装asyncio实现。本节主要介绍aioredi和aiohttp，其他库可参考官方示例。</p><h5 id="4-1-aioredis"><a href="#4-1-aioredis" class="headerlink" title="4.1 aioredis"></a>4.1 aioredis</h5><p>&#8195;&#8195;aioredis实现的api很丰富，支持sentinel连接，运行原生redis命令的接口，但是它不支持cluster集群的连接：<code>Current release (1.3.0) of the library does not support Redis Cluster in a full manner.</code><br>单个连接示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">coro</span>(<span class="params">key,value</span>):</span></span><br><span class="line">    redis = <span class="keyword">await</span> aioredis.create_redis(</span><br><span class="line">        <span class="string">&#x27;redis://127.0.0.1&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> redis.<span class="built_in">set</span>(key,value)</span><br><span class="line">    val = <span class="keyword">await</span> redis.get(key)</span><br><span class="line">    redis.close()</span><br><span class="line">    <span class="keyword">await</span> redis.wait_closed()</span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure><p>下面使用单例模式和协程with协议，实现基本的async redis 类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AsynRedis</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    _instance = <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,redis_uri=<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,pool=<span class="literal">False</span>,max_conn=<span class="number">100</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span></span>):</span></span><br><span class="line">        self._redis_uri = redis_uri</span><br><span class="line">        self._encoding = encoding</span><br><span class="line">        self._pool=pool</span><br><span class="line">        self._max_conn=max_conn</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">__aenter__</span>(<span class="params">self</span>):</span> <span class="comment"># with协议入口</span></span><br><span class="line">        <span class="keyword">await</span> self.get_conn()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">__aexit__</span>(<span class="params">self, exc_type, exc, tb</span>):</span> <span class="comment">#with 协议出口</span></span><br><span class="line">        <span class="keyword">await</span> self.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_conn</span>(<span class="params">self</span>):</span> <span class="comment"># 单例模式创建redis连接，可选pool或者单连接</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._instance:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._pool:</span><br><span class="line">                self._instance = <span class="keyword">await</span> aioredis.create_redis(self._redis_uri)</span><br><span class="line">            self._instance=<span class="keyword">await</span> aioredis.create_redis_pool(self._redis_uri,maxsize=self._max_conn)</span><br><span class="line">        <span class="keyword">return</span> self._instance</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">asyn_set</span>(<span class="params">self,*args,**kwargs</span>):</span> </span><br><span class="line">        response=<span class="keyword">await</span> self._instance.<span class="built_in">set</span>(*args,**kwargs)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">asyn_get</span>(<span class="params">self,key</span>):</span></span><br><span class="line">        value=<span class="keyword">await</span> self._instance.get(key,encoding=self._encoding)</span><br><span class="line">        <span class="keyword">return</span> value </span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._instance:</span><br><span class="line">            self._instance.close()</span><br><span class="line">            <span class="keyword">await</span> self._instance.wait_closed()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_coro</span>(<span class="params">index</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsynRedis() <span class="keyword">as</span> f: <span class="comment"># 异步的aenter和aexit实现with协议</span></span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> start at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        key=<span class="string">&#x27;foo-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index)</span><br><span class="line">        result=<span class="keyword">await</span> f.asyn_set(key,get_time()) <span class="comment"># 将时刻作为value，用于观察协程并发,获取set操作返回值，True表示set成功。</span></span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,key is set? <span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    tasks=[redis_coro(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">coro-9 start at 57:15</span><br><span class="line">coro-4 start at 57:15</span><br><span class="line">coro-2 start at 57:15</span><br><span class="line">coro-5 start at 57:15</span><br><span class="line">coro-0 start at 57:15</span><br><span class="line">coro-8 start at 57:15</span><br><span class="line">coro-6 start at 57:15</span><br><span class="line">coro-7 start at 57:15</span><br><span class="line">coro-3 start at 57:15</span><br><span class="line">coro-1 start at 57:15</span><br><span class="line"></span><br><span class="line">coro-9 done at 57:16,key is set? True</span><br><span class="line">coro-4 done at 57:16,key is set? True</span><br><span class="line">coro-2 done at 57:16,key is set? True</span><br><span class="line">coro-5 done at 57:16,key is set? True</span><br><span class="line">coro-0 done at 57:16,key is set? True</span><br><span class="line">coro-8 done at 57:16,key is set? True</span><br><span class="line">coro-6 done at 57:16,key is set? True</span><br><span class="line">coro-7 done at 57:16,key is set? True</span><br><span class="line">coro-3 done at 57:16,key is set? True</span><br><span class="line">coro-1 done at 57:16,key is set? True</span><br></pre></td></tr></table></figure><p>可以看到10个redis协程同一时刻并发set key，并且同一时刻完成。<br>在redis-cli查看key，所有的key都value都是同一时刻，说明协程并发运行正确，而在多线程方式，则需要创建<code>10个线程</code>才可以实现<code>单线程+10协程</code>的效果，若当并发量高达1万+时，可以想象多线程将消耗大量系统资源以及线程切换，效率必然不高。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys foo*</span><br><span class="line"> 1) &quot;foo-8&quot;</span><br><span class="line"> 2) &quot;foo-3&quot;</span><br><span class="line"> 3) &quot;foo-9&quot;</span><br><span class="line"> 4) &quot;foo-0&quot;</span><br><span class="line"> 5) &quot;foo-1&quot;</span><br><span class="line"> 6) &quot;foo-7&quot;</span><br><span class="line"> 7) &quot;foo-5&quot;</span><br><span class="line"> 8) &quot;foo-6&quot;</span><br><span class="line"> 9) &quot;foo-2&quot;</span><br><span class="line">10) &quot;foo-4&quot;</span><br><span class="line">127.0.0.1:6379&gt; get foo-8</span><br><span class="line">&quot;57:16&quot;</span><br><span class="line">127.0.0.1:6379&gt; get foo-0</span><br><span class="line">&quot;57:16&quot;</span><br></pre></td></tr></table></figure><p>这里用两个魔法方法<code>__aenter__</code>,<code>__aexit__</code>实现with协议，但要注意的是，协程的with方法只能在协程函数内部使用，这个with的上下文就是该协程的上下文。如果写在主线程外部，则提示语法出错：<br><img src="https://img-blog.csdnimg.cn/20200104125027492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这是因为协程上下文代表协程自己的栈等信息，肯定不是主线程的上下文，所以不能把<code>async with</code>写在程序的全局位置<br>注意：aioredis似乎有个bug，当MacOS系统的文件描述最大限制已设为10000，aioredis不管使用单连接或者pool方式，当并发数设为大值例如1000，部分协程完成后，剩余部分协程都会被阻塞（暂未找到原因）。</p><p>在项目中，如果要使用aioredis，可以用asyncio的semaphore信号量限制并发数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aioredis</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_coro</span>(<span class="params">semaphore,index</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> semaphore:</span><br><span class="line">        r=<span class="keyword">await</span> aioredis.create_redis(<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,db=<span class="number">0</span>)</span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> start at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">0.05</span>)</span><br><span class="line">        key=<span class="string">&#x27;bar-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(index)</span><br><span class="line">        result=<span class="keyword">await</span> r.<span class="built_in">set</span>(key,get_time(),expire=<span class="number">100</span>)</span><br><span class="line">        print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,key is set? <span class="subst">&#123;result&#125;</span>&#x27;</span>)    </span><br><span class="line">        r.close()</span><br><span class="line">        <span class="keyword">await</span> r.wait_closed()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">max_task=<span class="number">100</span></span>):</span></span><br><span class="line">    sem=asyncio.Semaphore(<span class="number">10</span>)</span><br><span class="line">    tasks=[redis_coro(sem,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_task)]</span><br><span class="line">    <span class="keyword">await</span> asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(run())</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><h5 id="4-1-aiohttp"><a href="#4-1-aiohttp" class="headerlink" title="4.1 aiohttp"></a>4.1 aiohttp</h5><p>&#8195;&#8195;Asynchronous HTTP client/server framework for asyncio and Python<br>aiohttp应该是除了request库外最强大的HTTP库，而且是异步实现，三个主要功能：</p><ul><li>Supports both Client and HTTP Server.</li><li>Supports both Server WebSockets and Client WebSockets out-of-the-box without the Callback Hell.</li><li>Web-server has Middlewares, Signals and pluggable routing.<br>除了基本client和server端服务，还支持websocket、web-server模块还支持可插拔的中间件，官方也给了详细的aiohttp demo代码，<a href="https://github.com/aio-libs/aiohttp-demos">链接</a></li></ul><p>以下代码逻辑：使用协程并发get html页面，并使用协程方式存储html到文件或者存到redis</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime,time</span><br><span class="line"><span class="keyword">import</span> asyncio,aiohttp,aioredis,aiofiles</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> d.strftime(<span class="string">&#x27;%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">save_to_file</span>(<span class="params">html,index</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;将html文本存放到本地目录，使用异步aiofiles实现。&quot;&quot;&quot;</span></span><br><span class="line">    file_name=<span class="string">f&#x27;/opt/test_aiohttp/html-<span class="subst">&#123;index&#125;</span>.txt&#x27;</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(file_name,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">await</span> f.write(html)</span><br><span class="line">    print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">save_to_redis</span>(<span class="params">html,index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将html文本存放到redis，采用协程模式 &quot;&quot;&quot;</span></span><br><span class="line">    r=<span class="keyword">await</span> aioredis.create_redis(<span class="string">&#x27;redis://127.0.0.1&#x27;</span>,db=<span class="number">0</span>)</span><br><span class="line">    coro_key=<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span>&#x27;</span></span><br><span class="line">    result=<span class="keyword">await</span> r.<span class="built_in">set</span>(coro_key,html,expire=<span class="number">60</span>)</span><br><span class="line">    print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> done at <span class="subst">&#123;get_time()&#125;</span>,is saved? <span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line">    r.close()</span><br><span class="line">    <span class="keyword">await</span> r.wait_closed()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_html</span>(<span class="params">semaphore,session,url,index,container=<span class="string">&#x27;file&#x27;</span></span>):</span></span><br><span class="line">   <span class="string">&quot;&quot;&quot;获取url对应的html文本，并调用存放文本的协程，这里就是前面章节提到的嵌套协层，用await调用外部协程 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> semaphore:<span class="comment"># 由外部传入的信号量，控制并发数</span></span><br><span class="line">            print(<span class="string">f&#x27;coro-<span class="subst">&#123;index&#125;</span> started at <span class="subst">&#123;get_time()&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">with</span> session.get(url,timeout=<span class="number">5</span>) <span class="keyword">as</span> response:</span><br><span class="line">                    <span class="keyword">if</span> response.status==<span class="number">200</span>:</span><br><span class="line">                        <span class="keyword">if</span> container==<span class="string">&#x27;file&#x27;</span>:</span><br><span class="line">                            container_=save_to_file</span><br><span class="line">                        <span class="keyword">else</span>:container_=save_to_redis</span><br><span class="line">                        html_text=<span class="keyword">await</span> response.read()</span><br><span class="line">                        <span class="keyword">await</span> container_(html_text,index)</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">semaphore,url,max_workers=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        urls=[url]*max_workers</span><br><span class="line">        tasks=[asyncio.ensure_future(get_html(semaphore,session,each_url,index)) <span class="keyword">for</span> index,each_url <span class="keyword">in</span> <span class="built_in">enumerate</span>(urls)]</span><br><span class="line">        <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop=asyncio.get_event_loop()</span><br><span class="line">    semaphore=asyncio.Semaphore(<span class="number">5</span>)</span><br><span class="line">    loop.run_until_complete(run(semaphore,<span class="string">&#x27;http://spark.apachecn.org/#/&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 官方推荐使用Zero-sleep保证底层的一些socket连接完成关闭</span></span><br><span class="line">    loop.run_until_complete(asyncio.sleep(<span class="number">0</span>))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><p>打印结果：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">coro-0 started at 49:26</span><br><span class="line">coro-1 started at 49:26</span><br><span class="line">coro-2 started at 49:26</span><br><span class="line">coro-3 started at 49:26</span><br><span class="line">coro-4 started at 49:26</span><br><span class="line"></span><br><span class="line">coro-0 done at 49:26</span><br><span class="line">coro-5 started at 49:26</span><br><span class="line">coro-2 done at 49:26</span><br><span class="line">coro-6 started at 49:26</span><br><span class="line">coro-4 done at 49:26</span><br><span class="line">coro-7 started at 49:26</span><br><span class="line">coro-1 done at 49:26</span><br><span class="line">coro-8 started at 49:26</span><br><span class="line">coro-3 done at 49:26</span><br><span class="line">coro-9 started at 49:26</span><br><span class="line"></span><br><span class="line">coro-6 done at 49:26</span><br><span class="line">coro-5 done at 49:27</span><br><span class="line">coro-7 done at 49:27</span><br><span class="line">coro-8 done at 49:27</span><br><span class="line">coro-9 done at 49:27</span><br></pre></td></tr></table></figure><p>这里使用asyncio.Semaphore(5)控制并发数，从输出可以看出，一开始有5个协程启动，最先5个协程运行结束后，另外5个协程同时启动。因为10个任务，分了2轮进行。</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本文内容相对较多且杂，主要是asyncio协程库有较多api，这些api与同步编程的python库有较大的区别，结合asyncio实现的几个第三方协层库来看，可以看到协程在python生态位置较为小众，如果在项目（例如高性能web接口服务）中引入协程异步编程，可以考虑Tornado以及Twisted。（想想Go语言还是相当强大，协程生态成熟）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面的文章，已经通过gevent实现高并发的协程，本文将详细讨论Python标准库异步IO——asyncio。在Python3.4中引入了协程的概念以及asyncio。asyncio底层调用yield from语法，将任务变成生成器后挂起，这种方式无法实现协程之间的自动切换，在Python3.5中正式确立引入了async和await 的语法，所有的这些工作都使得Python实现异步编程变得更容易上手。&lt;/p&gt;</summary>
    
    
    
    <category term="Python进阶" scheme="https://yield-bytes.gitee.io/blog/categories/Python%E8%BF%9B%E9%98%B6/"/>
    
    
    <category term="协程" scheme="https://yield-bytes.gitee.io/blog/tags/%E5%8D%8F%E7%A8%8B/"/>
    
    <category term="asyncio" scheme="https://yield-bytes.gitee.io/blog/tags/asyncio/"/>
    
  </entry>
  
  <entry>
    <title>gevent与协程</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/28/gevent%E4%B8%8E%E5%8D%8F%E7%A8%8B/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/28/gevent%E4%B8%8E%E5%8D%8F%E7%A8%8B/</id>
    <published>2019-12-28T06:42:52.000Z</published>
    <updated>2020-02-03T07:01:40.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1、-yield-实现协程"><a href="#1、-yield-实现协程" class="headerlink" title="1、 yield 实现协程"></a>1、 yield 实现协程</h4><h5 id="1-1-yield-同步执行"><a href="#1-1-yield-同步执行" class="headerlink" title="1.1 yield 同步执行"></a>1.1 yield 同步执行</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>():</span></span><br><span class="line">    send_msg=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer</span></span><br><span class="line">        output=<span class="keyword">yield</span> send_msg</span><br><span class="line">        print(<span class="string">&#x27;[consumer] consuming &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(output))</span><br><span class="line">        send_msg=<span class="string">&#x27;ok&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">consumer_obj,num</span>):</span></span><br><span class="line">    <span class="comment"># 1、启动consumer()生成器</span></span><br><span class="line">    <span class="built_in">next</span>(consumer_obj)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,num+<span class="number">1</span>):</span><br><span class="line">        print(<span class="string">&#x27;[producer] producing &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、通过send()切换到consumer()执行</span></span><br><span class="line">        receive_msg=consumer_obj.send(i)</span><br><span class="line">        print(<span class="string">&#x27;[producer] received a message &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(receive_msg))</span><br><span class="line">    consumer_obj.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    c=consumer()</span><br><span class="line">    producer(c,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[producer] producing <span class="number">1</span></span><br><span class="line">[consumer] consuming <span class="number">1</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">2</span></span><br><span class="line">[consumer] consuming <span class="number">2</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">3</span></span><br><span class="line">[consumer] consuming <span class="number">3</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">4</span></span><br><span class="line">[consumer] consuming <span class="number">4</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line">[producer] producing <span class="number">5</span></span><br><span class="line">[consumer] consuming <span class="number">5</span></span><br><span class="line">[producer] received a message ok</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>整个过程无锁，由一个线程执行，producer和consumer协作完成任务，但是以上无法实现并发，生产1个，消费1个，也即1个生产者对应1个消费者</p><h5 id="1-2-启动多个yield模拟consumer并发"><a href="#1-2-启动多个yield模拟consumer并发" class="headerlink" title="1.2 启动多个yield模拟consumer并发"></a>1.2 启动多个yield模拟consumer并发</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time,datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span>():</span></span><br><span class="line">    d=datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>(<span class="params">consumer_index</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;consumer-&#123;&#125; started at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(consumer_index,get_time()))</span><br><span class="line">    send_msg=<span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 消费者保持监听producer的发来的信息</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer</span></span><br><span class="line">        output=<span class="keyword">yield</span> send_msg</span><br><span class="line">        print(<span class="string">&#x27;[consumer-&#123;&#125;] consuming &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(consumer_index,output,get_time()))</span><br><span class="line">        time.sleep(<span class="number">1</span>) <span class="comment"># 模拟IO耗时操作</span></span><br><span class="line">        send_msg=<span class="string">&#x27;ack&#x27;</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">consumer_obj,consumer_num,count</span>):</span></span><br><span class="line">    <span class="comment"># 1、启动n个consumer()生成器，相当于用协程方式模拟并发</span></span><br><span class="line">    consumers=[consumer_obj(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(consumer_num) ]</span><br><span class="line">    <span class="keyword">for</span> each_cons <span class="keyword">in</span> consumers:</span><br><span class="line">        <span class="built_in">next</span>(each_cons)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count):</span><br><span class="line">        print(<span class="string">&#x27;[producer] producing &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i,get_time()))</span><br><span class="line">        <span class="comment"># 2、对每个consumer_obj使用send()切换到consumer()执行</span></span><br><span class="line">        <span class="keyword">for</span> index,each_cons <span class="keyword">in</span> <span class="built_in">enumerate</span>(consumers):</span><br><span class="line">            receive_msg=each_cons.send(i)</span><br><span class="line">            print(<span class="string">&#x27;[producer] received &#123;&#125; from consumer-&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(receive_msg,index,get_time()))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">for</span> each_cons <span class="keyword">in</span> consumers:</span><br><span class="line">        each_cons.close()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    producer(consumer,<span class="number">5</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>1个producer，5个consumer</p><p>输出</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">consumer-0 started at 21:12:45</span><br><span class="line">consumer-1 started at 21:12:45</span><br><span class="line">consumer-2 started at 21:12:45</span><br><span class="line">consumer-3 started at 21:12:45</span><br><span class="line">consumer-4 started at 21:12:45</span><br><span class="line">[producer] producing 0 at 21:12:45</span><br><span class="line">[consumer-0] consuming 0 at 21:12:45</span><br><span class="line">[producer] received ack from consumer-0</span><br><span class="line">[consumer-1] consuming 0 at 21:12:46</span><br><span class="line">[producer] received ack from consumer-1</span><br><span class="line">[consumer-2] consuming 0 at 21:12:47</span><br><span class="line">[producer] received ack from consumer-2</span><br><span class="line">[consumer-3] consuming 0 at 21:12:48</span><br><span class="line">[producer] received ack from consumer-3</span><br><span class="line">[consumer-4] consuming 0 at 21:12:49</span><br><span class="line">[producer] received ack from consumer-4</span><br><span class="line">[producer] producing 1 at 21:12:50</span><br><span class="line">[consumer-0] consuming 1 at 21:12:50</span><br><span class="line">[producer] received ack from consumer-0</span><br><span class="line">[consumer-1] consuming 1 at 21:12:51</span><br><span class="line">[producer] received ack from consumer-1</span><br><span class="line">[consumer-2] consuming 1 at 21:12:52</span><br><span class="line">[producer] received ack from consumer-2</span><br><span class="line">[consumer-3] consuming 1 at 21:12:53</span><br><span class="line">[producer] received ack from consumer-3</span><br><span class="line">[consumer-4] consuming 1 at 21:12:54</span><br><span class="line">[producer] received ack from consumer-4</span><br></pre></td></tr></table></figure><p>以上运行过程确实是协程运行，但yield无法自动切换协程，上面的运行过程打印出的实际可以发现代码同步执行：<br>5个consumer同时启动，当producer生产1个数据，consumer-0消费数据，而consumer内部有IO耗时操作（time.sleep(1)模拟IO），此时代码逻辑没有把线程当前控制权从consumer-0自动切换到consumer-1，consumer-1等待前面1秒后，才能接着干活。</p><h4 id="2-、greenlet实现的协程"><a href="#2-、greenlet实现的协程" class="headerlink" title="2 、greenlet实现的协程"></a>2 、greenlet实现的协程</h4><h5 id="2-1-简单gevent协程例子"><a href="#2-1-简单gevent协程例子" class="headerlink" title="2.1 简单gevent协程例子"></a>2.1 简单gevent协程例子</h5><p>&#8195;&#8195;greenlet是一个用C实现的协程模块，相比于上面使用python的yield实现协程，greenlet可以无需将函数声明为generator的前提下，用手动方式在任意函数之间切换。但在实际使用，往往不会直接使用greenlet，因为它遇到有IO地方是不会自动切换，而Gevent库可以实现这个需求，gevent是对greenlet的封装，实现自动切换，大体的设计逻辑如下：<br>&#8195;&#8195;当一个greenlet（你可以认为这个greenlet是一个协程对象，类比于线程对象thread）遇到IO操作时，比如访问读取文件或者网络socket连接，它会自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换原来位置继续执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent,datetime</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f1 started at&#x27;</span>,get_time())</span><br><span class="line">    <span class="comment"># gevent模拟IO耗时操作，并且gevent会在此保留现场后自动切换到其它函数</span></span><br><span class="line">    gevent.sleep(<span class="number">4</span>) </span><br><span class="line">    print(<span class="string">&#x27;f1 done at&#x27;</span>,get_time())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f2 started at&#x27;</span>,get_time())</span><br><span class="line">    gevent.sleep(<span class="number">2</span>) </span><br><span class="line">    print(<span class="string">&#x27;f2 done at&#x27;</span>,get_time())</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f3</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;f3 started at&#x27;</span>,get_time())</span><br><span class="line">    gevent.sleep(<span class="number">3</span>) </span><br><span class="line">    print(<span class="string">&#x27;f3 done at&#x27;</span>,get_time())</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gevent.joinall([gevent.spawn(f1),gevent.spawn(f2),gevent.spawn(f3)])</span><br></pre></td></tr></table></figure><p>打印结果</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f1 started at  09:21:17.066251</span><br><span class="line">f2 started at  09:21:17.066355</span><br><span class="line">f3 started at  09:21:17.066388</span><br><span class="line">f2 done at  09:21:19.067741</span><br><span class="line">f3 done at  09:21:20.067747</span><br><span class="line">f1 done at  09:21:21.067812</span><br></pre></td></tr></table></figure><p>可以看到，gevent在同一时刻运行3个函数，并且，f2先完成，接着f3完成，最后IO耗时最长的f1完成，三个函数共同完成耗时为4秒，说明三个函数并发执行了。如果是同步运行，整个过程耗时为4+2+3=9秒耗时，协程优势凸显。</p><h5 id="2-2-gevent-高并发测试"><a href="#2-2-gevent-高并发测试" class="headerlink" title="2.2 gevent 高并发测试"></a>2.2 gevent 高并发测试</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>(<span class="params">task_index</span>):</span></span><br><span class="line">    gevent.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">&#x27;task-&#123;&#125; done at &#123;&#125; &#x27;</span>.<span class="built_in">format</span>(task_index,datetime.datetime.now()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">syn</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        task(i)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    syn(<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>同步情况下，耗时4秒</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">task-0 done at  09:41:14.075001 </span><br><span class="line">task-1 done at  09:41:15.076049 </span><br><span class="line">task-2 done at  09:41:16.077101 </span><br><span class="line">task-3 done at  09:41:17.078055 </span><br></pre></td></tr></table></figure><p>gevent实现的协程异步</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">    coroutines=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    gevent.joinall(coroutines)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    asyn(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>原本需要4秒的执行流，现在只需1秒完成所有任务。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">task-0 done at  09:44:30.535495 </span><br><span class="line">task-1 done at  09:44:30.535749 </span><br><span class="line">task-2 done at  09:44:30.535801 </span><br><span class="line">task-3 done at  09:44:30.535833 </span><br></pre></td></tr></table></figure><p>尝试启动10万个任务，用line_profiler 查看函数中耗时操作（line_profiler 目前不兼容3.7，最好用pyenv 切换到3.6进行测试）。只需要在asyn函数上加@profile装饰器即可</p><p>创建asyn.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent,datetime,time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>(<span class="params">task_index</span>):</span></span><br><span class="line">    gevent.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(&#x27;task-&#123;&#125; done at &#123;&#125; &#x27;.format(task_index,datetime.datetime.now()))</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@profile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">    threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    gevent.joinall(threads)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start=time.time()</span><br><span class="line">    asyn(<span class="number">100000</span>)</span><br><span class="line">    cost=time.time()-start</span><br><span class="line">    print(cost)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(spk) [root@nn spv]# kernprof -l -v asyn.py </span><br><span class="line">5.805598974227905</span><br><span class="line">Wrote profile results to asyn.py.lprof</span><br><span class="line">Timer unit: 1e-06 s</span><br><span class="line"></span><br><span class="line">Total time: 5.73735 s</span><br><span class="line">File: asyn.py</span><br><span class="line">Function: asyn at line 6</span><br><span class="line"></span><br><span class="line">Line #      Hits         Time  Per Hit   % Time  Line Contents</span><br><span class="line">==============================================================</span><br><span class="line">     6                                           @profile</span><br><span class="line">     7                                           def asyn(n):</span><br><span class="line">     8         1    1204525.0 1204525.0     21.0      threads=[gevent.spawn(task,i) for i in range(n)]</span><br><span class="line">     9         1    4532823.0 4532823.0     79.0      gevent.joinall(threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>非常清晰看到，理论上：10万个任务使用协程实现并发运行，总耗时1秒，但实际上，因为需要创建大量greenlet对象，列表创建10万个项耗时1.2秒，gevent joinall 10万个greenlet对象耗时4.5秒，所以整个程序完成总耗时实际为5.7秒左右。</p><p>使用memory_profiler库查看asyn.py内存使用情况，使用也简单与line_profiler相似，使用@profile装饰器来标识需要追踪的函数即可。使用协程，10万个对象消耗300多M，鉴于其并发效率高，而且所有的执行都只在一个线程实现 了，因此内存消耗可接受。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(spk) [root@nn spv]<span class="comment"># python -m memory_profiler asyn.py</span></span><br><span class="line">Filename: asyn.py</span><br><span class="line"></span><br><span class="line">Line <span class="comment">#    Mem usage    Increment   Line Contents</span></span><br><span class="line">================================================</span><br><span class="line">     <span class="number">6</span>   <span class="number">36.137</span> MiB   <span class="number">36.137</span> MiB   @profile</span><br><span class="line">     <span class="number">7</span>                             <span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">     <span class="number">8</span>  <span class="number">229.531</span> MiB    <span class="number">0.773</span> MiB       threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">     <span class="number">9</span>  <span class="number">366.270</span> MiB  <span class="number">136.738</span> MiB       gevent.joinall(threads)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意：内存的单位MiB，表示的mebibyte，</p><p>MB/s的意思是每秒中传输10^6 byte的数据，以10为底数的指数</p><p>MiB/s的意思是每秒中传输2^20 byte的数据，以2为底数的指数</p><p>1 MiB =0.9765625 MB</p><p>创建100万个task，再看看kernprof -l -v asyn.py ，内存方面使用top可以直观看到asyn.py 占用了2G*0.816=1632 MiB</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KiB Swap:  2097148 total,  1039272 free,  1057876 used.    14348 avail Mem </span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                </span><br><span class="line">    30 root      20   0       0      0      0 S 45.1  0.0   0:47.13 kswapd0                                </span><br><span class="line"> 29380 root      20   0 2319772   1.4g     56 R 42.8 81.6   1:04.53 kernprof</span><br></pre></td></tr></table></figure><p>将协程并发数设为100万，总共耗时为534秒，时间略长，使用gevent在单台服务器上，并发数不要设太离谱，1000个并发足以应付普通项目的需求，例如爬虫，例如做服务端接收客户端发来的socket流数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(spk) [root@nn spv]<span class="comment"># kernprof -l -v asyn.py           </span></span><br><span class="line"><span class="number">534.7752296924591</span></span><br><span class="line">Wrote profile results to asyn.py.lprof</span><br><span class="line">Timer unit: <span class="number">1e-06</span> s</span><br><span class="line"></span><br><span class="line">Total time: <span class="number">532.165</span> s</span><br><span class="line">File: asyn.py</span><br><span class="line">Function: asyn at line <span class="number">6</span></span><br><span class="line"></span><br><span class="line">Line <span class="comment">#      Hits         Time  Per Hit   % Time  Line Contents</span></span><br><span class="line">==============================================================</span><br><span class="line">     <span class="number">6</span>                                           @profile</span><br><span class="line">     <span class="number">7</span>                                           <span class="function"><span class="keyword">def</span> <span class="title">asyn</span>(<span class="params">n</span>):</span></span><br><span class="line">     <span class="number">8</span>         <span class="number">1</span>   <span class="number">82927089.0</span> <span class="number">82927089.0</span>     <span class="number">15.6</span>      threads=[gevent.spawn(task,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">     <span class="number">9</span>         <span class="number">1</span>  <span class="number">449238172.0</span> <span class="number">449238172.0</span>     <span class="number">84.4</span>      gevent.joinall(threads)</span><br></pre></td></tr></table></figure><h5 id="2-3-理解gevent的monkey-patch-all"><a href="#2-3-理解gevent的monkey-patch-all" class="headerlink" title="2.3  理解gevent的monkey.patch_all()"></a>2.3  理解gevent的monkey.patch_all()</h5><p>&#8195;&#8195;在接下有关gevent的实际项目中，py程序都会引用monkey.patch_all()这个方法，它的作用是用非阻塞模块替换python自带的阻塞模块，这就是所谓”猴子补丁”，原理是运行时用非阻塞的对象属性替换对应阻塞对象的属性，或者用自己实现的同名非阻塞模块，替换对应的阻塞模块。<br>&#8195;&#8195;注意：这里说的模块就是“有完整功能的一个.py文件“或者”由多个py文件组成的一个完整功能的模块“<br>&#8195;&#8195;例如下面要实现这么一个需求：server.py运行时，将thread.py模块的synfoo函数替换为自定义的mythread.py模块里面asynfoo函数</p><p>thread.py 模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;builtin method synfoo of thread.py&#x27;</span>)</span><br></pre></td></tr></table></figure><p>自定义的mythread.py模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;builtin method asynfoo of mythread.py&#x27;</span>)</span><br></pre></td></tr></table></figure><p>server.py程序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> thread</span><br><span class="line"><span class="comment"># 在本程序的modules字典里面删除原thread模块</span></span><br><span class="line"><span class="keyword">del</span> sys.modules[<span class="string">&#x27;thread&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用异步的模块替换当前thread模块</span></span><br><span class="line">sys.modules[<span class="string">&#x27;thread&#x27;</span>] = <span class="built_in">__import__</span>(<span class="string">&#x27;mythread&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载thread</span></span><br><span class="line"><span class="keyword">import</span> thread</span><br><span class="line">thread.foo() <span class="comment">#这里的thread已经是mythread模块</span></span><br><span class="line">print(thread)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># server.py运行时已经成功将内建同步模块替换为异步的模块</span><br><span class="line">builtin method asynfoo of mythread.py</span><br><span class="line"># thread的指向自定义模块mythread</span><br><span class="line">&lt;module &#x27;mythread&#x27; from &#x27;/opt/spv/mythread.py&#x27;&gt;</span><br></pre></td></tr></table></figure><p>以上就是monkey.patch_all()大致逻辑，gevent可以把python内建的多个模块在程序运行时替换为它写的异步模块，默认是把内建的socket、thread、queue等模块替换为非阻塞同名模块</p><p>（site-packages/gevent/monkey.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_all</span>(<span class="params">socket=<span class="literal">True</span>, dns=<span class="literal">True</span>, time=<span class="literal">True</span>, select=<span class="literal">True</span>, thread=<span class="literal">True</span>, os=<span class="literal">True</span>, ssl=<span class="literal">True</span>, httplib=<span class="literal">False</span></span></span></span><br><span class="line"><span class="function"><span class="params">              subprocess=<span class="literal">True</span>, sys=<span class="literal">False</span>, aggressive=<span class="literal">True</span>, Event=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              builtins=<span class="literal">True</span>, signal=<span class="literal">True</span></span>):</span></span><br><span class="line">    _warnings, first_time = _check_repatching(**<span class="built_in">locals</span>())</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _warnings <span class="keyword">and</span> <span class="keyword">not</span> first_time:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> os:</span><br><span class="line">        patch_os()</span><br><span class="line">    <span class="keyword">if</span> time:</span><br><span class="line">        patch_time()</span><br><span class="line">    <span class="keyword">if</span> thread:</span><br><span class="line">        patch_thread(Event=Event, _warnings=_warnings)</span><br><span class="line">    <span class="comment"># sys must be patched after thread. in other cases threading._shutdown will be</span></span><br><span class="line">    <span class="comment"># initiated to _MainThread with real thread ident</span></span><br><span class="line">    <span class="keyword">if</span> sys:</span><br><span class="line">        patch_sys()</span><br><span class="line">    <span class="keyword">if</span> socket:</span><br><span class="line">        patch_socket(dns=dns, aggressive=aggressive)</span><br><span class="line">    <span class="keyword">if</span> select:</span><br><span class="line">        patch_select(aggressive=aggressive)</span><br><span class="line">    <span class="keyword">if</span> ssl:</span><br><span class="line">        patch_ssl()</span><br><span class="line">    <span class="keyword">if</span> httplib:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;gevent.httplib is no longer provided, httplib must be False&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> subprocess:</span><br><span class="line">        patch_subprocess()</span><br><span class="line">    <span class="keyword">if</span> builtins:</span><br><span class="line">        patch_builtins()</span><br></pre></td></tr></table></figure><p>如果不想gevent对某个内建模块覆盖为非阻塞，可以将该模块设为False：monkey.patch_all(thread=False)<br>或者在monkey.patch_all(thread=False) 语句后面追加import threading<br>目的是内建同步模块再次覆盖前面的gevent异步模块，例如server.py文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">monkey.patch_all(thread=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 或者 import threading</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>():</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">syn</span>():</span></span><br><span class="line">t=threading.thread(target=task,args=())</span><br><span class="line">t.start()</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span></span><br><span class="line">gevent.joinall...</span><br></pre></td></tr></table></figure><p>由此可知，gevent的patch_all针对模块的覆盖是有顺序的，因为当使用gevent时，import的模块顺序很重要，内建模块在patch_all前面或者在patch_all后面，对应是同步还是异步模块导入。</p><h6 id="2-2-1-locals-方法"><a href="#2-2-1-locals-方法" class="headerlink" title="2.2.1 locals()方法"></a>2.2.1 locals()方法</h6><p>&#8195;&#8195;locals()返回一个字典，它可以获取当前模块所有的局部变量以及当前模块引入的其他模块，例如myFoo.py模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">local_module_dict=<span class="built_in">locals</span>()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>():</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line">print(local_module_dict[<span class="string">&#x27;threading&#x27;</span>])</span><br><span class="line">print(local_module_dict[<span class="string">&#x27;socket&#x27;</span>])</span><br><span class="line">输出</span><br><span class="line"><span class="comment"># &lt;module &#x27;threading&#x27; from &#x27;/root/.pyenv/versions/3.7.5/lib/python3.7/threading.py&#x27;&gt;</span></span><br><span class="line"><span class="comment"># &lt;module &#x27;socket&#x27; from &#x27;/root/.pyenv/versions/3.7.5/lib/python3.7/socket.py&#x27;&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>gevent通过locals()获取这些模块后，将需要替换的模块都替换gevent自己实现的非阻塞模块</p><h6 id="2-2-2-gevent-替换非阻塞的模块的思路"><a href="#2-2-2-gevent-替换非阻塞的模块的思路" class="headerlink" title="2.2.2 gevent 替换非阻塞的模块的思路"></a>2.2.2 gevent 替换非阻塞的模块的思路</h6><p>&#8195;&#8195;在前面的例子中，使用gevent.sleep()可以让协程自动切换实现异步方式执行，如果使用内建的time.sleep()，则变成同步执行，下面看看gevent如何使用patch_time()方法为sleep打补丁：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_time</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Replace :func:`time.sleep` with :func:`gevent.sleep`.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">from</span> gevent.hub <span class="keyword">import</span> sleep</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="comment"># 用gevent.hub.sleep方法替换内建的sleep方法</span></span><br><span class="line">    patch_item(time, <span class="string">&#x27;sleep&#x27;</span>, sleep)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_item</span>(<span class="params">module, attr, newitem</span>):</span></span><br><span class="line">    olditem = <span class="built_in">getattr</span>(module, attr, _NONE)</span><br><span class="line">    <span class="keyword">if</span> olditem <span class="keyword">is</span> <span class="keyword">not</span> _NONE:</span><br><span class="line">        saved.setdefault(module.__name__, &#123;&#125;).setdefault(attr, olditem)</span><br><span class="line">    <span class="built_in">setattr</span>(module, attr, newitem)</span><br></pre></td></tr></table></figure><p>实现原理跟2.2提到monke.patch_all()一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gevent.hub <span class="keyword">import</span> sleep <span class="comment"># 先导入gevent的sleep</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment"># 再导入内建time模块</span></span><br><span class="line">print(<span class="built_in">getattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>)) <span class="comment"># 获取原内建sleep</span></span><br><span class="line"><span class="comment"># &lt;function time.sleep&gt;</span></span><br><span class="line"><span class="built_in">setattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>,sleep) <span class="comment"># 将gevent的异步sleep方法替换原sleep方法</span></span><br><span class="line">print(<span class="built_in">getattr</span>(time,<span class="string">&#x27;sleep&#x27;</span>)) <span class="comment"># 打印运行是sleep方法看看是内建的还是gevent实现的</span></span><br><span class="line"><span class="comment"># &lt;function gevent.hub.sleep(seconds=0, ref=True)&gt;</span></span><br></pre></td></tr></table></figure><p>对于模块的导入，则需要使用<code>getattr自省模式创建一个对象，然后通过__import__引入</code></p><p>以gevent替换os为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_os</span>():</span></span><br><span class="line">    patch_module(<span class="string">&#x27;os&#x27;</span>)</span><br></pre></td></tr></table></figure><p>具体实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用gevent的os替换内建的os模块，这里的name就是&#x27;os&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">patch_module</span>(<span class="params">name, items=<span class="literal">None</span></span>):</span></span><br><span class="line"><span class="comment"># 通过__import__方法导入用gevent的os</span></span><br><span class="line">    gevent_module = <span class="built_in">getattr</span>(<span class="built_in">__import__</span>(<span class="string">&#x27;gevent.&#x27;</span> + name), name)</span><br><span class="line">    module_name = <span class="built_in">getattr</span>(gevent_module, <span class="string">&#x27;__target__&#x27;</span>, name)</span><br><span class="line">    module = <span class="built_in">__import__</span>(module_name)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># 获取gevent_module里面跟os相关的方法</span></span><br><span class="line">        items = <span class="built_in">getattr</span>(gevent_module, <span class="string">&#x27;__implements__&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> items <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> AttributeError(<span class="string">&#x27;%r does not have __implements__&#x27;</span> % gevent_module)</span><br><span class="line">    <span class="keyword">for</span> attr <span class="keyword">in</span> items:</span><br><span class="line">    <span class="comment"># 用 gevent自己实现的os里面方法替换内建os指定方法</span></span><br><span class="line">        patch_item(module, attr, <span class="built_in">getattr</span>(gevent_module, attr))</span><br><span class="line">    <span class="keyword">return</span> module</span><br></pre></td></tr></table></figure><p>相信到了这里，已经可以理解 monkey.patch_all()为何要在gevent的程序头部引入，常见“模板”如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">monkey.patch_all()</span><br></pre></td></tr></table></figure><p>很多文章在讨论gevent的协程时，基本都是一句话“这是打补丁，将阻塞模块替换为非阻塞模块”简单带过，对于大部分人来说，这种说明一般会感到疑惑。</p><h4 id="3、gevent-examples"><a href="#3、gevent-examples" class="headerlink" title="3、gevent examples"></a>3、gevent examples</h4><p>&#8195;&#8195;本章主要结合一些场景给出gevent用法，参考了gevent官网给出的examples：<a href="http://www.gevent.org/examples/index.html">地址</a></p><h5 id="3-1-使用协程高并发爬网页"><a href="#3-1-使用协程高并发爬网页" class="headerlink" title="3.1 使用协程高并发爬网页"></a>3.1 使用协程高并发爬网页</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> random,datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line"></span><br><span class="line"><span class="comment"># patches stdlib (including socket and ssl modules) to cooperate with other greenlets</span></span><br><span class="line"><span class="comment"># 将标准lib打补丁，例如下面的https请求需要用到ssl模块，将该内建的ssl模块替换为gevent的ssl</span></span><br><span class="line">monkey.patch_all()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里给出的https协议来说明gevent可进行SSL的相关任务处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span> (<span class="params">workers=<span class="number">1000</span></span>):</span></span><br><span class="line">    start=time.time()</span><br><span class="line">    url_pool = [</span><br><span class="line">        <span class="string">&#x27;https://www.baidu.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.apple.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.qq.com/&#x27;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    urls=[ random.choice(url_pool) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(workers)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_head</span>(<span class="params">url</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;Starting &#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(url,datetime.datetime.now()))</span><br><span class="line">        data = requests.get(url).text <span class="comment"># gevent会在发生IO的位置实现协程自动切换</span></span><br><span class="line">        <span class="comment">#print(&#x27;%s: %s bytes: %r&#x27; % (url, len(data), data[:2]))</span></span><br><span class="line">    jobs = [gevent.spawn(print_head, _url) <span class="keyword">for</span> _url <span class="keyword">in</span> urls]</span><br><span class="line">    gevent.wait(jobs) <span class="comment"># 阻塞主线程，让所有协程得以持续运行</span></span><br><span class="line">    cost=time.time()-start</span><br><span class="line">    print(<span class="string">&#x27;cost:&#x27;</span>,cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure><p>以上1000个请求，只需运行一个线程，非常轻量且“低功耗”，而多线程方式，则需创建1000个线程，这就是协程的优势。</p><h5 id="3-2-gevent实现的socket高并发"><a href="#3-2-gevent实现的socket高并发" class="headerlink" title="3.2 gevent实现的socket高并发"></a>3.2 gevent实现的socket高并发</h5><p><strong>server.py端逻辑</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket,datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecodeErr</span>(<span class="params">Exception</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Server</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,host=<span class="string">&#x27;0.0.0.0&#x27;</span>,port=<span class="number">8090</span>,conns=<span class="number">100</span></span>):</span></span><br><span class="line">        self._s=socket.socket()</span><br><span class="line">        self._s.bind((host,port))</span><br><span class="line">        self._s.listen(conns)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_request</span>(<span class="params">self,conn</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 接收客户端发送的是比特字节，需要decode为str类型</span></span><br><span class="line">            msg=conn.recv(<span class="number">1024</span>).decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> msg:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            print(<span class="string">&#x27;got the msg:&#123;&#125; at &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(msg,self.recv_time()))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 发送给client需要byte类型</span></span><br><span class="line">            conn.send(<span class="built_in">bytes</span>(msg,encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">            <span class="keyword">if</span> msg ==<span class="string">&#x27;quit&#x27;</span>:</span><br><span class="line">                conn.shutdown(socket.SHUT_RDWR)</span><br><span class="line">                conn.close()</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line"><span class="meta">    @staticmethod            </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recv_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">serve_forever</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                client_conn,client_ip=self._s.accept()</span><br><span class="line">                <span class="comment"># 创建一个新的Greenlet服务新的请求</span></span><br><span class="line">                g=gevent.spawn(self.parse_request,client_conn)</span><br><span class="line">                print(<span class="string">&#x27;new client connected:&#123;&#125; &#123;&#125; serving...&#x27;</span>.<span class="built_in">format</span>(client_ip,g.name))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conns=<span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    server=Server(conns=conns)</span><br><span class="line">    server.serve_forever()                   </span><br></pre></td></tr></table></figure><p>另外打开2个终端使用 telnet 188.0.0.10 8090</p><p>输出：<br>可以看到每个client请求都是由新的greenlet来服务，这个Greenlet就是协程对象<code>&lt;Greenlet at 0x7f7dc87efa70: parse_request(&lt;gevent._socket3.socket object, fd=7, family=2, ty)&gt;</code>，而非多线程对象。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spv]# python server.py 100</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21042) Greenlet-0 serving...</span><br><span class="line">got the msg:foo</span><br><span class="line"> at 10:8:3</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21044) Greenlet-1 serving...</span><br><span class="line">got the msg:bar</span><br><span class="line"> at 10:8:37</span><br></pre></td></tr></table></figure><p>这里需要注意：<br>在parse_request里面关闭client的连接用conn.shutdown(socket.SHUT_WR)<br>shutdown 方法的 how 参数接受如下参数值： </p><ul><li> SHUT_RD：关闭 socket 的输入部分，程序还可通过该 socket 输出数据。(tcp半开状态)</li><li> SHUT_WR： 关闭该 socket 的输出部分，程序还可通过该 socket 读取数据。（(tcp半开)状态）</li><li> SHUT_RDWR：全关闭。该 socket 既不能读取数据，也不能写入数据。</li></ul><p>conn.close():关闭完整tcp连接通道<br>close方法不是立即释放，如果想立即释放，需在close之前使用shutdown方法<br>server.py 使用gevent实现可接受高并发连接，下面给出gevent版的client，高并发socket请求</p><p>*<em>client.py端代码**</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,server_ip,port,workers=<span class="number">10</span></span>):</span></span><br><span class="line">        self.server_ip=server_ip</span><br><span class="line">        self.port=port</span><br><span class="line">        self.workers=workers</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @staticmethod            </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recv_time</span>():</span></span><br><span class="line">        d=datetime.datetime.now()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;%s:%s:%s&#x27;</span>%(d.hour,d.minute,d.second)        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">asyn_sock</span>(<span class="params">self,msg</span>):</span></span><br><span class="line">        client=socket.socket()</span><br><span class="line">        client.connect((self.server_ip,self.port))</span><br><span class="line">        bmsg=<span class="built_in">bytes</span>(msg,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        client.sendall(bmsg)</span><br><span class="line">        recv_data=client.recv(<span class="number">1024</span>).decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;gevent object:&#123;&#125; data:&#123;&#125; at:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gevent.getcurrent(),recv_data,self.recv_time()))</span><br><span class="line">        client.close()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        threads=[gevent.spawn(self.asyn_sock,<span class="built_in">str</span>(i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.workers)]</span><br><span class="line">        gevent.joinall(threads)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    workers=<span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    c=Client(server_ip=<span class="string">&#x27;188.0.0.10&#x27;</span>,port=<span class="number">8090</span>,workers=workers)</span><br><span class="line">    c.start()</span><br></pre></td></tr></table></figure><p>服务器端启动100个连接数，客户端并发10个请求。</p><p>服务端打印如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spv]# python server.py 100</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21246) Greenlet-0 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21248) Greenlet-1 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21250) Greenlet-2 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21252) Greenlet-3 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21254) Greenlet-4 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21256) Greenlet-5 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21258) Greenlet-6 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21260) Greenlet-7 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21262) Greenlet-8 serving...</span><br><span class="line">new client connected:(&#x27;188.0.0.10&#x27;, 21264) Greenlet-9 serving...</span><br><span class="line">got the msg:0 at 10:11:5</span><br><span class="line">got the msg:1 at 10:11:5</span><br><span class="line">got the msg:2 at 10:11:5</span><br><span class="line">got the msg:3 at 10:11:5</span><br><span class="line">got the msg:4 at 10:11:5</span><br><span class="line">got the msg:5 at 10:11:5</span><br><span class="line">got the msg:6 at 10:11:5</span><br><span class="line">got the msg:7 at 10:11:5</span><br><span class="line">got the msg:8 at 10:11:5</span><br><span class="line">got the msg:9 at 10:11:5</span><br></pre></td></tr></table></figure><p>客户端打印如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spv]# python asyn.py 10</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a85f0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;9&#x27;)&gt; data:9 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a84d0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;8&#x27;)&gt; data:8 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a83b0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;7&#x27;)&gt; data:7 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8290: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;6&#x27;)&gt; data:6 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8170: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;5&#x27;)&gt; data:5 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41a9a8050: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;4&#x27;)&gt; data:4 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586ef0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;3&#x27;)&gt; data:3 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586b90: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;2&#x27;)&gt; data:2 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586cb0: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;1&#x27;)&gt; data:1 at:10:11:5</span><br><span class="line">gevent object:&lt;Greenlet at 0x7ff41b586a70: &lt;bound method Client.asyn_sock of &lt;__main__.Client object at 0x7ff41a9da210&gt;&gt;(&#x27;0&#x27;)&gt; data:0 at:10:11:5</span><br></pre></td></tr></table></figure><p>可以看到不管是服务器和客户端，都是由多个greenlet协程对象负责请求或者负责服务。<br>如果server.py端并发数设为10000，client.py并发也设为10000，那么会出现以下情况：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">init__</span><br><span class="line">OSError: [Errno 24] Too many open files</span><br><span class="line">During handling of the above exception, another exception occurred:</span><br></pre></td></tr></table></figure><p>这里因为centos限制用户级别在打开文件描述符的数量，可查看默认值：限制至多打开1024个文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spv]# ulimit -n</span><br><span class="line">1024</span><br></pre></td></tr></table></figure><p>linux 一般会在以下几个文件对系统资源做限制，例如用户级别（此外还有系统级别）的限制： /etc/security/limits.conf，和/etc/security/limits.d/目录，/etc/security/limits.d/里面配置会覆盖/etc/security/limits.conf的配置：</p><blockquote><p>系统限制用户的资源有：所创建的内核文件的大小、进程数据块的大小、Shell<br>进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell<br>进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。</p></blockquote><p>提升并发性能：临时修改：ulimit -n 100000；永久性修改：root权限下，在/etc/security/limits.conf中添加如下两行，*表示所有用户，重启/或者注销重登陆生效</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* soft nofile 102400</span><br><span class="line">* hard nofile 104800</span><br></pre></td></tr></table></figure><p>注意hard limit必须大于soft limit</p><p>这里将linux设为ulimit -n 100000，10万个描述符！ python server.py 20000个并发，python client.py 10000并发请求打过去，3秒内完成，而且这是因为程序加入print打印语句影响性能，去掉所有print语句，2万个客户端并发不到2秒内完成，gevent或者说底层Greenlet的并发性能非常强。</p><h5 id="3-3-gevent数据库操作"><a href="#3-3-gevent数据库操作" class="headerlink" title="3.3 gevent数据库操作"></a>3.3 gevent数据库操作</h5><p>&#8195;&#8195;这里将给出协程方式、多线程方式连接mysql数据库某实际项目备份表，15个字段，2万多条数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> socket,monkey</span><br><span class="line">monkey.patch_all()</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">        start=time.time()</span><br><span class="line">        func(*args,**kwargs)</span><br><span class="line">        cost=time.time()-start</span><br><span class="line">        print(<span class="string">&#x27;&#123;&#125; cost:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func.__name__,cost))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_db</span>(<span class="params">index</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;负责读数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(&#x27;start:&#x27;,index)</span></span><br><span class="line">    db = pymysql.connect(host = <span class="string">&#x27;****&#x27;</span>, user = <span class="string">&#x27;****&#x27;</span>, passwd = <span class="string">&#x27;****&#x27;</span>, db= <span class="string">&#x27;****&#x27;</span>)</span><br><span class="line">    cursor = db.cursor()</span><br><span class="line">    sql=<span class="string">&#x27;select count(1) from `article &#x27;</span></span><br><span class="line">    cursor.execute(sql)</span><br><span class="line">    nums = cursor.fetchall()</span><br><span class="line">    <span class="comment">#print(&#x27;total itmes:&#x27;,nums)</span></span><br><span class="line">    cursor.close()</span><br><span class="line">    db.close()</span><br><span class="line">    <span class="comment">#print(&#x27;end:&#x27;,index)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gevent_read</span>(<span class="params">workers</span>):</span></span><br><span class="line">    <span class="comment"># 创建多个greenlets协程对象</span></span><br><span class="line">    greenlets = [gevent.spawn(read_db,i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(workers)]</span><br><span class="line">    gevent.joinall(greenlets)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"><span class="comment"># 5次测试。这里每次间隔1秒，让客户端连接mysql的connections及时关闭，避免释放不及时导致超过数据库端的允许连接数</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        gevent_read(<span class="number">100</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><p>从代码逻辑可以看出，gevent使用协程非常简单，在头部引入相关模块，再使用gevent.spawn创建多个greenlets对象，最后joinall。以下是测试结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spv]# python asyn_mysql.py </span><br><span class="line">gevent_read cost:2.702486276626587</span><br><span class="line">gevent_read cost:2.120276689529419</span><br><span class="line">gevent_read cost:2.1487138271331787</span><br><span class="line">gevent_read cost:2.61714243888855</span><br><span class="line">gevent_read cost:2.1180896759033203</span><br></pre></td></tr></table></figure><h5 id="3-4-gevent-多线程"><a href="#3-4-gevent-多线程" class="headerlink" title="3.4 gevent 多线程"></a>3.4 gevent 多线程</h5><p>gevent也有自己线程池，使用的python的thread，两者没区别，如果用了多线程，那么gevent其实就没多大意义了，因为不是协程模式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> gevent</span><br><span class="line"><span class="keyword">from</span> gevent.threadpool <span class="keyword">import</span> ThreadPool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timeit</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">        start=time.time()</span><br><span class="line">        func(*args,**kwargs)</span><br><span class="line">        cost=time.time()-start</span><br><span class="line">        print(<span class="string">&#x27;&#123;&#125; cost:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(func.__name__,cost))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpool</span>(<span class="params">workers</span>):</span></span><br><span class="line">    pool = ThreadPool(workers)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        pool.spawn(time.sleep, <span class="number">1</span>)</span><br><span class="line">    gevent.wait()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gpool(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>输出3秒，10个任务，线程池只有4个worker，因此需分三轮工作，因为耗时3秒</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spv]# python gpool.py </span><br><span class="line">gpool cost:3.006455183029175</span><br></pre></td></tr></table></figure><h5 id="3-5-gevent-其他examples"><a href="#3-5-gevent-其他examples" class="headerlink" title="3.5 gevent 其他examples"></a>3.5 gevent 其他examples</h5><p>这里不再一一列出，可以参考gevent github的<a href="https://github.com/gevent/gevent/tree/master/examples">example目录</a></p><p>不过建议看看<strong>geventsendfile.py</strong>和<strong>wsgiserver_ssl.py</strong><br>第一是零拷贝技术的协程，第二个是基于https的协程webserver</p><h4 id="4、greenlet-eventlet-gevent的关系"><a href="#4、greenlet-eventlet-gevent的关系" class="headerlink" title="4、greenlet/eventlet/gevent的关系"></a>4、greenlet/eventlet/gevent的关系</h4><p>&#8195;&#8195;Greelent实现了一个比较易用(相比yeild)的协程切换的库。但是greenlet没有自己的调度过程，所以一般不会直接使用。<br>&#8195;&#8195;Eventlet在Greenlet的基础上实现了自己的GreenThread，实际上就是greenlet类的扩展封装，而与Greenlet的不同是，Eventlet实现了自己调度器称为Hub，Hub类似于Tornado的IOLoop，是单实例的。在Hub中有一个event loop，根据不同的事件来切换到对应的GreenThread。同时Eventlet还实现了一系列的补丁来使Python标准库中的socket等等module来支持GreenThread的切换。Eventlet的Hub可以被定制来实现自己调度过程。<br>&#8195;&#8195;Gevent基于libev和Greenlet。不同于Eventlet的用python实现的hub调度，Gevent通过Cython调用libev来实现一个高效的event loop调度循环。同时类似于Eventlet，Gevent也有自己的monkey_patch，在打了补丁后，完全可以使用python线程的方式来无感知的使用协程，减少了开发成本。<br>&#8195;&#8195;这里也顺便给出greenlet/eventlet/gevent和其他可以实现协程模式库的对比表格，该表来自Gruvi作者的项目介绍页。Gruvi是一个轻量且特别的协程库，项目作者因为不太认同常见python协程库的实现方式，而且也不认同不推荐使用monkey patch方式，所有他写了Gruvi，专注green thread：<a href="https://gruvi.readthedocs.io/en/latest/rationale.html">项目地址</a></p><table><thead><tr><th align="left">Feature</th><th align="left">Gruvi</th><th align="left">Asyncio</th><th align="left">Gevent</th><th align="left">Eventlet</th></tr></thead><tbody><tr><td align="left">IO library</td><td align="left"><a href="https://github.com/joyent/libuv">libuv</a></td><td align="left">stdlib</td><td align="left"><a href="http://libev.schmorp.de/">libev</a></td><td align="left">stdlib / <a href="http://libevent.org/">libevent</a></td></tr><tr><td align="left">IO abstraction</td><td align="left">Transports / Protocols</td><td align="left">Transports / Protocols</td><td align="left">Green sockets</td><td align="left">Green sockets</td></tr><tr><td align="left">Threading</td><td align="left"><a href="https://pypi.python.org/pypi/fibers">fibers</a></td><td align="left"><code>yield from</code></td><td align="left"><a href="https://pypi.python.org/pypi/greenlet">greenlet</a></td><td align="left"><a href="https://pypi.python.org/pypi/greenlet">greenlet</a></td></tr><tr><td align="left">Resolver</td><td align="left">threadpool</td><td align="left">threadpool</td><td align="left">threadpool / <a href="http://c-ares.haxx.se/">c-ares</a></td><td align="left">blocking / <a href="http://www.dnspython.org/">dnspython</a></td></tr><tr><td align="left">Python: 2.x</td><td align="left">YES (2.7)</td><td align="left">YES (2.6+, via <a href="https://bitbucket.org/enovance/trollius">Trollius</a>)</td><td align="left">YES</td><td align="left">YES</td></tr><tr><td align="left">Python: 3.x</td><td align="left">YES (3.3+)</td><td align="left">YES</td><td align="left">YES</td><td align="left">NO</td></tr><tr><td align="left">Python: PyPy</td><td align="left">NO</td><td align="left">NO</td><td align="left">YES</td><td align="left">YES</td></tr><tr><td align="left">Platform: Linux</td><td align="left">FAST</td><td align="left">FAST</td><td align="left">FAST</td><td align="left">FAST</td></tr><tr><td align="left">Platform: Mac OSX</td><td align="left">FAST</td><td align="left">FAST</td><td align="left">FAST</td><td align="left">FAST</td></tr><tr><td align="left">Platform: Windows</td><td align="left">FAST (IOCP)</td><td align="left">FAST (IOCP)</td><td align="left">SLOW (select)</td><td align="left">SLOW (select)</td></tr><tr><td align="left">SSL: Posix</td><td align="left">FAST</td><td align="left">FAST</td><td align="left">FAST</td><td align="left">FAST</td></tr><tr><td align="left">SSL: Windows</td><td align="left">FAST (IOCP)</td><td align="left">FAST (IOCP 3.5+)</td><td align="left">SLOW (select)</td><td align="left">SLOW (select)</td></tr><tr><td align="left">SSL: Contexts</td><td align="left">YES (also Py2.7)</td><td align="left">YES (also Py2.6+)</td><td align="left">NO</td><td align="left">NO</td></tr><tr><td align="left">HTTP</td><td align="left">FAST (via <a href="https://github.com/joyent/http-parser">http-parser</a>)</td><td align="left">NO (external)</td><td align="left">SLOW (stdlib)</td><td align="left">SLOW (stdlib)</td></tr><tr><td align="left">Monkey Patching</td><td align="left">NO</td><td align="left">NO</td><td align="left">YES</td><td align="left">YES</td></tr></tbody></table><p>本博客也会为Gruvi写一篇文章，主要是欣赏作者阐述的设计理念。从对比表格来看，Asyncio各方面都出色，而且完全由Python标准库实现，后面也有关于Asyncio深入讨论的文章。</p><h4 id="5、gevent-不适用的场合"><a href="#5、gevent-不适用的场合" class="headerlink" title="5、gevent 不适用的场合"></a>5、gevent 不适用的场合</h4><p>这里参考Stack Overflow的文章<a href="https://stackoverflow.com/questions/54254252/asyncio-vs-gevent">《Asyncio vs. Gevent 》</a></p><p>it wasn’t perfect:</p><ul><li>Back then, it didn’t work well on Windows (and it still has some limitations today). gevent在Windows 表现不佳</li><li>It couldn’t monkey-patch C extensions, so we coudn’t use MySQLdb,  for example. Luckily, there were many pure Python alternatives, like  PyMySQL. 由于gevent的 monkey-patch替换原理，参考上面2.2，它只支持对存python库打补丁，对于C语言实现的python库，例如MySQLdb，则不支持。</li></ul><p>这里篇文章大致意思是建议用asyncio，因为它是标准库，有着非常详细的文档以及稳定的python官方维护。gevent也可以用，但是自己要清楚项目演进的后续维护情况。</p><p>Supported Platforms</p><blockquote><p><a href="https://pypi.org/project/gevent/whatsnew_1_3.html">gevent 1.3</a> runs on Python 2.7 and Python 3. Releases 3.4, 3.5 and<br>3.6 of Python 3 are supported. (Users of older versions of Python 2<br>need to install gevent 1.0.x (2.5), 1.1.x (2.6) or 1.2.x (&lt;=2.7.8);<br>gevent 1.2 can be installed on Python 3.3.) gevent requires the<br><a href="https://greenlet.readthedocs.io/">greenlet</a> library and will install<br>the <a href="https://cffi.readthedocs.io/">cffi</a> library by default on Windows.</p></blockquote><h4 id="6、协程原理解析"><a href="#6、协程原理解析" class="headerlink" title="6、协程原理解析"></a>6、协程原理解析</h4><p>&#8195;&#8195;前面具体的gevent代码示例，对深入理解协程有一定帮助，因为在本文中，把原理性的讨论放在最后一节显得更为合理。谈到协程又不得不把进程、线程以及堆、栈相关概念抛出，以便从全局把握协程、线程和进程。</p><h5 id="6-1-进程与内存分配"><a href="#6-1-进程与内存分配" class="headerlink" title="6.1 进程与内存分配"></a>6.1 进程与内存分配</h5><p>&#8195;&#8195;进程是系统资源分配的最小单位，Linux系统由一个个在后台运行process提供所有功能的组成，你可以用<code>ll /proc |wc -l</code>或者<code>ps aus|less</code>查看系统运行的进程。进程自己是需要占用系统资源的，例如cpu、内存、网络，这里我们关注其<br>程序的内存分配。<br>这里以一个由C /C++编译的程序占用的内存分为以下几个部分为例说明，这段内容参考文章<a href="https://blog.csdn.net/ZXR_LJ/article/details/79440577">《堆栈的区别》</a>：</p><ul><li><p>栈区（stack）： 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p></li><li><p>堆区（heap）：一般由程序员（在代码里面自行申请内存）分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。</p></li><li><p>全局区（静态区）（static）：全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放</p></li><li><p>文字常量区 ：常量字符串就是放在这里的。 程序结束后由系统释放</p></li><li><p>程序代码区：存放函数体的二进制代码</p></li></ul><p>&#8195;&#8195;可以想象，系统创建一个新的进程都进行以上的复杂内存分配工作，而进程结束后系统还得进行大量内存回收清理工作，如果系统有成千上万个进程创建、切换以及销毁，可想而知，非常消耗资源，”疲于奔命，顾不上其他重要请求“（这就是Apache服务器的并发性的劣势，看看Nginx有多强大）。所以多进程做并发业务，显然不是一个理想方案。</p><h5 id="6-2-线程"><a href="#6-2-线程" class="headerlink" title="6.2 线程"></a>6.2 线程</h5><p>&#8195;&#8195;关于进程的描述，其实很多文章可以找到相关讨论，这里以线程和进程的区别作为说明：</p><ul><li><p>本质区别：进程是操作系统资源分配（分配CPU、内存、网络）的基本单位，而线程是任务（进行某种代码逻辑）调度和执行的基本单位</p></li><li><p>资源占用区别：每个进程都有独立的代码和程序上下文环境，进程之间的切换消耗较大系统资源（投入大，代价较高）；这里顺便说明为何代价高？因为进程之间切换涉及到用户空间（用户态）和内核空间（内核态）的切换。一个进程里面可以有多个线程运行，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的消耗的是当前进程占有的资源，代价较小，但也不低。</p></li><li><p>内存分配方面：系统在运行的时候会为每个进程分配不同的内存空间；而对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源。</p></li><li><p>所处环境：在操作系统中能同时运行多个进程（程序）；而在同一个进程中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行）</p></li></ul><h5 id="6-3-协程"><a href="#6-3-协程" class="headerlink" title="6.3 协程"></a>6.3 协程</h5><p>&#8195;&#8195;终于谈到本章的主角：协程，英文coroutine，它比线程更加轻量，你可以这样认为：一个进程可以拥有多个线程一样，而一个线程也可以拥有多个协程。<br>==<strong>协程与进程的区别</strong>==：</p><ul><li>执行流的调度者不同，进程是内核调度，而协程是在用户态调度，也就是说进程的上下文是在内核态保存恢复的，而协程是在用户态保存恢复的，很显然用户态的代价更低</li><li>进程会被强占，而协程不会，也就是说协程如果不主动让出CPU，那么其他的协程，就没有执行的机会。</li><li>对内存的占用不同，实际上协程可以只需要4K的栈就足够了，而进程占用的内存要大的多</li><li>从操作系统的角度讲，多协程的程序是单进程，单协程</li></ul><p>==<strong>协程与线程的区别</strong>==<br>&#8195;&#8195;一个线程里面可以包含多个协程，线程之间需要上下文切换成本相对协程来说是比较高的，尤其在开启线程较多时，线程的切换更多的是靠操作系统来控制，而协程之间的切换和运行由用户程序代码自行控制或者类似gevent这种自动切换，因此协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行），这将为用户可以设计出非常高性能的并发编程模式。如下图所示一个主线程负责使用gevent自动调度（自动切换运行）2个协程，大致逻辑如下：</p><ul><li>主线程（MainThread，也是根协程或者当前线程）创建（spawn）两个协程，只有有协程遇到IO event时候就把控制权交给当前线程，直到这个协程的IO event已经完成，主线程将控制权给这个协程。<br><img src="https://img-blog.csdnimg.cn/20191228112520473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h4 id="7、小结"><a href="#7、小结" class="headerlink" title="7、小结"></a>7、小结</h4>&#8195;&#8195;本文开启了Python的异步编程文章讨论篇章，算是比较进阶的内容，因为异步模式可让实际项目确实受益不少，在本博客之后有关异步的内容有：asyncio、文件描述符与IO多路复用。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h4 id=&quot;1、-yield-实现协程&quot;&gt;&lt;a href=&quot;#1、-yield-实现协程&quot; class=&quot;headerlink&quot; title=&quot;1、 yield 实现协程&quot;&gt;&lt;/a&gt;1、 yield 实现协程&lt;/h4&gt;&lt;h5 id=&quot;1-1-yield-同步执行&quot;&gt;&lt;a href=&quot;#1-1-yield-同步执行&quot; class=&quot;headerlink&quot; title=&quot;1.1 yield 同步执行&quot;&gt;&lt;/a&gt;1.1 yield 同步执行&lt;/h5&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; time&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;consumer&lt;/span&gt;():&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    send_msg=&lt;span class=&quot;string&quot;&gt;&amp;#x27;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# 3、consumer通过yield拿到producer发来的消息，又通过yield把结果send_msg返回给producer&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        output=&lt;span class=&quot;keyword&quot;&gt;yield&lt;/span&gt; send_msg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        print(&lt;span class=&quot;string&quot;&gt;&amp;#x27;[consumer] consuming &amp;#123;&amp;#125;&amp;#x27;&lt;/span&gt;.&lt;span class=&quot;built_in&quot;&gt;format&lt;/span&gt;(output))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        send_msg=&lt;span class=&quot;string&quot;&gt;&amp;#x27;ok&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;producer&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;consumer_obj,num&lt;/span&gt;):&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 1、启动consumer()生成器&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;next&lt;/span&gt;(consumer_obj)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,num+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        print(&lt;span class=&quot;string&quot;&gt;&amp;#x27;[producer] producing &amp;#123;&amp;#125;&amp;#x27;&lt;/span&gt;.&lt;span class=&quot;built_in&quot;&gt;format&lt;/span&gt;(i))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# 2、通过send()切换到consumer()执行&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        receive_msg=consumer_obj.send(i)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        print(&lt;span class=&quot;string&quot;&gt;&amp;#x27;[producer] received a message &amp;#123;&amp;#125;&amp;#x27;&lt;/span&gt;.&lt;span class=&quot;built_in&quot;&gt;format&lt;/span&gt;(receive_msg))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    consumer_obj.close()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; __name__==&lt;span class=&quot;string&quot;&gt;&amp;#x27;__main__&amp;#x27;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    c=consumer()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    producer(c,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="Python进阶" scheme="https://yield-bytes.gitee.io/blog/categories/Python%E8%BF%9B%E9%98%B6/"/>
    
    
    <category term="gevent" scheme="https://yield-bytes.gitee.io/blog/tags/gevent/"/>
    
    <category term="协程" scheme="https://yield-bytes.gitee.io/blog/tags/%E5%8D%8F%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>深入理解RDD弹性分布式数据集</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/26/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3RDD%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/26/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3RDD%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86/</id>
    <published>2019-12-26T11:37:19.000Z</published>
    <updated>2020-02-03T07:03:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面的博客<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》 </a>深入探讨了Spark架构原理内容，该文提到Stage的划分，为什么要做Stage划分？是为了得到更小的Task计算单元，分发给Executor的线程运行，将规模庞大、多流程的计算任务划分为有序的小颗粒的计算单元，实现更高效的计算。那么Stage划分怎么实现？需依赖RDD(Resilient Distributed Datasets，弹性分布式数据集)，可以说，RDD是Spark最为核心的概念。本文内容部分参考了<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_RDD.md">《弹性式数据集RDDs》</a>以及<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">《Spark官方文档》</a></p><a id="more"></a><h4 id="1、RDD简介"><a href="#1、RDD简介" class="headerlink" title="1、RDD简介"></a>1、RDD简介</h4><p>&#8195;&#8195;RDD 是分布式的、可容错的、只读的、分区记录的弹性数据集合（在开发角度来看，它是一种数据结构，并是绑定了多个方法和属性的一种object），支持并行操作，可以由外部数据集或其他 RDD 转换而来，细读Resilient Distributed  Dataset这三个单词，更深层的含义如下：</p><ul><li>Resilient: 弹性的，RDD如何体现出弹性呢？通过使用RDD血缘关系图——DAG，在丢失节点上重新计算上，弹性容错。</li><li>Distributed:分布式的，RDD的数据集驻留在多个节点的内存中或者磁盘上（一分多）。</li><li>Dataset: 在物理文件存储的数据</li></ul><p>更具体的说明：</p><ul><li><p>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数。<br>（这一特点体现出RDD的分布式，RDD的分区是在多个节点上指定的，注意不是指把master分区拷贝到其他节点上，spark强调的是”移动数据不如移动计算“，避免跨节点拷贝分区数据。做这样假设：RDD如果不设计为多个分区，那么一个RDD就是代表一个超大数据集，而且只能在单机行运行，这跟Pandas的DataFrame区别就不大了。）</p></li><li><p>Spark一个计算程序，往往会产生多个RDD，这些RDD会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。<br>（这个特点就体现了RDD可恢复性，）</p></li><li><p>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)。<br>（其实很多中间件或者组件只要涉及到Partition，必然少不了使用Partitioner(分区器)，例如kafka的partition，Producer可通过对消息key取余将消息写入到不同副本分区上，例如redis的key在slot上分配，也是通过对key取余。）</p></li><li><p>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</p></li></ul><h4 id="2、创建RDD"><a href="#2、创建RDD" class="headerlink" title="2、创建RDD"></a>2、创建RDD</h4><ul><li><p>由一个已经存在的Scala 数组创建或者Python列表创建。<br>val rdd0 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) # Scala<br>val rdd0 = sc.parallelize([1,2,3,4,5,6,7,8]) # Python</p></li><li><p>由外部存储系统的文件创建。包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、HBase等，其他流式数据：Socket流等。<br>val rdd2 = sc.textFile(“hdfs://nn:9000/data_files”)<br>sc是spark-shell内置创建的sparkcontext</p></li><li><p>已有的RDD经过算子转换生成新的RDD<br><code>val rdd1=rdd0.flatMap(_.split(&quot; &quot;))</code><br><code>val rdd2=rdd1.map((_, 1))</code></p></li></ul><h4 id="3、宽依赖和窄依赖"><a href="#3、宽依赖和窄依赖" class="headerlink" title="3、宽依赖和窄依赖"></a>3、宽依赖和窄依赖</h4><p>&#8195;&#8195;在<a href="https://blog.csdn.net/pysense/article/details/103641824">《深入理解Spark》 </a>的第6章节”理解Spark Stage的划分“内容，正是RDD 与它的父 RDD(s) 之间的依赖关系，才有Stage划分的基础，主要为以下两种不同的依赖关系：</p><ul><li>窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖，一对一关系。例如Map、Filter、Union等算子。（你可以这样形象理解Spark为什么要用narrow这个词：从下图中例如Map算子，一个父rdd到一个子rdd的依赖关系，一条线直连关系，用窄形容恰当）</li><li>宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDD 的多个分区所依赖，一对多关系。例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffling。（你可以这样形象理解Spark为什么要用wide这个词：从下图中例如groupByKey算子，一个父RDD有多条线连接到不同子RDD，用宽形容恰当）</li><li></li><li>Lineage（血统）<br>例如RDD0转换为下一个RDD1，RDD1转换为下一个RDD2，RDD2转换为下一个RDD3，…，这一系列的转换过程叫做血统，可以构成DAG图（执行计划）<br>有何作用？当RDD3丢失，只需要使用Lineage关系图和重新计算RDD2，即可恢复出RDD3数据集。而无需从头RDD0重新计算，省时省力。<br>对于下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：<br><img src="https://img-blog.csdnimg.cn/20191222202637659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">对于map、filter、union这些算子可以很直观看到它们所计算的rdd依赖关系是窄依赖<br>注意：对于join算子，如果是协同划分的话，两个父RDD之间， 父RDD与子RDD之间能形成一致的分区安排，即同一个Key保证被映射到同一个分区，这种join是窄依赖。（协同划分就是指定分区器以产生前后一致的分区安排）<br>如果不是协同划分，就会形成宽依赖。</li></ul><p>这两种依赖关系除了可以把一个Job划分多个Stage，还有以下最为重要的两点作用：<br>&#8195;&#8195;第一：窄依赖可实现在当前节点上（数据无需夸节点传输）以流水线的方式（pipeline管道方式）对父分区数据进行流水线计算，例如先执行 Map算子，接着执行Filter算子，这两个操作一气呵成。而宽依赖则需要先计算好所有父分区的数据，接着将数据通过跨节点传递后执行shuffling（不跨节点，又怎么能把不同节点但key相同的项归并到同一分区上呢？所以宽依赖必然要进行磁盘IO和Socket跨节点传数据），这一过程与 MapReduce 类似。<br>&#8195;&#8195;第二：窄依赖能够更有效地进行数据恢复，根据上面”Lineage（血统）“所提的逻辑，只需重新对丢失子rdd的父rdd进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据重新计算并再次 shuffling，效率低而且耗时。</p><h4 id="4、通过RDD的依赖关系构建DAG计算图"><a href="#4、通过RDD的依赖关系构建DAG计算图" class="headerlink" title="4、通过RDD的依赖关系构建DAG计算图"></a>4、通过RDD的依赖关系构建DAG计算图</h4><p>&#8195;&#8195;上面提到的多个RDD之间的两种依赖关系组成了 DAG，DAG 定义了这些 RDD之间的 Lineage链，通过血统关系（就像你手上拿了一张关系图谱），如果一个 RDD 的部分或者全部计算结果丢失了，也可以根据”这张关系图谱“重新进行计算出结果。Spark根据RDD依赖关系的不同将 DAG 划分为不同的执行阶段 (Stage)：</p><ul><li>对于窄依赖，由于分区的依赖关系是确定的，分区在当前节点上（数据无需夸节点传输）进行转换操作，也就是说可在同一个线程执行当前阶段计算任务，而且多个分区可以直接并行运行（因此分区数就决定并发计算的粒度，可用于Spark计算性能调优）。因此窄依赖的RDD可以划分到同一个执行阶段Stage；</li><li>对于宽依赖，由于 Shuffle 的存在，只能等多个父 RDD被 Shuffle 处理完成后（不同父分区的Shuffle导致数据需夸节点传输），才能开始对子RDD计算，因此遇到宽依赖就需要重新划分阶段。</li></ul><p><img src="https://img-blog.csdnimg.cn/20191224193522623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>正是有了以上对Stage的划分设计，Spark在执行作业时， 生成一个完整的、最优的执行计划，从而比MapReduce”更加聪明地利用资源地“完成计算作业。</p><h4 id="5、RDD-持久化"><a href="#5、RDD-持久化" class="headerlink" title="5、RDD 持久化"></a>5、RDD 持久化</h4><p>&#8195;&#8195;这一章节内容直接翻译spark：<br>&#8195;&#8195;RDD最重要的特点是可将数据集缓存在内存（或者磁盘上），这也是Spark之所以快的原因。使用RDD persist时，每个节点都会存储当前RDD计算的dataset，以便被其他RDD直接在内存加载使用，这种效果将使得之后的action算子至少提高10倍上计算速度。<br>&#8195;&#8195;通过调用persist()或者cache()方法即可触发RDD persist，而且持久化另外一个作用：Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.（可根据已有的算子重新计算丢失的RDD）<br>&#8195;&#8195;Spark 在持久化 RDDs 的时候提供了 3 种storage level：存在内存中的非序列化的 java 对象、存在内存中的序列化的数据以及存储在磁盘中。第一种选择的性能是最好的，因为 JVM 可以很快的访问 RDD 的每一个元素。第二种选择是在内存有限的情况下，使的用户可以以很低的性能代价而选择的比 java 对象图更加高效的内存存储的方式。如果内存完全不够存储的下很大的 RDDs，而且计算这个 RDD 又很费时的，那么选择第三种方式。</p><p>The full set of storage levels is:</p><table><thead><tr><th>Storage Level</th><th>Meaning</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will    not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td></tr><tr><td>MEMORY_AND_DISK</td><td>Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the    partitions that don’t fit on disk, and read them from there when they’re needed.</td></tr><tr><td>MEMORY_ONLY_SER   (Java and Scala)</td><td>Store RDD as <em>serialized</em> Java objects (one byte array per partition).    This is generally more space-efficient than deserialized objects, especially when using a    <a href="http://spark.apache.org/docs/latest/tuning.html">fast serializer</a>, but more CPU-intensive to read.</td></tr><tr><td>MEMORY_AND_DISK_SER   (Java and Scala)</td><td>Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of    recomputing them on the fly each time they’re needed.</td></tr><tr><td>DISK_ONLY</td><td>Store the RDD partitions only on disk.</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td>Same as the levels above, but replicate each partition on two cluster nodes.</td></tr><tr><td>OFF_HEAP (experimental)</td><td>Similar to MEMORY_ONLY_SER, but store the data in    <a href="http://spark.apache.org/docs/latest/configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to be enabled.</td></tr></tbody></table><p>Note: In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, and DISK_ONLY_2.<br>注意对使用Python来开发spark的话，它存储的对象都是由Pickle库实现序列化，因此对于python，除了OFF_HEAP这个level，其他level都适用。<br>Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it.<br>此外Spark在shuffle操作例如reduceByKey这类操作时会自动对RDD做持久化，以防止某些节点再做shuffle过程中挂了导致需要重新计算父RDD。Spark建议使用者在RDD需要重新使用的场景下可先持久化RDD。</p><h4 id="6、RDD的checkpointing机制"><a href="#6、RDD的checkpointing机制" class="headerlink" title="6、RDD的checkpointing机制"></a>6、RDD的checkpointing机制</h4><p>&#8195;&#8195;前面提到对于要恢复某些丢失的RDD，可根据父 RDDs 的血缘关系recomputed，但是如果这个血缘关系链很长的话（例如业务逻辑里面有10来个transmissions以及有多个actions），则recomputed需要耗费很长时间，因此在这种场景下，将一些 RDDs 的数据持久化到稳定存储系统中是有必要的。<br>&#8195;&#8195;checkpointing 对具有很长的血缘关系链且包含了宽依赖的 RDDs 是非常有用的，比如spark给出的PageRank 例子，在这些场景下，集群中的某个节点的失败会导致每一个父亲 RDD 的一些数据的丢失，最惨的是这些父RDD都是宽依赖以及很长的窄依赖链关系，显然需要重新所有的计算。<br>&#8195;&#8195;对于普通的窄依赖的 RDDs（spark给出例子中线性回归例子中的 points 和 PageRank 中的 link 列表数据），checkpointing 可能一点用都没有。如果一个节点失败了，spark照应可以很快在其他的节点中并行的重新计算出丢失了数据的分区，这个成本只是备份整个 RDD 的成本的一点点而已。</p><p>&#8195;&#8195;Spark 目前提供了一个 checkpointing 的 api（persist 中的标识为 REPLICATE，还有 checkpoint()），用户自行选择在一些最佳的 RDDs 来进行 checkpointing，以达到最小化恢复时间。（这就像你玩过关类游戏，总共20关，这个游戏本身没有自动保存检查点，必须用户自己触发，在第19关是最难的关卡耗费一周时间拿下，而你确忘记存档，能不崩溃吗）</p><h4 id="7、Spark分区与RDD分区（待更新）"><a href="#7、Spark分区与RDD分区（待更新）" class="headerlink" title="7、Spark分区与RDD分区（待更新）"></a>7、Spark分区与RDD分区（待更新）</h4><p>这部分内容难度较大，但确实能真正理解Spark和RDD并行计算的底层逻辑，相关知识点需回顾hadoop文件分区。这部分内容待更新</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面的博客&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103641824&quot;&gt;《深入理解Spark》 &lt;/a&gt;深入探讨了Spark架构原理内容，该文提到Stage的划分，为什么要做Stage划分？是为了得到更小的Task计算单元，分发给Executor的线程运行，将规模庞大、多流程的计算任务划分为有序的小颗粒的计算单元，实现更高效的计算。那么Stage划分怎么实现？需依赖RDD(Resilient Distributed Datasets，弹性分布式数据集)，可以说，RDD是Spark最为核心的概念。本文内容部分参考了&lt;a href=&quot;https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_RDD.md&quot;&gt;《弹性式数据集RDDs》&lt;/a&gt;以及&lt;a href=&quot;http://spark.apache.org/docs/latest/rdd-programming-guide.html&quot;&gt;《Spark官方文档》&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/categories/Spark/"/>
    
    
    <category term="RDD" scheme="https://yield-bytes.gitee.io/blog/tags/RDD/"/>
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Spark</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/22/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Spark/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/22/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Spark/</id>
    <published>2019-12-22T08:31:16.000Z</published>
    <updated>2020-02-04T08:39:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面博客文章里，已经把大数据实时分析项目在spark组件之前的各个组件原理、部署和测试都给出相关讨论，接下来是项目最核心的内容：实时计算部分，因为项目将使用spark streaming做微批计算（准实时计算），因此接下的文章内容将深入spark以及spark streaming架构原理，为后面实际计算编程做铺垫。</p><h4 id="1、Spark-是什么？"><a href="#1、Spark-是什么？" class="headerlink" title="1、Spark 是什么？"></a>1、Spark 是什么？</h4><p>&#8195;&#8195;Spark是一种分布式的并行计算框架，什么是计算框架？所谓的计算（在数据层面理解）其实是用于数据处理和分析的一套解决方案，例如Python的Pandas，相信用过Pandas都很容易理解Pandas擅长做什么，加载数据、对数据进行各类加工、分析数据等，只不过Pandas只适合在单机上的、数据量百万到千万级的计算组件，而Spark则是分布式的、超大型多节点可并行处理数据的计算组件。</p><p>&#8195;&#8195;Spark通常会跟MapReduce做对比，它与MapReduce 的最大不同之处在于Spark是基于内存的迭代式计算——Spark的Job处理的中间（排序和shuffling）输出结果可以保存在内存中，而不是在HDFS磁盘上反复IO浪费时间。除此之外，一个MapReduce 在计算过程中只有Map 和Reduce 两个阶段。而在Spark的计算模型中，它会根据rdd依赖关系预选设计出DAG计算图，把job分为n个计算阶段（Stage），因为它内存迭代式的，在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。<br>&#8195;&#8195;Spark提供了超过80种不同的Transformation和Action算子，如map，reduce，filter，reduceByKey，groupByKey，sortByKey，foreach等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小（尤其用Scala和Python写计算业务代码方面）。正是基于使用易用性，因此Spark能更好地用于基于分布式大数据的数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark生态如下：<br><img src="https://img-blog.csdnimg.cn/20191222113158864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><a id="more"></a><h4 id="2、Spark-运行模式"><a href="#2、Spark-运行模式" class="headerlink" title="2、Spark 运行模式"></a>2、Spark 运行模式</h4><p>目前最为常用的Spark运行模式有：</p><ul><li>Local：本地进程运行，例如启动一个pyspark交互式shell，一般用于开发调试Spark应用程序</li><li>Standalone：利用Spark自带的资源管理与调度器运行Spark集群，采用Master/Slave结构，可引入ZooKeeper实现spark集群HA</li><li>Hadoop YARN : 集群运行在YARN资源管理器上，资源管理交给YARN，Spark只负责进行任务调度和计算，参考本博客<a href="https://blog.csdn.net/pysense/article/details/103434832">《基于YARN HA集群的Spark HA集群》</a></li><li>Apache Mesos ：运行在著名的Mesos资源管理框架基础之上，该集群运行模式将资源管理交给Mesos，Spark只负责进行任务调度和计算<br>==<strong>Mesos和YARN两种资源有什么区别：</strong>==<br>之前看一个视频，对其给出的解释印象深刻：<br>Mesos：细腻度资源管控<br>YARN：粗粒度资源管控<br>例如有个老师要给45个学生上课，向教务处申请课室资源，若教务处以Mesos模式发放资源，那么它会发放只能容纳45个学生的课室，典型的按需分配；若教务处以YARN模式发放资源，那么它会发放能容200个学生的大教室，但实际上还有155个人位置资源空闲。这就是资源的细腻度和粗粒度的区别。</li></ul><h4 id="3、适合Spark的场景"><a href="#3、适合Spark的场景" class="headerlink" title="3、适合Spark的场景"></a>3、适合Spark的场景</h4><ul><li><p>Spark适用场景：<br>Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合，基于大数据的机器学习再适合不过，例如梯度下降法，需要不断迭代找到全局或局部最优解。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。<br>准实时计算场合：实时接收用户行为原始数据，并通过Spark Streaming计算（转换+加工），例如在广告、报表、推荐系统等业务上，在广告业务方面需要大数据做应用分析、效果分析、定向优化等，在推荐系统方面则需要大数据优化相关排名、个性化推荐以及热点点击分析等，这些业务天生适合大型的互联网巨头。</p></li><li><p>Spark不适用场景：</p><p>  内存消耗极大，在内存不足的情况下，Spark会下放到磁盘，会降低应有的性能<br>  有高实时性要求的流式计算业务，例如实时性要求毫秒级，对每一条数据都触发实时计算的，这种场合已经被Flink称霸。<br>  流线长或文件流量非常大的数据集不适合，这是因为这种场合rdd消耗极大的内存集群压力大时，一旦一个task失败会导致它前面一条线所有的前置任务全部重跑（尤其对于RDD 血缘关系链长且有多个宽依赖的情况），JVM的GC不够及时，内存不能及时释放，将会出现恶性循环导致更多的task失败，导致整个Application效率极低。所以为什么说Spark是适合“微批”处理，意味着每隔一段时间（1秒或者几秒不等）来一批次数据，Spark适合“一小口一小口准实时地吃数据”。</p></li></ul><h4 id="4、Spark相关术语"><a href="#4、Spark相关术语" class="headerlink" title="4、Spark相关术语"></a>4、Spark相关术语</h4><p><img src="https://img-blog.csdnimg.cn/20191221131426758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">一个完整的Spark应用程序，例如wordcount，在提交集群运行时:<br><code>./bin/spark-submit  --name word_count_app --master yarn  --deploy-mode cluster  --py-files word_count.py</code><br>它涉及到上图流程的相关术语： </p><ul><li><p>SparkContext:整个应用的上下文，控制应用的生命周期。</p></li><li><p>RDD：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型（鉴于RDD是Spark的核心概念，后面的有一篇博客给出相关讨论）。无法快速理解RDD？这里有两种方式可助于理解：<br>A、把它设想为分布式的Pandas dataframe，df也是一个数据集，也是被加载到内存上，然后利用pandas各个算子对df反复迭代最后得出计算结果。<br>B、<code>rdd = sc.parallelize(list(range(1000)), 10)</code>，这就创建了一个“轻量”的rdd数据集，设想下：这个数组有10亿项，分10个分区，有10个计算节点，每个节点负责1个分区的计算，也就是说从计算节点来看，每个节点负责1亿行的“小块rdd”；而从用户逻辑层面来看：就一个大rdd，包含10亿项数据</p></li><li><p>RDD的窄依赖和宽依赖</p><p>   A、窄依赖NarrowDependency（一对一）：不会产生分区之间的shuffle，所有的父RDD的partition会一一映射到子RDD的partition中，例如Map、FlatMap、Filter算子等</p><p>  B、宽依赖ShuffleDependency（一对多）：会引起多个分区之间的shuffle，父RDD中的partition会根据key的不同进行切分，划分到子RDD中对应的partition中，例如reduceByKey的任务。</p></li></ul><ul><li><p>DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系（关系链）。</p></li><li><p>Driver Program：Application中运行main函数并创建的SparkContext，创建SparkContext的目的是和集群的ClusterManager通信，进行计算资源的申请、任务的分配和监控等。因此也可认为SparkContext代表Driver控制程序。Driver负责对Application构建DAG图。</p></li><li><p>Cluster Manager：集群资源管理中心，例如YARN里面的ResourceManage，负责分配计算资源分配和回收。</p></li><li><p>Worker Node：启动Executor或Driver负责完成具体计算，在YARN模式中 Worker Node就是NodeManager节点。</p></li><li><p>Executor：是Application在Worker上面的一个进程，该进程会启动线程池方式去跑task，并负责把数据存在内存或者磁盘上。每个Application都有属于自己的一组Executors。在Spark on YARN模式下，Executor进程名为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor实例，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task。</p></li></ul><ul><li><p>Application：用户编写的Spark应用程序，例如下面启动一个名字为word_count_app的Application（简称app），该app的业务逻辑在word_count.py实现。一个Application包含多个Job。<br><code>./bin/spark-submit  --name word_count_app --master yarn  --deploy-mode cluster  --py-files word_count.py</code></p></li><li><p>Job：包含多个Task组成的并行计算，由Spark Action算子（collect、groupByKey、ReduceByKey、count、takeOrdered等）触发产生。一个action产生一个job，如果一个Application里面的业务代码有多少个action算子，就产生多少个Job。一个Job包含多个RDD及作用于相应RDD上的各种操作。</p></li></ul><ul><li><p>Task：任务，运行在Executor上的工作单元，是Executor中的一个线程（Executor是JVM进程，在YARN模式下，就是一个跑在container上JVM进程），与Hadoop MapReduce中的MapTask和ReduceTask一样，是运行Application的基本单位。多个Task组成一个Stage，而Task的调度和管理由TaskScheduler负责。</p></li><li><p>Stage：DAGScheduler将一个Job划分为若干Stages，每个Stage打包成一组Tasks，又称TaskSet。Stage的调度和划分由DAGScheduler负责。Stage又分为Shuffle Map Stage和Result Stage两种。Stage的边界就在发生Shuffle（rdd宽依赖）的地方。 </p></li><li><p>DAGScheduler：根据Job构建基于Stage的DAG（有向无环任务图），DAGScheduler会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，并提交Stage给TaskScheduler。</p></li><li><p>TaskScheduler：将任务(Task)分发给Executor，每个Executor负责运行什么Task由TaskScheduler分配。 </p></li><li><p>Shared Variables共享变量：Application在整个运行过程中，可能需要一些变量在每个Task中都使用，用于节省计算时间和IO。Spark有两种共享变量：一种缓存到各个节点的广播变量：broadcast；一种只支持加法操作：accumulator，一般用于对rdd求和sum以及累加计数counter。 这个概念需结合实际用例说明，否则难以理解。</p></li></ul><p>&#8195;&#8195;一句话：一个Application(其实就是你设计的Spark业务程序)由多个Job组成，一个Job由多个Stage组成，一个Stage由多个Task组成一个TaskSet。Stage是作业调度的基本单位，Task是执行计算和操作rdd的最小工作单元，以下面的wordcount语句对于的关系图说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20191221150736129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;这里为何画了2个Job，因为该计算任务包含两个Action：reduceByKey和sortBy。Application里面有多少个Action算子，Driver就给Application分配多少个Job。</p><p>&#8195;&#8195;Spark计算涉及的相关部件比较多而且相对抽象，需要在实际spark集群上跑几个测试application结合理解，本博客在前面的文章已经在测试项目中结合spark UI的截图就Job、Stage、Task等给出较为详细的说明，参考<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>第7章内容。</p><h4 id="5、Spark程序执行流程"><a href="#5、Spark程序执行流程" class="headerlink" title="5、Spark程序执行流程"></a>5、Spark程序执行流程</h4><p><img src="https://img-blog.csdnimg.cn/20191221131426758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;Spark 程序执行流程基于不同资源管理器其有不同的执行流程，以下以Standalone模式说明执行流程。其他资源管理模式的执行流程可以参考这篇文章：<a href="https://www.cnblogs.com/frankdeng/p/9301485.html">《Spark任务提交方式和执行流程》</a></p><ul><li><p>1)构建Spark Application的运行环境，启动SparkContext也即启动Driver，Driver向资源管理器注册并申请运行Executor资源；</p></li><li><p>2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；</p></li><li><p>3)Driver根据算子链预先构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task。</p></li><li><p>4)TaskScheduler将Task发送给Executor运行，同时Driver将应用程序代码传给Executor。</p></li><li><p>5)Task在Executor上运行，运行完毕释放所有资源。</p></li></ul><p>其实不管在哪种种资源模式下，Spark的程序执行流程一定离不开三条主线：</p><ul><li>Driver 申请资源用于启动Executor</li><li>Driver 构建DAG图，切分Stage，最终生产出多组TaskSet</li><li>Executor 领取Task和业务代码并执行</li></ul><h4 id="6、理解Spark-Stage的划分"><a href="#6、理解Spark-Stage的划分" class="headerlink" title="6、理解Spark Stage的划分"></a>6、理解Spark Stage的划分</h4><p>&#8195;&#8195;本部分内容比较重要，也是理解spark任务调度原理的关键，本章节内容参考<a href="http://sharkdtu.com/posts/spark-scheduler.html">《Spark Scheduler内部原理剖析》</a>其中任务调度内容</p><h5 id="6-1-Spark-Stage的划分"><a href="#6-1-Spark-Stage的划分" class="headerlink" title="6.1 Spark Stage的划分"></a>6.1 Spark Stage的划分</h5><p>&#8195;&#8195;这里以wordcount的Scala语句分析Spark对于一个application是如何划分stage的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;result&quot;)</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;以上就是一个Job，由rdd和Action方法封装而成，SparkContext将Job交给DAGScheduler提交，它会根据rdd的血缘关系的DAG图（你可以理解为预先规划的计算流程）进行切分，将一个Job划分为若干Stages，具体划分策略是，由处于末端的RDD不断通过依赖回溯判断父依赖是否为宽依赖，即以Shuffle为界，划分Stage。<br>划分的Stages分两类，一类叫做ResultStage，由Action方法决定，是DAG计算流程图最下游的Stage，这个Stage会最先被划分出来（因为是从末端回溯，因此首先遇到宽依赖reduceByKey，因此ResultStage最先被划出）。另一类叫做ShuffleMapStage，为下游ResultStage准备数据。<br>以下面的DAG流程作为说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;result&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20191221172609660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;Job由Action算子saveAsFile触发，该Job由rdd3和saveAsTextFile方法组成，根据rdd之间的依赖关系：</p><ul><li>首先从rdd3开始回溯搜索，在回溯搜索过程中，rdd3依赖rdd2，遇到reduceByKey需要宽依赖，所以在rdd3和rdd2之间划分Stage，该Stage为ResultStage</li><li>继续回溯，rdd2依赖rdd1，遇到Map窄依赖，不划分stage</li><li>rdd1依赖rdd0，遇到flapMap窄依赖，不划分Stage</li><li>最终回溯到源头rdd0，rdd0无父依赖，因此rdd2、rdd1和rdd0都划分到同一个Stage，即ShuffleMapStage。</li></ul><p>小结：一个Spark应用程序包括Job、Stage以及Task三个概念：</p><ul><li>Job是以Action方法(算子)为界，遇到一个Action方法则触发一个Job</li><li>Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次Stage划分</li><li>Task是Stage的子集，以并行度(分区数)来衡量，一个Stage有多少个partition就有多个task</li></ul><h5 id="6-2-Spark-DAG的可视化"><a href="#6-2-Spark-DAG的可视化" class="headerlink" title="6.2 Spark DAG的可视化"></a>6.2 Spark DAG的可视化</h5><p>&#8195;&#8195;为了更直观理解6.1 stage的划分，以及DAG可视化，这里用另外一个名为word-count的application语句跑一个测试，并在spark web UI查看相关划分情况以及执行计划：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(&quot;hdfs:&#x2F;&#x2F;nn:9000&#x2F;app&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false)</span><br></pre></td></tr></table></figure><p>从提交语句的逻辑即可直接看出分为两个Stage。在web端可直观看到Application有2个executor分别位于两个不同spark节点上。<br>==对于Stage 0：==<br>Stage 0 的Taskset有3个task，其中1个task在节点5上的executor0进程跑，另外2个task在节点6的executor1进程上跑。executor进程会使用多线程方式运行自己管辖的tasks<br><img src="https://img-blog.csdnimg.cn/20191222095742807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">==对于Stage 1：==<br>Stage 1 的Taskset有3个task，其中1个task在节点5上的executor0进程跑，另外2个task在节点6的executor1进程上跑。executor进程会使用多线程方式运行自己管辖的tasks<br><img src="https://img-blog.csdnimg.cn/20191222101000161.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这从两张图也可以看到Application的运行机制：引用官网文档<a href="http://spark.apache.org/docs/latest/job-scheduling.html">Scheduling Across Applications</a>的一句话：</p><blockquote><p>When running on a cluster, each Spark application gets an independent set of executor JVMs that only run tasks and store data for that application<br>当提交的application是在集群上跑时，每个application包含多个executor JVMs运行进程，这些executors（JVM进程）只负责为当前的application运行它的tasks任务和存储数据</p></blockquote><h4 id="7、Spark调度过程"><a href="#7、Spark调度过程" class="headerlink" title="7、Spark调度过程"></a>7、Spark调度过程</h4><p>&#8195;&#8195;有了前面内容铺垫后，本章节内容才能比较好理解，本章节内容参考<a href="http://sharkdtu.com/posts/spark-scheduler.html">《Spark Scheduler内部原理剖析》</a>其中的”Spark任务调度总览“，这篇文章质量不错。</p><h5 id="7-1-Spark的两级调度模型"><a href="#7-1-Spark的两级调度模型" class="headerlink" title="7.1 Spark的两级调度模型"></a>7.1 Spark的两级调度模型</h5><p>&#8195;&#8195;Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度。Stage级别的调度前面第6章节已经给出详细说明，至于Task级的调度相对复杂，原文作者给出了非常专业的、从源代码执行流程的说明，但这里不再重复累赘，毕竟重心还是以大数据项目在应用层的开发为主。<br>总体调度流程如下图所示。<br><img src="https://img-blog.csdnimg.cn/20191222111758872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ul><li>1)Spark RDD通过其Transactions和Action，形成了RDD的计划执行流程图，即DAG，最后通过Action的调用，触发Job并调度执行。</li><li>2)DAGScheduler负责Stage级的调度，主要是将DAG按照RDD的宽依赖切分成若干Stages，并将每个Stage里面的多个task打包成一个TaskSet。多个Stage就有多个TaskSets，这些TaskSets由DAGScheduler交给TaskScheduler调度。</li><li>3)TaskScheduler负责Task级的调度，TaskSets按照指定的调度策略分发到不同的Executor上执行。</li><li>4)调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多个实现API，分别对接不同的资源管理器YARN/Mesos/Standalone。你可以看成SchedulerBackend就是资源的代理，这个代理不断询问TaskScheduler是否需要资源去运行task。</li></ul><p>==这里会有一个疑问：每个Stage里面的task的数量怎么确定？==<br>每个Stage里面的task的数量是由该Stage中最后一个RDD的Partition数量所决定！</p><h5 id="7-2-以Spark-On-Yarn说明调度过程"><a href="#7-2-以Spark-On-Yarn说明调度过程" class="headerlink" title="7.2 以Spark On Yarn说明调度过程"></a>7.2 以Spark On Yarn说明调度过程</h5><p>&#8195;&#8195;Spark-On-Yarn模式下在任务调度期间，ApplicationMaster、Driver、DAGScheduler、TaskScheduler、Executor等内部模块的交互过程，以进一步巩固理解7.1章节的内容。<br>（从下图中有无发现一个有趣的现象：大数据多个组件理解难，其实不是那种像操作系统底层原理的难，而是在于：大数据的每个组件内部有很多个角色模块，每个角色负责的”工作“不一样，整个hadoop生态圈，每个组件和组件之间联系，这就会涉及到几十个实现模块，你要记忆和理解这几十个不同名字模块及其具体能做什么，以及他们之间的逻辑联系关系。<br>举个栗子：</p><ul><li>如果你是部门经理，部门只有9个人，3个岗位，如果现在接到一个开发项目，你当然很清楚和也容易安排每个岗位的人负责项目哪部分开发工作，并指定谁跟谁如何协调某部分内容。</li><li>如果你是市长，假设现在你接到一个上级大型项目，需要你亲自统筹和设计出如何让20个局完成该大型项目的方案。首先需要设计出10个局单位之间怎么配合，10个局单位共有100个岗位，每个局的每个岗位具体负责哪部分工作，并且你要设计出项目某部分内容是由哪个岗位和哪个岗位直接如何调度完成，够崩溃的。这就是大数据生态圈，概念多、杂，概念之间有一定逻辑关系，这些都需要你去理解和记忆，这就是难点所在。<br>）<br><img src="https://img-blog.csdnimg.cn/2019122211433139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li><li> Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver（这四部分是程序里面类或者模块，不是线程），并启动SchedulerBackend线程以及HeartbeatReceiver线程。</li><li>当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。</li><li>SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。</li><li>DAGScheduler负责的Stage调度</li><li>TaskScheduler负责的Task调度。</li><li>work node上Executor进程负责运行Task和把数据存在内存或者磁盘上</li></ul><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>&#8195;&#8195;本章内容对于博客接下来有关Spark以及Spark Streaming项目的文章理解起着关键的基础作用，下一篇文章，重点细讲RDDs。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面博客文章里，已经把大数据实时分析项目在spark组件之前的各个组件原理、部署和测试都给出相关讨论，接下来是项目最核心的内容：实时计算部分，因为项目将使用spark streaming做微批计算（准实时计算），因此接下的文章内容将深入spark以及spark streaming架构原理，为后面实际计算编程做铺垫。&lt;/p&gt;
&lt;h4 id=&quot;1、Spark-是什么？&quot;&gt;&lt;a href=&quot;#1、Spark-是什么？&quot; class=&quot;headerlink&quot; title=&quot;1、Spark 是什么？&quot;&gt;&lt;/a&gt;1、Spark 是什么？&lt;/h4&gt;&lt;p&gt;&amp;#8195;&amp;#8195;Spark是一种分布式的并行计算框架，什么是计算框架？所谓的计算（在数据层面理解）其实是用于数据处理和分析的一套解决方案，例如Python的Pandas，相信用过Pandas都很容易理解Pandas擅长做什么，加载数据、对数据进行各类加工、分析数据等，只不过Pandas只适合在单机上的、数据量百万到千万级的计算组件，而Spark则是分布式的、超大型多节点可并行处理数据的计算组件。&lt;/p&gt;
&lt;p&gt;&amp;#8195;&amp;#8195;Spark通常会跟MapReduce做对比，它与MapReduce 的最大不同之处在于Spark是基于内存的迭代式计算——Spark的Job处理的中间（排序和shuffling）输出结果可以保存在内存中，而不是在HDFS磁盘上反复IO浪费时间。除此之外，一个MapReduce 在计算过程中只有Map 和Reduce 两个阶段。而在Spark的计算模型中，它会根据rdd依赖关系预选设计出DAG计算图，把job分为n个计算阶段（Stage），因为它内存迭代式的，在处理完一个阶段以后，可以继续往下处理很多个阶段，而不只是两个阶段。&lt;br&gt;&amp;#8195;&amp;#8195;Spark提供了超过80种不同的Transformation和Action算子，如map，reduce，filter，reduceByKey，groupByKey，sortByKey，foreach等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小（尤其用Scala和Python写计算业务代码方面）。正是基于使用易用性，因此Spark能更好地用于基于分布式大数据的数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark生态如下：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191222113158864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>基于Sentinel模式部署高可用Redis</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/20/%E5%9F%BA%E4%BA%8ESentinel%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8Redis/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/20/%E5%9F%BA%E4%BA%8ESentinel%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8Redis/</id>
    <published>2019-12-20T13:31:37.000Z</published>
    <updated>2020-02-03T04:39:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在本博客前面的文章给出redis-cluster模式的配置和测试<a href="https://blog.csdn.net/pysense/article/details/100827689">《一篇文章掌握redis-cluster原理及其部署、测试》</a>，redis还有另外一种failover自动切换的部署方式，也即是本文给出的——Sentinel模式（哨兵模式），这两种方式部署的redis服务其实在普通的项目完全够用，例如个人在Django项目使用的Sentinel模式保证了”查询缓存服务以及一些频繁读取配置参数服务“的高可用。对于并发量大的需求，可以使用国内知名Codis——分布式Redis集群代理中间件，可配置规模更大的redis集群服务。</p><a id="more"></a><h4 id="1、安装redis"><a href="#1、安装redis" class="headerlink" title="1、安装redis"></a>1、安装redis</h4><p>&#8195;&#8195;为保持文章内容完整，这里给出redis的安装过程。两种方式，一种为yum 安装，另外一种下载包安装。这里选择bin包下载安装。目前redis稳定版为5.0.7，tar包为仅为1.7M，不愧为缓冲界的宠儿。安装包放在opt下，个人喜好将所有关开发的相关组件安装包放置于/opt目录，例如前面大数据各个组件的安装包，还是为了方便记忆、管理和查找。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# pwd</span><br><span class="line">/opt/redis-5.0.7</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget http://download.redis.io/releases/redis-5.0.7.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar xzf redis-5.0.7.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> redis-5.0.7</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make</span></span><br></pre></td></tr></table></figure><p>将redis启动命令所在的src路径加入系统变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# source ~/.bash_profile  </span><br><span class="line">PATH=$PATH:$HOME/bin:/opt/redis-5.0.7/src/  </span><br></pre></td></tr></table></figure><p>查看版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# redis-server -v</span><br><span class="line">Redis server v=5.0.7 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=864a7319aeb56c9b</span><br></pre></td></tr></table></figure><p>启动redis-server后台进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# redis-server &amp;</span><br><span class="line">[root@nn opt]# ps -ef |grep redis</span><br><span class="line">root      91054  60098  0 11:47 pts/0    00:00:00 redis-server *:6379</span><br><span class="line"></span><br><span class="line">[root@nn opt]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; set msg 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get msg</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure><h4 id="2、Sentinel-的配置说明"><a href="#2、Sentinel-的配置说明" class="headerlink" title="2、Sentinel 的配置说明"></a>2、Sentinel 的配置说明</h4><h5 id="2-1-官网有关Sentinel模式的基本信息"><a href="#2-1-官网有关Sentinel模式的基本信息" class="headerlink" title="2.1 官网有关Sentinel模式的基本信息"></a>2.1 官网有关Sentinel模式的基本信息</h5><blockquote><p>The current version of Sentinel is called <strong>Sentinel 2</strong>，A stable release of Redis Sentinel is shipped since Redis 2.8.<br>Sentinels by default run <strong>listening for connections to TCP port 26379</strong>,<br>If you are using the <code>redis-sentinel</code> executable， you can run Sentinel<br>with the following command line:<br><code>redis-sentinel /path/to/sentinel.conf</code></p></blockquote><p>redis要求启动Sentinel服务时必须带上其配置文件，否则直接返回启动失败。<br>启动Sentinel模式前的基本要求：</p><ul><li>You need at least three Sentinel instances for a robust deployment.（至少3个Sentinel实例，多数票选举）</li><li>最好在不同物理机上或者虚拟机上启动每个Sentinel 实例（在测试环境下，当然也可在同一台服务器里面，启动不同端口的多个实例也可完成测试。）</li><li>Sentinel + Redis distributed system does not guarantee that acknowledged writes are retained during failures，since Redis uses asynchronous replication.（Sentinel+redis分布式集群环境下，节点出现故障时，不保证写一致性，因redis异步复制方式实现集群数据同步）</li></ul><h5 id="2-2-redis官网Sentinel模式说明"><a href="#2-2-redis官网Sentinel模式说明" class="headerlink" title="2.2 redis官网Sentinel模式说明"></a>2.2 redis官网Sentinel模式说明</h5><p>有些缩写需要说明：</p><ul><li>Masters are called M1, M2, M3, …, Mn.</li><li>replicas are called R1, R2, R3, …, Rn (R stands for <em>replica</em>).（replica也就是slave角色，因为slave有歧视语义，很多中间件不再使用该词描述副角色，例如kafka备份分区的：replica）</li><li>Sentinels are called S1, S2, S3, …, Sn.</li><li>Clients are called C1, C2, C3, …, Cn.</li></ul><p>首先看官方首推的 basic setup with three boxes:It is based on three boxes, each box running both a Redis process and a Sentinel process. 每个box代表一个redis节点，确认master失败的选票数为2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">       +----+</span><br><span class="line">       | M1 |</span><br><span class="line">       | S1 |</span><br><span class="line">       +----+</span><br><span class="line">          |</span><br><span class="line">+----+    |    +----+</span><br><span class="line">| R2 |----+----| R3 |</span><br><span class="line">| S2 |         | S3 |</span><br><span class="line">+----+         +----+</span><br><span class="line"></span><br><span class="line">Configuration: quorum &#x3D; 2</span><br></pre></td></tr></table></figure><p>If the master M1 fails, S2 and S3 will agree about the failure and will<br>be able to authorize a failover, making clients able to continue.</p><p>如果主redis M1宕机（哨兵S1当然也会挂掉），那么其他节点上哨兵S2和哨兵S3发现与S1心跳失败，两者一致同意此时进入故障转移，选举R2为新的master M2。</p><p>redis sentinel实现高可用，但也会在某种程度下有丢失有些写数据。例如下面的情况：客户端C1原来与M1连接，写入M1，当M1挂了，到M2起来的这个过程，C1在这一过程写的部分数据会丢失。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">         +----+</span><br><span class="line">         | M1 |</span><br><span class="line">         | S1 | &lt;- C1 (writes will be lost)</span><br><span class="line">         +----+</span><br><span class="line">            |</span><br><span class="line">            &#x2F;</span><br><span class="line">            &#x2F;</span><br><span class="line">+------+    |    +----+</span><br><span class="line">| [M2] |----+----| R3 |</span><br><span class="line">| S2   |         | S3 |</span><br><span class="line">+------+         +----+</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;以上情况可通过以下两个配置实现数据丢失最小化。that allows to stop accepting writes if a master detects thatit is no longer able to transfer its writes to the specified number of replicas。<br>这里用到replica关键字单词，在本博客前面kafka文章里面，kafka也有自己的replica名词，不过kafka的replica是指top 分区后的副本，redis这里replica是从服务器（开源界不建议使用slave这个带有歧视的单词）。通过以下设置，只有当前master主机至少还有一个alive的replica才准许外部客户端写入数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br></pre></td></tr></table></figure><h4 id="3、一主两从的redis架构配置"><a href="#3、一主两从的redis架构配置" class="headerlink" title="3、一主两从的redis架构配置"></a>3、一主两从的redis架构配置</h4><p>&#8195;&#8195;第2部分的sentinel 2 高可用的前提是基于1主2两从的架构基础上实现的，如下架构，故首先得让一主两从的redis小集群跑起来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">       +----+</span><br><span class="line">       | M1 |</span><br><span class="line">       | S1 |</span><br><span class="line">       +----+</span><br><span class="line">          |</span><br><span class="line">+----+    |    +----+</span><br><span class="line">| R2 |----+----| R3 |</span><br><span class="line">| S2 |         | S3 |</span><br><span class="line">+----+         +----+</span><br><span class="line"></span><br><span class="line">Configuration: quorum &#x3D; 2</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;M1为1主，两从：R2、R3，在此基础上，每个节点运行sentinel进程，即可实现redis高可用架构。</p><h5 id="3-1-配置主从的redis-conf文件"><a href="#3-1-配置主从的redis-conf文件" class="headerlink" title="3.1 配置主从的redis.conf文件"></a>3.1 配置主从的<strong>redis.conf</strong>文件</h5><p>redis的配置文件的注释有分段说明，这里列出仅需修改的地方：<br>”一主redis“的配置说明：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# NETWORK</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定本机IP</span></span><br><span class="line">bind 182.0.0.10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ GENERAL</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台守护进程运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### SNAPSHOTTING</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放快照（数据日志文件的目录）dump.rdb</span></span><br><span class="line">dir /opt/redis-5.0.7/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ REPLICATION</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1主两从架构里，至少一个有个从服务器在线且在10秒以内延迟，主redis才能对外提供写服务器，否则客户端无法写</span></span><br><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br><span class="line"><span class="meta">#</span><span class="bash">这里也需设置，因为当该master挂了再重启，变成replica后，需要密码去认证新的master</span></span><br><span class="line">masterauth foo123</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SECURITY</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 为master设置认证密码</span></span><br><span class="line">requirepass foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## CLIENTS</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# MEMORY MANAGEMENT</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br></pre></td></tr></table></figure><p>”2个replica“节点的redis.conf配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# NETWORK</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定本机IP</span></span><br><span class="line">bind 182.0.0.11</span><br><span class="line"><span class="meta">#</span><span class="bash"> 另外一台从的IP为182.0.0.12</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ GENERAL</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 后台守护进程运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### SNAPSHOTTING</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放快照（数据日志文件的目录）dump.rdb</span></span><br><span class="line">dir /opt/redis-5.0.7/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ REPLICATION</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告诉从服务器主服务器的认证密码以及IP端口号，新版redis不再使用slave争议词，原版是slaveof</span></span><br><span class="line">replicaof 182.0.0.10 6379</span><br><span class="line">masterauth foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1主两从架构里，至少一个有个从服务器在线且在10秒以内延迟，主redis才能对外提供写服务器，否则客户端无法写</span></span><br><span class="line">min-replicas-to-write 1</span><br><span class="line">min-replicas-max-lag 10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SECURITY</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 从redis需要密码认证</span></span><br><span class="line">requirepass foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## CLIENTS</span></span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# MEMORY MANAGEMENT</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 按默认</span></span><br></pre></td></tr></table></figure><h5 id="3-2-启动和测试主从"><a href="#3-2-启动和测试主从" class="headerlink" title="3.2 启动和测试主从"></a>3.2 启动和测试主从</h5><p>启动所有主从redis-server，后台守护进程运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@p1 opt]# redis-server /opt/redis-5.0.7/redis.conf </span><br></pre></td></tr></table></figure><p>注意：<br>如果只启动master，从服务器还未启动，提示没有足够的从服务器在线，无法对外提供写服务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set test 1</span><br><span class="line">(error) NOREPLICAS Not enough good replicas to write.</span><br></pre></td></tr></table></figure><p>这是因为<code>min-replicas-to-write 1</code> 要求最少1个从redis在线后master才能接收客户端写数据。<br>在master set一个key</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@p1 opt]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; set foo 1</span><br><span class="line">​```shell</span><br><span class="line">在两个从服务器get key</span><br><span class="line">​```shell</span><br><span class="line">[root@p2 redis-5.0.7]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@p3 redis-5.0.7]# redis-cli -a foo123</span><br><span class="line">127.0.0.1:6379&gt; get foo</span><br><span class="line">&quot;1&quot;</span><br></pre></td></tr></table></figure><p>通过在master 查看主从信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; INFO Replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">min_slaves_good_slaves:2</span><br><span class="line">slave0:ip=182.0.0.11,port=6379,state=online,offset=1958,lag=0</span><br><span class="line">slave1:ip=182.0.0.12,port=6379,state=online,offset=1958,lag=1</span><br><span class="line">master_replid:1f69dd42ecea58d245859fd716c4eaee83a6e753</span><br><span class="line">master_replid2:0000000000000000000000000000000000000000</span><br><span class="line">master_repl_offset:1958</span><br><span class="line">second_repl_offset:-1</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:1</span><br><span class="line">repl_backlog_histlen:1958</span><br></pre></td></tr></table></figure><p>注：INFO [section]命令可以查看多个部分信息，也可指定查看某个section的信息<br>以上说明1主两从redis架构已经构建，该模式下，只有master才能写数据，replica只能get数据。如果尝试在replica上写数据，将提示readonly：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; <span class="built_in">set</span> bar <span class="number">2</span></span><br><span class="line">(error) READONLY You can<span class="string">&#x27;t write against a read only replica.</span></span><br></pre></td></tr></table></figure><h4 id="4、sentinel-高可用配置"><a href="#4、sentinel-高可用配置" class="headerlink" title="4、sentinel 高可用配置"></a>4、sentinel 高可用配置</h4><p>&#8195;&#8195;sentinel 高可用是基于主-从-从正常运行情况下配置，经过前面2.3点，相信很容易理解该sentinel的逻辑，</p><h5 id="4-1-配置sentinel-conf"><a href="#4-1-配置sentinel-conf" class="headerlink" title="4.1 配置sentinel.conf"></a>4.1 配置sentinel.conf</h5><p>&#8195;&#8195;主、从的sentinel.conf都一样，而且也很简单，更改两项属性即可，其他可以按默认值，如果需要调优，可自行参考conf的说明设置相应值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bind 0.0.0.0</span><br><span class="line">port 26379</span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;</span></span><br><span class="line">sentinel monitor  mymaster  182.0.0.10 6379 2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果master设置了密码，那么也告诉sentinel密码</span></span><br><span class="line">sentinel auth-pass mymaster foo123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  &lt;master-name&gt; 自行定义名称，这里使用mymaster默认值，后面的配置项都用了mymaster这个名，在django项目的settings，redis缓存设置也需要用到该master-name，无特殊需求不用改。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  &lt;quorum&gt; 裁定master挂了的最低通过票数</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Tells Sentinel to monitor this master, and to consider it <span class="keyword">in</span> O_DOWN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (Objectively Down) state only <span class="keyword">if</span> at least &lt;quorum&gt; sentinels agree.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 告诉Sentinel通监控master，如果有两个sentinel认为master挂了，说明master真的挂了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="4-2-测试redis高可用"><a href="#4-2-测试redis高可用" class="headerlink" title="4.2 测试redis高可用"></a>4.2 测试redis高可用</h5><p>启动所有主从的sentinel 服务，注意如果用 redis-server启动命令，需要带上选项 —-sentinel</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# redis-server /opt/redis-5.0.7/sentinel.conf --sentinel</span><br><span class="line">或者使用</span><br><span class="line">[root@nn opt]# redis-sentinel /opt/redis-5.0.7/sentinel.conf</span><br></pre></td></tr></table></figure><p>在master上查看sentinel状态，只需连接sentinel的工作端口即可，可以看到该master下带了两个replica，状态正常：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# redis-cli -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;182.0.0.10&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> 7) &quot;runid&quot;</span><br><span class="line"> 8) &quot;7cdc7e518b592168c94268f7d55fc2d237449118&quot;</span><br><span class="line"> 9) &quot;flags&quot;</span><br><span class="line">10) &quot;master&quot;</span><br><span class="line">11) &quot;link-pending-commands&quot;</span><br><span class="line">12) &quot;0&quot;</span><br><span class="line">13) &quot;link-refcount&quot;</span><br><span class="line">14) &quot;1&quot;</span><br><span class="line">15) &quot;last-ping-sent&quot;</span><br><span class="line">16) &quot;0&quot;</span><br><span class="line">17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">18) &quot;516&quot;</span><br><span class="line">19) &quot;last-ping-reply&quot;</span><br><span class="line">20) &quot;516&quot;</span><br><span class="line">21) &quot;down-after-milliseconds&quot;</span><br><span class="line">22) &quot;30000&quot;</span><br><span class="line">23) &quot;info-refresh&quot;</span><br><span class="line">24) &quot;3335&quot;</span><br><span class="line">25) &quot;role-reported&quot;</span><br><span class="line">26) &quot;master&quot;</span><br><span class="line">27) &quot;role-reported-time&quot;</span><br><span class="line">28) &quot;113804&quot;</span><br><span class="line">29) &quot;config-epoch&quot;</span><br><span class="line">30) &quot;0&quot;</span><br><span class="line">31) &quot;num-slaves&quot;</span><br><span class="line">32) &quot;2&quot;</span><br><span class="line">33) &quot;num-other-sentinels&quot;</span><br><span class="line">34) &quot;1&quot;</span><br><span class="line">35) &quot;quorum&quot;</span><br><span class="line">36) &quot;2&quot;</span><br><span class="line">37) &quot;failover-timeout&quot;</span><br><span class="line">38) &quot;180000&quot;</span><br><span class="line">39) &quot;parallel-syncs&quot;</span><br><span class="line">40) &quot;1&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看两个replica上的sentinel服务也正常运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:26379&gt; SENTINEL slaves mymaster</span><br><span class="line">1)  1) &quot;name&quot;</span><br><span class="line">    2) &quot;182.0.0.11:6379&quot;</span><br><span class="line">    3) &quot;ip&quot;</span><br><span class="line">    4) &quot;182.0.0.11&quot;</span><br><span class="line">    5) &quot;port&quot;</span><br><span class="line">    6) &quot;6379&quot;</span><br><span class="line">    7) &quot;runid&quot;</span><br><span class="line">    8) &quot;b7a9000c355584be472fe2409406b772b755b0ed&quot;</span><br><span class="line">    9) &quot;flags&quot;</span><br><span class="line">   10) &quot;slave&quot;</span><br><span class="line">   11) &quot;link-pending-commands&quot;</span><br><span class="line">   12) &quot;0&quot;</span><br><span class="line">   13) &quot;link-refcount&quot;</span><br><span class="line">   14) &quot;1&quot;</span><br><span class="line">   15) &quot;last-ping-sent&quot;</span><br><span class="line">   16) &quot;0&quot;</span><br><span class="line">   17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">   18) &quot;430&quot;</span><br><span class="line">   19) &quot;last-ping-reply&quot;</span><br><span class="line">   20) &quot;430&quot;</span><br><span class="line">   21) &quot;down-after-milliseconds&quot;</span><br><span class="line">   22) &quot;30000&quot;</span><br><span class="line">   23) &quot;info-refresh&quot;</span><br><span class="line">   24) &quot;9197&quot;</span><br><span class="line">   25) &quot;role-reported&quot;</span><br><span class="line">   26) &quot;slave&quot;</span><br><span class="line">   27) &quot;role-reported-time&quot;</span><br><span class="line">   28) &quot;169854&quot;</span><br><span class="line">   29) &quot;master-link-down-time&quot;</span><br><span class="line">   30) &quot;0&quot;</span><br><span class="line">   31) &quot;master-link-status&quot;</span><br><span class="line">   32) &quot;ok&quot;</span><br><span class="line">   33) &quot;master-host&quot;</span><br><span class="line">   34) &quot;182.0.0.10&quot;</span><br><span class="line">   35) &quot;master-port&quot;</span><br><span class="line">   36) &quot;6379&quot;</span><br><span class="line">   37) &quot;slave-priority&quot;</span><br><span class="line">   38) &quot;100&quot;</span><br><span class="line">   39) &quot;slave-repl-offset&quot;</span><br><span class="line">   40) &quot;24585&quot;</span><br><span class="line">2)  1) &quot;name&quot;</span><br><span class="line">    2) &quot;182.0.0.12:6379&quot;</span><br><span class="line">    3) &quot;ip&quot;</span><br><span class="line">    4) &quot;182.0.0.12.5&quot;</span><br><span class="line">    5) &quot;port&quot;</span><br><span class="line">    6) &quot;6379&quot;</span><br><span class="line">    7) &quot;runid&quot;</span><br><span class="line">    8) &quot;39edb70865b99916b1fd0013740e457135fe42e4&quot;</span><br><span class="line">    9) &quot;flags&quot;</span><br><span class="line">   10) &quot;slave&quot;</span><br><span class="line">   11) &quot;link-pending-commands&quot;</span><br><span class="line">   12) &quot;0&quot;</span><br><span class="line">   13) &quot;link-refcount&quot;</span><br><span class="line">   14) &quot;1&quot;</span><br><span class="line">   15) &quot;last-ping-sent&quot;</span><br><span class="line">   16) &quot;0&quot;</span><br><span class="line">   17) &quot;last-ok-ping-reply&quot;</span><br><span class="line">   18) &quot;430&quot;</span><br><span class="line">   19) &quot;last-ping-reply&quot;</span><br><span class="line">   20) &quot;430&quot;</span><br><span class="line">   21) &quot;down-after-milliseconds&quot;</span><br><span class="line">   22) &quot;30000&quot;</span><br><span class="line">   23) &quot;info-refresh&quot;</span><br><span class="line">   24) &quot;9197&quot;</span><br><span class="line">   25) &quot;role-reported&quot;</span><br><span class="line">   26) &quot;slave&quot;</span><br><span class="line">   27) &quot;role-reported-time&quot;</span><br><span class="line">   28) &quot;169855&quot;</span><br><span class="line">   29) &quot;master-link-down-time&quot;</span><br><span class="line">   30) &quot;0&quot;</span><br><span class="line">   31) &quot;master-link-status&quot;</span><br><span class="line">   32) &quot;ok&quot;</span><br><span class="line">   33) &quot;master-host&quot;</span><br><span class="line">   34) &quot;182.0.0.10&quot;</span><br><span class="line">   35) &quot;master-port&quot;</span><br><span class="line">   36) &quot;6379&quot;</span><br><span class="line">   37) &quot;slave-priority&quot;</span><br><span class="line">   38) &quot;100&quot;</span><br><span class="line">   39) &quot;slave-repl-offset&quot;</span><br><span class="line">   40) &quot;24585&quot;</span><br></pre></td></tr></table></figure><p>高可用测试：</p><p>kill 掉master的redis-server进程和redis-sentinel进程，模拟master服务器宕机情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@nn redis-5.0.7]# ps -ef |grep redis</span><br><span class="line">root       7385      1  0 *        00:02:35 redis-server 0.0.0.0:6379</span><br><span class="line">root      20367  20127  0 *    00:00:00 redis-cli -a foo123</span><br><span class="line">root      23760      1  0 *       00:00:03 redis-sentinel 0.0.0.0:26379 [sentinel]</span><br><span class="line">[root@nn redis-5.0.7]# kill -9 7385</span><br><span class="line">[root@nn redis-5.0.7]# kill -9 23760</span><br></pre></td></tr></table></figure><p>在replica 1查看目前是否已转移到：可以看到两个replica已经选举出新的master</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;182.0.0.11&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> </span><br></pre></td></tr></table></figure><p>也可通过redis-cli上查看：目前182.0.0.11已成为新的master，有个正常连接的replica</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info Replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:1</span><br><span class="line">min_slaves_good_slaves:1</span><br><span class="line">slave0:ip=182.0.0.12,port=6379,state=online,offset=146353,lag=1</span><br><span class="line">master_replid:4c1679086e6e4bb0bdd4956bcf979a6b964a8503</span><br><span class="line">master_replid2:ff0d944d22232a7b3489a8544c3109350aac6cd5</span><br><span class="line">master_repl_offset:146494</span><br><span class="line">second_repl_offset:145062</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:130700</span><br><span class="line">repl_backlog_histlen:15795</span><br></pre></td></tr></table></figure><p>在新maser set 值，可在剩余的一个replica get到相应的key</p><p>将原宕机的master恢复redis进程和sentinel进程，在新的master：182.0.0.11上，查看10节点已加入到replicas列表:<br>slave0:ip=182.0.0.12,port=6379,state=online,offset=211840,lag=1<br>slave1:ip=182.0.0.10,port=6379,state=online,offset=211840,lag=1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">min_slaves_good_slaves:2</span><br><span class="line">slave0:ip=182.0.0.12,port=6379,state=online,offset=211840,lag=1</span><br><span class="line">slave1:ip=182.0.0.10,port=6379,state=online,offset=211840,lag=1</span><br><span class="line">master_replid:4c1679086e6e4bb0bdd4956bcf979a6b964a8503</span><br><span class="line">master_replid2:ff0d944d22232a7b3489a8544c3109350aac6cd5</span><br><span class="line">master_repl_offset:211840</span><br><span class="line">second_repl_offset:145062</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:130700</span><br><span class="line">repl_backlog_histlen:81141</span><br></pre></td></tr></table></figure><p>以上完成redis 1主2从的高可用配置和测试，下面将在实际项目中引入。</p><h4 id="5、在python项目或者django的项目引入sentinel集群"><a href="#5、在python项目或者django的项目引入sentinel集群" class="headerlink" title="5、在python项目或者django的项目引入sentinel集群"></a>5、在python项目或者django的项目引入sentinel集群</h4><h5 id="5-1-python项目连接sentinel集群"><a href="#5-1-python项目连接sentinel集群" class="headerlink" title="5.1 python项目连接sentinel集群"></a>5.1 python项目连接sentinel集群</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> redis.sentinel <span class="keyword">import</span> Sentinel  </span><br><span class="line">In [<span class="number">4</span>]: st=Sentinel([(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="number">26379</span>),(<span class="string">&#x27;182.0.0.11&#x27;</span>,<span class="number">26379</span>),(<span class="string">&#x27;182.0.0.12&#x27;</span>,<span class="number">26379</span>)]) </span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: st.discover_master(<span class="string">&#x27;mymaster&#x27;</span>)                                     </span><br><span class="line">Out[<span class="number">7</span>]: (<span class="string">&#x27;182.0.0.10&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line">In [<span class="number">8</span>]: st.discover_slaves(<span class="string">&#x27;mymaster&#x27;</span>)                                     </span><br><span class="line">Out[<span class="number">8</span>]: [(<span class="string">&#x27;182.0.0.11&#x27;</span>, <span class="number">6379</span>), (<span class="string">&#x27;182.0.0.12&#x27;</span>, <span class="number">6379</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [5]: ?st.master_for                                                                               </span><br><span class="line">Signature:</span><br><span class="line">st.master_for(</span><br><span class="line">    service_name,</span><br><span class="line">    redis_class=&lt;class &#x27;redis.client.Redis&#x27;&gt;,</span><br><span class="line">    connection_pool_class=&lt;class &#x27;redis.sentinel.SentinelConnectionPool&#x27;&gt;,</span><br><span class="line">    **kwargs,</span><br><span class="line">)</span><br><span class="line">Docstring:</span><br><span class="line">Returns a redis client instance <span class="keyword">for</span> the ``service_name`` master.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建连接实例，kwargs参数跟redis.Redis入参一致</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意别漏了sentinel集群设了密码</span></span><br><span class="line">In [<span class="number">9</span>]: master_rd=st.master_for(service_name=<span class="string">&#x27;mymaster&#x27;</span>,password=<span class="string">&#x27;foo123&#x27;</span>,db=<span class="number">0</span>)  </span><br><span class="line">In [<span class="number">10</span>]: replica_rd=st.slave_for(service_name=<span class="string">&#x27;mymaster&#x27;</span>,password=<span class="string">&#x27;foo123&#x27;</span>,db=<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: master_rd              </span><br><span class="line">Out[<span class="number">19</span>]: Redis&lt;SentinelConnectionPool&lt;service=mymaster(master)&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向master写数据，不仅实现高可用，而且还实现读写分离</span></span><br><span class="line">In [<span class="number">23</span>]: master_rd.<span class="built_in">set</span>(<span class="string">&#x27;redis-HA&#x27;</span>,<span class="string">&#x27;good&#x27;</span>)                 </span><br><span class="line">Out[<span class="number">23</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: master_rd.get(<span class="string">&#x27;redis-HA&#x27;</span>)                </span><br><span class="line">Out[<span class="number">24</span>]: <span class="string">b&#x27;good&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果尝试向replica写数据则出错提示：replica只能读数据</span></span><br><span class="line">In [<span class="number">25</span>]: replica_rd.<span class="built_in">set</span>(<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;HA&#x27;</span>)    </span><br><span class="line">ReadOnlyError: You can<span class="string">&#x27;t write against a read only replica.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 从replica读数据，不仅实现高可用，而且还实现读写分离</span></span><br><span class="line"><span class="string">In [28]: replica_rd.get(&#x27;</span>redis-HA<span class="string">&#x27;)            </span></span><br><span class="line"><span class="string">Out[28]: b&#x27;</span>good<span class="string">&#x27;</span></span><br></pre></td></tr></table></figure><p>在服务器上把当前10节点master kill掉，再看看python取值是否被影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 经过3秒左右，再次获取最新的master可以看到已转移到11节点上，所以这3秒时间实际也是丢失数据的时间窗口</span></span><br><span class="line">In [<span class="number">43</span>]: st.discover_master(<span class="string">&#x27;mymaster&#x27;</span>)                                     Out[<span class="number">43</span>]: (<span class="string">&#x27;182.0.0.11&#x27;</span>, <span class="number">6379</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前集群仅有一个replica</span></span><br><span class="line">In [<span class="number">44</span>]: st.discover_slaves(<span class="string">&#x27;mymaster&#x27;</span>)</span><br><span class="line">Out[<span class="number">44</span>]: [(<span class="string">&#x27;182.0.0.12&#x27;</span>, <span class="number">6379</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主从切换后，不影响程序获取原有数据</span></span><br><span class="line">In [<span class="number">45</span>]: master_rd.get(<span class="string">&#x27;redis-HA&#x27;</span>)             </span><br><span class="line">Out[<span class="number">45</span>]: <span class="string">b&#x27;good&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: master_rd.<span class="built_in">set</span>(<span class="string">&#x27;bar&#x27;</span>,<span class="string">&#x27;test&#x27;</span>)               </span><br><span class="line">Out[<span class="number">47</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: replica_rd.get(<span class="string">&#x27;bar&#x27;</span>)         </span><br><span class="line">Out[<span class="number">48</span>]: <span class="string">b&#x27;test&#x27;</span></span><br></pre></td></tr></table></figure><h5 id="5-2-django项目中使用引入sentinel集群"><a href="#5-2-django项目中使用引入sentinel集群" class="headerlink" title="5.2 django项目中使用引入sentinel集群"></a>5.2 django项目中使用引入sentinel集群</h5><h6 id="5-2-1-单redis实例"><a href="#5-2-1-单redis实例" class="headerlink" title="5.2.1 单redis实例"></a>5.2.1 单redis实例</h6><p>在Django项目开发中，一般可以redis作为django后端cache的中间件，很多需求可以满足：例如验证码、session、缓存查询数据等。</p><p>首先需要django-redis这个库支持：pip install django-redis，具体用法参考<a href="http://niwinz.github.io/django-redis/latest/">官方doc</a></p><p>如果是单实例redis，在setting的设置相对简单：redis的0号db作为默认缓存，1号db作为hp这个App的数据缓存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将redis设为django缓存</span></span><br><span class="line">CACHES = &#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;BACKEND&#x27;</span>: <span class="string">&#x27;django_redis.cache.RedisCache&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;LOCATION&#x27;</span>: [<span class="string">&#x27;redis://182.0.0.10:6379/0&#x27;</span>], //连接redis url</span><br><span class="line">        <span class="string">&#x27;KEY_PREFIX&#x27;</span>: <span class="string">&#x27;dj&#x27;</span>,   //前缀名</span><br><span class="line">        <span class="string">&#x27;OPTIONS&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;CLIENT_CLASS&#x27;</span>: <span class="string">&#x27;django_redis.client.DefaultClient&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;CONNECTTON_POOL_KWARGS&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;max_connections&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;hp&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;BACKEND&#x27;</span>: <span class="string">&#x27;django_redis.cache.RedisCache&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;LOCATION&#x27;</span>: [<span class="string">&#x27;redis://182.0.0.10:6379/1&#x27;</span>],</span><br><span class="line">            <span class="string">&#x27;KEY_PREFIX&#x27;</span>: <span class="string">&#x27;dj:bi&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;OPTIONS&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;CLIENT_CLASS&#x27;</span>: <span class="string">&#x27;django_redis.client.DefaultClient&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;CONNECTTON_POOL_KWARGS&#x27;</span>: &#123;</span><br><span class="line">                    <span class="string">&#x27;max_connections&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&#x27;PASSWORD&#x27;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">SESSION_ENGINE = <span class="string">&#x27;django.contrib.sessions.backends.cache&#x27;</span></span><br><span class="line"><span class="comment">#django自身运行上下文使用默认数据库redis缓存</span></span><br><span class="line">SESSION_CACHE_ALIAS = <span class="string">&#x27;default&#x27;</span>  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>启动django shell测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@nn hp]<span class="comment"># python manage.py shell</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="keyword">from</span> django.core.cache <span class="keyword">import</span> cache,caches     </span><br><span class="line">In [<span class="number">4</span>]: cache.<span class="built_in">set</span>(<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;fofo&#x27;</span>)                     </span><br><span class="line">Out[<span class="number">4</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用default 库</span></span><br><span class="line">In [<span class="number">5</span>]: cache.get(<span class="string">&#x27;name&#x27;</span>)                                                   </span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">&#x27;fofo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用hp 库</span></span><br><span class="line">In [<span class="number">11</span>]: caches[<span class="string">&#x27;hp&#x27;</span>].<span class="built_in">set</span>(<span class="string">&#x27;code&#x27;</span>,<span class="number">133</span>)                                       </span><br><span class="line">Out[<span class="number">11</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: caches[<span class="string">&#x27;hp&#x27;</span>].get(<span class="string">&#x27;code&#x27;</span>)                                           </span><br><span class="line">Out[<span class="number">12</span>]: <span class="number">133</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意：如果使用cache方法，则默认使用default数据库，若需要使用其他名称的数据，需要用caches方法，并通过settings里面redis数据库名称来索引</p><p>在redis服务器查看default库相应的key：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) &quot;bar&quot;</span><br><span class="line">2) &quot;redis-HA&quot;</span><br><span class="line">3) &quot;dj:1:name&quot;</span><br></pre></td></tr></table></figure><p>因为在cache里面设置key的前缀为dj，加上cache会给key再打上<code>:1:</code>这个默认前缀，故实际存储的完整key名称:”dj:1:name”<br>同理查看db1号库的key</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; SELECT 1</span><br><span class="line">OK</span><br><span class="line"> </span><br><span class="line">127.0.0.1:6379[1]&gt; keys *</span><br><span class="line">1) &quot;dj:bi:1:code&quot;</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379[1]&gt; get dj:bi:1:code</span><br><span class="line">&quot;133&quot;</span><br></pre></td></tr></table></figure><p>若Django项目中需要使用更多进阶的原生client功能连接redis（支持redis所有方法)），需使用get_redis_connection，建议在实际项目使用该方法获取redis连接对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: <span class="keyword">from</span> django_redis <span class="keyword">import</span> get_redis_connection</span><br><span class="line">In [<span class="number">14</span>]: conn = get_redis_connection()                                     </span><br><span class="line">In [<span class="number">15</span>]: conn.<span class="built_in">set</span>(<span class="string">&#x27;fb&#x27;</span>,<span class="number">12</span>)                                                 </span><br><span class="line">Out[<span class="number">15</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: conn.get(<span class="string">&#x27;fb&#x27;</span>)                                                     </span><br><span class="line">Out[<span class="number">16</span>]: <span class="string">b&#x27;12&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h6 id="5-2-2-django项目的cache引入sentinel模式"><a href="#5-2-2-django项目的cache引入sentinel模式" class="headerlink" title="5.2.2  django项目的cache引入sentinel模式"></a>5.2.2  django项目的cache引入sentinel模式</h6><p>该模式需要安装新的django插件，<a href="https://github.com/KabbageInc/django-redis-sentinel">github地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install django-redis-sentinel</span><br><span class="line"><span class="meta">#</span><span class="bash">或者pip install django-redis-sentinel-redux 0.2.0</span> </span><br></pre></td></tr></table></figure><p>这个插件其实封装了redis.sentinel将其作为django_redis的插件之一，注意，该插件已不再维护，如果在重要项目上，不建议使用。（重要项目直接用codis，或者redis-cluster，以便可以pip install到相关的redis连接插件，或者自己参考模板写一个）</p><p>在setting.py中cache的设置：<br>Location 格式: master_name/sentinel_server:port,sentinel_server:port/db_id</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CACHES = &#123;</span><br><span class="line">        <span class="string">&quot;default&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;BACKEND&quot;</span>: <span class="string">&quot;django_redis.cache.RedisCache&quot;</span>,</span><br><span class="line">            <span class="string">&quot;LOCATION&quot;</span>: <span class="string">&quot;mymaster/188.0.0.10:26379,188.0.0.11:26379,188.0.0.12:26379/0&quot;</span></span><br><span class="line">            <span class="string">&quot;OPTIONS&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;PASSWORD&quot;</span>: <span class="string">&#x27;foo123&#x27;</span>,</span><br><span class="line">                <span class="string">&quot;CLIENT_CLASS&quot;</span>: <span class="string">&quot;django_redis_sentinel.SentinelClient&quot;</span>,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>该 django-redis-sentinel的插件代码实现不到120行，主要看看connect方法，写数据时用sentinel.discover_master(master_name)获取master去写，读数据时，随机取一个replica 读数据random.choice(sentinel.discover_slaves(master_name))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect</span>(<span class="params">self, write=<span class="literal">True</span>, SentinelClass=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates a redis connection with connection pool.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> SentinelClass <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        SentinelClass = Sentinel</span><br><span class="line">    self.log.debug(<span class="string">&quot;connect called: write=%s&quot;</span>, write)</span><br><span class="line">    master_name, sentinel_hosts, db = self.parse_connection_string(self._connection_string)</span><br><span class="line"></span><br><span class="line">    sentinel_timeout = self._options.get(<span class="string">&#x27;SENTINEL_TIMEOUT&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">    password = self._options.get(<span class="string">&#x27;PASSWORD&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    sentinel = SentinelClass(sentinel_hosts,</span><br><span class="line">                             socket_timeout=sentinel_timeout,</span><br><span class="line">                             password=password)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> write:</span><br><span class="line">        host, port = sentinel.discover_master(master_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            host, port = random.choice(sentinel.discover_slaves(master_name))</span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            self.log.debug(<span class="string">&quot;no slaves are available. using master for read.&quot;</span>)</span><br><span class="line">            host, port = sentinel.discover_master(master_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> password:</span><br><span class="line">        connection_url = <span class="string">&quot;redis://:%s@%s:%s/%s&quot;</span> % (password, host, port, db)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        connection_url = <span class="string">&quot;redis://%s:%s/%s&quot;</span> % (host, port, db)</span><br><span class="line">    <span class="keyword">return</span> self.connection_factory.connect(connection_url)</span><br></pre></td></tr></table></figure><p>注意：如果使用sentinel这个插件，那么需使用django_redis的get_redis_connection，而不是使用django.core.cache的cache</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> django_redis <span class="keyword">import</span> get_redis_connection   </span><br><span class="line">In [<span class="number">2</span>]: conn = get_redis_connection()       </span><br><span class="line">In [<span class="number">3</span>]: conn.<span class="built_in">set</span>(<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;airpods&#x27;</span>)               </span><br><span class="line">Out[<span class="number">3</span>]: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: conn.get(<span class="string">&#x27;apple&#x27;</span>)                              </span><br><span class="line">Out[<span class="number">4</span>]: <span class="string">b&#x27;airpods&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意：get_redis_connection()有bug，当使用conn实例获取哨兵集群的master或者replica，get_redis_connection()调用了<code>your pythonpath/python3.7/site-packages/redis/client.py </code>里面的<code> self.execute_command(&#39;SENTINEL MASTER&#39;, service_name)</code>，然而该方法是在6379端口连接的client执行<code>SENTINEL MASTER mymaster</code>，由于所有的SENTINEL 命令只能在26379端口下启动的client才能执行，因此直接用get_redis_connection()获取sentinel信息会提示未知命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: conn.sentinel_master(<span class="string">&#x27;mymaster&#x27;</span>) </span><br><span class="line">ResponseError: unknown command `SENTINEL`, <span class="keyword">with</span> args beginning <span class="keyword">with</span>: `MASTER`, `mymaster`, </span><br></pre></td></tr></table></figure><p>测试以上出错信息也简单：连接redis-server的6379端口client</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -a foo123 </span><br><span class="line">127.0.0.1:6379&gt; sentinel master mymaster</span><br><span class="line">(error) ERR unknown command `sentinel`, with args beginning with: `master`, `mymaster`, </span><br></pre></td></tr></table></figure><p>由于sentinel集群监听的是26379端口来执行有关查询命令（端口号在sentinel.conf文件配置），而get_redis_connection()使用的是6379，用此报错。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost redis-5.0.7]# redis-cli -a foo123 -p 26379</span><br><span class="line">127.0.0.1:26379&gt; sentinel master mymaster</span><br><span class="line"> 1) &quot;name&quot;</span><br><span class="line"> 2) &quot;mymaster&quot;</span><br><span class="line"> 3) &quot;ip&quot;</span><br><span class="line"> 4) &quot;188.0.0.10&quot;</span><br><span class="line"> 5) &quot;port&quot;</span><br><span class="line"> 6) &quot;6379&quot;</span><br><span class="line"> 7) &quot;runid&quot;</span><br><span class="line"> 8) &quot;5537ec765629633406942061f5993e475c42df8e&quot;</span><br><span class="line"> 9) &quot;flags&quot;</span><br><span class="line">10) &quot;master&quot;</span><br></pre></td></tr></table></figure><p>解决办法有三种种：<br>第一种：修改redis源码，改get_redis_connection，也不难<br>第二种：sentinel监听端口设为默认6379，redis-server设为其他端口号即可<br>第三种：不需要使用django redis插件，直接用redis原生库自行封装相关sentinel的操作</p><p>&#8195;&#8195;综上完成基于sentinel模式的redisHA配置以及详细解释了在python项目和django项目中如何使用sentinel集群模式，个人认为，目前该方案足够支持大部分中小企业内部自行开发项目。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在本博客前面的文章给出redis-cluster模式的配置和测试&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/100827689&quot;&gt;《一篇文章掌握redis-cluster原理及其部署、测试》&lt;/a&gt;，redis还有另外一种failover自动切换的部署方式，也即是本文给出的——Sentinel模式（哨兵模式），这两种方式部署的redis服务其实在普通的项目完全够用，例如个人在Django项目使用的Sentinel模式保证了”查询缓存服务以及一些频繁读取配置参数服务“的高可用。对于并发量大的需求，可以使用国内知名Codis——分布式Redis集群代理中间件，可配置规模更大的redis集群服务。&lt;/p&gt;</summary>
    
    
    
    <category term="Redis" scheme="https://yield-bytes.gitee.io/blog/categories/Redis/"/>
    
    
    <category term="Sentinel模式" scheme="https://yield-bytes.gitee.io/blog/tags/Sentinel%E6%A8%A1%E5%BC%8F/"/>
    
    <category term="redis集群" scheme="https://yield-bytes.gitee.io/blog/tags/redis%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>深入解析Python元类作用</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/18/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Python%E5%85%83%E7%B1%BB%E4%BD%9C%E7%94%A8/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/18/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Python%E5%85%83%E7%B1%BB%E4%BD%9C%E7%94%A8/</id>
    <published>2019-12-18T11:39:36.000Z</published>
    <updated>2020-02-03T07:04:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;python的元类使用场景一般在大型框架里面，例如Django的ORM框架、基于python实现的高级设计模式，元类的这部分内容相对晦涩，但也是作为python非常核心的知识点，通过解析其机制，有利于阅读和学习优秀中间件源代码的设计逻辑，在面向对象设计的重要性不言而喻。本博客后面的内容将会给出较为复杂的设计模式的文章，里面会出现较多的元类编程，因此有必要单独开一篇文章讨论python元类，相关内容将参考Stack Overflow上一篇很受欢迎的关于python metaclasses的文章：<a href="https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python">what-are-metaclasses-in-python</a></p><a id="more"></a><h4 id="1、python的class对象"><a href="#1、python的class对象" class="headerlink" title="1、python的class对象"></a>1、python的class对象</h4><h5 id="1-1-python的class也是一种object"><a href="#1-1-python的class也是一种object" class="headerlink" title="1.1 python的class也是一种object"></a>1.1 python的class也是一种object</h5><p>”在python的世界里，一切皆对象（object）“，这句话经常出现在很多python书籍中有关”面向对象或者类“文章里。如果你要深入python，首先面向对象的思维和面向对象的编程经历较为丰富。掌握对类的理解和运用，是理解元类的重要基础。<br>1.1 类即对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [1]: class Foo(object):</span><br><span class="line">   ...:     pass</span><br><span class="line">   ...:</span><br><span class="line">In [2]: my_instance&#x3D;Foo()</span><br><span class="line">In [3]: print(my_instance)</span><br><span class="line">&lt;__main__.Foo object at 0x108561b00&gt;</span><br></pre></td></tr></table></figure><p>这里创建了一个名为Foo的类，打印它的实例，可以看到该实例是一个Foo object 存放在内存地址：0x108561b00<br>这里只是说明Foo类的实例是object，怎么确认Foo是一个object呢？<br>两种方式可以回答：<br>方式一：在定义阶段：Foo(object)，Foo这个类继承object，所以Foo是object<br>方式二：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [8]: isinstance(Foo,object)</span><br><span class="line">Out[8]: True</span><br></pre></td></tr></table></figure><p>既然Foo是一个object，那么对于该object则可以扩展其功能：</p><ul><li><p>可赋值给变量</p></li><li><p>可被复制</p><ul><li>可添加属性</li><li>可将其当作函数参数传递</li><li>当然也可绑定新的类或者对象</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: NewFoo=Foo</span><br><span class="line">In [<span class="number">14</span>]: print(NewFoo)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Foo</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">In</span> [16]:</span> CloneFoo=copy.deepcopy(Foo)</span><br><span class="line">In [<span class="number">17</span>]: print(CloneFoo)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Foo</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">In</span> [20]:</span> Foo.new_attr=<span class="string">&#x27;bar&#x27;</span></span><br><span class="line">In [<span class="number">21</span>]: Foo.new_attr</span><br><span class="line">Out[<span class="number">21</span>]: <span class="string">&#x27;bar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [22]: def myfunc(obj):</span></span><br><span class="line"><span class="string">    ...:     print(obj.__name__)</span></span><br><span class="line"><span class="string">    ...:</span></span><br><span class="line"><span class="string">In [23]: myfunc(Foo)</span></span><br><span class="line"><span class="string">Foo</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [24]: class  NewFoo(object):</span></span><br><span class="line"><span class="string">    ...:     pass</span></span><br><span class="line"><span class="string">    ...:</span></span><br><span class="line"><span class="string">In [25]: Foo.x=NewFoo</span></span><br><span class="line"><span class="string">In [26]: Foo.x</span></span><br><span class="line"><span class="string">Out[26]: __main__.NewFoo</span></span><br></pre></td></tr></table></figure><p>总之，只要拿到一个object，你可以对其扩展任意你想得到效果</p></li></ul><h5 id="1-2-动态创建类"><a href="#1-2-动态创建类" class="headerlink" title="1.2 动态创建类"></a>1.2 动态创建类</h5><p>什么是动态创建类？只有运行这个程序后，通过判断给定参数来决定创建的是类A还是类B，而不是给程序写为固定生产类A。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">28</span>]: <span class="function"><span class="keyword">def</span> <span class="title">choose_class</span>(<span class="params">which</span>):</span></span><br><span class="line">    ...:     <span class="keyword">if</span> which ==<span class="string">&#x27;Foo&#x27;</span>:</span><br><span class="line">    ...:         <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:             <span class="keyword">pass</span></span><br><span class="line">    ...:         <span class="keyword">return</span> Foo</span><br><span class="line">    ...:     <span class="keyword">elif</span> which == <span class="string">&#x27;Bar&#x27;</span>:</span><br><span class="line">    ...:         <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:             <span class="keyword">pass</span></span><br><span class="line">    ...:         <span class="keyword">return</span> Bar</span><br><span class="line">    ...:</span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: myclass=choose_class(<span class="string">&#x27;Bar&#x27;</span>)</span><br><span class="line">In [<span class="number">32</span>]: print(myclass.__name__)</span><br><span class="line">Bar</span><br></pre></td></tr></table></figure><p>前面提到，既然Foo创建一个实例就是一个对象，把这个逻辑放在Foo身上：既然（某某某）创建一个对象就是一个Foo类，这个某某某是什么？可以做什么？<br>这个某某某就是type这个内建函数（函数也是一个对象），用type也可以像上面一样动态的创建一个类，用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type(要创建的类名，该类的所有父类名字组成的元组（若无父类，则为空元组），要创建该类需要用到入参：属性的字典)</span><br><span class="line">一般写成：</span><br><span class="line">type(class_name,class_bases,class_dict)</span><br></pre></td></tr></table></figure><p>经典方式一般如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    car=<span class="string">&#x27;Model 3&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=age</span><br><span class="line">        self.age=age</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;my name is &#123;&#125; and &#123;&#125; years old&#x27;</span>.<span class="built_in">format</span>(self.name,self.age))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>使用type函数动态创建以上Person类的过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为Person继承object，所以type的第二个位置参数为(object,)，Person类有三个属性因此class_dict为&#123;&#x27;car&#x27;:car,&#x27;__init__&#x27;:__init__,&#x27;info&#x27;:info&#125;)</span></span><br><span class="line">car = <span class="string">&#x27;Model 3&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">    self.name = name</span><br><span class="line">    self.age = age</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self</span>):</span></span><br><span class="line">    print(self.name,self.age)</span><br><span class="line"></span><br><span class="line">Person = <span class="built_in">type</span>(<span class="string">&#x27;Person&#x27;</span>,(<span class="built_in">object</span>,),&#123;<span class="string">&#x27;car&#x27;</span>:car,<span class="string">&#x27;__init__&#x27;</span>:__init__,<span class="string">&#x27;info&#x27;</span>:info&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: Person.__dict__</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">mappingproxy(&#123;<span class="string">&#x27;car&#x27;</span>: <span class="string">&#x27;Model 3&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__init__&#x27;</span>: &lt;function __main__.__init__(self, name, age)&gt;,</span><br><span class="line">              <span class="string">&#x27;info&#x27;</span>: &lt;function __main__.info(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;__module__&#x27;</span>: <span class="string">&#x27;__main__&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__dict__&#x27;</span>: &lt;attribute <span class="string">&#x27;__dict__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__weakref__&#x27;</span>: &lt;attribute <span class="string">&#x27;__weakref__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__doc__&#x27;</span>: <span class="literal">None</span>&#125;)</span><br><span class="line">In [<span class="number">5</span>]: person = Person(<span class="string">&#x27;Watt&#x27;</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: person</span><br><span class="line">Out[<span class="number">6</span>]: &lt;__main__.Person at <span class="number">0x10896f9e8</span>&gt;</span><br></pre></td></tr></table></figure><p>type创建完整Person类！本章内容主要通过类的创建，因此type这个函数并用其实现动态创建类，为元类这个话题做了铺垫，通过以上type创建的实例推出，python创建类必须要具备以下三个参数：</p><ul><li>1、类名class_name</li><li>2、继承关系class_bases</li><li>3、类的名称空间class_dict<br>这三个参数是揭开元类是如何改变类的秘密。</li></ul><h4 id="2-、Python的metaclass元类"><a href="#2-、Python的metaclass元类" class="headerlink" title="2 、Python的metaclass元类"></a>2 、Python的metaclass元类</h4><h5 id="2-1-认识type"><a href="#2-1-认识type" class="headerlink" title="2.1 认识type"></a>2.1 认识type</h5><p>前面的内容已经说明Python中的类也是对象，那么metaclass元类（元类自己也是对象）就是用来创建这些类的类，例如可以这样理解：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MyClass &#x3D; MetaClass()    #元类创建了类</span><br><span class="line">MyObject &#x3D; MyClass()     #被元类创建的类后，用它创建了实例</span><br></pre></td></tr></table></figure><p>在上一节内容，type可创建MyClass类：<br><code>MyClass = type(&#39;MyClass&#39;, (), &#123;&#125;)</code><br>MyClass是type()这个特殊类的一个实例，只不过这个实例直接就是类。<br>以上的逻辑主要说明一件事：type这个特殊类，就是python的一个元类，type是Python在背后用来创建所有类的元类，这句话如何理解？<br>首先，还是那句熟悉的话：在python的世界里，一切皆对象（object），包括各类数据结构、函数、类以及元类，它们都来源于一个“创物者”，这个强大的创物者这就是type元类。<br>查看每种对象的<code>__class__</code>属性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">55</span>]: num=<span class="number">10</span></span><br><span class="line">In [<span class="number">56</span>]: num.__class__</span><br><span class="line">Out[<span class="number">56</span>]: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: alist=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">In [<span class="number">59</span>]: alist.__class__</span><br><span class="line">Out[<span class="number">59</span>]: <span class="built_in">list</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">60</span>]: <span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    ...:     <span class="keyword">pass</span></span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: foo.__class__</span><br><span class="line">Out[<span class="number">61</span>]: function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: <span class="class"><span class="keyword">class</span> <span class="title">Bar</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    ...:     <span class="keyword">pass</span></span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: b=Bar()</span><br><span class="line">In [<span class="number">65</span>]: b.__class__</span><br><span class="line">Out[<span class="number">65</span>]: __main__.Bar</span><br></pre></td></tr></table></figure><p>说明每个对象都是某种类，<br>那么，一个<code>__class__</code>.<code>__class__</code>又是属于哪种类呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">69</span>]: num.__class__.__class__</span><br><span class="line">Out[<span class="number">69</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">70</span>]: foo.__class__.__class__</span><br><span class="line">Out[<span class="number">70</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: alist.__class__.__class__</span><br><span class="line">Out[<span class="number">71</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">72</span>]: b.__class__.__class__</span><br><span class="line">Out[<span class="number">72</span>]: <span class="built_in">type</span></span><br></pre></td></tr></table></figure><p>在继续往“创物者”方向靠近，发现最后都是type：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: foo.__class__.__class__.__class__</span><br><span class="line">Out[<span class="number">74</span>]: <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: foo.__class__.__class__.__class__.__class__</span><br><span class="line">Out[<span class="number">75</span>]: <span class="built_in">type</span></span><br></pre></td></tr></table></figure><p>以上说明：python的各类数据结构、函数、类以及元类，它们都来源于一个“创物者”，这个强大的创物者这就是type元类。</p><h5 id="2-2-认识-metaclass-属性"><a href="#2-2-认识-metaclass-属性" class="headerlink" title="2.2 认识__metaclass__属性"></a>2.2 认识<code>__metaclass__属性</code></h5><p>在python的元类的设计中，通常会出现<code>__metaclass__</code>属性，一般用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span>   <span class="comment">#python2版本的写法</span></span><br><span class="line">    __metaclass__ = something…</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params">metaclass=something</span>):</span>   <span class="comment">#python3版本的写法</span></span><br><span class="line">    __metaclass__ = something…</span><br></pre></td></tr></table></figure><p>当一个类的内部属性定义了<code>__metaclass__</code>属性，说明这个类将由某个元类来创建，当Foo类一旦被调用，因为设计类时可能有继承关系，因此会出现属性搜索过程：<br>1）Foo的定义里面有<code>__metaclass__</code>这个属性？如果有，解释器在内存中通过<code>something...</code>这个元类创建一个名字为Foo的类（对象）<br>2）如果在Foo的作用域内未找到<code>__metaclass__</code>属性，则继续在父类中寻找，若在父类找到，则用<code>something...</code>这个元类创建一个名字为Foo的类（对象）。<br>3）如果任何父类中都找不到<code>__metaclass__</code>属性，它就会在模块层次中去寻找<code>__metaclass__</code>，若找到，则用<code>something...</code>这个元类创建一个名字为Foo的类（对象）。<br>4）如果还是找不到<code>__metaclass__</code>,解释器最终使用内置的type来创建这个Foo类对象。</p><p>从上面过程可知，既然找到<code>something...</code>这个元类后它就可以创建类，说明它与type这个终极元类作用一样：都是用来创建类。<br>所以可推出：<code>__metaclass__</code>指向某个跟type功能相仿的元类———任何封装type的元类、继承type的子类、type本身</p><p>下面用元类实现的redis连接单例来感受下以上的逻辑：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisMetaSingleton</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;元类做初始化&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#super(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict) python2写法</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(class_name, class_bases, class_dict) <span class="comment"># python3写法</span></span><br><span class="line">        cls._instance =<span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">cls,host,port,db</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;call调用即完成类的实例化，用类的入参创建redis连接实例&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> cls._instance:</span><br><span class="line">            <span class="comment"># cls._instance = super(RedisMetaSingleton, cls).__call__(host,port,db) python2写法</span></span><br><span class="line">            cls._instance = <span class="built_in">super</span>().__call__(host,port,db)<span class="comment"># python3写法</span></span><br><span class="line">        <span class="keyword">return</span> cls._instance</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisSingleton</span>(<span class="params">metaclass=RedisMetaSingleton</span>):</span></span><br><span class="line">    <span class="string">&quot;redis操作专用类&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span>(<span class="params">self,host,port,db</span>):</span></span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.db=db</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conn</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>测试其实例是否为单例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">37</span>]: r1=RedisSingleton(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="string">&#x27;6379&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">In [<span class="number">38</span>]: r1</span><br><span class="line">Out[<span class="number">38</span>]: &lt;__main__.RedisSingleton at <span class="number">0x10fdc6080</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: r2=RedisSingleton(<span class="string">&#x27;182.0.0.10&#x27;</span>,<span class="string">&#x27;6379&#x27;</span>,<span class="number">0</span>)</span><br><span class="line">In [<span class="number">40</span>]: r1 <span class="keyword">is</span> r2</span><br><span class="line">Out[<span class="number">40</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>将单例逻辑放在定义元类这里，其他redis常用方法则放在子类实现。此外，该测试用例需要注意的两点：<br>1）RedisMetaSingleton的<code>__init__</code>和<code>__call__</code>第一个参数为cls，表示元类要创建的”类对象“，因此用cls而不是self。元类至于类对象（mataclass==&gt;class object），就像类至于实例(class==&gt;instance)，反复理解该句。<br>2）<code>__init__(cls,class_name,class_bases,class_dict)</code>，第2个到4个参数，其实就是type元类创建类的所需参数：<br>type（类名，父类元组（若无父类，则为空元组），类属性或内部方法的字典）<br>3）由于RedisMetaSingleton继承type，那么super(RedisMetaSingleton, cls)经过搜索后，父类就是type，因此<br>A: <code>super(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict)</code>的初始化就等价于<br><code>type.__init__(class_name, class_bases, class_dict)</code>的初始化<br>B:<code>super(RedisMetaSingleton, cls).__call__(host,port,db)</code>创建类对象就等价于<code>type.__call__(host,port,db)创建类对象</code><br>这就说明RedisSingleton指定由RedisMetaSingleton来创建，在RedisMetaSingleton内部最后交由<code>type.__init__</code>初始化，证明过程如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">119</span>]: <span class="class"><span class="keyword">class</span> <span class="title">RedisMetaSingleton</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">     ...:     <span class="string">&quot;&quot;&quot;在元类层面实现单例&quot;&quot;&quot;</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">     ...:         <span class="built_in">super</span>(RedisMetaSingleton, cls).__init__(class_name, class_bases, class_dict)</span><br><span class="line">     ...:         print(<span class="string">&#x27;class_name:&#123;&#125; class_bases:&#123;&#125; class_dict:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(class_name,class_bases,class_dict))</span><br><span class="line">     ...:         cls.cls_object =<span class="literal">None</span></span><br><span class="line">     ...:</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">cls,host,port,db</span>):</span></span><br><span class="line">     ...:         <span class="keyword">if</span> <span class="keyword">not</span> cls.cls_object:</span><br><span class="line">     ...:             cls.cls_object = <span class="built_in">super</span>(RedisMetaSingleton, cls).__call__(host,port,db)</span><br><span class="line">     ...:         <span class="keyword">return</span> cls.cls_object</span><br><span class="line">     ...:</span><br><span class="line">     ...: <span class="class"><span class="keyword">class</span> <span class="title">RedisSingleton</span>(<span class="params">metaclass=RedisMetaSingleton</span>):</span></span><br><span class="line">     ...:     <span class="string">&quot;redis操作专用类&quot;</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span>(<span class="params">self,host,port,db</span>):</span></span><br><span class="line">     ...:         self.host=host</span><br><span class="line">     ...:         self.port=port</span><br><span class="line">     ...:         self.db=db</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">conn</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         <span class="keyword">pass</span></span><br><span class="line">     ...:</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当以上代码在ipython解释器敲下去后，解释器对RedisMetaSingleton做了<code>__init__</code>初始化工作，故可得到以下打印信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class_name:RedisSingleton  # 类名</span><br><span class="line">class_bases:() # 父类元组</span><br><span class="line">class_dict:&#123;&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;RedisSingleton&#39;, &#39;__doc__&#39;: &#39;redis操作专用类&#39;, &#39;__init__&#39;: &lt;function RedisSingleton.__init__ at 0x10ffa0158&gt;, &#39;conn&#39;: &lt;function RedisSingleton.conn at 0x10ffa00d0&gt;&#125; # 要创建类的所有属性字典</span><br></pre></td></tr></table></figure><p>这三个参数就是type创建类的所需的参数:<br><code>type（类名，父类元组（若无父类，则为空元组），类属性或内部方法的字典）</code></p><p>以上内容略显复杂：归结起来，只要一个普通类指定需要元类创建，那么最终一定是由type这个终极元类来创建。</p><h5 id="2-3-自定义元类"><a href="#2-3-自定义元类" class="headerlink" title="2.3 自定义元类"></a>2.3 自定义元类</h5><p>对元类的构建和原理有一定认识后，那么可通过元类定制普通类，真正站在创物者的上帝视野来创建普通类。<br>现在有这样一个需求，要求创建的普通类的属性满足以下条件：<br>对于开头不是<code>__</code>的属性，都要大写，例如get_name(self)，在元类创建该普通类后都会被改为GET_NAME(self)<br>开头为<code>__</code>的属性，大小写保持不变。<br>从type创建普通类的“公式“可知：type（class_name,class_bases,class_dict）,class_dict就是放置了普通类属性或内部方法的字典），故只需要对其修改后，再重新传入type即可实现，需要基于type的<code>__new__</code>方法实现，type当然有<code>__new__</code>方法，因为type是元类，也是类。（元类必然有<code>__new__</code>方法，它创建的普通类例如Person、RedisConn才有这个内建的<code>__new__</code>方法。）<br>具体实现:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="comment"># 在这里，被创建的对象是类，因此第一个参数为cls，而不是类实例化的self，且需重写__new__方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        uppercase_attr = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, val <span class="keyword">in</span> class_dict.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>):</span><br><span class="line">                uppercase_attr[name.upper()] = val</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                uppercase_attr[name] = val</span><br><span class="line">        <span class="comment"># 用uppercase_attr替换了原class_dict，再传入到type，由type创建类，实现了自定义创建类的目标      </span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(class_name,class_bases, uppercase_attr)</span><br></pre></td></tr></table></figure><p>考虑到<code>return type(class_name,class_bases, uppercase_attr)</code>的写法不是pythone的OOP的写法（不够高级、抽象），因此又转化为以下OOP写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="comment"># 在这里，被创建的对象是类，因此第一个参数为cls，而不是实例self，且需重写__new__方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        attrs = ((name, value) <span class="keyword">for</span> name, value <span class="keyword">in</span> class_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>))</span><br><span class="line">        uppercase_attr  = <span class="built_in">dict</span>((name.upper(), value) <span class="keyword">for</span> name, value <span class="keyword">in</span> attrs)</span><br><span class="line">        <span class="comment"># 用uppercase_attr替换了原class_dict，再传入type，由type创建类，实现了自定义创建类的目标      </span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases, uppercase_attr)</span><br></pre></td></tr></table></figure><p>以上OOP风格在知名的python框架中到处可见！此外，我们知道通过super(UpperAttrMetaclass,cls)可以搜索到父类type，因此开发者会习惯写成以下形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        attrs = ((name, value) <span class="keyword">for</span> name, value <span class="keyword">in</span> class_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>))</span><br><span class="line">        uppercase_attr  = <span class="built_in">dict</span>((name.upper(), value) <span class="keyword">for</span> name, value <span class="keyword">in</span> attrs)  </span><br><span class="line">        <span class="comment"># return super(UpperAttrMetaclass,cls).__new__(cls,class_name,class_bases, uppercase_attr)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().__new__(cls,class_name,class_bases, uppercase_attr) <span class="comment"># python3的写法</span></span><br></pre></td></tr></table></figure><p>定义一个普通类测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">124</span>]: <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">metaclass=UpperAttrMetaclass</span>):</span></span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">     ...:         self.name=name</span><br><span class="line">     ...:         self.age=age</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">get_name</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         print(<span class="string">&#x27;name is:&#x27;</span>,self.name)</span><br><span class="line">     ...:     <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">     ...:         print(<span class="string">&#x27;age is:&#x27;</span>,self.age)</span><br><span class="line">     ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">125</span>]: Person.__dict__</span><br><span class="line">Out[<span class="number">125</span>]:</span><br><span class="line">mappingproxy(&#123;<span class="string">&#x27;GET_NAME&#x27;</span>: &lt;function __main__.Person.get_name(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;GET_AGE&#x27;</span>: &lt;function __main__.Person.get_age(self)&gt;,</span><br><span class="line">              <span class="string">&#x27;__module__&#x27;</span>: <span class="string">&#x27;__main__&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;__dict__&#x27;</span>: &lt;attribute <span class="string">&#x27;__dict__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__weakref__&#x27;</span>: &lt;attribute <span class="string">&#x27;__weakref__&#x27;</span> of <span class="string">&#x27;Person&#x27;</span> objects&gt;,</span><br><span class="line">              <span class="string">&#x27;__doc__&#x27;</span>: <span class="literal">None</span>&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当普通类Person定义后，解释器已经用元类UpperAttrMetaclass创建了Person普通类，其get_name和get_age方法名都被改为大写：GET_NAME和GET_AGE，其他双下划线的的方法名字保持不变。</p><p>此外，metaclass不局限于类的调用，也可以在任何对象内部调用，例如函数内部调用，例如以下一个模块upper_attr_by_func.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper_attr</span>(<span class="params">class_name,class_bases,class_dict</span>):</span></span><br><span class="line">    uppercase_attr = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name, val <span class="keyword">in</span> class_dict.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">&#x27;__&#x27;</span>):</span><br><span class="line">            uppercase_attr[name.upper()] = val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            uppercase_attr[name] = val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">type</span>(class_name,class_bases,class_dict)</span><br><span class="line"></span><br><span class="line">__metaclass__ = upper_attr  <span class="comment"># 该元类只能作用在本模块的所有类，对其他模块a.py、b.py无影响。</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">metaclass=UpperAttrMetaclass</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,age</span>):</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;name is:&#x27;</span>,self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;age is:&#x27;</span>,self.age)</span><br></pre></td></tr></table></figure><p>但需要注意的是：这种方式，元类的作用域将受到限制，仅能影响本模块upper_attr_by_func.py的所有类，对其他模块的类不产生作用。</p><p>综上，可总结元类定制普通类的创建一般如下过程：</p><ul><li><p>拦截一个普通类，一般在会使用<code>__new__，__init__ 和 __call__</code>，这些方法的内部可以放入对普通类进行不同定制的代码逻辑，其中：<br>A、<code>__new__</code>和<code>__init__</code>方法用于控制类的行为<br>B、 <code>__call__</code>方法用于控制类实例化的行为（</p></li><li><p>修改普通类，一般是指修改type(class_name,class_bases,class_dict）里面的三个参数，尤其对class_dict修改频繁，例如要求class_dict里面的必须要有<code>__doc__</code>属性，甚至针对父类元组class_bases来操作其继承关系。</p></li><li><p>通过<code>return super().__new__(cls,class_name,custom_bases, custom_class_dict)</code>创建并返回普通类</p></li></ul><p>元类一般用于复杂的框架上改变类的行为，对于普通简单的类，还有其他两种手段用来改变类：</p><ul><li>monkey patching</li><li>类装饰器<br>按Stack Overflow上的”建议“：<br>如果需要改变类，99%的情况下使用这两种方法，但其实98%的情况你根本不需要改变类。所以你看到很多较为简单的python轮子，一般是几个普通类就可以完成，根本无需动用元类来构建普通类。</li></ul><h4 id="3、元类定制普通类的示例——myORM"><a href="#3、元类定制普通类的示例——myORM" class="headerlink" title="3、元类定制普通类的示例——myORM"></a>3、元类定制普通类的示例——myORM</h4><p>本节内容参考了廖雪峰的<a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072">文章</a>，但其文章有很多关键的语句并无做更细致的说明，本节内容会在重要的元类实现逻辑上给出更详细的文字说明。<br>这里将实现一个轻量ORM——myORM：<br>在架构层面（不面向用户）</p><ul><li>架构定义了一个元类ModelMetaClass，用于拦截和修改普通类User定义阶段的class_dict属性字典，并用改造后的class_dict传入type来创建普通类User对象。</li><li>架构定义了一个Model类，用于把字段属性名和字段值封装在拼接的SQL语句，主要负责与数据库的增删查改。</li><li>架构定义了一个基本字段类Field：包含字段名和字段类型</li></ul><p>在用户层面（面向用户，用户可自行定义各种模型）</p><ul><li>用户定义一个整数字段类IntField，用于存放整型类型数据，例如id号，age</li><li>用户定义一个字符串字段类CharField，用于存放字符类型数据，例如name，email</li><li>创建一个User模型的一条行记录</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelMetaClass</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    例如定义User普通类,如下：</span></span><br><span class="line"><span class="string">    class User(Model):</span></span><br><span class="line"><span class="string">        id=IntField(&#x27;user_id&#x27;)</span></span><br><span class="line"><span class="string">        name=CharField(&#x27;user_name&#x27;)</span></span><br><span class="line"><span class="string">        email=CharField(&#x27;email&#x27;)</span></span><br><span class="line"><span class="string">        password=CharField(&#x27;password&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    那么元类ModelMetaClass捕获到的属性字典为： </span></span><br><span class="line"><span class="string">    class_dict=&#123;</span></span><br><span class="line"><span class="string">        &#x27;__module__&#x27;: &#x27;__main__&#x27;, </span></span><br><span class="line"><span class="string">        &#x27;__qualname__&#x27;: &#x27;User&#x27;,</span></span><br><span class="line"><span class="string">        &#x27;id&#x27;:IntField&lt;user_id,bigint&gt;,</span></span><br><span class="line"><span class="string">        &#x27;name&#x27;:CharField&lt;&#x27;user_name&#x27;,varchar(100)&gt;,</span></span><br><span class="line"><span class="string">        &#x27;email&#x27;:CharField&lt;&#x27;email&#x27;,varchar(100)),</span></span><br><span class="line"><span class="string">        &#x27;password&#x27;:CharField&lt;&#x27;password&#x27;,varchar(100)&gt;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls,class_name,class_bases,class_dict</span>):</span></span><br><span class="line">        <span class="keyword">if</span> class_name == <span class="string">&quot;Model&quot;</span>:</span><br><span class="line">            <span class="comment"># 如果创建的普通类为Model，不修改该类，直接创建即可</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases,class_dict)</span><br><span class="line">        print(<span class="string">&#x27;在ModelMetaClass元类捕获到普通类的属性字典:\n&#x27;</span>,class_dict)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 用于存放字段类型的属性</span></span><br><span class="line">        fields_dict=<span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> attr_name,attr <span class="keyword">in</span> class_dict.items():</span><br><span class="line">            <span class="comment"># 因为普通类属性字典还有__doc__,__qualname__等属性，因此要过滤出属于Field类型属性</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(attr,Field):</span><br><span class="line">                <span class="comment"># 打印结果：attr_name is &quot;id&quot;,field object is &quot;&lt;class &#x27;__main__.IntField&#x27;&gt;&quot;等字段信息</span></span><br><span class="line">                print(<span class="string">&#x27;attr_name is &quot;&#123;&#125;&quot;,field object is &quot;&#123;&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(attr_name,<span class="built_in">type</span>(attr)))</span><br><span class="line">                fields_dict[attr_name]=attr</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> field_name <span class="keyword">in</span> fields_dict.keys():</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            把Field类型的属性在原属性字典剔除：</span></span><br><span class="line"><span class="string">            u=User(name=&#x27;Wott&#x27;)</span></span><br><span class="line"><span class="string">            print(u.name)# 打印结果Wott</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            若不在原字典删除，那么u.name的值为：CharField&lt;email,varchar(100)&gt;</span></span><br><span class="line"><span class="string">            这个值是元类创建User类的属性值，它会覆盖了值为&#x27;Wott&#x27;的u.name实例属性，显然不符合实际情况。</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            class_dict.pop(field_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在原属性字典里增加一个私有属性（字典类型），这个私有属性保存了要创建字段的信息</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__fields_dict__&#x27;</span>]=fields_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取自定义模型中，指定Meta信息的数据库表名 </span></span><br><span class="line"><span class="string">        class User(Model):</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">            class Meta:</span></span><br><span class="line"><span class="string">                db_table=USER_T</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        attr_meta=class_dict.get(<span class="string">&#x27;Meta&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">        print(<span class="string">&#x27;User模型定义的Meta属性:&#x27;</span>,attr_meta) <span class="comment"># Meta: &lt;class &#x27;__main__.User.Meta&#x27;&gt;</span></span><br><span class="line">        meta_table_name=<span class="built_in">getattr</span>(attr_meta,<span class="string">&#x27;db_table&#x27;</span>,<span class="literal">None</span>)</span><br><span class="line">        print(<span class="string">&#x27;User模型在Meta指定的数据库表名为：&#x27;</span>,meta_table_name) <span class="comment"># User模型在Meta指定的数据库表名为：： USER_T</span></span><br><span class="line">        <span class="keyword">if</span> attr_meta <span class="keyword">and</span> meta_table_name:</span><br><span class="line">                table=meta_table_name</span><br><span class="line">                <span class="keyword">del</span> class_dict[<span class="string">&#x27;Meta&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">                table=class_name <span class="comment"># 若User模型没指定Meta中的数据库表名，则默认用User模型类名作为数据库表名</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__table_name__&#x27;</span>]=table </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把原Meta属性变成私有属性，这样创建出来的类更具OOP风格</span></span><br><span class="line">        class_dict[<span class="string">&#x27;__Meta__&#x27;</span>]=attr_meta</span><br><span class="line">        <span class="comment"># 以上完成对普通User模型的属性字典改造后，再重新把它传入到type，从而元类ModelMetaClass完成拦截=&gt;定制=&gt;创建普通类的过程。</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>.__new__(cls,class_name,class_bases,class_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params"><span class="built_in">dict</span>,metaclass=ModelMetaClass</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    1、Model类继承dict，目的是为了满足ORM中使用字典赋值和取值的方式例如</span></span><br><span class="line"><span class="string">    创建u=User(id=1,name=&#x27;Wott&#x27;,email=&#x27;11@11.com&#x27;,password=&#x27;1213&#x27;)</span></span><br><span class="line"><span class="string">2、Model内部通过拼接普通类的字段属性信息，封装了原生sql语句，例如save(),filter(),update()等方法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self,attr_name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重写内建getattr方法，可实现类似u.name这种点号获取属性值得方式,用起来更具ORM风格</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self[attr_name]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">raise</span> AttributeError(<span class="string">&quot;Model object has no attribute &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attr_name))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self,col_name,col_value</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        重写setattr方法，可实现类似u.name=&quot;Foo&quot;这种通过点号设置属性值的方式，用起来更具ORM风格</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self[col_name]=col_value</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 字段名列表</span></span><br><span class="line">        column_name_list=[]</span><br><span class="line">        place_holder_list=[]</span><br><span class="line">        <span class="comment"># 字段值列表</span></span><br><span class="line">        column_value_list=[]</span><br><span class="line">        fields_dict=self.__fields_dict__</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 存放字段的字典，key就是字段名，放在字段名列表，value就是字段值，放在字段值列表，两个列表用于拼接sql语句</span></span><br><span class="line">        <span class="keyword">for</span> attr_name,attr <span class="keyword">in</span> fields_dict.items():</span><br><span class="line">            <span class="comment"># 打印为：attr_name==&gt;id,attr==&gt;&lt;class &#x27;__main__.IntField&#x27;&gt;,attr.col_name==&gt;user_id</span></span><br><span class="line">            column_name_list.append(attr.col_name)</span><br><span class="line">            place_holder_list.append(<span class="string">&#x27;%s&#x27;</span>)</span><br><span class="line">            print(self) <span class="comment"># Model Dict:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125; </span></span><br><span class="line">            column_value_list.append(self[attr_name])</span><br><span class="line">            <span class="comment"># 或者column_value_list.append(getattr(self,attr_name))</span></span><br><span class="line"></span><br><span class="line">            sql = <span class="string">&#x27;insert into %s (%s) values (%s)&#x27;</span> % (self.__table_name__, <span class="string">&#x27;,&#x27;</span>.join(column_name_list), <span class="string">&#x27;,&#x27;</span>.join(place_holder_list))</span><br><span class="line">        print(<span class="string">&#x27;SQL语句:&#x27;</span>,sql)</span><br><span class="line">        print(<span class="string">&#x27;SQL的入参值列表:&#x27;</span>,column_value_list)</span><br><span class="line">            <span class="comment"># 连接mysql数据库后，使用cur.execute(sql,column_value_list)即可存入数据</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Model type:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">super</span>().__str__())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Field</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,col_name,col_type</span>):</span></span><br><span class="line">        self.col_name=col_name <span class="comment"># 字段名</span></span><br><span class="line">        self.col_type=col_type <span class="comment"># 字段类型</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        例如将会打印以下格式：</span></span><br><span class="line"><span class="string">        &#x27;id&#x27;: IntField&lt;user_id,bigint&gt;等</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&#123;&#125;&lt;&#123;&#125;,&#123;&#125;&gt;&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__,self.col_name,self.col_type)</span><br><span class="line">    </span><br><span class="line">    __repr__=__str__</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharField</span>(<span class="params">Field</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义字符型字段,默认可变字符类型长度为100</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, col_name, max_length=<span class="number">100</span></span>):</span></span><br><span class="line">        varchar_type=<span class="string">&quot;varchar(&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(max_length)</span><br><span class="line">        <span class="built_in">super</span>().__init__(col_name, varchar_type)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IntField</span>(<span class="params">Field</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义整型字段</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, col_name, col_type=<span class="string">&quot;bigint&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(col_name, col_type)</span><br></pre></td></tr></table></figure><p>ModelMetaClass负责顶层设计（改造），用户创建所有的普通类如User、Article、Department等，都会被该元类重设设计（改造它们的class_dict）后再创建出这些普通类。</p><p>用户定义了一个User模型，有四个字段，并指定创建为表名为USER_T</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">Model</span>):</span></span><br><span class="line">    <span class="built_in">id</span>=IntField(<span class="string">&#x27;user_id&#x27;</span>)</span><br><span class="line">    name=CharField(<span class="string">&#x27;user_name&#x27;</span>)</span><br><span class="line">    email=CharField(<span class="string">&#x27;email&#x27;</span>,max_length=<span class="number">200</span>)</span><br><span class="line">    password=CharField(<span class="string">&#x27;password&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        自定义数据库表名，这里虽然Meta定义为类，</span></span><br><span class="line"><span class="string">        但在元类ModelMetaClass的视角来看，它是一个属性，放在class_dict里面</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        db_table=<span class="string">&#x27;USER_T&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当用户定义完以上的普通类User后，Python解释器首先在当前类User的定义中查找metaclass，显然当前上下文环境没有找到，则继续在父类Model中查找metaclass，发现Model定义了metaclass=ModelMetaClass，故直接交由ModelMetaclass来创建该普通的User类。</p><p>用户创建了User实例并尝试向db插入该条数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">u=User(<span class="built_in">id</span>=<span class="number">1</span>,name=<span class="string">&#x27;Wott&#x27;</span>,email=<span class="string">&#x27;11@11.com&#x27;</span>,password=<span class="string">&#x27;1213&#x27;</span>) <span class="comment"># Model继承dict，因此Model子类User当然可用字典创建方式来创建实例</span></span><br><span class="line">u[<span class="string">&#x27;name&#x27;</span>]=<span class="string">&#x27;Foo&#x27;</span><span class="comment"># Model继承dict，因此Model子类User当然可使用字典方式赋值</span></span><br><span class="line">u.password=<span class="string">&#x27;Pa33Wood&#x27;</span><span class="comment"># Model内部定义__setattr__方法，故可用点号给属性赋值</span></span><br><span class="line">print(u.email) <span class="comment"># Model内部定义__getattr__方法，故可用点号取属性值</span></span><br><span class="line">u.save()</span><br></pre></td></tr></table></figure><p>以上代码各个位置上的print输出结果如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">第一句print输出：</span><br><span class="line">在ModelMetaClass元类捕获到普通类的属性字典:</span><br><span class="line"> &#123;&#x27;__module__&#x27;: &#x27;__main__&#x27;, &#x27;__qualname__&#x27;: &#x27;User&#x27;, &#x27;id&#x27;: IntField&lt;user_id,bigint&gt;, &#x27;name&#x27;: CharField&lt;user_name,varchar(100)&gt;, &#x27;email&#x27;: CharField&lt;email,varchar(200)&gt;, &#x27;password&#x27;: CharField&lt;password,varchar(100)&gt;, &#x27;Meta&#x27;: &lt;class &#x27;__main__.User.Meta&#x27;&gt;&#125;</span><br><span class="line"></span><br><span class="line">第二句print输出：</span><br><span class="line">attr_name is &quot;id&quot;,field object is &quot;&lt;class &#x27;__main__.IntField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;name&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;email&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line">attr_name is &quot;password&quot;,field object is &quot;&lt;class &#x27;__main__.CharField&#x27;&gt;&quot;</span><br><span class="line"></span><br><span class="line">第三句print输出：</span><br><span class="line">User模型定义的Meta属性: &lt;class &#x27;__main__.User.Meta&#x27;&gt;</span><br><span class="line"></span><br><span class="line">第四句print输出：</span><br><span class="line">User模型在Meta指定的数据库表名为： USER_T</span><br><span class="line"></span><br><span class="line">第五句print输出：</span><br><span class="line">11@11.com</span><br><span class="line"></span><br><span class="line">第六句print输出：</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line">Model type:&#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Foo&#x27;, &#x27;email&#x27;: &#x27;11@11.com&#x27;, &#x27;password&#x27;: &#x27;Pa33Wood&#x27;&#125;</span><br><span class="line"></span><br><span class="line">第七句print输出：</span><br><span class="line">SQL语句: insert into USER_T (user_id,user_name,email,password) values (%s,%s,%s,%s)</span><br><span class="line"></span><br><span class="line">第八句print输出：</span><br><span class="line">SQL的入参值列表: [1, &#x27;Foo&#x27;, &#x27;11@11.com&#x27;, &#x27;Pa33Wood&#x27;]</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;python的元类使用场景一般在大型框架里面，例如Django的ORM框架、基于python实现的高级设计模式，元类的这部分内容相对晦涩，但也是作为python非常核心的知识点，通过解析其机制，有利于阅读和学习优秀中间件源代码的设计逻辑，在面向对象设计的重要性不言而喻。本博客后面的内容将会给出较为复杂的设计模式的文章，里面会出现较多的元类编程，因此有必要单独开一篇文章讨论python元类，相关内容将参考Stack Overflow上一篇很受欢迎的关于python metaclasses的文章：&lt;a href=&quot;https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python&quot;&gt;what-are-metaclasses-in-python&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Python进阶" scheme="https://yield-bytes.gitee.io/blog/categories/Python%E8%BF%9B%E9%98%B6/"/>
    
    
    <category term="python元类" scheme="https://yield-bytes.gitee.io/blog/tags/python%E5%85%83%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>基于YARN HA集群的Spark HA集群</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/08/%E5%9F%BA%E4%BA%8EYARN%20HA%E9%9B%86%E7%BE%A4%E7%9A%84Spark%20HA%E9%9B%86%E7%BE%A4/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/08/%E5%9F%BA%E4%BA%8EYARN%20HA%E9%9B%86%E7%BE%A4%E7%9A%84Spark%20HA%E9%9B%86%E7%BE%A4/</id>
    <published>2019-12-08T10:06:08.000Z</published>
    <updated>2020-02-03T07:04:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面的<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>，这篇是基于非HA模式下hadoop集群的spark集群HA配置，而本文将给出基于HA模式下hadoop集群的spark集群HA配置，并将yarn HA集群映入到spark中，做资源管理。为何要做些环境的配置呢？因为到本篇文章为止，已经完成hadoop HA集群、hbaseHA集群，hive集群（非HA）、sparkHA集群、flumeHA集群、kafka HA集群，实现实时数据流动，接下的文章重点探讨spark streaming、spark以及pyspark相关知识，这将涉及多个计算任务以及相关计算资源的分配，因此需要借助yarn HA集群强大的资源管理服务来管理spark的计算任务，从而实现完整的、接近生产环境的、HA模式下的大数据实时分析项目的架构。</p><a id="more"></a><p>服务器资源分配表(仅列出yarn和spark)：</p><table><thead><tr><th>节点</th><th>yarn 角色</th><th>spark 角色</th></tr></thead><tbody><tr><td>nn</td><td>ResourceManager， NodeManager</td><td>Master，Worker</td></tr><tr><td>dn1</td><td>NodeManager</td><td>Worker</td></tr><tr><td>dn2</td><td>ResourceManager， NodeManager</td><td>Master，Worker</td></tr></tbody></table><p>&#8195;&#8195;这里再提下yarn管理大数据集群计算中对资源有效管理（主要指CPU、物理内存以及虚拟内存）的重要性：</p><blockquote><p>&#8195;&#8195;整个集群的计算任务由ResourceManager和NodeManager共同完成，其中，ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为计算任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。</p></blockquote><p>&#8195;&#8195;因为spark就是负责计算，有大量计算任务要运行，每个任务总得分配cpu和内存给它用，否则某些计算任务会被“饿死”（巧妇难为无米之炊），这种比喻比较形象。</p><h3 id="YARN-HA模式的配置"><a href="#YARN-HA模式的配置" class="headerlink" title="YARN HA模式的配置"></a>YARN HA模式的配置</h3><p>&#8195;&#8195;yarn HA模式的运行是于hadoop HA模式运行的，关于hadoop HA部署和测试可以参考本博客文章<a href="https://blog.csdn.net/pysense/article/details/102635656">《基于Hadoop HA集群部署HBase HA集群（详细版）》</a>的第6章内容，考虑到后面文章将会给出各种spark计算任务，结合测试服务器本身cpu和内存资源有限，这里主要重点介绍yarn-site.xml和mapred-site.xml配置文件说明。</p><h4 id="完整-yarn-site-xml配置"><a href="#完整-yarn-site-xml配置" class="headerlink" title="完整 yarn-site.xml配置"></a>完整 yarn-site.xml配置</h4><p>&#8195;&#8195;yarn-site的配置其实分为两大块：第一部分为yarn HA集群的配置，第二部分为根据现有测试服务器资源来优化yarn配置。<br>==yarn-site.xml在三个节点上都使用相同配置，无需更改==<br>第一部分：yarn HA集群的配置<br>（注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code>&lt;configuration&gt;&lt;/configuration&gt;</code>）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 启用yarn HA高可用性 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定resourcemanager的名字，自行命名，跟服务器hostname无关 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hayarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定nn节点为rm1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定dn2节点为rm2  --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;dn2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定当前机器nn作为主rm1 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定zookeeper集群机器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn:2181,dn1:2181,dn2:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上将nn和dn2作为yarn集群主备节点，对应的id为rm1、rm2</p><p>第二部分：yarn的优化配置<br>A、禁止检查每个任务正使用的物理内存量、虚拟内存量是否可用<br>若任务超出分配值，则将其杀掉。考虑到作为测试环境，希望看到每个job都能正常运行，以便记录其他观测事项，这里将其关闭。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>B、配置RM针对单个Container能申请的最大资源或者RM本身能配置的最大内存<br>配置解释：单个容器可申请的最小与最大内存，Application在运行申请内存时不能超过最大值，小于最小值则分配最小值，例如在本文测试中，因计算任务较为简单，无需太多资源，故最小值设为512M，最大值设为1024M。注意最大最不小于1G，因为yarn给一个executor分配512M时，还需要另外动态的384M内存（Required executor memory (512), overhead (384 MB)）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;512&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>若将yarn.scheduler.maximum-allocation-mb设为例如512M，spark on yarn就会启动失败。</p><p>C、NM的内存资源配置，主要是通过下面两个参数进行的</p><p>第一个参数：每个节点可用的最大内存，默认值为-1，代表着yarn的NodeManager占总内存的80%，本文中，物理内存为1G</p><p>第二个参数：NM的虚拟内存和物理内存的比率，默认为2.1倍</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;nm向本机申请的最大物理内存，默认8G&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>vmem-pmem-ratio的默认值为2.1，由于本机器中，每个节点的物理内存为1G，因此单个RM拿到最大虚拟内存为2.1G，例如在跑spark任务，会出现<code>2.5 GB of 2.1 GB virtual memory used. Killing container</code>的提示，Container申请的资源为2.5G，已经超过默认值2.1G，当改为3倍时，虚拟化够用，故解决可该虚拟不足的情况。</p><h4 id="mapred-site-xml的配置文件说明"><a href="#mapred-site-xml的配置文件说明" class="headerlink" title="mapred-site.xml的配置文件说明"></a>mapred-site.xml的配置文件说明</h4><p>mapred-site的配置其实分为两大块：第一部分为mapreduce的基本配置，第二部分为根据现有测试服务器资源来优化mapreduce计算资源分配的优化配置。<br>==mapred-site.xml在三个节点上都需要配置，只需把nn主机名改为当前节点的主机名即可==<br>第一部分：mapreduce的基本配置<br>（注意这里仅给出property，若复制该配置内容，需在xml文件里面加入<code>&lt;configuration&gt;&lt;/configuration&gt;</code>）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"> &lt;!-- 使用yarn框架来管理MapReduce --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!-- mp所需要hadoop环境 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;!-- 打开Jobhistory --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定nn作为jobhistory服务器 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;nn:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!--存放已完成job的历史日志 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放正在运行job的历史日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/history/done_intermediate&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--存放yarn stage的日志 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/history/staging&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这里主要配置开启jobhistory服务以及MapReduce多种日志存放</p><p>第二部分：mapreduce的优化项</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;100&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个mapper任务的物理内存限制&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;200&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;每个reducer任务的物理内存限制&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;每个mapper任务申请的虚拟cpu核心数，默认1&lt;/description&gt; </span><br><span class="line"> &lt;/property&gt; </span><br><span class="line"> </span><br><span class="line"> &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;每个reducer任务申请的虚拟cpu核心数，默认1&lt;/description&gt; </span><br><span class="line"> &lt;/property&gt; </span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx100m&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;mapper阶段的JVM的堆大小&lt;/description&gt;     </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx200m&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;reduce阶段的JVM的堆大小&lt;/description&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>根据当前服务器物理配置资源，在内存和CPU方面给mapper和reducer任务进行调优。</p><h4 id="yarn-HA的启动"><a href="#yarn-HA的启动" class="headerlink" title="yarn HA的启动"></a>yarn HA的启动</h4><p>首先确保hadoop HA集群已正常启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState nn</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# hdfs haadmin -getServiceState dn2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure><p>启动yarn HA服务，只需在nn节点启动yarn后，其他节点会自动启动相应服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@nn sbin]# start-yarn.sh </span><br><span class="line">[root@nn sbin]# yarn rmadmin -getServiceState rm1</span><br><span class="line">active</span><br><span class="line">[root@nn sbin]# yarn rmadmin -getServiceState rm2</span><br><span class="line">standby</span><br></pre></td></tr></table></figure><p>以上完成yarn HA配置，因为涉及hadoop HA和调优，因此不建议刚入门的同学就按此配置继续测试，建议从最原始、最简单的非HA hadoop开始着手。<br>下面开始配置spark。</p><h3 id="spark-HA-集群及其基本测试"><a href="#spark-HA-集群及其基本测试" class="headerlink" title="spark HA 集群及其基本测试"></a>spark HA 集群及其基本测试</h3><h4 id="修改spark配置"><a href="#修改spark配置" class="headerlink" title="修改spark配置"></a>修改spark配置</h4><p>&#8195;&#8195;经历第1章节繁琐的yarn HA配置后， 当资源管理问题得到妥善解决，那么接下的计算任务将实现的非常流畅。<br>spark HA集群详细的部署和测试，请参考<a href="https://blog.csdn.net/pysense/article/details/102536716">《基于hadoop3.1.2分布式平台上部署spark HA集群》</a>的第8章节，本文不再累赘。<br>&#8195;&#8195;把spark 的任务交给yarn管理还需要在HA集群上再加入部分配置，改动也简单 ，只需在spark-defaults.conf和spark-env.sh改动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf]# vi spark-defaults.conf</span><br><span class="line"></span><br><span class="line">#spark.master                     spark:&#x2F;&#x2F;nn:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"></span><br><span class="line"># spark.eventLog.dir               hdfs:&#x2F;&#x2F;nn:9000&#x2F;directory</span><br><span class="line">spark.eventLog.dir               hdfs:&#x2F;&#x2F;hdapp&#x2F;directory</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              512m</span><br><span class="line">spark.driver.cores               1</span><br><span class="line">spark.yarn.jars                  hdfs:&#x2F;&#x2F;hdapp&#x2F;spark_jars&#x2F;*</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey&#x3D;value -Dnumbers&#x3D;&quot;one two three&quot;</span><br></pre></td></tr></table></figure><p>重点配置项目说明：</p><p>原standalone模式下：spark.master设为 spark://nn:7077</p><p>因为spark已经配成HA模式，因此无需指定master是谁，交由zookeeper管理。</p><p>spark.eventLog.dir              hdfs://hdapp/directory<br>这里hdfs路径从nn:9000改为hdapp，是因为hadoop已经配置为HA模式，注意集群模式下是不需要加上端口： hdfs://hdapp:9000/directory，这会导致NameNode无法解析host部分。</p><p>spark.yarn.jars                  hdfs://hdapp/spark_jars/*<br>这里需要将spark跟目录下的jar包都上传到hdfs指定的spark_jars目录下，若不这么处理，每次提交spark job时，客户端每次得先上传这些jar包到hdfs，然后再分发到每个NodeManager，导致任务启动很慢。而且启动spark也会提示：<br>==WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.==</p><p>解决办法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -mkdir  &#x2F;spark_jars</span><br><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# hdfs dfs -put  jars&#x2F;*  &#x2F;spark_jars</span><br></pre></td></tr></table></figure><p>spark-defaults.conf在三个节点上使用相同配置。</p><p>spark-env.sh的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;conf</span><br><span class="line">[root@nn conf]# vi spark-env.sh</span><br><span class="line"># 基本集群配置</span><br><span class="line">export SCALA_HOME&#x3D;&#x2F;opt&#x2F;scala-2.12.8</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_161</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url&#x3D;nn:2181,dn1:2181,dn2:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark&quot;</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line"></span><br><span class="line"># yarn模式下的调优配置</span><br><span class="line"># Options read in YARN client&#x2F;cluster mode</span><br><span class="line">export SPARK_WORKER_MEMORY&#x3D;512M</span><br><span class="line"># - SPARK_CONF_DIR, Alternate conf dir. (Default: $&#123;SPARK_HOME&#125;&#x2F;conf) 无需设置，使用默认值</span><br><span class="line"># - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;hadoop-3.1.2&#x2F;etc&#x2F;hadoop</span><br><span class="line"># - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN 上面HADOOP_CONF_DIR以已设置即可</span><br><span class="line"># - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1). 无需设置，默认使用1个vcpu</span><br><span class="line"># - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">export SPARK_EXECUTOR_MEMORY&#x3D;512M</span><br><span class="line"># - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">export SPARK_EXECUTOR_MEMORY&#x3D;512M</span><br><span class="line"></span><br><span class="line"># 存放计算过程的日志</span><br><span class="line">export SPARK_HISTORY_OPTS&#x3D;&quot;</span><br><span class="line">-Dspark.history.ui.port&#x3D;9001</span><br><span class="line">-Dspark.history.retainedApplications&#x3D;5</span><br><span class="line">-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;hdapp&#x2F;directory&quot;</span><br></pre></td></tr></table></figure><p>以上的driver和executor的可用内存设为512M，考虑到测试服务器内存有限的调优。若生产服务器，一般32G或者更大的内存，则可以任性设置。</p><h4 id="启动spark集群"><a href="#启动spark集群" class="headerlink" title="启动spark集群"></a>启动spark集群</h4><p>在nn节点上，启动wokers： start-slaves.sh，该命令自动启动其他节点的worker<br>在nn节点和dn2节点启动master进程：start-master.sh<br>查看nn:8080和dn2:8080的spark web UI是否有active以及standby模式。<br>跑一个wordcount例子，测试spark集群能否正常计算结果。<br>创建一个本地文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  vi &#x2F;opt&#x2F;foo.txt</span><br><span class="line">spark on yarn</span><br><span class="line">yarn </span><br><span class="line">spark HA</span><br></pre></td></tr></table></figure><p>启动pyspark，连接到spark集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  .&#x2F;bin&#x2F;pyspark --name bar --driver-memory 512M   --master  spark:&#x2F;&#x2F;nn:7077</span><br><span class="line"># 读取本地文件&#x2F;opt&#x2F;foo.txt</span><br><span class="line">&gt;&gt;&gt; df&#x3D;sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;opt&#x2F;foo.txt&quot;)</span><br><span class="line"># 切分单词，过滤空值</span><br><span class="line">&gt;&gt;&gt; words &#x3D; df.flatMap(lambda line: line.split(&#39; &#39;)).filter(lambda x: x !&#x3D;&quot;&quot;)</span><br><span class="line">&gt;&gt;&gt; words.collect()</span><br><span class="line">[u&#39;spark&#39;, u&#39;on&#39;, u&#39;yarn&#39;, u&#39;yarn&#39;,u&#39;spark&#39;, u&#39;HA&#39;]</span><br><span class="line"># 将个word映射为（word，1）这样的元组，在reduce汇总。</span><br><span class="line">&gt;&gt;&gt; counts &#x3D; words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)</span><br><span class="line">&gt;&gt;&gt; counts.collect()</span><br><span class="line">[(u&#39;spark&#39;, 2), (u&#39;yarn&#39;, 2), (u&#39;on&#39;, 1), (u&#39;HA&#39;, 1)]   </span><br></pre></td></tr></table></figure><p>以上完成spark HA集群和测试。</p><h3 id="spark-on-yarn"><a href="#spark-on-yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h3><p>spark on yarn意思是将spark计算人任务提交到yarn集群上运行。</p><h4 id="spark集群跑在yarn上的两种方式"><a href="#spark集群跑在yarn上的两种方式" class="headerlink" title="spark集群跑在yarn上的两种方式"></a>spark集群跑在yarn上的两种方式</h4><p>根据spark官网的<a href="http://spark.apache.org/docs/latest/running-on-yarn.html">文档说明</a>，这里引用其内容：</p><blockquote><p>There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p></blockquote><p>cluster模式下，spark driver 在 AM里运行，客户端（或者应用程序）在提交完任务（初始化）后可直接退出，作业会继续在 YARN 上运行。显然cluster 模式不适合交互式操作。cluster模式的spark计算结果可以保持到<br>外部数据库，例如hbase。这部分内容将是spark streaming可以完成的环境，spark streaming以yarn cluster模式运行，实时将处理结果存到hbase里，web BI 应用再从hbase取数据。</p><p>client模式下，spark driver是在本地环境运行，AM仅负责向yarn请求计算资源（Executor 容器），例如交互式运行基本的操作。</p><p>在前面第2节的word count例子里，用下面的启动命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]# pyspark --name bar --driver-memory 512M   --master  spark:&#x2F;&#x2F;nn:7077</span><br></pre></td></tr></table></figure><p>该命令启动是一个spark shell进程，没有引入yarn管理其资源，因此在yarn集群的管理页面<code>http://nn:8088/cluster/apps/RUNNING</code>，将不会 bar这个application。</p><h4 id="测试spark-on-yarn"><a href="#测试spark-on-yarn" class="headerlink" title="测试spark on yarn"></a>测试spark on yarn</h4><p>只需在启动spark shell时，将<code>--master spark://nn:7077</code> 改为<br><code> --master yarn --deploy-mode cluster</code>或者<code> --master yarn --deploy-mode client</code>，那么spark提交的任务就会交由yarn集群管理<br>还是以word count为例，使用yarn client模式启动spark<br>创建测试文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;opt&#x2F;yarn-word-count.txt</span><br><span class="line">spark on yarn </span><br><span class="line">spark HA </span><br><span class="line">yarn HA</span><br></pre></td></tr></table></figure><p>启动driver</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@nn spark-2.4.4-bin-hadoop2.7]#  pyspark --name client_app --driver-memory 512M  --executor-memory 512M  --master yarn --deploy-mode client</span><br><span class="line">Python 2.7.5 (default, Oct 30 2018, 23:45:53) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     &#x2F; __&#x2F;__  ___ _____&#x2F; &#x2F;__</span><br><span class="line">    _\ \&#x2F; _ \&#x2F; _ &#96;&#x2F; __&#x2F;  &#39;_&#x2F;</span><br><span class="line">   &#x2F;__ &#x2F; .__&#x2F;\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\_\   version 2.4.4</span><br><span class="line">      &#x2F;_&#x2F;</span><br><span class="line"></span><br><span class="line">Using Python version 2.7.5 (default, Oct 30 2018 23:45:53)</span><br><span class="line">SparkSession available as &#39;spark&#39;.</span><br><span class="line">&gt;&gt;&gt; sc</span><br><span class="line">&lt;SparkContext master&#x3D;yarn appName&#x3D;client_app&gt;</span><br></pre></td></tr></table></figure><p>这里driver和executor都是以最小可用内存512来启动spark-shell<br>因为该spark 任务是提交到yarn 上运行，所以在spark web ui后台：<code>http://nn:8080</code>，running application 为0<br><img src="https://img-blog.csdnimg.cn/20191208110757819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这是需要去yarn后台入口：<code>http://nn:8088</code>，可以看到刚提交的计算任务：<br><img src="https://img-blog.csdnimg.cn/20191208111208596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">可以看到该application（计算任务）分配了3个Container<br><img src="https://img-blog.csdnimg.cn/20191208111547983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">通过查看该applicationMaster管理页面，可以看到client-yarn这个app更为详细的计算过程，例如该wordcount在reduceByKey DAG可视化过程。<br><img src="https://img-blog.csdnimg.cn/20191208111947653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>yarn cluster模式下，因为它不是打开一个spark shell让你交互式输入数据处理逻辑，所以需先把处理逻辑封装成一个py模块。<br>以上面的word count为例：<br>word_count.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_count</span>():</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&#x27;cluster-yarn&#x27;</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    <span class="comment"># 统计文件中包含mape的行数，并打印第一行</span></span><br><span class="line">    df = sc.textFile(<span class="string">&quot;/tmp/words.txt&quot;</span>)</span><br><span class="line">    words = df.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&#x27; &#x27;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x !=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> words.collect()</span><br><span class="line">    counts = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">    <span class="built_in">print</span> counts.collect()</span><br><span class="line">    sc.stop</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">word_count()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>需要使用spark-submit 提交到yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@dn2 spark-2.4.4-bin-hadoop2.7]#  ./bin/spark-submit  --driver-memory 512M  --executor-memory 512M  --master yarn  --deploy-mode cluster  --py-files word_count.py</span><br></pre></td></tr></table></figure><p>在yarn管理也可以看到该app，application的命名好像直接用脚本名字，而不是指定的cluster-yarn<br><img src="https://img-blog.csdnimg.cn/20191208120528893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">关于如何提交py文件，官方也给出指引：</p><blockquote><p>For Python, you can use the –py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application. If you depend on multiple Python files we recommend packaging them into a .zip or .egg.</p></blockquote><p>如有多个py文件（例如1.py依赖2.py和3.py），需要通过将其打包为.zip或者.egg包： –py-files tasks.zip</p><h4 id="提交spark-application的多种方式"><a href="#提交spark-application的多种方式" class="headerlink" title="提交spark application的多种方式"></a>提交spark application的多种方式</h4><p>spark运行有standalone模式（分local、cluster）、on yarn模式（分client、cluster）还有on k8s，而且可以附带jar包或者py包，多种提交的方式的命令模板怎么写？网上其实很多类似文章，但都是给的某个模式的某种文件的提交方式，其实在spark官网的<a href="http://spark.apache.org/docs/latest/submitting-applications.html">submitting-applications</a>章节给出详细的多种相关命令模板。这里统一汇总：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run application locally on 8 cores 本地模式</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master local[8] \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  standalone 集群下的client模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  standalone 集群下的cluster模式</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> on yarn 集群，且用的class文件和jar包</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a YARN cluster</span></span><br><span class="line">export HADOOP_CONF_DIR=XXX</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \  # can be client for client mode</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里给出如何传入py文件，可以不写 --py-files 选项</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Mesos cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master mesos://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Kubernetes cluster <span class="keyword">in</span> cluster deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master k8s://xx.yy.zz.ww:443 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>&#8195;&#8195;本文内容主要为后面的文章——spark streaming 与kafka集群的实时数据计算做铺垫，考虑到测试环境环境资源有限，在做spark streaming的时候，将不会以spark HA模式运行，也不会将任务提交到yarn集群上，而是用一节点作为spark streaming计算节点，具体规划参考该文。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面的&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/102536716&quot;&gt;《基于hadoop3.1.2分布式平台上部署spark HA集群》&lt;/a&gt;，这篇是基于非HA模式下hadoop集群的spark集群HA配置，而本文将给出基于HA模式下hadoop集群的spark集群HA配置，并将yarn HA集群映入到spark中，做资源管理。为何要做些环境的配置呢？因为到本篇文章为止，已经完成hadoop HA集群、hbaseHA集群，hive集群（非HA）、sparkHA集群、flumeHA集群、kafka HA集群，实现实时数据流动，接下的文章重点探讨spark streaming、spark以及pyspark相关知识，这将涉及多个计算任务以及相关计算资源的分配，因此需要借助yarn HA集群强大的资源管理服务来管理spark的计算任务，从而实现完整的、接近生产环境的、HA模式下的大数据实时分析项目的架构。&lt;/p&gt;</summary>
    
    
    
    <category term="Spark" scheme="https://yield-bytes.gitee.io/blog/categories/Spark/"/>
    
    
    <category term="YARN集群" scheme="https://yield-bytes.gitee.io/blog/tags/YARN%E9%9B%86%E7%BE%A4/"/>
    
    <category term="Spark集群" scheme="https://yield-bytes.gitee.io/blog/tags/Spark%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>flume集群高可用连接kafka集群</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/05/flume%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9E%E6%8E%A5kafka%E9%9B%86%E7%BE%A4/</id>
    <published>2019-12-05T13:00:22.000Z</published>
    <updated>2020-11-21T16:55:06.010Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面blog文章中：<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>和<a href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示：</p><p><img src="https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;flume NG集群前向的source是各类实时的log数据，通过flume sink将这些日志实时sink到后向kafka集群，所有flume sink其实是本架构里kafka的producer角色，kafka集群后向连接spark streaming，用于消费kafka的实时消息（log日志数据）流。</p><a id="more"></a><p>组件版本：<br>flume-1.9.0、kafka-2.12 </p><h4 id="1-在kafka集群上创建相应的topic"><a href="#1-在kafka集群上创建相应的topic" class="headerlink" title="1.在kafka集群上创建相应的topic"></a>1.在kafka集群上创建相应的topic</h4><p>&#8195;&#8195;在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是直接sink到hdfs上。<br>创建topic需要做一定规划，考虑到目前有三个broker节点，分别为nn、dn1以及dn2节点，所以创建了3个分区，每个分区有三个replica，topic名为：sparkapp</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka_2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181&#x2F;kafka-zk --replication-factor 3 --partitions 3 --topic sparkapp </span><br><span class="line">Created topic sparkapp.</span><br><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181&#x2F;kafka-zk --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>-zookeeper nn:2181/kafka-zk：因为kafka在zk的所有znode都统一放置在/kafka-zk路径下，所以启动时需要注意加上该路径。</p><h4 id="2-单节点配置flume的agent-sink"><a href="#2-单节点配置flume的agent-sink" class="headerlink" title="2.单节点配置flume的agent sink"></a>2.单节点配置flume的agent sink</h4><h5 id="2-1-配置flume-文件"><a href="#2-1-配置flume-文件" class="headerlink" title="2.1 配置flume 文件"></a>2.1 配置flume 文件</h5><p>&#8195;&#8195;这里首先给出单节点的flume是如何连接到kafka集群，在nn节点上启动flume进程。在第3章节，将给出flume集群连接kafka集群，实现两组件之间的高可用实时数据流。<br>flume source的数据源为<code>/opt/flume_log/web_log/access.log</code><br>这里拷贝一份新的配置文件，配置过程简单，相关组件的参数说明可以参考flume官网最新文档：<br><a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-sink">kafka-sink章节</a><br><code>[root@nn conf]# cp flume-conf.properties flume-kafka.properties</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">vi flume-kafka.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">设置<span class="built_in">source</span>组件<span class="built_in">exec</span>模式用 tail -F实时读取文本新的数据行</span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel组件，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定sinktype 为kafka sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的broker列表，用逗号（英文）隔开</span></span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> 前面创建的topic名称</span></span><br><span class="line">a1.sinks.k1.kafka.topic = sparkapp</span><br><span class="line"><span class="meta">#</span><span class="bash"> How many messages to process <span class="keyword">in</span> one batch. flume一次写入kafka的消息数</span></span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 20</span><br><span class="line"><span class="meta">#</span><span class="bash"> ack=1 说明只要求producer写入leader主分区即完成（<span class="built_in">wait</span> <span class="keyword">for</span> leader only)）</span></span><br><span class="line"><span class="meta">#</span><span class="bash">How many replicas must acknowledge a message before its considered successfully written.</span> </span><br><span class="line">a1.sinks.k1.kafka.producer.acks = 1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以设置分区ID，这里使用默认，也即kafka自己分区器</span></span><br><span class="line"><span class="meta">#</span><span class="bash">a1.sinks.k1.kafka.defaultPartitionId</span></span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 5</span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息的压缩类型</span></span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>组件和sinks组件绑定channel组件</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里sink的channel是单数</span></span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>这里的linger.ms=5主要是处理下情况：<br>producer是按照batch进行发送，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。<br>这里设置producer请求延时5ms才会被发送。</p><h5 id="2-2-测试数据消费情况"><a href="#2-2-测试数据消费情况" class="headerlink" title="2.2 测试数据消费情况"></a>2.2 测试数据消费情况</h5><p>在dn1节点上启动kafka consumer进程，等待flume sink，因为kafka已经集群，所以–bootstrap-server 选任意一个节点都可以，前提所选节点需在线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  saprkapp</span><br></pre></td></tr></table></figure><p>在nn节点上启动flume agent，这里不是后台启动，目的是为了实时观测flume agent 实时数据处理情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-kafka.properties --name a1</span><br></pre></td></tr></table></figure><p>手动在source文件最加新的文本行<br>[root@nn web_log]# echo “testing flume to kafka” &gt;&gt;access.log<br>在dn1 上可以看到实时输出nn节点上flume sink过来的消息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092 --topic  sparkapp</span><br><span class="line">testing flume to kafka</span><br></pre></td></tr></table></figure><p>以上完成单节点flume实时数据到kafka的配置和测试，下面将使用flume集群模式sink到kafka集群</p><h4 id="3-flume-NG集群连接kafka集群"><a href="#3-flume-NG集群连接kafka集群" class="headerlink" title="3.flume NG集群连接kafka集群"></a>3.flume NG集群连接kafka集群</h4><p><img src="https://img-blog.csdnimg.cn/20191201225119507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">图3 为flume NG集群连接kafka集群的示意图<br>配置也相对简单，只需要把<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件》</a>第4.2章节的collector配置的sinks部分改为kafkasink，agent1、agent2和agent3用第4.1章节所提的配置文件内容即可，本文不再给出。<br>给个flume节点角色分布表<br>| 节点 |  flume 角色|kafka角色|<br>|–|–|–|<br>| nn | agent1，collector 1|kafka broker<br>| dn1 | agen2 |kafka broker<br>| dn2 | agent3，collector2 |kafka broker</p><h5 id="3-1-配置collector"><a href="#3-1-配置collector" class="headerlink" title="3.1 配置collector"></a>3.1 配置collector</h5><p>因为测试环境计算资源有限，每个flume进程和kafka进程都在同一服务器上运行，实际生产环境flume和kafka分别在不同服务器上。<br>配置collector：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume-1.9.0&#x2F;conf</span><br><span class="line">[root@nn conf]# vi avro-collector.properties</span><br><span class="line"># 定义三个组件</span><br><span class="line">collector1.sources &#x3D; r1</span><br><span class="line">collector1.sinks &#x3D; k1</span><br><span class="line">collector1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 定义source：这里的source配成avro，连接flume agent端sink avro</span><br><span class="line">collector1.sources.r1.type &#x3D; avro</span><br><span class="line"># bind的属性：dn2节点对应改为dn2</span><br><span class="line">collector1.sources.r1.bind &#x3D; nn</span><br><span class="line">collector1.sources.r1.port &#x3D; 52020</span><br><span class="line"></span><br><span class="line">#定义channel</span><br><span class="line">collector1.channels.c1.type &#x3D; memory</span><br><span class="line">collector1.channels.c1.capacity &#x3D; 500</span><br><span class="line">collector1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># 指定sinktype 为kafka sink，从而使得flume collector成为kafka的producer</span><br><span class="line">collector1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">collector1.sinks.k1.kafka.bootstrap.servers &#x3D; nn:9092,dn1:9092,dn2:9092</span><br><span class="line">collector1.sinks.k1.kafka.topic &#x3D; sparkapp</span><br><span class="line">collector1.sinks.k1.kafka.flumeBatchSize &#x3D; 20</span><br><span class="line">collector1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line">collector1.sinks.k1.kafka.producer.linger.ms &#x3D; 5</span><br><span class="line">collector1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"></span><br><span class="line"># source组件和sinks组件绑定channel组件</span><br><span class="line">collector1.sources.r1.channels &#x3D; c1</span><br><span class="line">collector1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure><h5 id="3-2-启动flume-ng集群服务"><a href="#3-2-启动flume-ng集群服务" class="headerlink" title="3.2 启动flume-ng集群服务"></a>3.2 启动flume-ng集群服务</h5><p>在nn和dn2节点上使用nohup &amp; 后台启动flume collector进程<br>nn节点上collector进程，jps可以看到Application进程<br> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console &amp;</span><br><span class="line">[1] 26895</span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">27168 Jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">23589 Kafka</span><br><span class="line">26895 Application</span><br></pre></td></tr></table></figure><br> dn2同样操作，这里不再给出。</p><p>分别在nn、dn1和dn2节点上使用nohup &amp; 后台启动启动flume agent进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# nohup bin/flume-ng agent -c conf -f conf/avro-agent.properties --name  agent1 -Dflume.root.logger=INFO,console &amp; </span><br><span class="line">[root@nn flume-1.9.0]# jps</span><br><span class="line">15508 QuorumPeerMain</span><br><span class="line">324 Jps</span><br><span class="line">23589 Kafka</span><br><span class="line">30837 Application</span><br><span class="line">32462 Application</span><br></pre></td></tr></table></figure><p>可以在nn节点看到两个 Application进程，一个是flume collector进程，另外一个是flume agent进程。<br>dn1和dn2取同样的后台启动方式，这里不再给出。</p><h5 id="3-3-测试flume与kafka高可用"><a href="#3-3-测试flume与kafka高可用" class="headerlink" title="3.3 测试flume与kafka高可用"></a>3.3 测试flume与kafka高可用</h5><p>任找一个节点，启动kafka consumer 进程，这里在dn1节点上启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka_2.12]# .&#x2F;bin&#x2F;kafka-console-consumer.sh --bootstrap-server nn:9092,dn1:9092,dn2:9092 --topic sparkapp</span><br></pre></td></tr></table></figure><p>因为nn、dn1、dn2三个节点上都有flume agent进程，分别在每个节点下的access.log追加新数据行，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># nn节点追加新数据行</span><br><span class="line">[root@nn web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@nn web_log]# echo &quot;flume&amp;kafka HA--msg from nn&quot; &gt;&gt;access.log</span><br><span class="line"># dn1节点追加新数据行</span><br><span class="line">[root@dn1 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn1 web_log]#echo &quot;flume&amp;kafka HA--msg from dn1&quot; &gt;&gt;access.log</span><br><span class="line"># dn2节点追加新数据行</span><br><span class="line">[root@dn2 web_log]# pwd</span><br><span class="line">&#x2F;opt&#x2F;flume_log&#x2F;web_log</span><br><span class="line">[root@dn2 web_log]# echo &quot;flume&amp;kafka HA--msg from dn2&quot; &gt;&gt;access.log</span><br></pre></td></tr></table></figure><p>在kafka consumer shell可以看到实时接收三条记录：<br><img src="https://img-blog.csdnimg.cn/20191203235843219.png" alt="在这里插入图片描述"><br>能否关闭其中collector所在服务器中的一台用于同时测试flume和kafka的高可用，例如关闭nn服务器？<br>不能，因为这里只有三个节点，zookeeper集群必须要三个及以上才能保证高可用，因此这里只需要kill nn节点上的collector，此时flume集群只有dn2的collector在工作，按上面的步骤，在三个节点上都给access.log新增数据行，同样可以正常观测到dn1的kafka consumer拿到3条message</p><h4 id="4-小结"><a href="#4-小结" class="headerlink" title="4.小结"></a>4.小结</h4><p>本文给出了flume与kafka连接的高可用部署过程，设置相对简单，考虑到测试环境资源限制，这里把flume集群和kafka集群放在同一服务器，生产环境中，flume集群有独立的服务器提供，kafka集群也由独立的服务器提供。后面几篇文章将给出有关spark的深度理解和kafka与spark streaming整合内容，目的为用于实现实时处理批数据的环节。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面blog文章中：&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103214906&quot;&gt;《在hadoopHA节点上部署flume高可用组件》&lt;/a&gt;和&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103225653&quot;&gt;《在hadoopHA节点上部署kafka集群组件》&lt;/a&gt;，已经实现大数据实时数据流传输两大组件的部署和测试，本文将讨论flume组件连接kafka集群相关内容，两组件在项目架构图的位置如下图1红圈所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191201160247293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot;&gt;&amp;#8195;&amp;#8195;flume NG集群前向的source是各类实时的log数据，通过flume sink将这些日志实时sink到后向kafka集群，所有flume sink其实是本架构里kafka的producer角色，kafka集群后向连接spark streaming，用于消费kafka的实时消息（log日志数据）流。&lt;/p&gt;</summary>
    
    
    
    <category term="Flume" scheme="https://yield-bytes.gitee.io/blog/categories/Flume/"/>
    
    
    <category term="flume连接kafka" scheme="https://yield-bytes.gitee.io/blog/tags/flume%E8%BF%9E%E6%8E%A5kafka/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Kafka</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/12/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/12/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Kafka/</id>
    <published>2019-12-01T04:16:41.000Z</published>
    <updated>2020-11-22T10:55:35.661Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面的文章<a href="https://blog.csdn.net/pysense/article/details/103225653">《在hadoopHA节点上部署kafka集群组件》</a>，介绍大数据实时分析平台生态圈组件——kafka，前向用于连接flume，后向连接spark streaming。在研究Kafka过程中，发现该中间件的设计很巧妙，因此专设一篇文章用于深入理解Kafka核心知识。Kafka已经纳入个人目前最欣赏的中间件list：redis，zookeeper，kafka</p><h4 id="1、kafka集群架构图"><a href="#1、kafka集群架构图" class="headerlink" title="1、kafka集群架构图"></a>1、kafka集群架构图</h4><p>以下为kafka集群一种经典的架构图，该图以《在hadoopHA节点上部署kafka集群组件》文章的kafka集群以及sparkapp topic作为示例绘成，本文的内容将以该图为标准作为说明。<br><img src="https://img-blog.csdnimg.cn/20191127231934698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="kafka集群架构图">图1 kafka集群架构图</p><a id="more"></a><h4 id="2、kafka-高性能读写的设计"><a href="#2、kafka-高性能读写的设计" class="headerlink" title="2、kafka 高性能读写的设计"></a>2、kafka 高性能读写的设计</h4><h5 id="2-1、利用read-ahead-和-write-behind提升写性能"><a href="#2-1、利用read-ahead-和-write-behind提升写性能" class="headerlink" title="2.1、利用read-ahead 和 write-behind提升写性能"></a>2.1、利用read-ahead 和 write-behind提升写性能</h5><p>&#8195;&#8195;kafka底层设计高度依赖现代磁盘优化技术和文件系统的优化技术。在kafka官方文档的：<a href="http://kafka.apache.org/documentation/#design">don’t fear the filesystem</a>章节说明了kafka是如何利用磁盘已有的高性能读写技术：read-ahead 和 write-behind 实现日志在磁盘山高性能顺序写。<br>&#8195;&#8195;read-ahead 是以大的 data block 为单位预先读取数据。write-behind（后写） 是将多个小型的逻辑写合并成一次大型的物理磁盘写入，producer向kafka写入消息日志时，因为消息是一条一条的过来，而且消息本身payload很小，如果每条消息进来立刻执行写入磁盘，显然IO非常高，因此需要将进来的消息先缓存，然后到一定数量或者到一定容量时再触发写入磁盘，kafka用了pagecache实现write-behind而不是通过内存。<br>&#8195;&#8195;官方举例说明用廉价的RAID-5模式sata硬盘可以去到600MB/秒，但随机写入的性能仅约为100k/秒，相差6000倍以上。</p><h5 id="2-2、使用pagecache缓存程序数据提升读写性能"><a href="#2-2、使用pagecache缓存程序数据提升读写性能" class="headerlink" title="2.2、使用pagecache缓存程序数据提升读写性能"></a>2.2、使用pagecache缓存程序数据提升读写性能</h5><p>&#8195;&#8195;同样，在kafka官方文档的：<a href="http://kafka.apache.org/documentation/#design">don’t fear the filesystem</a>章节还提到另外一个技术：pagecache。kafka利用了现代操作系统主动将所有空闲内存用作磁盘caching这一机制（代价是在内存回收时性能会有所降低），再次提升基于filesystem的读写性能的效果。<br>&#8195;&#8195;kafka 跑在 jvm之上，那么jvm一定会有复杂的GC情况：</p><ul><li>对象的内存开销非常高，通常是所存储的数据的两倍(甚至更多)。</li><li>随着堆中数据的增加，Java 的垃圾回收变得越来越复杂和缓慢。</li></ul><p>&#8195;&#8195;受这些因素影响， 维护in-memory cache就会显得很复杂，而kafka通过文件系统方式和 pagecache 读写消息反而显得更有优势（避免复杂低效率的GC），通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番，例如32GB内存的服务器，它的 pagecache缓存容量可以达到28-30GB，并且不会产生额外的 GC 负担。kafka自己也说还有重要一点：简化核心代码。<br>为何这么设计？<br>&#8195;&#8195;kafka自己这么解释：因为相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将消息数据 flush 到文件系统的，kafka写过程把这个过程倒过来：所有消息数据一开始就被写入（write-behind）到文件系统的持久化日志中，而不用在in-memory cache 空间不足的时候 flush 到磁盘。实际上，是先把数据被转移到了内核的 pagecache 中。<br>&#8195;&#8195;这里可以联想到Hbase的MemStore设计：MemStore基于in-memory cache，MemStore 在内存中存在，保存修改key-value数据，当MemStore的大小达到一个阀值（默认64MB）时，MemStore里面的数据会被flush到Hfile文件上，也就是flush到磁盘上。</p><p>==为何page cache 会加速读过程？==<br>linux的文件cache分为两层，一个是page cache，另一个是buffer cache；每一个page cache包含若干个buffer cache，结构图如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191130122756221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>page cache：文件系统层级的缓存，从磁盘里读取数据缓存到page cache（属于内核空间，而不是应用用户的空间），这样应用读磁盘数据会被加速，例如使用find等命令查找文件时，第一次会慢很多，第二次查找相同文件时会瞬间读取到。如果page cache的数据被修改过后，也即脏数据，等到写入磁盘时机到来时，会把数据转移到buffer cache 而不是直接写入到磁盘。<br>buffer cache：磁盘等块设备的缓冲。<br>大致流程：<br><img src="https://img-blog.csdnimg.cn/20191130120633422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">page cache其优化读的工作过程如下：<br>A、文件的第一次读请求<br>系统读入所请求的page页并读入紧随其后的的少数几个页面，这种读取方式称为同步预读。<br>B、文件的第二次读请求:<br>如果page页不在第一次的cache中，说明不是顺序读，所以又会重新继续第一次那种同步预读过程。</p><p>==如果page页面在cache中，说明是顺序读，Linux会将预读group扩大一倍==，继续把不在首次cache中的文件数据读进来，此为异步预读。kafka之所以设计按顺序读写，完全就是按照底层page cahe的这种预读机制来设计，所以在文件系统底层就已经有不错的性能了。</p><h5 id="2-3-通过sendfile（零拷贝机制）提高消费者端的读吞吐量"><a href="#2-3-通过sendfile（零拷贝机制）提高消费者端的读吞吐量" class="headerlink" title="2.3 通过sendfile（零拷贝机制）提高消费者端的读吞吐量"></a>2.3 通过sendfile（零拷贝机制）提高消费者端的读吞吐量</h5><p>&#8195;&#8195;在kafka官方文档的Efficiency章节解释了kafka通过使用sendfile （零拷贝技术）继续提高消费者端的读性能。<br>&#8195;&#8195;前面2.1和2.2解释了kafka里利用相关底层机制，解决了磁盘访问模式不佳的情况。接下来，还需要解决以下两个影响kafka性能的情况：<br>too many small I/O operations, and excessive byte copying<br>（大量的小型 I/O 操作以及过多的字节拷贝 ）</p><ul><li><p>A、 The small I/O problem happens both between the client and the server and in the server’s own persistent operations.<br>（大量小型的 I/O 操作表现在client和broker之间以及broker服务端自身持久化操作中）<br>解决方式：kafka用一个称为 “消息块” 的抽象基础上，合理将消息分组。 这使得网络请求将多个消息打包成一组，而不是每次发送一条消息，从而使整组消息分担网络中往返的开销。consumer 每次获取多个大型有序的消息块，并由服务端依次将消息块一次加载到它的日志中。<br>这个简单的优化对速度有着数量级的提升。批处理允许更大的网络数据包，更大的顺序读写磁盘操作，连续的内存块等等</p></li><li><p>B、excessive byte copying<br>另一个低效率的操作是字节拷贝，在消息量少时，这不是什么问题，但是在高负载的情况下，影响就不容忽视。为了避免这种情况，kafka在producer、broker 和 consumer 都是用相同标准化的二进制消息格式，这样数据块不用修改就能在他们之间传递。<br>broker 维护的消息日志本身就是一个文件目录，每个segment文件都由一系列以相同格式消息组成，保持这种通用格式将非常有利于消息日志文件的网络传输的效率。 现代的unix 操作系统提供了一个高度优化的编码方式，用于将数据从 pagecache 转移到 socket 网络连接中，减少内核拷贝次数；在 Linux 中系统调用<a href="http://man7.org/linux/man-pages/man2/sendfile.2.html"> sendfile </a>方式做到这一点。 </p></li></ul><p>先看看数据从磁盘文件到套接字的拷贝过程：<br><code>File.read(fileDesc, buf, len);</code><br><code>Socket.send(socket, buf, len);</code><br>以上两个操作是java语义的读取文件和socket发送数据包，一共有两次拷贝？当然不是的：<br>1） 操作系统从磁盘读取数据到内核空间的 page cache<br>2）应用程序从内核空间 page cache读取数据到用户空间的缓冲区（应用程序的地址空间）<br>3）应用程序将数据(用户空间的缓冲区)写回内核空间到套接字缓冲区(内核空间)<br>4）操作系统将数据从套接字缓冲区(内核空间)复制到通过网络发送的 NIC 缓冲区<br>以上过程有四次 copy 操作和两次系统调用，如果数据传输吞吐量大时，对Kafka来说低效率，如何减少拷贝次数？<br>使用 内核提供的sendfile 方法，使用零拷贝的应用程序要求内核直接将数据从磁盘文件拷贝到套接字，而无需通过应用程序。零拷贝不仅大大地提高了应用程序的性能，而且还减少了内核与用户模式间的上下文切换。例如一个 topic 被多消费者消费时，使用上面zero-copy（零拷贝）优化，消息在使用时只会被复制到pagecache 中一次，节省了每次拷贝到用户空间内存中，再从用户空间进行读取的消耗。这使得消息能够以接近服务器网卡Gb级别的网速来进行消费。</p><blockquote><p>在应用程序和网络之间提供更快的数据传输方法，从而可以有效地降低通信延迟，提高网络吞吐率。零拷贝技术是实现主机或者路由器等设备高速网络接口的主要技术之一。举例来说，一个 1 GHz 的处理器可以对 1Gbit/s 的网络链接进行传统的数据拷贝操作，但是如果是 10 Gbit/s 的网络，那么对于相同的处理器来说，零拷贝技术就变得非常重要了。</p></blockquote><p>page cache 和 sendfile 的组合使用意味着，在一个kafka集群中，大多数 consumer 消费时，==将看不到磁盘上的读取活动，因为数据将完全由缓存提供==。<br>有关zero-copy以及Linux IO详细内容，推荐IBM Developer中国区四篇高质量文章：<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-directio/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中直接 I/O 机制的介绍》</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中的零拷贝技术，第 1 部分》</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《Linux 中的零拷贝技术，第 2 部分》</a><br><a href="https://www.ibm.com/developerworks/cn/java/j-zerocopy/index.html?mhsrc=ibmsearch_a&mhq=%E9%9B%B6%E6%8B%B7%E8%B4%9D">《通过零拷贝实现有效数据传输》</a>，这篇文章翻译了IBM Developer官方英文原文：<a href="https://developer.ibm.com/articles/j-zerocopy/">《Efficient data transfer through zero copy》</a><br>阅读这几篇文章可以说收益匪浅，不仅深度理解了kafka使用filesystem作为消息队列的底层文件IO，而且也有利于理解任何基于文件系统上的中间件的部分实现机制。</p><h4 id="3、kafka的repilcas副本机制"><a href="#3、kafka的repilcas副本机制" class="headerlink" title="3、kafka的repilcas副本机制"></a>3、kafka的repilcas副本机制</h4><h5 id="3-1-主分区的副本"><a href="#3-1-主分区的副本" class="headerlink" title="3.1 主分区的副本"></a>3.1 主分区的副本</h5><p>&#8195;&#8195;Kafka 允许 topic 的 partition 拥有若干副本，也就是说每个partition都有一个 leader 和零或多个 followers，例如图1 kafka集群架构图中，sparkapp这个topic，在broker1有主分区（leader）partition-0，在broker-1和broker-2有followers副本分区（replica）partition-0。  总的副本数是包含 leader 分区的总和。 所有的读写操作都由 leader 处理，各分区的 leader 均 匀的分布在brokers 中，一个topic在当前broker只能有一个leader主分区。followers节点就像普通的 consumer 那样从 leader 节点那里拉取消息并保存在自己的日志文件中。</p><h5 id="3-2-leade如何管理follower节点"><a href="#3-2-leade如何管理follower节点" class="headerlink" title="3.2 leade如何管理follower节点"></a>3.2 leade如何管理follower节点</h5><p>&#8195;&#8195;Kafka 判断节点是否存活有两种方式。</p><ul><li>首先follower所在的broker服务器在线，Zookeeper 通过心跳机制检查每个broker的连接，对应的znode路径/brokers/ids。</li><li>要求follower角色的同步进程 ，它必须能及时的同步 leader 的写操作，并且延时不能太多。 </li></ul><p>&#8195;&#8195;kafka认为满足这两个条件的节点处于 “in sync” 状态， Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本集合ISR中移除。这个ISR列表在zookeeper可以看到，a set of In-Sync Replicas，简称：ISR：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 36] get &#x2F;brokers&#x2F;topics&#x2F;sparkapp&#x2F;partitions&#x2F;1&#x2F;state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:6,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:2,&quot;isr&quot;:[11,12,10]&#125;</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;以上说明：sparkapp的partition-1这个主分区在brokerid为10的服务器上，其他follower的brokerid分别为11和12。<br>可配置在ISR移除follower的触发条件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 如果leader发现follower超过10秒没有向它发起同步请求，那么leader会认为follower无法正常同步主分区日志，就把它从ISR集合中中移除。</span><br><span class="line"> rerplica.lag.time.max.ms&#x3D;10000 # 默认值</span><br><span class="line"> # 相差1000条就从ISR集合移除该follower</span><br><span class="line"> rerplica.lag.max.messages&#x3D;1000# 默认值</span><br></pre></td></tr></table></figure><h5 id="3-3-Replica如何均匀分布到整个kafka集群"><a href="#3-3-Replica如何均匀分布到整个kafka集群" class="headerlink" title="3.3 Replica如何均匀分布到整个kafka集群"></a>3.3 Replica如何均匀分布到整个kafka集群</h5><p>&#8195;&#8195;为了更好的做负载均衡以及HA，Kafka尽量降所有的replicas均匀分配到整个集群上。为了更直观partition的副本是如何被分布到不同节点上，这里以一个小例子为例：创建一个fooTopic（可以先把它理解为消息队列queue，类似RabbitMQ的队列）且有五个分区，每个分区有三个副本replica，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin&#x2F;kafka-topics.sh --create --zookeeper nn:2181 --replication-factor 3 --partitions 5 --topic fooTopic</span><br><span class="line">[root@dn1 kafka-2.12]#  bin&#x2F;kafka-topics.sh --describe --zookeeper nn:2181 --topic fooTopic</span><br><span class="line">Topic:fooTopic  PartitionCount:5        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: fooTopic Partition: 0    Leader: 11      Replicas: 11,10,12      Isr: 11,10,12</span><br><span class="line">        Topic: fooTopic Partition: 1    Leader: 12      Replicas: 12,11,10      Isr: 12,11,10</span><br><span class="line">        Topic: fooTopic Partition: 2    Leader: 10      Replicas: 10,12,11      Isr: 10,12,11</span><br><span class="line">        Topic: fooTopic Partition: 3    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: fooTopic Partition: 4    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Kafka分配资源跟很多中间件一样：通过取余实现，具体的规则如下：<br> 1）序号为i的Partition分配到第（i mod n）个Broker上，n为集群的broker总数<br> 2）序列号为i的Partition的第j个Replica分配到第（(i + j) mod n）个Broker上</p><p>以上述fooTopic为例，给出其分布过程：首先查看broker ids的列表为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[11, 12, 10]</span><br></pre></td></tr></table></figure><p>[11, 12, 10]列表的项的索引从0开始，因为kafka是用Scala语言开发，Scala获取zk这个ids值后，肯定是转为Scala数组类型，它的索引从0开始。当然也适用replicas数组（列表）。（这里为何不是[10, 11, 12]？因为本次获取结果是最新的集群选举结果数组）<br>假设a=[11, 12, 10]那么a[0]=11,a=[1]=12,a[2]=10</p><ul><li><p>按规则1）对于partition的序号i，它会分配到第（i mod n）个broker上：<br>那么对于partition0，0 mod 3=0，所以该在a[0]=11这个broker上，<br>同理有：<br>partition1，1 mod 3=1，所以该在a[1]=12这个broker上<br>partition2，2 mod 3=2，所以该在a[2]=10这个broker上<br>partition3，3 mod 3=0，所以该在a[0]=11这个broker上<br>partition4，4 mod 3=1，所以该在a[1]=12这个broker上</p></li><li><p>按规则 2）序列号为i的Partition的第j个Replica分配到第（(i + j) mod n）个Broker上<br>那么对于序列号为4的partition和序列号为0的replica，（4+0）mod 3=1，所以该在a[1]=12这个broker上，那么这个就是主leader分区，符合规则1partition4在12这个broker的计算结果。<br>那么对于序列号为4的partition和序列号为1的replica，（4+1）mod 3=2，所以该在a[2]=10这个broker上，<br>那么对于序列号为4的partition和序列号为2的replica，（4+2）mod 3=0，所以该在a[0]=11这个broker上<br>也就说partition4的replicas为[12,10,11]</p></li></ul><h4 id="4、Kafka消息的ack机制"><a href="#4、Kafka消息的ack机制" class="headerlink" title="4、Kafka消息的ack机制"></a>4、Kafka消息的ack机制</h4><p>&#8195;&#8195;这里是指producer向broker写消息的确认机制，这直接影响到Kafka集群的吞吐量和消息可靠性。而吞吐量和可靠性是矛盾的，两者不可兼得，只能平衡。<br>&#8195;&#8195;在第3章节提到leader和follower节点日志同步的内容，kafka动态维护了一个同步状态的副本的集合，在这个集合中的节点都是和leader保持高度一致的，任何一条消息只有被这个集合中的每个节点读取并追加到日志中，才会向外部通知说“这个消息已经committed。<br>&#8195;&#8195;也就是说只有当消息被ISR上所有的followers加入到日志中时，才算是“committed”，只有committed的消息才会发送给consumer，这样就不用担心一旦leader down掉了消息会丢失。这一环节就是决定了消息队列吞吐量和可靠性的环节。消息从leader复制到follower，可通过producer是否等待消息被提交的通知(ack)来区分同步复制和异步复制。<br>ack有3个可选值，分别是1，0，-1，可通过server.properties进行配置：<br>request.required.asks=0<br>==ack=0==:相当于异步的，producer给broker发送一次就不再发送了，不管本条消息是否在leader和follower都写入成功。可靠性低，吞吐量当然高。<br>==ack=1==：producer等待leader这个主分区成功写入了消息，producer才会认为消息发送成功，这是默认值，显然是吞吐量与可靠性的一个折中方案<br>==ack=-1==：当所有的follower都同步消息成功后，leader再向producer发送ack，producer才认为此消息发送成功，显然牺牲了吞吐量，因为如果leader有多个followers，同步需要一定时间，producer当然要等待一段时间后，再能继续向leader发送新消息。</p><p>当然ack=1的情况下，消息也可能会丢失，这是因为：<br>producer只要收到分区leader成功写入的通知就会认为消息发送成功了，但是也有这样的情况：leader成功写入后，还没来得及把数据同步到follower节点超时等，这时候消息会丢失。</p><p>清楚了kafka的ack机制后，来看看ack=-1的同步复制过程：<br>1）producer首先取zookeeper找到指定topic的主分区leader，向leader发送消息<br>2）leader收到消息写入到本地segment文件<br>3）所有follower从leader pull消息并写入自己segment文件<br>4）所有follower向leader发送ack消息<br>5）leader收到所有follower的ack消息<br>6）leader向producer发送ack<br>7）producer收到成功写入的响应</p><h4 id="5、kafka-消息索引机制"><a href="#5、kafka-消息索引机制" class="headerlink" title="5、kafka 消息索引机制"></a>5、kafka 消息索引机制</h4><p>&#8195;&#8195;前面的四节内容更多是kafka集群本身的一些机制，其实对于consumer侧，当consumer去broker pull一条的消息时，broker是如何快速找出相应的消息呢？<br>&#8195;&#8195;在前面部署Kafka集群已经知道每个partition都是一个文件目录，每个目录下有成index文件和log文件，它们是成对出现，后缀 “.index” 和 “.log” 分表表示 segment 索引文件和数据文件（存放消息的地方），segment的大小以及相关配置可在server.properties进行设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#segment文件的大小，默认为 1G</span><br><span class="line">log.segment.bytes&#x3D;1024*1024*1024</span><br><span class="line">#滚动生成新的segment文件的最大时长</span><br><span class="line">log.roll.hours&#x3D;24*7</span><br><span class="line">#segment文件保留的最大时长，超过7天将被删除</span><br><span class="line">log.retention.hours&#x3D;24*7</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;Segment 是 Kafka 存储消息的最小单位，Segment 文件命名规则：对于某个partition，例如partition0，全局的第一个 Segment 从 0 开始，后续每个 Segment 文件名为上一个 Segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用 0 填充。如 00000000000000368769.index 和 00000000000000368769.log。<br>对于parition1，它全局的第一个 Segment 也是从 0 开始，切勿认为parition0的最后一个segment的offset会顺延到partition1！<br>以下图为例，分析costumer如何根据offset拿到消息数据：<br>（因为集群测试环境还没有太多segment数据，所以这里参考这篇<a href="https://mp.weixin.qq.com/s/fX26tCdYSMgwM54_2CpVrw">文章的内容</a>：）<br><img src="https://img-blog.csdnimg.cn/20191130215819665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">&#8195;&#8195;假设现在研究partition0，在它.index文件中，存储的是key-value格式的值：&lt;n,m&gt;，n代表在.log中按顺序开始第n条消息，m代表该消息的位置偏移m，这里以索引文件中元数据 <code>&lt;3, 497&gt;</code> 为例，表示该368769.log文件中第 3 条Message，该消息所在物理位置为 497。<br>现在consumer要取offset为368773的消息，以下为查找过程：<br>1）在partition0下，有多个segment的index文件，根据二分法，可以快速地位到368772条消息在368769.index上<br>2）在368769.index索引文件，找出&lt;n,m&gt;的具体值，n=368774-368769=4（一般称为base offset），因为索引文件是稀疏结构，4这个值不在索引文件上<br>3）再根据二分法，很快找到3这个base offset&lt;3,497&gt;，因为索引值4没有，只能用不大于4的索引值3。<br>4）再回到368769.log上，从物理位置497开始按顺序查找，当物理位置到达830时，offset为368773的消息被找到。</p><p>&#8195;&#8195;从上图可以知道 Index 文件也不是每次递增 1 的，这是因为 Kafka 采取稀疏索引存储的方式，每隔一定字节的数据建立一条索引。它减少了索引文件大小，使得能够把 Index 映射到内存，降低了查询时的磁盘 IO 开销，同时也并没有给查询带来太多的时间消耗。<br>&#8195;&#8195;要满足以上的搜索策略：Kafka为在 Partition 中的每一条 Message 都定义了以下三个属性：</p><ul><li>Offset：表示 Message 在当前 Partition 中的偏移量，是一个逻辑上的值，唯一确定了在当前Partition 中的一条 Message</li><li>MessageSize：表示 Message 内容 Data 的大小。</li><li>Data：Message 的具体内容。<br>因此只要消费者只需要订阅的topic后，一旦拿到消息的offset，broker就会按以上检索策略将消息取出。</li></ul><h4 id="6、consumer-group的工作机制"><a href="#6、consumer-group的工作机制" class="headerlink" title="6、consumer group的工作机制"></a>6、consumer group的工作机制</h4><p>&#8195;&#8195;在图1可看到有多个consumer group，本章节主要探讨为何kafka使用consumer group的设计。<br>&#8195;&#8195;consumer group是kafka实现单播和广播两种消息模型的方式：<br>同一个topic的消息，可以广播给不同的group；<br>同一个topic的消息，每个group里面只能被其中的一个consumer消费。group内的consumer可以使用多线程或多进程来实现，consumer数量建议与partition成整数倍关系，因为kafka设计一个partition只能被group内一个consumer消费，也即是单播模式。</p><h5 id="6-1-一个topic为何需要被多个consumer消费？"><a href="#6-1-一个topic为何需要被多个consumer消费？" class="headerlink" title="6.1  一个topic为何需要被多个consumer消费？"></a>6.1  一个topic为何需要被多个consumer消费？</h5><p>&#8195;&#8195;以图1的sparkapp这个topic为例，假设没有consumer group，假设只有一个consumer去消费kafka集群的sparkapp，考虑到consumer只能从leader分区消费，相当于以下架构图：<br><img src="https://img-blog.csdnimg.cn/20191201103918787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">从单个consumer视角观察sparkapp，集群为consumer提供三个分区消费：（leader）partition0、（leader）partition1、（leader）partition2，所以上图简化成下图：</p><p><img src="https://img-blog.csdnimg.cn/20191201104854685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">但如果当producer写入消息的速度比consumer读取的速度快呢？结果是：消息堆积越来越严重，对于这种情况，需要增加多个消费者来进行水平扩展消息的读取。<br>可以用实际案例说明：<br>例如这个sparkapp的消息是待发送邮箱的内容和用户邮箱地址，如果仅有一个consumer去读取消息再发邮箱通知用户，那么随消息堆积越来越严重，将会有大量用户不能及时收到邮件通知。<br>解决办法：增加多个consumer，一般是跟leader分区的数目一致，例如本例3个leader分区，对应3个consumer进行消费，每个consumer消费分别对应一个分区进行消费，如下图所示：<br><img src="https://img-blog.csdnimg.cn/2019120111063329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">这就保证了生产的消息能够及时被consumer消费处理掉，表现在实际应用场景的效果：大量用户能够及时收到邮箱通知。<br>==注意：consumer数量建议与partition成整数倍关系，例如上面的1倍关系，因为kafka设计一个partition只能被group内一个consumer消费，也即是单播模式。<br>consumer数量由客户端自己通过多线程方式或者多进程方式实现。==</p><h5 id="6-2-同一个partition能否被多个consumer同时消费？"><a href="#6-2-同一个partition能否被多个consumer同时消费？" class="headerlink" title="6.2 同一个partition能否被多个consumer同时消费？"></a>6.2 同一个partition能否被多个consumer同时消费？</h5><p>例如（leader）partition0同时被两个consumer消费，如下所示：<br><img src="https://img-blog.csdnimg.cn/20191201111807824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">以邮箱通知这个为例子：<br>两个consumer将拿到重复的消息，在用户侧的效果就是：所有用户都会重复收到同一内容邮箱通知，显然不能接受。<br>kafka设计早已考虑到这些情况，所有kafka不允许同一个consumer group中的两个consumer读取同一个partition。</p><h5 id="6-3-kafka为何设计多个consumer-group这样的模型？"><a href="#6-3-kafka为何设计多个consumer-group这样的模型？" class="headerlink" title="6.3 kafka为何设计多个consumer group这样的模型？"></a>6.3 kafka为何设计多个consumer group这样的模型？</h5><p>以邮箱通知这个为例子：<br>这个sparkapp的消息是待发送邮箱的内容和用户邮箱地址，现在有两个应用需要拉取sparkapp这个topic的消息，一个是邮箱通知应用A，另外一个是存储邮箱内容和用户邮箱地址的应用B。</p><h6 id="6-3-1-无consumer-group，应用A和应用B会出现什么情况？"><a href="#6-3-1-无consumer-group，应用A和应用B会出现什么情况？" class="headerlink" title="6.3.1 无consumer group，应用A和应用B会出现什么情况？"></a>6.3.1 无consumer group，应用A和应用B会出现什么情况？</h6><p><img src="https://img-blog.csdnimg.cn/20191201114330792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">因为kafka设计每个consumer只能消费一个partition，如上图示，<br>==对于应用A，它开了2个线程==，消费(leader)partition0和(leader)partition1，在用户侧的出现情况：有一部分用户根本没收到邮件通知，漏了(leader)partition2这部分的数据。<br>==对于应用B，它只能开一个线程==，也即是一个consumer，而且只能拿到(leader)partition2的消息，最终出现的情况：数据库里面，根本没有存储到一部分用户的邮件记录。显然是漏了(leader)partition0和(leader)partition1的数据。<br>如何解决上述问题？</p><h6 id="6-3-2-为应用建立consumer-group，观测应用A和应用B的情况。"><a href="#6-3-2-为应用建立consumer-group，观测应用A和应用B的情况。" class="headerlink" title="6.3.2 为应用建立consumer group，观测应用A和应用B的情况。"></a>6.3.2 为应用建立consumer group，观测应用A和应用B的情况。</h6><p>考虑到两个应用，这里对应两个group，如下图示：<br><img src="https://img-blog.csdnimg.cn/20191201120617771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">group A 对应应用A，group B 对应应用B<br>kafka限定：<br>group内的consumer只能消费一个分区<br>同一分区（的一条信息）可以被不同group消费，广播模式。<br>从上图的结构可以看出：<br>应用A可以把三个分区的邮箱通知内容都发送到所有用户，不会出现像6.3.1 的情况：遗漏部分数据。<br>应用B可以把三个分区的邮箱通知内容都存储到数据库，不会出现像6.3.1 的情况：遗漏部分数据。<br>这就是kafka的consumer group的设计逻辑。<br>小结：<br>1）如果一个应用需要读取全量消息，那么可为该应用设置一个消费组；<br>如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。<br>2） kafka支持写入的一条消息能够被若干个应用读取这条消息。也就是说：<br>每个应用都可以读到全量的消息，通过为每个应用设置自己的消费组。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面的文章&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103225653&quot;&gt;《在hadoopHA节点上部署kafka集群组件》&lt;/a&gt;，介绍大数据实时分析平台生态圈组件——kafka，前向用于连接flume，后向连接spark streaming。在研究Kafka过程中，发现该中间件的设计很巧妙，因此专设一篇文章用于深入理解Kafka核心知识。Kafka已经纳入个人目前最欣赏的中间件list：redis，zookeeper，kafka&lt;/p&gt;
&lt;h4 id=&quot;1、kafka集群架构图&quot;&gt;&lt;a href=&quot;#1、kafka集群架构图&quot; class=&quot;headerlink&quot; title=&quot;1、kafka集群架构图&quot;&gt;&lt;/a&gt;1、kafka集群架构图&lt;/h4&gt;&lt;p&gt;以下为kafka集群一种经典的架构图，该图以《在hadoopHA节点上部署kafka集群组件》文章的kafka集群以及sparkapp topic作为示例绘成，本文的内容将以该图为标准作为说明。&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191127231934698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70&quot; alt=&quot;kafka集群架构图&quot;&gt;图1 kafka集群架构图&lt;/p&gt;</summary>
    
    
    
    <category term="Kafka" scheme="https://yield-bytes.gitee.io/blog/categories/Kafka/"/>
    
    
    <category term="Kafka原理" scheme="https://yield-bytes.gitee.io/blog/tags/Kafka%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>在HadoopHA节点上部署Kafka集群组件</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/11/28/%E5%9C%A8HadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2kafka%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/11/28/%E5%9C%A8HadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2kafka%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6/</id>
    <published>2019-11-28T14:02:03.000Z</published>
    <updated>2020-11-22T10:56:02.478Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面的文章中<a href="https://blog.csdn.net/pysense/article/details/103214906">《在hadoopHA节点上部署flume高可用组件 》</a>已经介绍了flume实时收集acces.log，同时给出flume是如何实现数据流向的高可用环境测试。在后面的文章中会给出实时大数据项目的开发，实时数据源由flume sink到kafka的topic里，而不是前面提到的hdfs，目的是利用kafka强大的分布式消息组件用于分发来自flume的实时数据流。<br>kafka集群在Hadoop实时大数据项目的位置，如下图所示：<br><img src="https://img-blog.csdnimg.cn/20191124162957995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">   </p><a id="more"></a><h4 id="1、Kafka的基本介绍"><a href="#1、Kafka的基本介绍" class="headerlink" title="1、Kafka的基本介绍"></a>1、Kafka的基本介绍</h4><h5 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h5><p>Kafka 是一种分布式的，基于发布/订阅的消息系统（redis也可以实现该功能），主要设计目标如下：<br>以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。<br>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。<br>支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。<br>同时支持离线数据处理和实时数据处理。<br>Scale out：支持在线水平扩展。</p><h5 id="1-2-kafka-应用场景"><a href="#1-2-kafka-应用场景" class="headerlink" title="1.2 kafka 应用场景"></a>1.2 kafka 应用场景</h5><ul><li>日志收集：可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer。不过在本文中，flume用于收集数据日志，kafka组件用于接受来自flume的event</li><li>流式处理：spark streaming，在上面的架构图也可以清楚看到kafka组件的下游为spark streaming，它消费来自kafka topic的实时数据消息。</li><li>消息系统：解耦生产者和消费者、缓存消息等。</li><li>用户活动跟踪：kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后消费者通过订阅这些topic来做实时的监控分析，亦可保存到hbase、mangodb等数据库。</li><li>运营指标：kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，</li><li>生产各种操作的集中反馈，比如报警和报告。<br>可以看出kafka在大数据实时处理以及互联网产品方面应用最为突出。</li></ul><h5 id="1-3-kafka相关术语"><a href="#1-3-kafka相关术语" class="headerlink" title="1.3 kafka相关术语"></a>1.3 kafka相关术语</h5><ul><li><p>producer : 生产者，生产message发送到topic，例如flume sink就是生产者</p></li><li><p>consumer : 消费者，订阅topic并消费message, consumer作为一个线程来消费，例如实时处理的spark streaming。</p></li><li><p>Broker：Kafka节点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群，在大数据项目中，直接利用已有的hadoop节点服务器配置成kafka集群。整个 Kafka 集群架构会有一个 zookeeper集群，通过 zookeeper 管理集群配置，选举 kafka Leader，以及在 Consumer Group 发生变化时进行 Rebalance。</p></li><li><p>topic：一类消息，消息存放的目录即主题，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发</p></li><li><p>massage： Kafka中最基本的传递对象。</p></li><li><p>partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的segment以及index：partition在物理上已一个文件夹的形式存在，由多个segment文件组成和多个index文件，它们是很对出现，每个Segment存着message信息，每个index存放着message的offset</p></li><li><p>replica：partition 的副本，保障 partition 的高可用。个人建议写成replica partition–副本分区</p></li><li><p>leader：这里的leader要理解为某个partition 作为主分区，也即称为leader partition，要注意该partition所在的服务器不能称为leader，否认会被误认为是kafka集群的master服务器（Kafka把master服务器称为controller）。 producer 和 consumer 只跟 leader petition交互。</p></li><li><p>replicas：leader 角色的partition加上replica角色的partition，一起成为replicas，也就是该topic总共有多少个副本数，副本数包含一个主分区副本和其余的副本分区。</p></li><li><p>controller：为了避免更leader这个词混淆，开发者将kafka 集群中的其中一台服务器称为controller，用于对每个topic的partition leader选举以及实现对partition的failover。</p></li><li><p>consumer Group：消费者组，一个Consumer Group包含多个consumer</p></li><li><p>offset：偏移量，理解为消息partition中的索引</p></li></ul><h4 id="2、kafka-单点部署与测试"><a href="#2、kafka-单点部署与测试" class="headerlink" title="2、kafka 单点部署与测试"></a>2、kafka 单点部署与测试</h4><h5 id="2-1-配置文件"><a href="#2-1-配置文件" class="headerlink" title="2.1 配置文件"></a>2.1 配置文件</h5><p>目前官方kafka最新稳定版本为2.3.1<br>按官方建议以下建议，项目用到scala2.1.3，kafka用了官方的建议版本2.1.2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">We build for multiple versions of Scala. This only matters if you are using Scala and you want a version built for the same Scala version you use. Otherwise any version should work (2.12 is recommended). </span><br></pre></td></tr></table></figure><p>kafka组件同样被放置在/opt目录下，该目录放置所有Hadoop及其组件，便于统一管理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hbase-2.1.7   kafka-2.12      scala-2.13.1             </span><br><span class="line">flume_log     hive-3.1.2    mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    xcall.sh          </span><br></pre></td></tr></table></figure><p>配置server.properties。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@nn config]# vi server.properties </span><br><span class="line">[root@nn config]# pwd</span><br><span class="line">/opt/kafka-2.12/config</span><br><span class="line"><span class="meta">#</span><span class="bash"> The id of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果是kafka集群，需配置全局id</span></span><br><span class="line">broker.id=10</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Socket Server Settings #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以不设置，kafka自动获取hostname</span></span><br><span class="line">listeners=PLAINTEXT://nn:9092</span><br><span class="line">advertised.listeners=PLAINTEXT://nn:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Basics #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终存放消息的路径,建议放在kafka组件目录下，方便管理</span></span><br><span class="line">log.dirs=/opt/kafka-2.12/kafka-logs</span><br><span class="line">num.partitions=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Zookeeper #############################</span></span></span><br><span class="line">zookeeper.connect=nn:2181,dn1:2181,dn2:2181/kafka-zk</span><br><span class="line"><span class="meta">#</span><span class="bash"> Timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br></pre></td></tr></table></figure><p>已更新配置：<del>zookeeper.connect=nn:2181,dn1:2181,dn2:2181</del><br>考虑到后面项目中，对kafka在zk上方便更为管理，用了新的配置：zookeeper.connect=nn:2181,dn1:2181,dn2:2181/kafka-zk<br>因为kafka默认在zk的根路径下创建多个节点路径，当需要去zk查看kafka相关的元数据时显得有点混乱，因此这里要求kafka将它要创建的所有znode都统一放在/kafka-zk这个路径下，方便集中查看和管理kafka的元数据，如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0]  ls /kafka-zk</span><br><span class="line">[cluster, controller_epoch, controller, brokers, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config]</span><br></pre></td></tr></table></figure><p>本文后面内容所有kafka命令中，若有<code>--zookeeper nn:2181</code> 这样启动参数，都需要改为<code>--zookeeper nn:2181/kafka-zk</code></p><h5 id="2-2-启动kafka进程"><a href="#2-2-启动kafka进程" class="headerlink" title="2.2 启动kafka进程"></a>2.2 启动kafka进程</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh config/server.properties </span><br></pre></td></tr></table></figure><p>启动后提示内存不足<br>“There is insufficient memory ”<br>因为kafka的启动脚本为最大堆申请1G内存，由于使用虚拟机跑项目，资源有限，将 kafka-server-start.sh的export KAFKA_HEAP_OPTS=”-Xmx1G -Xms1G”修改为export KAFKA_HEAP_OPTS=”-Xmx256M -Xms128M”，最大堆空间为256M，初始堆空间为128M。<br>使用后台进程方式启动kafka服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过jps也可以看到kafka进程</span></span><br><span class="line">[root@nn kafka-2.12]# jps</span><br><span class="line">4609 QuorumPeerMain</span><br><span class="line">14436 JournalNode</span><br><span class="line">2454 HMaster</span><br><span class="line">2552 Jps</span><br><span class="line">14185 DataNode</span><br><span class="line">15017 NodeManager</span><br><span class="line">2365 Kafka</span><br><span class="line">13983 NameNode</span><br><span class="line">14879 ResourceManager</span><br></pre></td></tr></table></figure><h5 id="2-3-测试topic"><a href="#2-3-测试topic" class="headerlink" title="2.3 测试topic"></a>2.3 测试topic</h5><p>创建无备份的topic,名称为hadoop，分区数1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]#bin/kafka-topics.sh --create --zookeeper nn:2181 --replication-factor 1  --partitions 1 --topic hadoop</span><br><span class="line">Created topic hadoop.</span><br></pre></td></tr></table></figure><p>查看新建的topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nn bin]# kafka-topics.sh --list --zookeeper nn:2181</span><br><span class="line">hadoop</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看kafka在zookeeper上创建的topic znode上可以看到 hadoop这个topic</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls /brokers</span><br><span class="line">[ids, topics, seqid]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /brokers/topics</span><br><span class="line">[hadoop]</span><br></pre></td></tr></table></figure><p>启动producer进程，这是一个console，可以命令式发送message</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list nn:9092 --topic hadoop</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">spark</span></span><br></pre></td></tr></table></figure><p>新打开一个shell用于启动consumer进程，订阅hadoop这个topic，该进程会持续监听9092端口，一旦上面producer的console写入信息，这边consumer就会立刻打印同样信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic hadoop</span><br><span class="line">hello kafka</span><br><span class="line">spark</span><br></pre></td></tr></table></figure><p>查看hadoop这个topic的所在的物理文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里的hadoop-0就是hadoop topic的parition</span></span><br><span class="line">[root@nn hadoop-0]# pwd</span><br><span class="line">/opt/kafka-2.12/kafka-logs/hadoop-0</span><br><span class="line"></span><br><span class="line">[root@nn hadoop-0]# ls</span><br><span class="line">00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  leader-epoch-checkpoint</span><br></pre></td></tr></table></figure><p>有index、log文件，新版本的kafka还多了timeindex时间索引。至此完成kafka单节点的配置和测试。</p><h4 id="3、kafka集群部署与测试"><a href="#3、kafka集群部署与测试" class="headerlink" title="3、kafka集群部署与测试"></a>3、kafka集群部署与测试</h4><h5 id="3-1-配置server-properties"><a href="#3-1-配置server-properties" class="headerlink" title="3.1 配置server.properties"></a>3.1 配置server.properties</h5><p>kafka集群部署要求所在节点上已经运行zookeeper集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@nn config]# vi server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个节点id需唯一nn设10，dn1设11，dn2设12</span></span><br><span class="line">broker.id=10</span><br><span class="line">ip和端口这里可以不配置，kafka自动读取，也方便把整个kafka目录分发到其他节点上</span><br><span class="line"><span class="meta">#</span><span class="bash">listeners=PLAINTEXT://:9092</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放的日志，kafka自动创建</span></span><br><span class="line">log.dirs=/opt/kafka-2.12/kafka-logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置zk集群</span></span><br><span class="line">zookeeper.connect=nn:2181,dn1:2181,dn2:2181</span><br></pre></td></tr></table></figure><p>其他属性项基本是调优项目，这里不再一一给出，后面用单独一篇文章给出讨论。<br>将kafka-2.12目录拷贝到dn1和dn2节点上，修改对应的broker.id即可</p><h5 id="3-2-集群测试"><a href="#3-2-集群测试" class="headerlink" title="3.2 集群测试"></a>3.2 集群测试</h5><p>分布在三个节点上启动kafka服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@dn1 kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line">[root@dn2 kafka-2.12]# bin/kafka-server-start.sh -daemon config/server.properties </span><br><span class="line"><span class="meta">#</span><span class="bash"> jps可以看到每个节点上都已经有kafka进程</span></span><br><span class="line">[root@nn opt]# sh xcall.sh jps |grep ka</span><br><span class="line">10569 Kafka</span><br><span class="line">12836 Kafka</span><br><span class="line">28243 Kafka</span><br></pre></td></tr></table></figure><p>创建一个新的topic：sparkapp，3份拷贝，3分区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-topics.sh --create --zookeeper nn:2181,dn1:2181,dn2:2181 --replication-factor 3 --partitions 3 --topic sparkapp</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看sparkapp主分区及其副本分区的情况</span></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-topics.sh --describe --zookeeper nn:2181 --topic sparkapp</span><br><span class="line">Topic:sparkapp  PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: sparkapp Partition: 0    Leader: 10      Replicas: 10,11,12      Isr: 10,11,12</span><br><span class="line">        Topic: sparkapp Partition: 1    Leader: 11      Replicas: 11,12,10      Isr: 11,12,10</span><br><span class="line">        Topic: sparkapp Partition: 2    Leader: 12      Replicas: 12,10,11      Isr: 12,10,11</span><br></pre></td></tr></table></figure><p>该命令其实就是读取/brokers/topics/sparkapp/partitions/**/state 所有分区的state节点值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] get  &#x2F;brokers&#x2F;topics&#x2F;sparkapp&#x2F;partitions&#x2F;0&#x2F;state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:22,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[10,11,12]&#125;</span><br></pre></td></tr></table></figure><p>在nn节点启动producer进程，连接broker分别为nn自己、dn1节点和dn2节点，都能正常连接，同理，dn1、dn2的producer进程使用dn1、dn2、nn节点都能正常连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list nn:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br><span class="line"></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list dn1:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br><span class="line"></span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-producer.sh --broker-list dn2:9092 --topic sparkapp</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">sparkapp</span></span><br></pre></td></tr></table></figure><p>在nn节点启动producer进程，然后在dn1节点、dn2节点以及nn新shell分别启动consumer，看看一个producer生产msg，其他三个节点能否同时收到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dn1 kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br><span class="line">[root@dn2 kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br><span class="line">[root@nn kafka-2.12]# bin/kafka-console-consumer.sh --bootstrap-server nn:9092 --topic sparkapp</span><br></pre></td></tr></table></figure><p>查看kafka-cluster这个topic的partition在物理文件上的分布</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@nn kafka-logs]# ls sparkapp-</span><br><span class="line">sparkapp-0/ sparkapp-1/ sparkapp-2/ </span><br><span class="line">[root@nn kafka-logs]# ls sparkapp-0/</span><br><span class="line">00000000000000000000.index  00000000000000000000.timeindex</span><br><span class="line">00000000000000000000.log    leader-epoch-checkpoint</span><br></pre></td></tr></table></figure><p>可以看到三个分区对于三个文件目录，每个目录有索引文件和数据文件</p><h5 id="3-3-在zk上查看集群情况"><a href="#3-3-在zk上查看集群情况" class="headerlink" title="3.3 在zk上查看集群情况"></a>3.3 在zk上查看集群情况</h5><p>kafka在zk上的数据存储结构：<br>brokers列表：ls /brokers/ids<br>某个broker信息：get /brokers/ids/10<br>topic信息：get /brokers/topics/sparkapp<br>partition信息：get /brokers/topics/sparkapp/partitions/0/state<br>controller中心节点变更次数：get /controller_epoch<br>conrtoller信息：get /controller<br>[zk: localhost:2181(CONNECTED) 2] get /controller<br>{“version”:1,”brokerid”:10,”timestamp”:”***”}，可以看到当前kafka集群的controller节点为nn服务器brokerid为10.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 集群的brokers信息在/brokers持久节点下，ids节点用于存放上线的brokers id号，topics：集群上所有的topces都放在在节点下</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 20] ls /brokers</span><br><span class="line">[ids, topics, seqid]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> brokers在持久ids节点下注册临时节点，节点名称就是broker自己的id号，这里说明为何在server.properties里面的broker.id要设为唯一，因为利用zookeeper的临时节点以及保证节点命名唯一。</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 19] ls /brokers/ids</span><br><span class="line">[10,11,12]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取其中一个broker id节点的信息，例如dn2这个broker</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 24] get /brokers/ids/12</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://dn2:9092&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;dn2&quot;,&quot;timestamp&quot;:&quot;*****&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看sparkapp这个topics的分区数量</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /brokers/topics/sparkapp/partitions</span><br><span class="line">[0, 1, 2]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前kafka集群的leader状态，通过在topic的分区的state节点可以看到当前leader是节点dn1，对应的broker id为1</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 36] get /brokers/topics/sparkapp/partitions/1/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:6,&quot;leader&quot;:10,&quot;version&quot;:1,&quot;leader_epoch&quot;:2,&quot;isr&quot;:[11,12,10]&#125;</span><br></pre></td></tr></table></figure><p>至此，完成Kafka的集群配置和测试</p><h4 id="4、-小结"><a href="#4、-小结" class="headerlink" title="4、 小结"></a>4、 小结</h4><p>为hadoop环境配置kafka组件的过程相对简单，鉴于Kafka这个中间件具有非常不错应用价值，本blog继续用1到2篇文章深入探讨有关Kafka核心内容。此外还用另外一篇文章用于给出flume和kafka两者的整合——<a href="https://blog.csdn.net/pysense/article/details/103335495">《flume集群高可用连接kafka集群》</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面的文章中&lt;a href=&quot;https://blog.csdn.net/pysense/article/details/103214906&quot;&gt;《在hadoopHA节点上部署flume高可用组件 》&lt;/a&gt;已经介绍了flume实时收集acces.log，同时给出flume是如何实现数据流向的高可用环境测试。在后面的文章中会给出实时大数据项目的开发，实时数据源由flume sink到kafka的topic里，而不是前面提到的hdfs，目的是利用kafka强大的分布式消息组件用于分发来自flume的实时数据流。&lt;br&gt;kafka集群在Hadoop实时大数据项目的位置，如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20191124162957995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot;&gt;   &lt;/p&gt;</summary>
    
    
    
    <category term="Kafka" scheme="https://yield-bytes.gitee.io/blog/categories/Kafka/"/>
    
    
    <category term="kafka集群" scheme="https://yield-bytes.gitee.io/blog/tags/kafka%E9%9B%86%E7%BE%A4/"/>
    
    <category term="hadoop HA" scheme="https://yield-bytes.gitee.io/blog/tags/hadoop-HA/"/>
    
  </entry>
  
  <entry>
    <title>在hadoopHA节点上部署flume高可用组件</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/11/24/%E5%9C%A8hadoopHA%E8%8A%82%E7%82%B9%E4%B8%8A%E9%83%A8%E7%BD%B2flume%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/</id>
    <published>2019-11-24T08:17:22.000Z</published>
    <updated>2020-11-21T16:54:58.607Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式</p><a id="more"></a><table><thead><tr><th>hadoop节点</th><th>数据源节点</th><th>角色</th></tr></thead><tbody><tr><td>nn</td><td>nn</td><td>NameNode,DataNode数据源</td></tr><tr><td>dn2</td><td>dn2</td><td>NameNode,DateNode数据源</td></tr><tr><td>dn1</td><td>dn1</td><td>DataNode,数据源</td></tr></tbody></table><p>其他hbase、hive等不再此表给出，因为前面的文件已有相关表格。</p><h4 id="1、flume-的基本介绍"><a href="#1、flume-的基本介绍" class="headerlink" title="1、flume 的基本介绍"></a>1、flume 的基本介绍</h4><h5 id="1-1-基本介绍"><a href="#1-1-基本介绍" class="headerlink" title="1.1 基本介绍"></a>1.1 基本介绍</h5><blockquote><p>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.  It uses a simple extensible data model that allows for online analytic application.</p></blockquote><p>简单来说：flume 是一个分布式、高可靠、高可用的用来收集、聚合、转移不同来源的大量日志数据到中央数据仓库的工具</p><p>目前flume最新版本为今年1月发布的 Flume 1.9.0，具体优化的内容：</p><blockquote><ul><li>Better SSL/TLS support</li><li>Configuration Filters to provide a way to inject sensitive information like passwords into the configuration</li><li>Float and Double value support in Context</li><li>Kafka client upgraded to 2.0</li><li>HBase 2 support</li></ul></blockquote><p>环境要求：</p><p>Java Runtime Environment - Java 1.8 or later</p><h5 id="1-2-数据流模型"><a href="#1-2-数据流模型" class="headerlink" title="1.2 数据流模型"></a>1.2 数据流模型</h5><p>这里以Flume收取web的日志再将其写入到hdfs作为数据流模型示例说明。<br>（需要注意的flume支持多级配置、扇入、扇出，这里仅介绍单级、单输入、单输出的模式）<br><img src="https://img-blog.csdnimg.cn/20191123154225919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>==Event==<br>Event是Flume定义的一个数据流传输的最小单元。Agent就是一个Flume的实例，本质是一个JVM进程，该JVM进程控制Event数据流从外部日志生产者那里传输到目的地（或者是下一个Agent）。<br>在Flume中，event表示数据传输的一个最小单位，从上图可以看出Agent就是Flume的一个部署实例， 一个完整的Agent中包含了三个组件Source、Channel和Sink，Source是指数据的来源和方式，Channel是一个数据的缓冲池，Sink定义了数据输出的方式和目的地。</p><p>==Source组件==<br>Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。（对于实时大数据项目，这个source其实就是不断滚动的log数据文件。）<br>Flume提供了各种source的实现，具体参考官方文档：<br>Flume Sources、Avro Source、Thrift Source、Exec Source、JMS Source、Taildir Source、Kafka Source、NetCat TCP Source、NetCat UDP Source、Syslog Sources、HTTP Source、Custom Source等</p><p>==Channel组件==<br>Channel是连接Source和Sink的组件，可以看作一个数据的缓冲区，它可以将事件暂存到内存中也可以持久化到磁盘上， 直到Sink处理完该事件。flume提供多个channel提供对event数据的缓存：<br>Memory Channel、JDBC Channel、Kafka Channel、File Channel、Spillable Memory Channel、Pseudo Transaction Channel、Custom Channel<br>（在本大数据实时项目中，使用Memory Channel即可，生产环境需要根据具体需求而定）</p><p>==Sink组件==<br>Sink从Channel取出event数据，并将其写入到文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。<br>Flume也提供了各种sink的实现，具体参考官方说明：<br>HDFS Sink、Hive Sink、Logger Sink、Avro Sink、Thrift Sink、IRC Sink、File Roll Sink、Null Sink、HBaseSinks、AsyncHBaseSink、ElasticSearchSink、Kite Dataset Sink、Kafka Sink、HTTP Sink、Custom Sink<br>（在实时大数据项目中，用sink配置将数据写入kafka sink）</p><p>以上的数据模型的详细介绍可以参考中文翻译文档：<a href="https://flume.liyifeng.org/">地址</a><br>(注意：该翻译文档对应flume1.8版本的内容，若想查阅最新的flume，参考官网1.9原文)</p><h4 id="2、flume的配置文件说明"><a href="#2、flume的配置文件说明" class="headerlink" title="2、flume的配置文件说明"></a>2、flume的配置文件说明</h4><p>flume配置遵循Java properties文件格式的文本文件。一个或多个Agent配置可放在同一个配置文件里。配置文件包含Agent的source，sink和channel的各个属性以及他们的数据流连接。</p><p>完整的配置流程如下：<br>命名各个组件（定义流）–&gt;连接各个组件–&gt;配置各个组件的属性–&gt;启动Agent</p><h5 id="2-1-配置过程"><a href="#2-1-配置过程" class="headerlink" title="2.1 配置过程"></a>2.1 配置过程</h5><p>这里以后面文章——flume整合kafka的配置文件说明。<br>（有较多的blog文章会照搬a1.sources=r1,a1.sinks=k1,a1.channels=c1这样的写法，a1?r1?k1?c1?到底指代什么？所以建议要参考官方文档，否则一头雾水？当然熟悉flume后，可以使用短命名，设置配置属性字符串不会显得太长）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、列出Agent的所有Source、Channel、Sink</span></span><br><span class="line">&lt;Agent&gt;.sources = &lt;Source&gt;</span><br><span class="line">&lt;Agent&gt;.sinks = &lt;Sink&gt;</span><br><span class="line">&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接各个组件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Source的关联</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Channel和Sink的关联</span></span><br><span class="line">&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、为每个组件配置属性，这些属性就是flume的性能参数，控制flume的各种工作方式，调优配置就在这部分了。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sources</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> channels</span></span><br><span class="line">&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sinks</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br></pre></td></tr></table></figure><p>对于本blog的实时大数据项目的配置，Agent名字为：agent_log从本地服务器读取log数据文件，使用内存channel缓存，然后通过kafka Sink从channel读取后发送到kafka集群，它的配置文件应该这样配：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、定义Agent的所有<span class="built_in">source</span>、sink和channel组件</span></span><br><span class="line">agent_log.sources = log-src</span><br><span class="line">agent_log.sinks = kafka-sink</span><br><span class="line">agent_log.channels = log-mem-channel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、连接三个组件</span></span><br><span class="line">agent_log.sources.log-src.channels =log-mem-channel    # 指定与source:log-src相连接的channel是log-mem-channel</span><br><span class="line">agent_foo.sinks.kafka-sink.channel = log-mem-channel   # 指定与sink:kafka-sink相连接的channel是log-mem-channel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、配置各个组件的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置avro-AppSrv-source的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> log-src 的类型是spooldir，官方建议不要使用tail -F抽取数据文件因会出现丢失</span></span><br><span class="line">agent_log.sources.log-src.type = spooldir         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置<span class="built_in">source</span>数据文件的路径</span></span><br><span class="line">agent_log.sources.log-src.spoolDir = /opt/flume_agent/web_log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置log-mem-channel的属性</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> channel的类型是内存channel</span></span><br><span class="line">agent_log.channels.log-mem-channel.type = memory  </span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> channel的最大容量是1000</span></span><br><span class="line">agent_log.channels.log-mem-channel.capacity = 1000         </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>和sink每次从channel写入和读取的Event数量</span></span><br><span class="line">agent_log.channels.log-mem-channel.transactionCapacity = 100    </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置kafka-sink的属性，将数据写入到kafka集群指定topic，实现Flume与Kafka集成</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接channel到kafkasink</span></span><br><span class="line">agent_log.sinks.kafka-sink.channel = log-mem-channel </span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定kafka sink</span></span><br><span class="line">agent_log.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka存放数据的topic</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.topic = webtopic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka sink 使用的 kafka 集群的实例列表</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.bootstrap.servers = nn:9092,dn1:9092,dn2:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每批要发送到kafka的消息数量</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.flumeBatchSize = 20</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在成功写入之前，要求有1个副本必须确认消息，保证数据一致性</span></span><br><span class="line">agent_log.sinks.kafka-sink.kafka.producer.acks = 1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>通过上面的配置，就形成了[log-src]-&gt;[log-mem-channel]-&gt;[log-sink]的数据流，这将使Event通过内存channel（log-mem-channel）从log-src流向Kafka-sink，从而实现数据源-flume-kafka的实时数据流动。</p><h4 id="3、单点flume-agent测试"><a href="#3、单点flume-agent测试" class="headerlink" title="3、单点flume agent测试"></a>3、单点flume agent测试</h4><p>本节主要在name节点上部署单个flume agent，用于测试单agent的使用。<br>数据流向：<br>手动写日志内容—&gt;flume spooldir抽取—&gt;flume sink到hadoop集群文件系统上</p><h5 id="3-1-基本安装"><a href="#3-1-基本安装" class="headerlink" title="3.1 基本安装"></a>3.1 基本安装</h5><p>个人习惯将所有的hadoop组件都放置在同一个dir下，方便管理，如下所示</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@nn opt]# ls</span><br><span class="line">flume-1.9.0   hive-3.1.2       xcall.sh       </span><br><span class="line">hadoop-3.1.2  jdk1.8.0_161    scala-2.13.1         </span><br><span class="line">hbase-2.1.7   mariadb-10.4.8  spark-2.4.4-bin-hadoop2.7  zookeeper-3.4.14</span><br></pre></td></tr></table></figure><p>配置flume-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@nn conf]# pwd</span><br><span class="line">/opt/flume-1.9.0/conf</span><br><span class="line">[root@nn conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line">vi flume-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置java1.8的路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> JAVA_HOME=/opt/jdk1.8.0_16</span></span><br></pre></td></tr></table></figure><p>这里的配置要注意的点：如果已经在系统的环境变量配置JAVA_HOME，那么flume-env.sh可以不用再配置java路径</p><p>配置flume-conf.properties<br>这里就是用于配置flume agent的文件。有了第2章节的介绍后，这里有关source、channel、sink配置则相对简单，因此可以使用短字符进行配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出三个组件</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>数据源为本地某个文件目录，flume监听这个目录下日志文件</span></span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里不需要写成web_log/</span></span><br><span class="line">a1.sources.r1.spoolDir = /opt/flume_log/web_log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel，使用本节点的内存缓存event</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录，hdapp为hadoop集群名称，不需要加入端口，否则flume无法写入，</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType=SequenceFile</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat=Writable</span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放在hdfs的文件文件命名方式，其实还有更详细的配置，这里仅给出简单示例，具体可参考官网。</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从flume过来的数据，每128M分割成一个文件</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128000000  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 最终在hdfs的文件名称为：%Y-%m-%d.TimeStamp.txt</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这里在配置source.type要注意的是，配成spooldir类型后：允许把要收集的文件放入磁盘上的某个指定目录。它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来。数据解析逻辑是可配置的。<br>与Exec Source不同，Spooling Directory Source是可靠的，即使Flume重新启动或被kill，也不会丢失数据。<br>但这种可靠有一定的代价和限制：指定目录中的文件必须是不可变的、唯一命名的。Flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常：<br>如果文件在写入完成后又被再次写入新内容，Flume将向其日志文件（这是指Flume自己logs目录下的日志文件）打印错误并停止处理。如果在以后重新使用以前的文件名，Flume将向其日志文件打印错误并停止处理。<br>为了避免上述问题，最好在生成新文件的时候文件名加上时间戳，可以通过加入属性项实现：a1.sinks.k1.hdfs.useLocalTimeStamp = true</p><h5 id="3-2-启动flume-agent进程"><a href="#3-2-启动flume-agent进程" class="headerlink" title="3.2 启动flume agent进程"></a>3.2 启动flume agent进程</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]# pwd</span><br><span class="line">/opt/flume-1.9.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动flume agent 实例</span></span><br><span class="line">[root@nn flume-1.9.0]# bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p> 命令含义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">commands:</span><br><span class="line">  help                      display this help text</span><br><span class="line">  agent                     run a Flume agent</span><br><span class="line">  avro-client               run an avro Flume client</span><br><span class="line">  version                   show Flume version info</span><br><span class="line"> global options:</span><br><span class="line">    --conf,-c &lt;conf&gt;          use configs in &lt;conf&gt; directory</span><br><span class="line"> agent options:</span><br><span class="line">    --name,-n &lt;name&gt;          the name of this agent (required)</span><br><span class="line">  --conf-file,-f &lt;file&gt;     specify a config file (required if -z missing)</span><br></pre></td></tr></table></figure><p>创建数据文件，测试flume 能否成功把目录下的文件推到hdfs指定目录上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@nn web_log]# pwd</span><br><span class="line">/opt/flume_log/web_log</span><br><span class="line">[root@nn web_log]#  vi log.txt</span><br><span class="line">aaa</span><br><span class="line">bbb</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当文件创建后，发现该log.txt被命名为log.txt.COMPLETED，说明已经被flume 读取过</span></span><br><span class="line">[root@nn web_log]# ls</span><br><span class="line">log.txt.COMPLETED</span><br></pre></td></tr></table></figure><p>hdfs上可看到数据文件已经上传到到/flume/web_log（这里打码了时间）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn web_log]# hdfs dfs -ls /flume/web_log </span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup        161 **** /flume/web_log/2019-**-**.15**0.txt</span><br></pre></td></tr></table></figure><h5 id="3-3-将source-type配成tail-F"><a href="#3-3-将source-type配成tail-F" class="headerlink" title="3.3 将source.type配成tail F"></a>3.3 将source.type配成tail F</h5><p>Spooling Directory Source是可靠的，它会将监视这个目录中产生的新文件，并在新文件出现时从新文件中解析数据出来，当此种方式不适合本blog后面开发的实时大数据项目需求。具体说明如下：<br>本blog后面开发的实时大数据项目需求：<br>实时抽取access.log的访问日志，access.log每插入一行，flume 就会把它实时sink到hdfs上（本文用于测试所以先sink到hdfs，若已经到开发阶段，这里会改为sink到kalka集群上）。<br>对于spooldir模式，当log.txt被sink后其文件名变为log.txt.COMM，若继续向log.txt.COMPLETED append数据行，flume不会再抽取该文件，也说明无法把新来的数据sink到hdfs上，显然不符合需求。<br>source需做以下调整，使用exec source：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改<span class="built_in">source</span> <span class="built_in">type</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure><p>这里sink的文件滚动策略很重要，若配置不当，flume sink会在hdfs不断滚动生成多个小文件，例如access.log新增一行，触发flume sink在hdfs新增一个对应的文件。<br>以下的配置：access.log在hdfs存放的形式为：<br>/flume/web_log/2019-05-31.1579*.txt<br>每达到128M则开始滚动新建一个文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将evevt数据写到hadoop文件系统的指定目录下</span></span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需自行创建该目录</span></span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 128M</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix=.txt</span><br></pre></td></tr></table></figure><p>但exec source方式也有缺点：会丢失数据，例如当flume 挂了重启，之前进来的日志行将不会被重启后flume抽取到，正官方的提示：<br>The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost.</p><p>这种数据丢失其实还可以接受，毕竟大部分日志收集应用场景还没到高级事务的严格标准，而且服务器集群运行以及进程运行稳定，即使宕机、断电再重启，也只是一小部分日志行丢失。</p><p>==测试结果：==<br>启动flume agent，并将日志实时打印在shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn flume-1.9.0]#  bin/flume-ng agent -c conf -f conf/flume-conf.properties --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将日志追加新数据行</span></span><br><span class="line">[root@nn web_log]# echo &#x27;test&#x27;&gt;&gt;access.log </span><br></pre></td></tr></table></figure><p>在hdfs上，存放的文件会以文件名.txt.tmp形式保持打开状态，供flume实时写入，若达到滚动条件，则会生成日期+时间戳.txt的数据文件，再新建另外一个日期+时间戳.txt.tmp文件。</p><p>至此，已完成flume的部署，下一步，在三个节点上配置高可用的flume集群。</p><h4 id="4、flume高可用配置"><a href="#4、flume高可用配置" class="headerlink" title="4、flume高可用配置"></a>4、flume高可用配置</h4><p>Flume高可用又称Flume NG高可用，NG：Next Generation。<br>flume高可用的实现思路比较清晰：多个节点flume agent  avro sink 到 多个flume collector avro source上，这些flume collector 会有优先级，优先级高的collector负责把数据sink到hdfs或者kafka上。因为agent和collector是多节点运行，在agent端：某个agent挂了，还有其他agent工作；在collecor端，某个collector挂了，还有其他collector继续工作。<br>架构图如下：<br><img src="https://img-blog.csdnimg.cn/20191124103515197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3B5c2Vuc2U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">各个节点规划，考虑到测试虚拟机资源有限，其中两个节点都分布运行agent和collector进程。<br>| 节点 |  flume 角色|<br>|–|–|<br>| nn | agent1，collector 1|<br>| dn1 | agen2 |<br>| dn2 | agent3，collector2 |</p><h5 id="4-1-三个agent的flume配置"><a href="#4-1-三个agent的flume配置" class="headerlink" title="4.1  三个agent的flume配置"></a>4.1  三个agent的flume配置</h5><p>三个agent的配置其实都一样，不同的部分：每个agent命名不同。<br>在nn节点的/opt/flume-1.9.0/conf新建一个avro-agent.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出agent1的组件，sinks有两个，分别去到collector1和collector2</span></span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">source</span>属性</span></span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /opt/flume_log/web_log/access.log</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置channel</span></span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector1</span></span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = nn</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置sink到collector2</span></span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = dn2</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建sink groups，将多个sinks绑定为一个组，agent会向这些组sink 数据，将k1和k2设置负载均衡模式，也可以设置为failover模式，本文使用load_balance</span></span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">agent1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agent1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">agent1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">agent1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> failover模式，只有collector1工作。仅当collector1挂了后，collector2才能启动服务。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> agent1.sinkgroups.g1.processor.type = failover</span></span><br><span class="line"><span class="meta">#</span><span class="bash">值越大，优先级越高，collector1优先级最高</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k1 = 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.priority.k2 = 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">发生异常的sink最大故障转移时间（毫秒），这里设为10秒</span></span><br><span class="line"><span class="meta">#</span><span class="bash">agent1.sinkgroups.g1.processor.maxpenalty = 10000</span></span><br></pre></td></tr></table></figure><p>将avro-agent.properties拷贝到dn1和dn2节点，agent1这个名字可改，可不改。</p><h5 id="4-2-配置-collector"><a href="#4-2-配置-collector" class="headerlink" title="4.2  配置 collector"></a>4.2  配置 collector</h5><p>分别在nn和dn2节点的/opt/flume-1.9.0/conf新建一个avro-collector.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在dn2节点上，则改为collector2，不改也没关系，这里只是为了区分两个collector</span></span><br><span class="line">collector1.sources = r1</span><br><span class="line">collector1.sinks = k1</span><br><span class="line">collector1.channels = c1</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义<span class="built_in">source</span>：这里的<span class="built_in">source</span>配成avro，从而连接agent端sink avro</span></span><br><span class="line">collector1.sources.r1.channels = c1</span><br><span class="line">collector1.sources.r1.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span>的属性：dn2节点需改为dn2</span></span><br><span class="line">collector1.sources.r1.bind = nn</span><br><span class="line">collector1.sources.r1.port = 52020</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">定义channel</span></span><br><span class="line">collector1.channels.c1.type = memory</span><br><span class="line">collector1.channels.c1.capacity = 1000</span><br><span class="line">collector1.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">定义sinks：由collector将数据event推到hdfs上</span></span><br><span class="line">collector1.sinks.k1.channel=c1</span><br><span class="line">collector1.sinks.k1.type=hdfs</span><br><span class="line">collector1.sinks.k1.hdfs.path=hdfs://hdapp/flume/web_log</span><br><span class="line">collector1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">collector1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">collector1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">collector1.sinks.k1.hdfs.minBlockReplicas=1</span><br><span class="line">collector1.sinks.k1.hdfs.idleTimeout=0</span><br><span class="line">collector1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">collector1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">collector1.sinks.k1.hdfs.fileSuffix=.txt</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h5 id="4-3-测试flume高可用"><a href="#4-3-测试flume高可用" class="headerlink" title="4.3 测试flume高可用"></a>4.3 测试flume高可用</h5><p>在nn节点和dn2节点启动各自的collector</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">nn节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector1，所以这里启动的--name 为collector1</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">dn2节点启动collector进程，因为该节点的avro-collector.properties agent名字为collector2，所以这里启动的--name 为collector2</span></span><br><span class="line">[root@nn flume-1.9.0]# </span><br><span class="line"> bin/flume-ng agent -c conf -f conf/avro-collector.properties --name  collector2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>在nn、dn1和dn2节点启动各自的agent，在shell可以看到以下agent 进程打印的信息，说明三个agent都可以连接到两个collector的source组件k1和k2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k1 started.</span><br><span class="line"> ......</span><br><span class="line">Monitored counter group for type: SINK, name: k2: Successfully registered new MBean.</span><br><span class="line"> Rpc sink k2 started.</span><br></pre></td></tr></table></figure><p>在nn节点上的access.log新增信息 echo ‘foo’ &gt;&gt;access.log后，在hdfs上可以看到***.txt.tmp文件可以相应的文件内容<br>停止collector1经常，此时测试collector2可以正常接替服务。</p><p>至此，已完成本文内容。下一篇文章为Hadoop引入Kafka组件，在实时大数据项目中，实时数据是被flume sink到kafka的topic里，而不是本文测试的hdfs。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;前面的blog已实现了hadoopHA的项目环境，本文继续为该hadoop环境引入flume组件，用于实时大数据项目的开发。考虑到项目已经使用了hadoopHA，那么flume的组件也相应的部署成HA模式&lt;/p&gt;</summary>
    
    
    
    <category term="Flume" scheme="https://yield-bytes.gitee.io/blog/categories/Flume/"/>
    
    
    <category term="flume高可用" scheme="https://yield-bytes.gitee.io/blog/tags/flume%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>hadoop集群平台网络配置bond模式实现高可用</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/11/19/hadoop%E9%9B%86%E7%BE%A4%E5%B9%B3%E5%8F%B0%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AEbond%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/11/19/hadoop%E9%9B%86%E7%BE%A4%E5%B9%B3%E5%8F%B0%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AEbond%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8/</id>
    <published>2019-11-19T12:54:57.000Z</published>
    <updated>2020-11-21T16:56:57.296Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在前面文章关于hadoop大数据项目开发中，每台服务器之间其实存在大量的IO，例如NameNode服务器和DataNode服务器同步fimag文件件，DataNode之间的数据IO，为压榨服务器对于超大数据的吞吐量，在服务器的网络层使用bond模式配置Linux网卡，可以将该服务器上多个物理网卡虚拟成一张网卡，由该虚拟网卡对外提供统一网络服务，实现多个网卡自适应负载均衡以及提高数据的吞吐量，同时也实现链路双备份功能，保证服务器底层网络高可用性。bond虚拟网卡可以绑定多个网络，例如绑定2两个网卡，一般物理服务器有4个千兆网卡，所以也可以绑定4个网卡，绑定的网卡类型需要一致，否则无法进行bond模式配置。<br>&#8195;&#8195;以centos7.5 的两个网卡配成bond模式作为示例，其中在之前的hadoop的相关文章里，nn节点与dn2节点构成HA，dn节点作为DataNode，当这三个节点的底层网络都配成bond模式，将进一步提高本blog之前搭建大数据开发平台的高可用性。作为全栈开发者，这些涉及简单网络的配置，应该需要掌握。</p><a id="more"></a><h4 id="1、为测试服务器添加多个网卡"><a href="#1、为测试服务器添加多个网卡" class="headerlink" title="1、为测试服务器添加多个网卡"></a>1、为测试服务器添加多个网卡</h4><p>在VMware workstations中，给相应服务器新增一个网卡，选择NAT模式，这样这是基于虚拟机操作，实际生成环境中，我的开发项目会部署在真实服务器上，真实服务器不需要NAT模式。</p><h4 id="2、查看测试服务器的网卡信息"><a href="#2、查看测试服务器的网卡信息" class="headerlink" title="2、查看测试服务器的网卡信息"></a>2、查看测试服务器的网卡信息</h4><p>原网卡ens33是配有IP的，新增的网卡ens37无IP绑定。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ~]# ip a</span><br><span class="line"></span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.188.0.5/24 brd 192.188.0.255 scope global noprefixroute ens33</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: ens37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 ***/64 scope link noprefixroute </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>可以看到两个网卡ens33和ens37，都是千兆网速，都为up状态</p><h4 id="3、配置两个网卡"><a href="#3、配置两个网卡" class="headerlink" title="3、配置两个网卡"></a>3、配置两个网卡</h4><h5 id="3-1-这里需要先把原网卡配置拷贝一份作为备份。"><a href="#3-1-这里需要先把原网卡配置拷贝一份作为备份。" class="headerlink" title="3.1 这里需要先把原网卡配置拷贝一份作为备份。"></a>3.1 这里需要先把原网卡配置拷贝一份作为备份。</h5><p>备份网卡ens33的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# pwd</span><br><span class="line">/etc/sysconfig/network-scripts</span><br><span class="line">[root@nn network-scripts]# cp ifcfg-ens33 ifcfg-ens33.bak</span><br></pre></td></tr></table></figure><p>新增网卡ens37是没有对应配置文件，只需要从ens33拷贝一份即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# cp ifcfg-ens33 ifcfg-ens37</span><br></pre></td></tr></table></figure><h5 id="3-2-将ens33和ens37配成slave模式"><a href="#3-2-将ens33和ens37配成slave模式" class="headerlink" title="3.2 将ens33和ens37配成slave模式"></a>3.2 将ens33和ens37配成slave模式</h5><p>这里不再需要配置ip和dns，mac地址也不需要配置，linux自动识别。</p><p>配置ens33，</p><p>注意该网卡已配置有IP，在改完该网卡配置后，请勿直接重启网卡，否则ssh无法远程连接，因为slave模式下，该网卡配置文件里面是无IP地址的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=ens33</span><br><span class="line">NAME=ens33</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line"><span class="meta">#</span><span class="bash"> bond网卡的名字</span></span><br><span class="line">MASTER=bond0</span><br><span class="line"><span class="meta">#</span><span class="bash"> ens33网卡启用slave模式</span></span><br><span class="line">SLAVE=yes</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>配置ens37 ，同上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=ens37</span><br><span class="line">NAME=ens37</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NM_CONTROLLED=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line"><span class="meta">#</span><span class="bash"> bond网卡的名字</span></span><br><span class="line">MASTER=bond0</span><br><span class="line"><span class="meta">#</span><span class="bash"> ens33网卡启用slave模式</span></span><br><span class="line">SLAVE=yes</span><br></pre></td></tr></table></figure><p>若有更多的网卡需要绑定为bond0模式，则按以上配置改即可。</p><h4 id="4、配置bond0虚拟网卡"><a href="#4、配置bond0虚拟网卡" class="headerlink" title="4、配置bond0虚拟网卡"></a>4、配置bond0虚拟网卡</h4><p>在 <code>/etc/sysconfig/network-scripts</code>下，需要创建一个<code>ifcfg-bond0</code>文件，用于配置名字为bond0的虚拟网卡，注意这里bond0只是一个名字，可以按需要命名，例如mybond。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# vi ifcfg-bond0</span><br></pre></td></tr></table></figure><p>配置文件如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=bond0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">NM_CONTROLLER=no</span><br><span class="line">BONDING_OPTS=&quot;miimon=100 mode=6&quot;</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">USERCTL=no</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.188.0.5</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.188.0.1</span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line">DNS2=114.114.114.115</span><br></pre></td></tr></table></figure><p>若不想在bond0配置文件写入<code>BONDING_OPTS=&quot;miimon=100 mode=6&quot;</code></p><p>也可以在/etc/modprobe.d/dist.conf</p><p>新增如下两行：</p><p><code>alias bond0 bonding options bond0 miimon=100 mode=6</code></p><p>但个人建议直接在bond0网卡配置mod模式，方便后期查看和更改。</p><h4 id="5、bond0的配置说明和工作原理"><a href="#5、bond0的配置说明和工作原理" class="headerlink" title="5、bond0的配置说明和工作原理"></a>5、bond0的配置说明和工作原理</h4><p>重点配置项为：BONDING_OPTS=”miimon=100 mode=6”</p><p>该配置文件:miimon=100，意思是linux每100ms监测一次本服务器网络链路连接状态（这里说的网络连接，是指该服务器网卡与接入层交换机端口连接的状态），如果有其中网卡例如ens33中断，那么bond0会将网络连接切到可用ens37；</p><p>mode的值表示工作模式，共有0，1，2，3，4，5，6六种模式，常用为0，6，1三种</p><ul><li><p>mode=0，表示load balancing (round-robin)为负载均衡方式，两块网卡都工作，但是与网卡相连的交换必须做特殊配置（ 这两个端口需要配置成聚合方式），因为做bonding的这两块网卡是使用同一个MAC地址</p></li><li><p>mode=1，表示fault-tolerance (active-backup)提供冗余功能，工作方式是主备的工作方式，也就是说默认情况下只有一块网卡工作，另一块做备份 。</p></li><li><p>mode=6，表示load balancing (round-robin)为负载均衡方式，两块网卡都工作，该模式下无需配置交换机，因为做bonding的这两块网卡是使用不同的MAC地址。</p><p>mode6因为可以做负载均衡，因此实际场景使用效果好，mode6的工作原理：bond0虚拟网卡的mac地址为两张网卡mac地址其中的一个，bond0通过更改自身与某个网卡一样的Mac地址，以达到随时切换转发数据包到相应正常工作的网卡中。从外部交换机来看，交换机只看到bond0这个网卡，bond0就像Nginx，都是代理角色，代理后面有多个实体。</p><p>mode6负载均衡的是这么实现的：</p><p>假设测试服务器的网卡ens33连接交换机33号端口，ens37连接交换机37端口。</p><p>当bond0检测到ens33流量超过阈值时，则bond0会将自己Mac地址切换到en37的mac地址，所以交换机通过发arp广播包，找到37端口就是连接ens37网卡，所以网络流量就从交换机37端口转发到ens37网卡。</p></li></ul><h4 id="6、-加载内核bond模块-modprobe-bonding"><a href="#6、-加载内核bond模块-modprobe-bonding" class="headerlink" title="6、 加载内核bond模块 modprobe bonding"></a>6、 加载内核bond模块 modprobe bonding</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# modprobe bonding</span><br><span class="line">[root@nn network-scripts]# lsmod |grep bond </span><br><span class="line">bonding               152656  0 </span><br></pre></td></tr></table></figure><p>重启网络</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# service network restart</span><br><span class="line">Restarting network (via systemctl):                        [  OK  ]</span><br></pre></td></tr></table></figure><h4 id="7、查看bond0虚拟网卡状态并测试主备网卡切换"><a href="#7、查看bond0虚拟网卡状态并测试主备网卡切换" class="headerlink" title="7、查看bond0虚拟网卡状态并测试主备网卡切换"></a>7、查看bond0虚拟网卡状态并测试主备网卡切换</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# cat /proc/net/bonding/bond0 </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 负载均衡模式</span></span><br><span class="line">Bonding Mode: adaptive load balancing</span><br><span class="line">Primary Slave: None</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前对外连接网络额度是网卡ens33</span></span><br><span class="line">Currently Active Slave: ens33</span><br><span class="line"><span class="meta">#</span><span class="bash"> 状态up</span></span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens33</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f1</span><br><span class="line">Slave queue ID: 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f2</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure><p>查看bond0的mac地址，跟ens33的mac地址一样：12:1d:21:fg:43:f1 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@nn ~]# ip a</span><br><span class="line"></span><br><span class="line">2: ens33: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: ens37: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line"></span><br><span class="line">191: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 12:1d:21:fg:43:f1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.142.4/24 brd 192.168.142.255 scope global noprefixroute bond0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>关闭ens33网卡，看看bond0是否会切换到ens37</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@nn network-scripts]# ifdown ens33</span><br><span class="line">Device &#x27;ens33&#x27; successfully disconnected.</span><br><span class="line"></span><br><span class="line">[root@localhost network-scripts]# cat /proc/net/bonding/bond0 </span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line">Bonding Mode: adaptive load balancing</span><br><span class="line">Primary Slave: None</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到当ens33不工作后，网络连接已经切换到ens37</span></span><br><span class="line">Currently Active Slave: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: ens37</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: 12:1d:21:fg:43:f2</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在前面文章关于hadoop大数据项目开发中，每台服务器之间其实存在大量的IO，例如NameNode服务器和DataNode服务器同步fimag文件件，DataNode之间的数据IO，为压榨服务器对于超大数据的吞吐量，在服务器的网络层使用bond模式配置Linux网卡，可以将该服务器上多个物理网卡虚拟成一张网卡，由该虚拟网卡对外提供统一网络服务，实现多个网卡自适应负载均衡以及提高数据的吞吐量，同时也实现链路双备份功能，保证服务器底层网络高可用性。bond虚拟网卡可以绑定多个网络，例如绑定2两个网卡，一般物理服务器有4个千兆网卡，所以也可以绑定4个网卡，绑定的网卡类型需要一致，否则无法进行bond模式配置。&lt;br&gt;&amp;#8195;&amp;#8195;以centos7.5 的两个网卡配成bond模式作为示例，其中在之前的hadoop的相关文章里，nn节点与dn2节点构成HA，dn节点作为DataNode，当这三个节点的底层网络都配成bond模式，将进一步提高本blog之前搭建大数据开发平台的高可用性。作为全栈开发者，这些涉及简单网络的配置，应该需要掌握。&lt;/p&gt;</summary>
    
    
    
    <category term="Hadoop" scheme="https://yield-bytes.gitee.io/blog/categories/Hadoop/"/>
    
    
    <category term="hadoop集群网络配置" scheme="https://yield-bytes.gitee.io/blog/tags/hadoop%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>Python开发常用的虚拟环境管理配置</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/11/18/Python%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E7%9A%84%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/11/18/Python%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E7%9A%84%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE/</id>
    <published>2019-11-18T11:24:21.000Z</published>
    <updated>2020-11-21T17:00:10.412Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;在某些python的开发项目中，或跑一些demo，例如tensorflow的demo，要求python3.5以上的版本，若原系统环境只有python2.7.5，显然无法满足测试环境。若为系统安装python3.5+，有些库又会造成版本冲突，因此需要使用python的虚拟环境工具来解决这些矛盾。当然也可采用python的docker镜像，使用一个镜像独立环境运行项目，但相比于python虚拟化工具来说，这种docker镜像显得有点重。</p><a id="more"></a><h4 id="1、python虚拟工具介绍"><a href="#1、python虚拟工具介绍" class="headerlink" title="1、python虚拟工具介绍"></a>1、python虚拟工具介绍</h4><p>目前有几种方式创建python的虚拟环境，在python3中有标准库venv，而第三方库例如virtualenv、virtualenvwrapper、pyenv，那么在项目或者说在实际开发里面，选哪种工具更为适合？</p><h5 id="1-1-virtualenv"><a href="#1-1-virtualenv" class="headerlink" title="1.1 virtualenv"></a>1.1 virtualenv</h5><p>virtualenv 是目前较为常用的 python 虚拟环境配置工具。它不仅同时支持 python2 和 python3，而且可以为每个虚拟环境指定 python 解释器（要求系统已经安装了不同版本的python），并选择不继承基础版本的site-packages。</p><h5 id="1-2-virtualenvwrapper"><a href="#1-2-virtualenvwrapper" class="headerlink" title="1.2 virtualenvwrapper"></a>1.2 virtualenvwrapper</h5><p>virtualenvwrapper是virtualenv的一个封装，目的是使后者更好用。virtualenv在使用中，每次得去虚拟环境所在目录下的 bin 目录下 source  activate，也即当有多个虚拟环境时，得每次都去找对应的目录，virtualenvwrapper将所有的虚拟环境目录全都集中起来统一管理，避免每次开启虚拟环境时候的source 项目目录操作。</p><h5 id="1-3-venv"><a href="#1-3-venv" class="headerlink" title="1.3 venv"></a>1.3 venv</h5><p>Python 从3.3 版本开始，自带了一个虚拟环境 <a href="https://docs.python.org/3/library/venv.html">venv</a>，在 <a href="http://legacy.python.org/dev/peps/pep-0405/">PEP-405</a> 中可以看到它的详细介绍。它的很多操作都和 virtualenv 类似。也支持linux和win。venv也有局限性，例如当前系统python版本为3.5，那么venv只能在当前安装的python3.5版本，不能创建其它Python 3.x的版本以及Python 2的环境。</p><h5 id="1-4-pyenv"><a href="#1-4-pyenv" class="headerlink" title="1.4 pyenv"></a>1.4 pyenv</h5><p><code>pyenv</code>主要用来安装、管理Python的版本及其虚拟环境，比如一个项目需要Python2.x，一个项目需要Python3.x。而virtualenv主要用来管理Python包的依赖。不同项目需要依赖的包版本不同，则需要使用虚拟环境。<code>pyenv</code>通过系统修改环境变量来实现Python不同版本的切换。前面的三个工具都是用于虚拟环境切换，pyenv是 Python 版本环境切换工具，将这两套工具结合使用，可以完美解决 python 多版本环境的问题。具体实例在第4节给出。</p><h4 id="2、使用virtualenv创建和管理虚拟环境"><a href="#2、使用virtualenv创建和管理虚拟环境" class="headerlink" title="2、使用virtualenv创建和管理虚拟环境"></a>2、使用virtualenv创建和管理虚拟环境</h4><h5 id="2-1-为多个python版本安装相应的pip"><a href="#2-1-为多个python版本安装相应的pip" class="headerlink" title="2.1 为多个python版本安装相应的pip"></a>2.1 为多个python版本安装相应的pip</h5><p>centos7.5默认没有pip包，因此需求手动安装，本文系统已经安装python2.7和python3.6。这里给出python2.7的pip安装和python3.6的pip3安装。<br>pip安装依赖setuptools，首先为python2和python3安装相应setuptools</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost local]# pwd</span><br><span class="line">/usr/local</span><br><span class="line">[root@localhost local]# wget https://files.pythonhosted.org/packages/ab/41/ab6ae1937191de0c9cbc115d0e91e335f268aa1cd85524c86e5970fdb68a/setuptools-42.0.0.zip</span><br><span class="line">[root@localhost local]# unzip setuptools-42.0.0.zip</span><br><span class="line">[root@localhost local] cd setuptools-42.0.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这里的python命令是连接到python2.7，所以setuptools库只在python2.7环境生效</span></span><br><span class="line">[root@localhost setuptools-42.0.0]# python2.7 setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给python3.6安装setuptools</span></span><br><span class="line">[root@localhost setuptools-42.0.0]# python3.6 setup.py install</span><br></pre></td></tr></table></figure><p>这里为何使用python2.7或者python3.6，因为如果想要构建更多python版本，其shell执行命令例如python3.5，python3.7则会显得清晰而不混乱。</p><p>为python2和python3安装相应pip，跟setuptools安装流程一致。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@localhost local]# pwd</span><br><span class="line">/usr/local</span><br><span class="line">[root@localhost local] wget https://files.pythonhosted.org/packages/ce/ea/9b445176a65ae4ba22dce1d93e4b5fe182f953df71a145f557cffaffc1bf/pip-19.3.1.tar.gz</span><br><span class="line">[root@localhost local]# unzip pip-19.3.1.tar.gz</span><br><span class="line">[root@localhost local] cd pip-19.3.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 为python2.7 安装pip，最终命令执行路径在：/usr/<span class="built_in">local</span>/bin/pip</span></span><br><span class="line">[root@localhost pip-19.3.1] python2.7 setup.py install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 为pytho3.6 安装pip，最终命令执行路径：/usr/<span class="built_in">local</span>/bin/pip3</span></span><br><span class="line">[root@localhost pip-19.3.1] python3.6 setup.py install</span><br></pre></td></tr></table></figure><h5 id="2-2-安装virtualenv"><a href="#2-2-安装virtualenv" class="headerlink" title="2.2  安装virtualenv"></a>2.2  安装virtualenv</h5><p>上面已经配置了python2.7环境和python3.6环境，virtualenv库无需在两种环境安装，这里安装到python2.7库下即可。</p><p>这里要注意：如果系统已经安装python3版本，且shell已经设定python命令是软链接到python3</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# ls -al /usr/bin/python</span><br><span class="line">**** /usr/bin/python -&gt; /usr/bin/python3</span><br></pre></td></tr></table></figure><p>那么需要使用pip3安装 virtualenv，这样virtualenv库才会安装到python3的site-packages目录下，</p><p>如果shell已经设定python命令是软链接到python2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# ls -al /usr/bin/python</span><br><span class="line">**** /usr/bin/python -&gt; /usr/bin/python2</span><br></pre></td></tr></table></figure><p>那么需要使用pip安装 virtualenv，这样virtualenv库才会安装到python2的site-packages目录下。</p><p>若不按照上述的环境情况，安装virtualenv，会出现以下情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# virtualenv -v</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/virtualenv&quot;, line 2, in &lt;module&gt;</span><br><span class="line">    import virtualenv</span><br><span class="line">ModuleNotFoundError: No module named &#x27;virtualenv&#x27;</span><br></pre></td></tr></table></figure><p>当前python是软链到python3，而pip安装的virtualenv是在python2.7路径下，python3并没有virtualenv这个库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# python3</span><br><span class="line">Python 3.6.8 (default, Apr 25 2019, 21:02:35) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import virtualenv</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">ModuleNotFoundError: No module named &#x27;virtualenv</span><br></pre></td></tr></table></figure><p>virtualenv在python2.7路径下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# python</span><br><span class="line">Python 2.7.5 (default, Aug  7 2019, 00:51:29) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import virtualenv</span></span><br></pre></td></tr></table></figure><p>解决办法：将当前python命令软链到python2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]# ln -s /usr/bin/python2 /usr/bin/python</span><br></pre></td></tr></table></figure><h5 id="2-3、virtualenv创建虚拟环境"><a href="#2-3、virtualenv创建虚拟环境" class="headerlink" title="2.3、virtualenv创建虚拟环境"></a>2.3、virtualenv创建虚拟环境</h5><p>在/opt/pvenv_test目录下，创建一个使用python2.7解释器、且独立安装第三方库的运行环境、名字为crmapp的python2.7项目环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost pvenv_test]# virtualenv  --python=python2.7    --no-site-packages  crmapp</span><br><span class="line">Running virtualenv with interpreter /usr/bin/python2.7</span><br><span class="line">Already using interpreter /usr/bin/python2.7</span><br><span class="line">  No LICENSE.txt / LICENSE found in source</span><br><span class="line">New python executable in /opt/pvenv_test/crmapp/bin/python2.7</span><br><span class="line">Also creating executable in /opt/pvenv_test/crmapp/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...</span><br><span class="line"></span><br><span class="line">[root@localhost pvenv_test]# ls</span><br><span class="line">crmapp</span><br><span class="line">[root@localhost crmapp]# ls</span><br><span class="line">bin  include  lib  lib64</span><br><span class="line"><span class="meta">#</span><span class="bash"> 相关执行命令在bin目录下</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 激活当前环境</span></span><br><span class="line">[root@localhost crmapp]# source bin/activate</span><br><span class="line">(crmapp) [root@localhost crmapp]# </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当前虚拟环境pip包没有引入系统的pip包，从而实现包不冲突</span></span><br><span class="line">(crmapp) [root@localhost crmapp]# pip list</span><br><span class="line">pip (9.0.1)</span><br><span class="line">setuptools (28.8.0)</span><br><span class="line">wheel (0.29.0)</span><br></pre></td></tr></table></figure><p>在/opt/pvenv_test目录下，创建一个使用python3.6解释器、且独立安装第三方库的运行环境、名字为djwebsocket的python3.6项目环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost pvenv_test]# virtualenv  --python=python3.6    --no-site-packages  djwebsocket</span><br><span class="line">Running virtualenv with interpreter /usr/bin/python3.6</span><br><span class="line">Already using interpreter /usr/bin/python3.6</span><br><span class="line">Using base prefix &#x27;/usr&#x27;</span><br><span class="line">  No LICENSE.txt / LICENSE found in source</span><br><span class="line">New python executable in /opt/pvenv_test/djwebsocket/bin/python3.6</span><br><span class="line">Also creating executable in /opt/pvenv_test/djwebsocket/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...</span><br><span class="line">done.</span><br><span class="line"></span><br><span class="line">[root@localhost pvenv_test]# source djwebsocket/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到当前pip包为干净的</span></span><br><span class="line">(djwebsocket) [root@localhost pvenv_test]# pip list</span><br><span class="line">Package    Version</span><br><span class="line">---------- -------</span><br><span class="line">pip        19.3.1 </span><br><span class="line">setuptools 42.0.0 </span><br><span class="line">wheel      0.33.6 </span><br></pre></td></tr></table></figure><p>这里的 –python=python3  其实就是 /usr/bin/python3对于的python3.6解释器</p><h4 id="3、pyenv终极python版本和虚拟化环境管理工具"><a href="#3、pyenv终极python版本和虚拟化环境管理工具" class="headerlink" title="3、pyenv终极python版本和虚拟化环境管理工具"></a>3、pyenv终极python版本和虚拟化环境管理工具</h4><p>有关virtualenvwrapper或者venv的用法，因内容较为简单，这里不再讨论，本文推荐使用pyenv管理任意基于python项目的虚拟环境。</p><h5 id="3-1-pyenv的设计原理"><a href="#3-1-pyenv的设计原理" class="headerlink" title="3.1 pyenv的设计原理"></a>3.1 pyenv的设计原理</h5><p><code>pyenv</code>主要用来管理Python的版本，比如一个项目需要Python2.x，一个项目需要Python3.x。而virtualenv主要用来管理Python包的依赖。不同项目需要依赖的包版本不同，则需要使用虚拟环境。</p><p><code>pyenv</code>通过系统修改环境变量来实现Python不同版本的切换。而vitualenv通过Python包安装到一个目录来作为Python虚拟包环境，通过切换目录来实现不同包环境间的切换。</p><p><code>pyenv</code>的设计巧妙的地方在于，在PATH 的最前面插入了一个垫片路径（shims）：~/.pyenv/shims:/usr/local/bin:/usr/bin:/bin。所有对 Python 可执行文件的查找都会首先被这个 shims 路径截获，从而使后方的系统路径失效。</p><p>对于系统环境变量 PATH ，里面包含了一串由冒号分隔的路径，例如 /usr/local/bin:/usr/bin:/bin。每当在系统中执行一个命令时，例如 python 或 pip，操作系统就会在 PATH 的所有路径中从左至右依次寻找对应的命令。因为是依次寻找，因此排在左边的路径具有更高的优先级。在PATH 最前面插入一个 <code>$(pyenv root)/shims </code>目录，<code>$(pyenv root)/shims</code>目录里包含名称为python以及pip等可执行脚本文件；当用户执行python或pip命令时，根据查找优先级，系统会优先执行shims目录中的同名脚本。pyenv 正是通过这些脚本，来灵活地切换至我们所需的Python版本。</p><p>需要手工去查找python版本的所在路径，如果有多个版本，这种手工管理显得有点繁琐。</p><h5 id="3-2-安装pyenv"><a href="#3-2-安装pyenv" class="headerlink" title="3.2 安装pyenv"></a>3.2 安装pyenv</h5><p>pyenv安装python需要依赖底层的系统库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin] yum install -y gcc make patch gdbm-devel openssl-devel sqlite-devel readline-devel zlib-devel bzip2-devel ncurses-devel libffi-devel</span><br></pre></td></tr></table></figure><p>若系统库不全，pyenv安装python后，会有如下提示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# pyenv install 3.7.5</span><br><span class="line">Installing Python-3.7.5...</span><br><span class="line">WARNING: The Python bz2 extension was not compiled. Missing the bzip2 lib?</span><br><span class="line">WARNING: The Python readline extension was not compiled. Missing the GNU readline lib?</span><br><span class="line">WARNING: The Python sqlite3 extension was not compiled. Missing the SQLite3 lib?</span><br><span class="line">Installed Python-3.7.5 to /root/.pyenv/versions/3.7.5</span><br></pre></td></tr></table></figure><p>有三个warning提示，系统环境缺少 bzip2、readline、SQLite3。</p><p>下载pyenv</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget git <span class="built_in">clone</span> https://github.com/pyenv/pyenv.git ~/.pyenv</span><br></pre></td></tr></table></figure><p>将<code>PYENV_ROOT</code>和<code>pyenv init</code>加入bash的<code>~/.bashrc</code>（或zsh的<code>~/.zshrc</code>）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;export PATH=~/.pyenv/bin:$PATH&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &#x27;export PYENV_ROOT=~/.pyenv&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">echo &#x27;eval &quot;$(pyenv init -)&quot;&#x27; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><p>激活<code>pyenv</code>（zsh为<code>~/.zshrc</code>）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><p>指定python版本在线安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# pyenv install 3.7.5</span><br></pre></td></tr></table></figure><p>常用命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pyenv install --list # 列出可安装版本</span><br><span class="line">pyenv install &lt;version&gt; # 安装对应版本</span><br><span class="line">pyenv uninstall &lt;version&gt; # 卸载对应版本的python</span><br><span class="line">pyenv uninstall &lt;venv name&gt; # 删除指定虚拟环境</span><br><span class="line">pyenv install -v &lt;version&gt; # 安装对应版本，若发生错误，可以显示详细的错误信息</span><br><span class="line">pyenv versions # 显示当前使用的python版本</span><br><span class="line">pyenv which python # 显示当前python安装路径</span><br><span class="line">pyenv global &lt;version&gt; # 设置默认Python版本</span><br></pre></td></tr></table></figure><h5 id="3-3pyenv离线安装python各版本"><a href="#3-3pyenv离线安装python各版本" class="headerlink" title="3.3pyenv离线安装python各版本"></a>3.3pyenv离线安装python各版本</h5><p>pyenv自动去官网拉取python安装包，如果要为离线服务器安装，则只需在<code>.pyenv</code>创建cache目录，将指定版本的python安装包放在该目录。</p><p>python安装包下载地址：<a href="https://www.python.org/ftp/python/">https://www.python.org/ftp/python/</a></p><p>只需下载Python-***.tar.xz 即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost cache]# pwd</span><br><span class="line">/root/.pyenv/cache</span><br><span class="line">[root@localhost cache]# ls</span><br><span class="line">Python-3.7.5.tar.xz </span><br><span class="line"></span><br><span class="line">[root@localhost .pyenv]# pyenv install 3.7.5</span><br><span class="line">Installing Python-3.7.5...</span><br><span class="line">Installed Python-3.7.5 to /root/.pyenv/versions/3.7.5</span><br></pre></td></tr></table></figure><h5 id="3-4-使用多个python版本"><a href="#3-4-使用多个python版本" class="headerlink" title="3.4 使用多个python版本"></a>3.4 使用多个python版本</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">pyenv设为系统原有python版本：</span><br><span class="line">[root@localhost opt]# pyenv global system </span><br><span class="line">[root@localhost opt]# python -V</span><br><span class="line">Python 2.7.5</span><br><span class="line"></span><br><span class="line">pyenv更换系统python3.7.5版本后：</span><br><span class="line">[root@localhost opt]# pyenv global 3.7.5</span><br><span class="line">[root@localhost opt]# python -V</span><br><span class="line">Python 3.7.5</span><br><span class="line">[root@localhost opt]# pyenv versions</span><br><span class="line">  system</span><br><span class="line">* 3.7.5 (set by /root/.pyenv/version)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python3.7.5所在目录，所有的安装python都会集中放置在.pyenv/versions目录下</span></span><br><span class="line">[root@localhost bin]# pwd</span><br><span class="line">/root/.pyenv/versions/3.7.5/bin</span><br><span class="line">[root@localhost bin]# ls</span><br><span class="line">2to3              idle     pip3    pydoc3.7   python3.7-config   python3-config</span><br><span class="line">2to3-3.7          idle3    pip3.7  python     python3.7-gdb.py   python-config</span><br><span class="line">easy_install      idle3.7  pydoc   python3    python3.7m         pyvenv</span><br><span class="line">easy_install-3.7  pip      pydoc3  python3.7  python3.7m-config  pyvenv-3.7</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>pyenv在python版本方面得心应手。</p><h5 id="3-5-pyenv结合-pyenv-virtualenv实现灵活的虚拟环境管理"><a href="#3-5-pyenv结合-pyenv-virtualenv实现灵活的虚拟环境管理" class="headerlink" title="3.5 pyenv结合/pyenv-virtualenv实现灵活的虚拟环境管理"></a>3.5 pyenv结合/pyenv-virtualenv实现灵活的虚拟环境管理</h5><p>在第3.2章节介绍了virtualenv用于管理pip包的虚拟环境，virtualenv有个不足地方是：系统需已安装多个python版本，一般会通过手工安装，也即编译时指定不同路径，避免冲突。pyenv作者同时也开发pyenv-virtualenv的工具，跟virtualenv功能一致，也是用于管理pip包以及虚拟环境，结合pyenv，可以实现任意python版本以及pip包环境的切换以及使用，高效提高个人开发效率。</p><p>pyenv-virtualenv放在.pyenv/plugins 这个插件目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost plugins]# pwd</span><br><span class="line">/root/.pyenv/plugins</span><br><span class="line">[root@localhost plugins]# git clone https://github.com/yyuu/pyenv-virtualenv.git</span><br><span class="line">[root@localhost plugins]# ls</span><br><span class="line">pyenv-virtualenv  python-build</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新系统环境变量</span></span><br><span class="line">[root@localhost plugins]# exec &quot;$SHELL&quot;</span><br></pre></td></tr></table></figure><p>创建python3.7.5版本、虚拟环境名为tf375</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv virtualenv 3.7.5 tf375</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有使用python3.7.5版本创建的虚拟环境都在此目录：/root/.pyenv/versions/3.7.5/envs</span></span><br><span class="line">[root@localhost envs]<span class="comment"># ls</span></span><br><span class="line">pyspark375  tf375</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf375虚拟环境的目录在这里：</span></span><br><span class="line">[root@localhost opt]<span class="comment"># ls ~/.pyenv/versions/3.7.5/envs/tf375/</span></span><br><span class="line"><span class="built_in">bin</span>/        include/    lib/        lib64/      pyvenv.cfg  </span><br><span class="line"></span><br><span class="line"><span class="comment">#pyenv uninstall tf375 # 删除虚拟环境,同时也会删除其目录</span></span><br></pre></td></tr></table></figure><p>激活tf375环境，pyenv支持对环境名的自动补全，非常方便，pip包默认不继承系统包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv activate tf375 </span></span><br><span class="line"></span><br><span class="line">(tf375) [root@localhost envs]<span class="comment"># pip list</span></span><br><span class="line">Package    Version</span><br><span class="line">---------- -------</span><br><span class="line">pip        <span class="number">19.2</span><span class="number">.3</span> </span><br><span class="line">setuptools <span class="number">41.2</span><span class="number">.0</span> </span><br></pre></td></tr></table></figure><p>创建python3.6.5版本、虚拟环境名为spk的pyspark开发环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost opt]<span class="comment"># pyenv virtualenv 3.6.5 pyspark_proj</span></span><br><span class="line">[root@localhost opt]<span class="comment"># pyenv activate pyspark_proj</span></span><br><span class="line">(pyspark_proj) [root@localhost opt]<span class="comment"># pip list</span></span><br><span class="line">pip (<span class="number">9.0</span><span class="number">.3</span>)</span><br><span class="line">setuptools (<span class="number">39.0</span><span class="number">.1</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;在某些python的开发项目中，或跑一些demo，例如tensorflow的demo，要求python3.5以上的版本，若原系统环境只有python2.7.5，显然无法满足测试环境。若为系统安装python3.5+，有些库又会造成版本冲突，因此需要使用python的虚拟环境工具来解决这些矛盾。当然也可采用python的docker镜像，使用一个镜像独立环境运行项目，但相比于python虚拟化工具来说，这种docker镜像显得有点重。&lt;/p&gt;</summary>
    
    
    
    <category term="Python进阶" scheme="https://yield-bytes.gitee.io/blog/categories/Python%E8%BF%9B%E9%98%B6/"/>
    
    
    <category term="python虚拟环境" scheme="https://yield-bytes.gitee.io/blog/tags/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>Pandas数据预处理的常用函数</title>
    <link href="https://yield-bytes.gitee.io/blog/2019/11/17/Pandas%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
    <id>https://yield-bytes.gitee.io/blog/2019/11/17/Pandas%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</id>
    <published>2019-11-17T15:30:19.000Z</published>
    <updated>2020-11-21T17:01:08.749Z</updated>
    
    <content type="html"><![CDATA[<p>&#8195;&#8195;以往项目中也有引入Pandas，用于有关数据处理和分析的环节，结合Python的Web开发，很容易开发出一款轻量BI系统。Pandas、Numpy、Scipy、matplotlib、scikit-learn和Jupyter Notebook结合使用，完全可以组成非常出色的数据分析与挖掘的生产环境工具，数据方面的应用，比matlab强不少，以至于本人也不断强化这方面的积累。单独拿出这方面技能，即可完成数据分析师的相关工作（又称提数工程师）。本文主要归档一些高频使用的预处理方面的函数，注意本文不涉及Pandas数据挖掘和数理统计方面的知识点（会在另外blog给出）。</p><a id="more"></a><h3 id="1、读取数据文件"><a href="#1、读取数据文件" class="headerlink" title="1、读取数据文件"></a>1、读取数据文件</h3><p>&#8195;&#8195;读取数据的相关接口使用在pandas官网的document有非常详细的说明:在<a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html">IO tools部分</a>。pandas 不仅能读取基本常用的Excel、csv、文本，还可以读取hadoop文件，或者直接读取数据库等</p><h4 id="1-1-读取excel数据文件"><a href="#1-1-读取excel数据文件" class="headerlink" title="1.1  读取excel数据文件"></a>1.1  读取excel数据文件</h4><ul><li><p>加载Excel表，使用skiprows=1跳过首行<br>并指定加载的列，注意数据文件的编码，默认utf-8，常用还有gb2312，根据自身数据而定。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">raw_pd = pd.read_excel(data_file,,skiprows=<span class="number">1</span>,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br><span class="line"><span class="number">181</span> ms ± <span class="number">1.32</span> ms per loop (mean ± std. dev. of <span class="number">7</span> runs, <span class="number">1</span> loop each)</span><br></pre></td></tr></table></figure><p>  这里可以为每个执行单元之前加入<code>%%timeit</code>，观察其耗时情况。</p></li><li><p>加载Excel表，使用header=0跳过有列标题的首行<br>除了使用skiprows=1可跳过首行，header=0也可以实现同样效果</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd = pd.read_excel(data_file,header=<span class="number">0</span>,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>加载Excel表，首行为数据，不是列标题<br>若该表第一行不是列标题行而是数据行，则需要指定header=None，否则读取后，第一行数据会被作为column name</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>加载Excel表，读取前n行数据<br>若数据文件大小为几个G，显然若直接全量读取，内存会挤爆，因此可以先读取前n看看。使用nrows=500，表示只读取前500行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>, nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>加载Excel表，跳过所有空白行<br>若有些表出现了不定数量的空白行，可以使用skip_blank_lines=True处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd=pd.read_excel(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,skip_blank_lines = <span class="literal">True</span>, nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>加载Excel表，通过自定规则，跳过满足规则的行<br>例如跳过有值为单数的行，定义更复杂的函数，用于跳过满足复杂规则的行。不过，除非这些行很多，否则可以在读取后，直接用正则drop掉来得更有效。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv(data_file, skiprows=<span class="keyword">lambda</span> x: x % <span class="number">2</span> != <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li></ul><h4 id="1-2-读取csv文件"><a href="#1-2-读取csv文件" class="headerlink" title="1.2 读取csv文件"></a>1.2 读取csv文件</h4><p>&#8195;&#8195;读取csv文件跟读取Excel文件区别不大，这里简单给出示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd=pd.read_csv(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>,nrows=<span class="number">500</span>,encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure><p>读取文件，需要注意的地方一般是选择编码，数据文件的编码决定读取数据后，是否正常显示。</p><h4 id="1-3-读取数据时，跳过尾行"><a href="#1-3-读取数据时，跳过尾行" class="headerlink" title="1.3 读取数据时，跳过尾行"></a>1.3 读取数据时，跳过尾行</h4><p>有些报表一般会在表（例如财务系统导出）的后几行写上制表人、制表日期<br>这里要注意，若使用c engine，则无法使用从尾部跳过数据的功能：</p><blockquote><p>skipfooter : int, default <code>0</code></p><p>Number of lines at bottom of file to skip (unsupported with engine=’c’).</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd=pd.read_csv(data_file,usecols=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],name=[<span class="string">&#x27;item_id&#x27;</span>,<span class="string">&#x27;item_name&#x27;</span>,<span class="string">&#x27;price&#x27;</span>],header=<span class="literal">None</span>, skipfooter=<span class="number">5</span>,encoding=<span class="string">&#x27;gb2312&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="1-4-读取特定分割符的数据文件"><a href="#1-4-读取特定分割符的数据文件" class="headerlink" title="1.4 读取特定分割符的数据文件"></a>1.4 读取特定分割符的数据文件</h4><p>read_csv也可以读取任意文本文件，只需要指定列分割符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd=pd.read_csv(<span class="string">&#x27;data_file.txt&#x27;</span>,sep=<span class="string">&#x27;||&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="1-5-使用c或者python作为读取文件的引擎"><a href="#1-5-使用c或者python作为读取文件的引擎" class="headerlink" title="1.5 使用c或者python作为读取文件的引擎"></a>1.5 使用c或者python作为读取文件的引擎</h4><p>pd.read_*** 方法默认使用python解释器作为读取文件engine，若数据文件大，可选择c engine</p><blockquote><p>engine : {<code>&#39;c&#39;</code>, <code>&#39;python&#39;</code>}</p><p>Parser engine to use. The C engine is faster while the Python engine is currently more feature-complete.</p></blockquote><h4 id="1-6-使用迭代器读取超大文件"><a href="#1-6-使用迭代器读取超大文件" class="headerlink" title="1.6 使用迭代器读取超大文件"></a>1.6 使用迭代器读取超大文件</h4><p>参考官网文档给出的示例，使用<code>iterator=True</code>， 或者chunksize=4读取超大文件，返回的是TextFileReader，是一个文件迭代器</p><p>chunksize方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">187</span>]: reader = pd.read_csv(<span class="string">&#x27;tmp.sv&#x27;</span>, sep=<span class="string">&#x27;|&#x27;</span>, chunksize=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">188</span>]: reader</span><br><span class="line">Out[<span class="number">188</span>]: &lt;pandas.io.parsers.TextFileReader at <span class="number">0x7f2b428c17f0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">189</span>]: <span class="keyword">for</span> chunk <span class="keyword">in</span> reader:</span><br><span class="line">   .....:     print(chunk)</span><br></pre></td></tr></table></figure><p>iterator=True方式：<br>使用iterator=True方式，值读取前面5行，放回的也是df对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">190</span>]: reader = pd.read_csv(<span class="string">&#x27;tmp.sv&#x27;</span>, sep=<span class="string">&#x27;|&#x27;</span>, iterator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">191</span>]: chunk_pd=reader.get_chunk(<span class="number">5</span>)</span><br><span class="line">chunk_pd.head()</span><br></pre></td></tr></table></figure><p>当然最佳的方式是两者结合使用：返回迭代器方式，并指定分块读取，例如分64k读取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iter_df=pd.read_csv(large_file,iterator=<span class="literal">True</span>，chunksize=<span class="number">64</span>*<span class="number">1024</span>)</span><br></pre></td></tr></table></figure><h3 id="2、查看数据的基本信息"><a href="#2、查看数据的基本信息" class="headerlink" title="2、查看数据的基本信息"></a>2、查看数据的基本信息</h3><p>读入数据后，一般需要对数据进行基本的观察</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">raw_pd.head(<span class="number">5</span>) <span class="comment"># 查看数据基本信息（前5行）</span></span><br><span class="line">raw_pd.tail(<span class="number">5</span>) <span class="comment"># 查看末尾5行</span></span><br><span class="line">raw_pd.sample(<span class="number">5</span>) <span class="comment"># 随机抽取5行查看</span></span><br><span class="line">raw_pd.dtypes <span class="comment"># 查看每列数据类型</span></span><br><span class="line">raw_pd.columns    <span class="comment">#查看列名</span></span><br><span class="line">raw_pd.info()     <span class="comment">#查看各字段的信息</span></span><br><span class="line">raw_pd.shape      <span class="comment">#查看数据集行列分布，几行几列</span></span><br><span class="line">raw_pd.describe() <span class="comment"># 快速查看数据的基本统计信息</span></span><br></pre></td></tr></table></figure><h3 id="3、有关空值处理"><a href="#3、有关空值处理" class="headerlink" title="3、有关空值处理"></a>3、有关空值处理</h3><p>空值：在pandas中的空值是””<br>缺失值：在dataframe中为NaN或者NaT（缺失时间），在series中为none或者nan</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试数据</span></span><br><span class="line">raw_pd = pd.DataFrame(&#123;<span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;aoo&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;college&quot;</span>: [np.nan, <span class="string">&#x27;SACT&#x27;</span>, <span class="string">&#x27;AACT&#x27;</span>],</span><br><span class="line">                  <span class="string">&quot;birth_date&quot;</span>: [pd.NaT, pd.Timestamp(<span class="string">&quot;2000-10-01&quot;</span>),pd.NaT]&#125;)</span><br></pre></td></tr></table></figure><h4 id="3-1-行的空值处理"><a href="#3-1-行的空值处理" class="headerlink" title="3.1 行的空值处理"></a>3.1 行的空值处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">axis：0或者&#x27;index&#x27;代表行操作（默认）  1或者&#x27;column&#x27;：列操作</span></span><br><span class="line"><span class="string">how：any-只要有空值就删除（默认），all-全部为空值才删除</span></span><br><span class="line"><span class="string">inplace：False-返回新的数据集（默认），True-在愿数据集上操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用频率高：查看name列中，有多少行为空值行,value_counts其实是一个统计方法</span></span><br><span class="line">raw_pd[<span class="string">&#x27;name&#x27;</span>].isnull().value_counts()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用频率高：any表示行的任意一列有空值，则删除该行；all表示该行全部为空，则删除</span></span><br><span class="line">raw_pd.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 行的任意一列有空值,且出现2个空值才删除这些行。例如该行有3列，其中2列都是为空，那么可以删掉该行。</span></span><br><span class="line">使用频率低：raw_pd.dropna(axis=<span class="number">0</span>, how=<span class="string">&#x27;any&#x27;</span>,thresh=<span class="number">2</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-2-列的空值处理"><a href="#3-2-列的空值处理" class="headerlink" title="3.2 列的空值处理"></a>3.2 列的空值处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用频率高：指定某几列，若这些列中出现了空值，则直接删除所在行</span></span><br><span class="line">raw_pd.dropna(subset=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;birth_date&#x27;</span>],inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="3-3-空值的填充"><a href="#3-3-空值的填充" class="headerlink" title="3.3 空值的填充"></a>3.3 空值的填充</h4><p>最简单的用法，对全部数据记录里面的空值填充指定值</p><p>df.fillna(value=’bar’)</p><p>频繁使用：对指定列的空值进行填充</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd[<span class="string">&#x27;name&#x27;</span>]=raw_pd[<span class="string">&#x27;name&#x27;</span>].fillna(value=<span class="string">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>高级填充方式</strong><br>使用与空值单元相邻近的值来填充。该用法一般用在大量数据统计分析的场景或者图像的像素值填充、实验室的实验数据。相邻是指可以使用上下左右四个方向的值实现前向或者后向填充</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">DataFrame.fillna(value=<span class="literal">None</span>, method=<span class="literal">None</span>, axis=<span class="literal">None</span>, inplace=<span class="literal">False</span>, limit=<span class="literal">None</span>, downcast=<span class="literal">None</span>, **kwargs)</span><br><span class="line"></span><br><span class="line">method : &#123;‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, <span class="literal">None</span>&#125;, default <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">Method to use <span class="keyword">for</span> filling holes <span class="keyword">in</span> reindexed Series pad / ffill: </span><br><span class="line">propagate last valid observation forward to <span class="built_in">next</span> valid backfill / bfill: use NEXT valid observation to fill gap</span><br><span class="line"></span><br><span class="line">axis : &#123;<span class="number">0</span> <span class="keyword">or</span> ‘index’, <span class="number">1</span> <span class="keyword">or</span> ‘columns’&#125;</span><br><span class="line">limit:限制填充的个数</span><br></pre></td></tr></table></figure><p>这里使用比较频繁的是纵向填充，因为纵向代表的是列，从相邻样本的同一特征中填值，对每列的空值实施前项或者后项填充。</p><p><code>df.fillna(method=&#39;ffill&#39;)</code> or df.fillna(method=’bfill’)</p><h4 id="3-4-空值使用所在列或者所在行的均值、中位数来填补"><a href="#3-4-空值使用所在列或者所在行的均值、中位数来填补" class="headerlink" title="3.4  空值使用所在列或者所在行的均值、中位数来填补"></a>3.4  空值使用所在列或者所在行的均值、中位数来填补</h4><p>这里以均值填充为例，当然也可以用该列的预测值填充</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean_value = df[<span class="string">&#x27;age&#x27;</span>].mean()</span><br><span class="line">df[<span class="string">&#x27;age&#x27;</span>].fillna(mean_value, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="4、dataframe-取（定位）数据的操作"><a href="#4、dataframe-取（定位）数据的操作" class="headerlink" title="4、dataframe 取（定位）数据的操作"></a>4、dataframe 取（定位）数据的操作</h3><h4 id="4-1-按给定列名取数，类似字典操作：df-‘列名’"><a href="#4-1-按给定列名取数，类似字典操作：df-‘列名’" class="headerlink" title="4.1 按给定列名取数，类似字典操作：df[‘列名’]"></a>4.1 按给定列名取数，类似字典操作：df[‘列名’]</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd= raw_pd[<span class="string">&#x27;name&#x27;</span>]</span><br></pre></td></tr></table></figure><p>取出多列数据，入参为包含多个字段的list：[‘name’,’college’]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_pd[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;college&#x27;</span>]]</span><br></pre></td></tr></table></figure><h4 id="4-2-按行默认的行索引号选取数据：df-loc"><a href="#4-2-按行默认的行索引号选取数据：df-loc" class="headerlink" title="4.2  按行默认的行索引号选取数据：df.loc"></a>4.2  按行默认的行索引号选取数据：df.loc</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">&quot;name&quot;</span>: [<span class="string">&#x27;aoo&#x27;</span>, <span class="string">&#x27;boo&#x27;</span>, <span class="string">&#x27;coo&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;college&quot;</span>: [np.nan, <span class="string">&#x27;SACT&#x27;</span>, <span class="string">&#x27;AACT&#x27;</span>],</span><br><span class="line">                  <span class="string">&quot;birth_date&quot;</span>: [pd.NaT, pd.Timestamp(<span class="string">&quot;2000-10-01&quot;</span>),pd.NaT]&#125;)</span><br><span class="line"><span class="comment"># 查看该df的行索引</span></span><br><span class="line">df.index</span><br><span class="line">RangeIndex(start=<span class="number">0</span>, stop=<span class="number">3</span>, step=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印                  </span></span><br><span class="line">birth_date college name</span><br><span class="line"><span class="number">0</span> NaT NaN aoo</span><br><span class="line"><span class="number">1</span> <span class="number">2000</span>-<span class="number">10</span>-01 SACT boo</span><br><span class="line"><span class="number">2</span> NaT AACT coo </span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的0,1,2就是pandas默认给加载的数据提供的行索引号</span></span><br></pre></td></tr></table></figure><p>按索引取数据，跟列表使用slice切片获取数据的用法一致<br>df.loc[1] 获取行索引1的行数据，df.loc[1:2]获取1到2行数据</p><p>若行索引号不是int，例如将以上数据的默认index序列，改成字符索引序列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.index=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line"> birth_date college name</span><br><span class="line">a NaT NaN aoo</span><br><span class="line">b <span class="number">2000</span>-<span class="number">10</span>-01 SACT boo</span><br><span class="line">c NaT AACT coo</span><br></pre></td></tr></table></figure><p>获取索引为b的数据：df.loc[‘b’]<br>或者索引为b、c的数据：df.loc[[‘b’,’c’]]</p><h4 id="4-3-按给定列名以及行索引取出数据"><a href="#4-3-按给定列名以及行索引取出数据" class="headerlink" title="4.3 按给定列名以及行索引取出数据"></a>4.3 按给定列名以及行索引取出数据</h4><p>例如取出college列的b、c行数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[[<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>],<span class="string">&#x27;college&#x27;</span>]</span><br></pre></td></tr></table></figure><p>例如取出college列、birth_date列的b、c行数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.loc[[<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>],[<span class="string">&#x27;college&#x27;</span>,<span class="string">&#x27;birth_date&#x27;</span>]]</span><br><span class="line"><span class="comment"># 打印</span></span><br><span class="line">college birth_date</span><br><span class="line">b SACT <span class="number">2000</span>-<span class="number">10</span>-01</span><br><span class="line">c AACT NaT</span><br></pre></td></tr></table></figure><h4 id="4-4-df-iloc利用index获取行数据或者列数据"><a href="#4-4-df-iloc利用index获取行数据或者列数据" class="headerlink" title="4.4  df.iloc利用index获取行数据或者列数据"></a>4.4  df.iloc利用index获取行数据或者列数据</h4><p>df.iloc只能使用整型切片获取数据:例如df.iloc[0:10]<br>而df.loc可以使用字符型索引等来取数</p><h3 id="5、通过复杂规则取数"><a href="#5、通过复杂规则取数" class="headerlink" title="5、通过复杂规则取数"></a>5、通过复杂规则取数</h3><p>在sql中经常会在where子句使用筛选条件：<br>select * from emp e  where e.age&gt;20 and e.dep_name=’dev’ and e.city&lt;&gt;’foo’<br>在pandas里面则使用方式如下：<br>单个筛选条件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">20</span>]</span><br><span class="line">或者</span><br><span class="line">df.loc[df[<span class="string">&#x27;age&#x27;</span>]&gt;<span class="number">20</span>]</span><br></pre></td></tr></table></figure><p>多个筛选条件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[(df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">20</span>)&amp;(df[<span class="string">&#x27;dep_name&#x27;</span>]==<span class="string">&#x27;dev&#x27;</span>)&amp;~(df[<span class="string">&#x27;city&#x27;</span>]==<span class="string">&#x27;foo&#x27;</span>)]</span><br></pre></td></tr></table></figure><p>使用isin方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;city&#x27;</span>].isin([<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>])]</span><br></pre></td></tr></table></figure><p>根据时间范围取值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Excel加载的日期，如果格式不对，有可能是object类型，需将其转为datetime64[ns]类型，否则无法进行日期筛选比较</span></span><br><span class="line">df[<span class="string">&#x27;date_col&#x27;</span>]= df.to_datetime(df[<span class="string">&#x27;date_col&#x27;</span>])</span><br><span class="line">start_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment">#或者pd.Timestamp(&#x27;2017-02-01&#x27;)</span></span><br><span class="line">end_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">14</span>) <span class="comment">#或者pd.Timestamp(&#x27;2017-02-14&#x27;)</span></span><br><span class="line"><span class="comment"># 注意以上的实际范围其实是 2017-02-01 00:00:00 ~2017-02-14 00:00:00</span></span><br><span class="line"><span class="comment"># 或者截止到当天最后一秒</span></span><br><span class="line">end_time=datetime.datetime(<span class="number">2017</span>,<span class="number">2</span>,<span class="number">14</span>,<span class="number">23</span>,<span class="number">59</span>,<span class="number">59</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找指定时间范围内的数据行</span></span><br><span class="line">filter_df=df[(df[<span class="string">&#x27;start_time&#x27;</span>]&gt;=start_time) &amp; (df[<span class="string">&#x27;end_time&#x27;</span>]&lt;=end_time)]</span><br></pre></td></tr></table></figure><p>还有另外这一种方式是选择一个时间列，变将其设为当前df的时间索引，</p><p>常用：根据某个字段，取出获取删除其值出现频率排在前n的数据行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  对name字段进行group_by</span></span><br><span class="line">groupby_df=df.groupby(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line"><span class="comment"># groupby_df:&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000010190C18&gt;</span></span><br><span class="line"></span><br><span class="line">resl_df=groupby_df[<span class="string">&#x27;name&#x27;</span>].count()</span><br><span class="line"><span class="comment"># resl_df 就像透视表的数据形式</span></span><br><span class="line">name</span><br><span class="line">foo    <span class="number">10</span></span><br><span class="line">bar    <span class="number">5</span></span><br><span class="line">coo    <span class="number">4</span></span><br><span class="line">dee    <span class="number">2</span></span><br><span class="line">Name: bar, dtype: int64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找name字段里，字符出现频率排在前2位，例如上述的例子：foo，bar。按降序返回一个python的列表</span></span><br><span class="line">top_2_list=resl_df.sort_values(ascending=<span class="literal">False</span>).head(<span class="number">2</span>)</span><br><span class="line">print(top_2_list)</span><br><span class="line">[<span class="string">&#x27;foo&#x27;</span>,<span class="string">&#x27;bar&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将出现频率排在前2的内容拼接成用于正则匹配的字符串</span></span><br><span class="line">pattern_str=<span class="string">&#x27;|&#x27;</span>.join(top_2_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pandas提供的正则方法，剔除name字段中出现频率排在前2的数据行</span></span><br><span class="line">filtered_df= df[~df[<span class="string">&#x27;name&#x27;</span>].<span class="built_in">str</span>.contains(pattern_str, case=<span class="literal">False</span>, na=<span class="literal">False</span>,regex=<span class="literal">True</span>)]</span><br></pre></td></tr></table></figure><p>==使用时间索引选取数据行==<br>个人认为，这种方式是时间选取数据场景最高效的手段<br>例如有数据df，其中create_date是该df唯一的日期字段，通常做法:<br>新增一列命名为back_up_date，用于备份create_date<br><code>df[&#39;back_up_date&#39;]=df[&#39;create_date&#39;]</code><br>将crete_date置为该df的时间索引<br><code>df=df.set_index(&#39;create_date&#39;)</code><br>当时间索引设置后，那么根据时间筛选数据将变得异常简单<br>取2000年的数据行<br><code>df[&#39;2000&#39;]</code><br>取2000年到2019年的数据行<br><code>df[&#39;2000&#39;:&#39;2019&#39;]</code><br>某天具体时间到某天具体时间的数据行<br><code>df[&#39;2015-03-15 11:11:10&#39;:&#39;2015-05-15 10:30:00&#39;]</code><br>有关pandas时间的专题，官方文档给出了非常详细的用法示例，这里不再赘述，<a href="https://pandas.pydata.org/pandas-docs/version/0.25/user_guide/timeseries.html">timeseries链接</a></p><h3 id="6、调整列位置、列的增、删"><a href="#6、调整列位置、列的增、删" class="headerlink" title="6、调整列位置、列的增、删"></a>6、调整列位置、列的增、删</h3><p>交换birth_date和college列的位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[[<span class="string">&#x27;birth_date&#x27;</span>,<span class="string">&#x27;college&#x27;</span>]]=df[[<span class="string">&#x27;college&#x27;</span>,<span class="string">&#x27;birth_date&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>删除指定列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop(columns=[<span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>])</span><br></pre></td></tr></table></figure><p>删除指定行，使用行索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop([<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>删除重复行:df.drop_duplicates</p><p>直接删除重复行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop_duplicates()</span><br></pre></td></tr></table></figure><p>删除name列、age列存在重复的行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop_duplicates([<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>],keep=<span class="string">&#x27;first&#x27;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>请注意：以上删除行的操作，会破坏df原有的0到n的连续索引，例如原行索引为：0，1，2，3…n，其中索引1，2为空行，删除空行后，df的索引变为0，3…n，显然不连续，因此需要重置索引：df.reset_index(drop=True)，重置后，索引变为0，1，2，3…n</p><h3 id="7、单元格的字符相关处理"><a href="#7、单元格的字符相关处理" class="headerlink" title="7、单元格的字符相关处理"></a>7、单元格的字符相关处理</h3><p>例如有字段app_id，有部分值字符串为数字：‘12331’，需转成int64<br>有部分值为字符加数字：‘TD12331’，去掉字符TD并转成int64<br>有些值为非id例如：‘ # llsd’，需对此类值用固定数值填充。因此需要对其统一处理成整型id<br>使用replace方法去掉值里面的TD字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>].replace(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>,regex=<span class="literal">True</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>使用apply方法通过定义简单的lambda去掉值里面的TD字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].apply(<span class="keyword">lambda</span>:item:item.replace(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure><p>其实apply才是终极方法，适用各种自定义的数据行或者列的处理，例如对同一列的值有多种判断需要处理，则可以在外部定义要处理的函数，再用apply广播到该列的每个cell中。例如上面的例子：如果单元格数值含有TD则去掉TD字符保留其数值部分，如果单元格出现非数值，则将其设为NaN空值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_id</span>(<span class="params">cell</span>):</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">&#x27;\d&#123;3&#125;&#x27;</span>,cell):</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    <span class="keyword">elif</span> re.match(<span class="string">&#x27;TD&#x27;</span>,cell):</span><br><span class="line">        <span class="keyword">return</span> re.sub(<span class="string">&#x27;TD&#x27;</span>,<span class="string">&#x27;&#x27;</span>,cell)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.nan</span><br><span class="line">        </span><br><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].apply(filter_id)</span><br></pre></td></tr></table></figure><p>apply方法另外一种常用的方式:对数值进行分级，例如10&lt;item&lt;30为D，30&lt;=item&lt;60为C，60&lt;=item&lt;90为B。此外，货币进行转化、时间转换也是常用的场景</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="number">10</span>&lt;=item&lt;<span class="number">30</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;D&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">30</span>&lt;=item&lt;<span class="number">60</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;C&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="number">60</span>&lt;=item&lt;<span class="number">90</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;B&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;A&#x27;</span></span><br><span class="line">    </span><br><span class="line">df[<span class="string">&#x27;level&#x27;</span>]=df[<span class="string">&#x27;level&#x27;</span>].apply(level)    </span><br></pre></td></tr></table></figure><p>使用astype转成整型id号，具体其他数据类型不再列出。astype要求整列数据是完整的同一数据类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;app_id&#x27;</span>]=df[<span class="string">&#x27;app_id&#x27;</span>].astype(<span class="string">&#x27;int64&#x27;</span>)</span><br></pre></td></tr></table></figure><p>使用频繁：使用pandas.Series.str.contains方法处理列的值</p><blockquote><p>Series.str.contains(pat, case=True, flags=0, na=nan, regex=True)<br>pat : str<br>    Character sequence or regular expression.<br>case : bool, default True<br>    If True, case sensitive.<br>flags : int, default 0 (no flags)<br>    Flags to pass through to the re module, e.g. re.IGNORECASE.<br>na : default NaN<br>    Fill value for missing values.<br>regex : bool, default True<br>    If True, assumes the pat is a regular expression.<br>    If False, treats the pat as a literal string.</p></blockquote><p>删除name列中含有foo字符串的行，默认使用正则匹配<br>df=df[~df[‘name’].str.contains(‘foo’, case=false, flags=re.IGNORECASE, na=False)]</p><p>或者使用正则匹配<br>df=df[~df[‘name’].str.contains(‘^foo’, case=false, flags=re.IGNORECASE, na=False)]</p><h3 id="8、有关遍历行的处理"><a href="#8、有关遍历行的处理" class="headerlink" title="8、有关遍历行的处理"></a>8、有关遍历行的处理</h3><p>单表处理，请勿使用循环！！ 效率很低！有apply方法足以，底层是矩阵操作。</p><p>遍历行，一般用在两个表之间，表A字段’date’与表B字段’date‘的比较<br>使用iterrows遍历行<br>iterate over DataFrame rows as (index, Series) pairs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这种方式可以把索引和行数据遍历出，其中row的数据结构为nametuple</span></span><br><span class="line"><span class="keyword">for</span> index,row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    print(index,row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这种方式其实就是itertuples(index=False)的遍历</span></span><br><span class="line"><span class="keyword">for</span> _,row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    print(row.A,row.B)</span><br></pre></td></tr></table></figure><p>使用itertuples遍历行<br>Iterate over DataFrame rows as namedtuples of the values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">s = pd.Series(pd.date_range(<span class="string">&#x27;2012-1-1&#x27;</span>, periods=<span class="number">10</span>, freq=<span class="string">&#x27;D&#x27;</span>))</span><br><span class="line">td = pd.Series([pd.Timedelta(days=i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: s, <span class="string">&#x27;B&#x27;</span>: td&#125;)</span><br><span class="line"><span class="comment">#这种方式，取出的每行为nametuple</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">    print(row.A,row.B)</span><br></pre></td></tr></table></figure><p>使用iteritems遍历列<br>这种方式以横向遍历列数据，每次返回该列名和该列Series</p><h3 id="9、DataFrame和DataFrame合并、关联查询等"><a href="#9、DataFrame和DataFrame合并、关联查询等" class="headerlink" title="9、DataFrame和DataFrame合并、关联查询等"></a>9、DataFrame和DataFrame合并、关联查询等</h3><h4 id="9-1-DataFrame和DataFrame合并"><a href="#9-1-DataFrame和DataFrame合并" class="headerlink" title="9.1 DataFrame和DataFrame合并"></a>9.1 DataFrame和DataFrame合并</h4><p>合并具有相同结构的df<br>将多个DataFrame按垂直方向或者水平方向合并：这种场合使用批量处理具有相同字段结构的多份报表或数据源</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认是按垂直方向合并三个子df</span></span><br><span class="line">frames = [df1, df2, df3]</span><br><span class="line">result = pd.concat(frames)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在合并后，还可以为每个子df设定相应key</span></span><br><span class="line">result = pd.concat(frames, keys=[<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;cee&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用上面key，可以一次性取回合并前的df1</span></span><br><span class="line">df1=result.loc[<span class="string">&#x27;foo&#x27;</span>]</span><br></pre></td></tr></table></figure><p>合并字段不同的df</p><h4 id="9-2-DataFrame和DataFrame之间的关联查询"><a href="#9-2-DataFrame和DataFrame之间的关联查询" class="headerlink" title="9.2 DataFrame和DataFrame之间的关联查询"></a>9.2 DataFrame和DataFrame之间的关联查询</h4><p>因为关联查询基本是数据分析里面重要的、使用频繁的需求，例如实现报表1和报表的用vlookup关联查询、sql中多个表的关联查询（内连接、左连接、右连接、全连接）。pandas的doc官方文档在这部分的内容已经非常详细，并且有相应的关联前后的图文说明，本文不再一一赘述，仅给出简单的关联用法。<br>以内连接为例：<br>实现类似sql使用两表的多个外键关联：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select t1.*,t2.* from t1,t2 where t1.a&#x3D;t2.a</span><br><span class="line">and t1.b&#x3D;t2.b</span><br><span class="line">and t1.c&#x3D;t2.c</span><br></pre></td></tr></table></figure><p>pandas的方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.merge(df2, on=[ key1 ,  key2 ,  key ])</span><br></pre></td></tr></table></figure><p>使用单个字段(外键)关联两表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.merge(df2, on=<span class="string">&#x27;dept_id&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="10、groupby基本用法"><a href="#10、groupby基本用法" class="headerlink" title="10、groupby基本用法"></a>10、groupby基本用法</h3><p>groupby可以说面对不同的数据需求，有不同用法，对sql熟悉的人应该无需多说。这里仅给出一些简单用法。</p><p>按季度分组，提取每个分组前n个数据行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_n</span>(<span class="params">df,n=<span class="number">3</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> df[<span class="number">0</span>:n]</span><br><span class="line"><span class="comment"># 这里的n是top_n自定义的关键字参数n，不是apply的参数    </span></span><br><span class="line">df.groupby(<span class="string">&#x27;quarter&#x27;</span>).apply(top_n,n=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>按产品种类分组，提取每个分组里最大值和最小值之差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个产品种类的数值跨度范围，也即最大值减去最小值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_min</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">return</span> item.<span class="built_in">max</span>() - item.<span class="built_in">min</span>()</span><br><span class="line">df.groupby(<span class="string">&#x27;prod&#x27;</span>).agg(max_min)</span><br></pre></td></tr></table></figure><p>按产品种类分组，一次性取出每组的最值、均值、数值跨度范围，这里需要注意agg的入参为方法的列表，内置方法使用其字符名，自定义方使用其函数名</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;prod&#x27;</span>).agg([<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,max_min])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;#8195;&amp;#8195;以往项目中也有引入Pandas，用于有关数据处理和分析的环节，结合Python的Web开发，很容易开发出一款轻量BI系统。Pandas、Numpy、Scipy、matplotlib、scikit-learn和Jupyter Notebook结合使用，完全可以组成非常出色的数据分析与挖掘的生产环境工具，数据方面的应用，比matlab强不少，以至于本人也不断强化这方面的积累。单独拿出这方面技能，即可完成数据分析师的相关工作（又称提数工程师）。本文主要归档一些高频使用的预处理方面的函数，注意本文不涉及Pandas数据挖掘和数理统计方面的知识点（会在另外blog给出）。&lt;/p&gt;</summary>
    
    
    
    <category term="数据分析与挖掘" scheme="https://yield-bytes.gitee.io/blog/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98/"/>
    
    
    <category term="pandas" scheme="https://yield-bytes.gitee.io/blog/tags/pandas/"/>
    
  </entry>
  
</feed>
